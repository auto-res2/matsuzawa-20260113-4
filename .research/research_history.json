{
  "research_topic": "Data-efficient image classification methods using novel regularization and augmentation techniques",
  "queries": [
    "data-efficient image classification",
    "regularization image classification",
    "augmentation strategies image classification",
    "consistency regularization image",
    "mixup augmentation regularization",
    "semi supervised data efficient",
    "entropy minimization regularization",
    "virtual adversarial training",
    "contrastive augmentation methods",
    "label efficient learning"
  ],
  "research_study_list": [
    {
      "title": "Explanation-based Data Augmentation for Image Classification",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Logarithmic Lenses: Exploring Log RGB Data for Image Classification",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Contextual Squeeze-and-Excitation for Efficient Few-Shot Image Classification",
      "full_text": "Contextual Squeeze-and-Excitation for Efﬁcient Few-Shot Image Classiﬁcation Massimiliano Patacchiola University of Cambridge mp2008@cam.ac.uk John Bronskill University of Cambridge jfb54@cam.ac.uk Aliaksandra Shysheya University of Cambridge as2975@cam.ac.uk Katja Hofmann Microsoft Research kahofman@microsoft.com Sebastian Nowozin∗ nowozin@gmail.com Richard E. Turner University of Cambridge ret26@cam.ac.uk Abstract Recent years have seen a growth in user-centric applications that require effective knowledge transfer across tasks in the low-data regime. An example is personaliza- tion, where a pretrained system is adapted by learning on small amounts of labeled data belonging to a speciﬁc user. This setting requires high accuracy under low computational complexity, therefore the Pareto frontier of accuracy vs. adaptation cost plays a crucial role. In this paper we push this Pareto frontier in the few-shot image classiﬁcation setting with a key contribution: a new adaptive block called Contextual Squeeze-and-Excitation (CaSE) that adjusts a pretrained neural network on a new task to signiﬁcantly improve performance with a single forward pass of the user data (context). We use meta-trained CaSE blocks to conditionally adapt the body of a network and a ﬁne-tuning routine to adapt a linear head, deﬁning a method called UpperCaSE. UpperCaSE achieves a new state-of-the-art accuracy relative to meta-learners on the 26 datasets of VTAB+MD and on a challenging real-world personalization benchmark (ORBIT), narrowing the gap with leading ﬁne-tuning methods with the beneﬁt of orders of magnitude lower adaptation cost. 1 Introduction In recent years, the growth of industrial applications based on recommendation systems (Bennett et al., 2007), speech recognition (Xiong et al., 2018), and personalization (Massiceti et al., 2021) has sparked an interest in machine learning techniques that are able to adapt a model on small amounts of data belonging to a speciﬁc user. A key factor in many of these applications is the Pareto frontier of accuracy vs. computational complexity (cost to adapt). For example, in a real-time classiﬁcation task on a phone, a pretrained model must be personalized by exploiting small amounts of data on the user’s device (context). In these applications the goal is twofold: maximize the classiﬁcation accuracy on unseen data (target) while avoiding any latency and excessive use of computational resources. Methods developed to face these challenges in the few-shot classiﬁcation setting can be grouped in two categories: meta-learning and ﬁne-tuning. Meta-learning is based on the idea of learning-how-to- learn by improving the algorithm itself (Schmidhuber, 1987; Hospedales et al., 2020). Meta-learners are trained across multiple tasks to ingest a labeled context set, adapt the model, and predict the class membership of an unlabeled target point. Fine-tuning methods adjust the parameters of a pretrained neural network on the task at hand by iterative gradient-updates (Chen et al., 2019; Triantaﬁllou et al., 2019; Tian et al., 2020; Kolesnikov et al., 2020; Dumoulin et al., 2021). ∗Work done while the author was at Microsoft Research – Cambridge (UK) 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2206.09843v3  [cs.CV]  11 Jan 2023We can gain an insight on the differences between those two paradigms by comparing them in terms of accuracy and adaptation cost. Figure 1 illustrates this comparison by showing on the vertical axis the average classiﬁcation accuracy on the 18 datasets of the Visual Task Adaptation Benchmark (VTAB, Dumoulin et al. 2021), and on the horizontal axis the adaptation cost measured as the number of multiply–accumulate operations (MACs) required to adapt on a single task (see Appendix C.1 for details). Overall, ﬁne-tuners achieve a higher classiﬁcation accuracy than meta-learners but are more expensive to adapt. The comparison between two state-of-the-art methods for both categories, Big Transfer (BiT, Kolesnikov et al. 2020) and LITE (Bronskill et al., 2021), shows a substantial performance gap of 14% in favor of the ﬁne-tuner but at a much higher adaptation cost, with BiT requiring 526 ×1012 MACs and LITE only 0.2 ×1012 MACs. Figure 1: Accuracy and adaptation cost on VTAB for meta-learners (blue), ﬁne- tuners (red), and hybrids (blue-red). Black dotted-line is the previous Pareto front across categories. UpperCaSE nar- rows the gap with the leading ﬁne-tuning method and represents the best trade-off in terms of accuracy/adaptation-cost. It is crucial to ﬁnd solutions that retain the best of both worlds: the accuracy of ﬁne-tuners and low adaptation cost of meta-learners. The main bottleneck that hampers the adaptation of ﬁne-tuners is the need for multiple gradi- ent adjustments over the entire set of network parameters. Restricting those adjustments to the last linear layer (head) signiﬁcantly speeds up ﬁne-tuning, but it harms perfor- mance (e.g. see experiments in Section 5.1). Finding a way to rapidly adapt the feature extractor (body) is there- fore the main obstacle to bypass. In this paper we propose a hybrid solution to this issue, exploiting meta-learned adapters for rapidly adjusting the body and a ﬁne-tuning routine for optimizing the head. At the core of our approach is a novel extension of the popular Squeeze-and-Excitation block proposed by Hu et al. (2018) to the meta-learning setting that we call Contextual Squeeze-and-Excitation (CaSE). We exploit CaSE as building block of a hybrid training protocol called UpperCaSE which is based on the idea of adjusting the body of the network in a single forward pass over the context, and reserving the use of expensive ﬁne-tuning routines for the linear head, similarly to methods like MetaOptNet (Lee et al., 2019), R2D2 (Bertinetto et al., 2018), and ANIL (Raghu et al., 2019). Figure 1 shows how UpperCaSE substantially improves the performance in the low-cost regime, outperforming meta-learners, ﬁne-tuners such as MD-Transfer (Triantaﬁllou et al., 2019), and reducing the gap with the current state of the art (BiT). When adaptation cost is critical, UpperCaSE is the best method currently available since it can provide substantial computation savings and compelling classiﬁcation performance. Our contributions can be summarized as follows: 1. We introduce a new adapter calledContextual Squeeze-and-Excitation (CaSE), based on the popular Squeeze-and-Excitation model proposed by Hu et al. (2018), that outperforms other adaptation mechanisms (e.g. the FiLM generators used in Bronskill et al. 2021) in terms of parameter efﬁciency (a 75% reduction in the number of adaptation parameters) and classiﬁcation accuracy (a 1.5% improvement on MetaDataset and VTAB). The code is released with an open-source license 1. 2. We use CaSE adaptive blocks in conjuction with a ﬁne-tuning routine for the linear head in a model called UpperCaSE, reporting an improved classiﬁcation accuracy compared to the SOTA meta-learner (Bronskill et al., 2021) on the 8 datasets of MDv2 (+2.5% on average) and the 18 datasets of VTAB (+6.8% on average), narrowing the gap with BiT (Kolesnikov et al., 2020) with the beneﬁt of orders of magnitude lower adaptation cost. 3. We showcase the potential of UpperCaSE in a real-world personalization task on the ORBIT dataset (Massiceti et al., 2021), where it compares favorably with the leading methods in the challenging cross-domain setting (training on MDv2, testing on ORBIT). 1https://github.com/mpatacchiola/contextual-squeeze-and-excitation 22 Contextual Squeeze-and-Excitation (CaSE) Problem formulation In this paragraph we introduce the few-shot learning notation, as this will be used to describe the functioning of a CaSE adaptive block. Let us deﬁne a collection of meta-training tasks as D= {τ1,...,τ D}where τi = (Ci,Ti) represents a generic task composed of a context set Ci = {(x,y)1,..., (x,y)M}and a target set Ti = {(x,y)1,..., (x,y)D}of input-output pairs. Following common practice we use the term shot to identify the number of samples per class (e.g. 5-shot is 5 samples per class) and the term way to identify the number of classes (e.g. 10-way is 10 classes per task). Given an evaluation task τ∗ = {C∗,x∗}the goal is to predict the true label y∗ of the unlabeled target point x∗ conditioned on the context set C∗. In ﬁne-tuning methods, we are given a neural network fθ(·), with parameters θ estimated via standard supervised-learning on a large labeled dataset (e.g. ImageNet). Given a test task τ∗ adaptation consists of minimizing the lossL(·) via gradient updates to ﬁnd the task-speciﬁc parameters θτ∗ ←G(ϵ,L,τ∗,fθ), where ϵis a learning rate, and G(·) is a functional representing an iterative routine that returns the adapted parameters θτ∗ (used for prediction). This procedure is particularly effective because it can exploit efﬁcient mini-batching, parallelization, and large pretrained models. In meta-learning methods training and evaluation are performed episodically (Vinyals et al., 2016), with training tasks sampled from a meta-train dataset and evaluation tasks sampled from an unseen meta-test dataset. The distinction in tasks is exploited to deﬁne a hierarchy. The parameters are divided in two groups: φtask-common parameters shared across all tasks (top of the hierarchy), and ψτ task-speciﬁc parameters estimated on the task at hand as part of an adaptive mechanism (bottom of the hierarchy). The way φand ψτ come into play is method dependent; they can be estimated via gradient updates (e.g. MAML, Finn et al. 2017), learned metrics (e.g. ProtoNets, Snell et al. 2017), or Bayesian methods (Gordon et al., 2018; Patacchiola et al., 2020; Sendera et al., 2021). Standard Squeeze-Excite (SE) We brieﬂy introduce standard SE (Hu et al., 2018), as we are going to build on top of this work. SE is an adaptive layer used in the supervised learning setting to perform instance based channel-wise feature adaptation, which is trained following a supervised protocol together with the parameters of the neural network backbone. Given a convolutional neural network, consider a subset of Llayers and associate to each one of them a Multi-Layer Perceptron (MLP), here represented as a function gφ(·). The number of hidden units in the MLP is deﬁned by the number of inputs divided by a reduction factor. Given a mini-batch of B input images, each convolution produces an output of size B×C×H×W where Cis the number of channels, Hthe height, and W the width of the resulting tensor. For simplicity we split this tensor into sub-tensors that are grouped into a set {H1,..., HB}with Hi ∈RC×H×W. To avoid clutter, we suppress the layer indexing when possible. SE perform a spatial pooling that produces a tensor of shape B×C×1 ×1; this can be interpreted as a set of vectors {h1,..., hB}with hi ∈RC. For each layer l, the set is passed to the associated MLP that will generate an individual scale vector γi ∈RC, where γ(l) 1 = g(l) φ ( h(l) 1 ) ··· γ(l) B = g(l) φ ( h(l) B ) . (1) An elementwise product is then performed between the scale vector and the original tensor ˆH(l) 1 = H(l) 1 ∗γ(l) 1 ··· ˆH(l) B = H(l) B ∗γ(l) B , (2) with the aim of modulating the activation along the channel dimension. This operation can be interpreted as a soft attention mechanism, with the MLP conditionally deciding which channel must be attended to. A graphical representation of SE is provided in Figure 2 (left). Contextual Squeeze-Excite (CaSE) Standard SE is an instance-based mechanism that is suited for i.i.d. data in the supervised setting. In a meta-learning setting we can exploit the distinction in tasks to deﬁne a new version of SE for task-based channel-wise feature adaptation. For a task τ = (C,T), consider the N images from the context set C, and the tensors produced by each convolution in the layers of interest {H1,..., HN}with Hi ∈RC×H×W. As in standard SE, we ﬁrst apply a spatial pooling to each tensor Hi which produces N vectors {h1,..., hN}of shape hi ∈RC. Then a context pooling is performed; this corresponds to an empirical mean over {h1,..., hN}(see Appendix A for more details about context pooling). The pooled representation is passed to the associated MLP to produce a single scale-vector for that layer γ(l) = g(l) φ ( ¯h(l) ) with ¯h(l) = 1 N ( h(l) 1 + ··· + h(l) N ) , (3) 3Figure 2: Comparison between the standard Squeeze-Excite (left) and the proposed Contextual Squeeze-Excite (right). Red frames highlight the two key differences between SE and CaSE: context pooling and scale transfer from context to target. B = mini-batch size, C = channels, H = height, W = width, N = context-set size, M = target-set size, ∗elementwise multiplication. which is then multiplied elementwise by the original tensors ˆH(l) 1 = H(l) 1 ∗γ(l) ··· ˆH(l) N = H(l) N ∗γ(l). (4) The scale vector is estimated in adaptive mode and transferred to the target points T in inference mode (no forward pass on the MLPs), as shown in the rightmost part of Figure 2. In synthesis, the three major differences between SE and CaSE are: (i) CaSE uses a contextual pooling with the aim of generating an adaptive vector per-task instead of per-instance as in SE; (ii) CaSE distinguishes between an adaptive mode and an inference mode that transfers the scale from context to target, while SE does not make such a distinction; and (iii) CaSE parameters are estimated via episodic meta-training while SE parameters via standard supervised-training. In Section 5.1 we show that those differences are fundamental to achieve superior performance in the few-shot setting. A representation of a CaSE block is reported in Figure 2 (right), additional technical details are provided in Appendix A. Comparison with other adapters Popular adaptation mechanisms for few-shot learning are based on Feature-wise Linear Modulation layers (FiLM, Perez et al. 2018). Those mechanisms perform adaptation using a separate convolutional set-encoder to produce an embedding of the context set. The embedding is forwarded to local MLPs to produce the scale and shift vectors of the FiLM layers that modulate a pretrained model. Variations of this adapter have been used in several methods, such as TADAM (Oreshkin et al., 2018), CNAPs (Requeima et al., 2019), SimpleCNAPs (Bateni et al., 2020), CA VIA (Zintgraf et al., 2019), and LITE (Bronskill et al., 2021). We will use the generic term FiLM generator to refer to these adapters and the term FiLM to refer to the scale and shift vectors used to modulate the activations. There are two key differences between FiLM and CaSE: (i) CaSE exploits context pooling to aggregate the activations of the backbone instead of a separate set-encoder as in FilM generators (see Appendix A for details) which is more efﬁcient in terms of parameter count and implementation overhead; and (ii) FiLM uses scale and shift to modulate the activations, CaSE only the scale, therefore 50% less parameters are stored in memory and transferred during inference. In Section 5.1 we compare CaSE and the FiLM generators used in a recent SOTA method (LITE, Bronskill et al. 2021), showing that CaSE is superior in terms of accuracy while using a fraction of the amortization parameters. 3 UpperCaSE: system description and optimization protocol We exploit CaSE blocks as part of UpperCaSE, a hybrid training protocol based on Coordinate- Descent (CD). We call this protocolhybrid because it combines a meta-training procedure to optimize the CaSE parameters (body) with a ﬁne-tuning routine to estimate the task-speciﬁc parameters (head). Preliminaries We are given a feature extractor (body) pretrained with supervised learning on a large dataset (e.g. ImageNet), deﬁned as bθ(·) where θare the pretrained parameters. CaSE blocks, 4parameterized by φ, are added to the model at speciﬁc locations to give bθ,φ(·) (see Appendix A for details about this step). We are interested in learning the CaSE parameters φkeeping constant the pretrained parameters θ(omitted from here to keep the notation uncluttered). At training time, we are given a series of tasks τ = {C,T}∼D , where Dis the training set. The number of classes (way) is calculated from the context set and used to deﬁne a linear classiﬁcation head hψτ (·) parameterized by ψτ. The complete model is obtained by nesting the two functions as hψτ (bφ(·)). We indicate a forward pass through the body over the context inputs with the shorthand bφ(Cx) →{z1,..., zN}, where zn is the context embedding for the input xn. All the context embeddings and the associated labels are stored in M= {(zn,yn)}N n=1. Optimization challenges We have two sets of learnable parameters,φthe CaSE parameters, and ψτ the parameters of the linear head for the taskτ. While φis shared across all tasks (task-common), ψτ must be inferred on the task at hand (task-speciﬁc). In both cases, the objective is the minimization of a classiﬁcation loss L. There are some challenges in optimizing the CaSE parameters in the body, as shown by the decomposition of the full gradient dL dφ = ∑ τ (∂Lτ ∂ψτ dψτ dφ + ∂Lτ ∂φ ) . (5) The ﬁrst term ∂Lτ/∂ψτ (sensitivity of the loss w.r.t. the head) and the direct gradient ∂L/∂φ (sensitivity of the loss w.r.t. the adaptation parameters with a ﬁxed head) can be obtained with auto-differentiation as usual. The second term dψτ/dφ(sensitivity of the head w.r.t. the adaptation parameters) is problematic because ψτ is obtained iteratively after a sequence of gradient updates. Backpropagating the gradients to φincludes a backpropagation through all the gradient steps per- formed to obtain the task-speciﬁc ψτ. Previous work has showed that this produces instability, vanishing gradients, and high memory consumption (Antoniou et al., 2018; Rajeswaran et al., 2019). Meta-training via Coordinate-Descent A potential solution to these issues is the use of implicit gradients (Chen et al., 2020; Rajeswaran et al., 2019; Chen et al., 2022). The main problem with implicit gradients is the computation and inversion of the Hessian matrix as part of Cauchy’s implicit function theorem, which is infeasible when the number of parameters in the linear head is large. Another possible solution is the use of an alternating-optimization scheme, similar to the one proposed in a number of recent methods such as MetaOptNet (Lee et al., 2019), R2D2 (Bertinetto et al., 2018), and ANIL (Raghu et al., 2019). These methods share the idea of inner-loop-head/outer- loop-body meta-training, and they ﬁnd the parameters of the linear head with closed form solutions or by stochastic optimization. Starting from similar assumptions we propose a simple yet effective alternating-optimization scheme, which we formalize using Coordinate-Descent (CD) (Wright, 2015). The idea behind CD is to consider the minimization of a complex multi-variate function as a set of simpler objectives that can be solved one at a time. In our case, we can consider the joined landscape w.r.t. φand ψτ as composed of two separate sets of coordinates (block CD, Wright 2015). By minimizing ψτ ﬁrst, we reach a local minimum where ∂Lτ/∂ψτ ≈0. Therefore CD induces a direct optimization objective w.r.t.φ, with Equation (5) reducing to ∂Lτ/∂φ(no red term). The time complexity of this method is only affected by the number of classes but is constant w.r.t. the number of training points due to the use of mini-batching, which scales well with large tasks (e.g. those in MetaDataset and VTAB). See Appendix B for more details. In practice, at each training iteration we sample a task τ = (C,T) ∼D, perform a forward pass on the body (with CaSE in adaptive mode) to get bφ(Cx) →{z1,..., zN}. (6) The context embeddings are temporarily stored in a buffer with their associated labels M = {(zn,yn)}N n=1 to avoid expensive calls to bφ(·). We then set the head parameters to zero, and solve the ﬁrst minimization problem (inner-loop), obtaining the task-speciﬁc parameters ψτ via ψτ ←G ( α,M,L,hψτ ) (7) where αis a learning rate, and G(·) is a functional representing an iterative gradient-descent routine for parameter estimation (e.g. maximum likelihood estimation or maximum a posteriori estimation). Note that the iterative routine in Equation(7) only relies on the headhψτ (·) and not on the bodybφ(·), which is the primary source of memory savings and the crucial difference with common ﬁne-tuning methods. Moreover, the inner-loop is agnostic to the choice of optimizer, it can handle many gradient steps without complications, exploit parallelization and efﬁcient mini-batching. 5We then turn our attention to the second coordinate: the task-common parameters of the CaSE blocks in the body. For a single task, the update consists of a single optimization step w.r.t.φ(outer-loop) given support/target points and the task-speciﬁc parameters ψτ identiﬁed previously. The ﬁnal form of the equation depends on the optimizer, for a generic SGD the update is given by φ←φ−β∇φL ( Cy ∪Qy,hψτ ,bφ ) , (8) where βis a learning rate. CaSE blocks must be in adaptive mode to allow the backpropagation of the gradients to the MLPs. The process repeats, alternating the minimization along the two sets of coordinates. The pseudo-code for train and test is provided in Appendix B. Inference on unseen tasks After the training phase, we are given an unseen task τ∗ = (C∗,x∗) where x∗ is a single target input and y∗ the associate true label to estimate. Inference consists of three steps: (i) forward pass on the body for all the context inputs with CaSE set to adaptive mode as in Equation (6) and embeddings/labels stored in M, (ii) estimation of the task-speciﬁc parameters ψ∗ via iterative updates as in Equation (7), and (iii) inference of the target-point membership via a forward pass over body and head ˆy∗ = hψ∗ (bφ(x∗)) with CaSE in inference mode. 4 Related work Meta-learning There has been a large volume of publications related to meta-learning. Here we focus on those methods that are the most related to our work, and refer the reader to a recent survey for additional details (Hospedales et al., 2020). LITE (Bronskill et al., 2021) is a protocol for training meta-learners on large images, that achieved SOTA accuracy on VTAB+MD. LITE is particularly relevant in this work, as its best performing method is based on Simple CNAPs (Bateni et al., 2020) that exploits FiLM for fast body adaptation. We compare against LITE in Section 5.2 showing that UpperCaSE is superior in terms of classiﬁcation accuracy and parameter efﬁciency. Fine-tuning Chen et al. (2019) were the ﬁrst to expose the potential of simple ﬁne-tuning baselines for transfer learning. MD-Transfer has been proposed in Triantaﬁllou et al. (2019) as an effective ﬁne-tuning baseline for the MetaDataset benchmark. More recently Kolesnikov et al. (2020) have presented Big Transfer (BiT), showing that large models pretrained on ILSVRC-2012 ImageNet and the full ImageNet-21k are very effective at transfer learning. MD-Transfer and BiT differ in terms of classiﬁcation head, learning schedule, normalization layers, and batching. Fine-tuning only the last linear layer can be effective (Bauer et al., 2017; Tian et al., 2020). We compare against this baseline in Section 5.1, showing that adapting the body via CaSE signiﬁcantly boosts the performance. Overall, ﬁne-tuners have consistently outperformed meta-learners in terms of classiﬁcation accuracy, only under particular conditions (e.g. strong class-imbalance) the trend is reversed (Ochal et al., 2021a,b). Hybrids Hybrid methods are trained episodically like meta-learners but rely on ﬁne-tuning routines for adaptation. Model Agnostic Meta-Learning (MAML, Finn et al. 2017) ﬁnds a set of parameters that is a good starting point for adaptation towards new tasks in a few gradient steps. MAML has been the inspiration for a series of other models such as MAML++ (Antoniou et al., 2018), ProtoMAML (Triantaﬁllou et al., 2019), and Reptile (Nichol et al., 2018). Dynamic networks CaSE blocks belong to the wider family of dynamic networks, models that can adapt their structure or parameters to different inputs (Han et al., 2021). Adaptive components have been used in a variety of applications, such as neural compression (Veit and Belongie, 2018; Wu et al., 2018), generation of artistic styles (Dumoulin et al., 2016; Huang and Belongie, 2017), or routing (Guo et al., 2019). Residual adapters (Rebufﬁ et al., 2017, 2018) have been used in transfer learning (non few-shot) but they rely on ﬁne-tuning routines which are signiﬁcantly slow during adaptation. More recently, Li et al. (2022) have used serial and residual adapters in the few-shot setting, with the task-speciﬁc weights being adapted from scratch on the context set. This approach has similar limitations, since it requires backpropagation to the task-speciﬁc weights in the body of the network which is costly. In Sun et al. (2019) the authors introduce a Meta-Transfer Learning (MTL) method for the few-shot setting. In MTL a series of scale and shift parameters are meta-learned across tasks and then dynamically adapted during the test phase via ﬁne-tuning. This method suffers of similar limitations, as the ﬁne-tuning stage is expensive during adaptation. Moreover, MTL relies on scale and shift vectors to perform adaptation whereas CaSE only relies on a scale vector, meaning that it needs to store and transfer 50% less parameters at test time. 6Figure 3: Left: CaSE vs Squeeze-and-Excitation (SE) (both methods use EfﬁcientNetB0, 84 × 84 inputs, Mahalanobis-distance head). CaSE outperforms SE in all conditions. Center: CaSE vs. FiLM generators (Bronskill et al., 2021) and a baseline with no body adaptation (all methods use EfﬁcientNetB0, 84 ×84 inputs, Mahalanobis-distance head). CaSE outperforms FiLM generators in all conditions. Right: boxplot of CaSE activations at different depth of an EfﬁcientNetB0 for 800 tasks sampled from the MDv2 test set (224 ×224 inputs, UpperCaSE). The modulation of CaSE is minimal at early stages for general-purpose ﬁlters and increases at deeper stages. 5 Experiments In this section we report on experiments on VTAB+MD (Dumoulin et al., 2021) and ORBIT (Massiceti et al., 2021). VTAB+MD has become the standard evaluation protocol for few-shot approaches, and it includes a large number of datasets (8 test dataset for MD, 18 for VTAB). For a description of ORBIT, see Section 5.2. In all experiments we used the following pretrained (on ImageNet) backbones: EfﬁcientNetB0 from the ofﬁcial Torchvision repository; ResNet50x1-S released with BiT (Kolesnikov et al., 2020). We used three workstations (CPU 6 cores, 110GB of RAM, and a Tesla V100 GPU), the meta-training protocol of Bronskill et al. (2021) ( 10K training tasks, updates every 16 tasks), the Adam optimizer with a linearly-decayed learning rate in [10−3,10−5] for both the CaSE and linear-head. The head is updated 500 times using a random mini-batch of size 128. MD test results are averaged over 1200 tasks per-dataset (conﬁdence intervals in appendix). We did not use data augmentation. Code to reproduce the experiments is available at https://github.com/mpatacchiola/contextual-squeeze-and-excitation . 5.1 Analysis of CaSE blocks In this sub-section we report empirical results related to CaSE blocks in three directions: 1) we compared standard SE (Hu et al., 2018) and CaSE on MDv2 and VTAB, conﬁrming that a) adaptation helps over not adapting, b) contextual adaptation (CaSE) outperforms instance based adaptation (SE); 2) we compare CaSE against a SOTA FiLM generator (Bronskill et al., 2021), showing that CaSE is signiﬁcantly more efﬁcient using 75% fewer parameters while boosting the classiﬁcation accuracy on average by +1.5% on VTAB and MD2; and 3) we provide an insight on the effectiveness of CaSE blocks with a series of qualitative analysis. Comparing SE vs. CaSE We compare standard SE and the proposed CaSE on VTAB and MD- v2. For a fair comparison we keep constant all factors of variation (backbone, training schedule, hyperparameters, etc.) and use the same reduction of 32 (0.8M adaptive parameters). In order to compare the results with the other experiments in this section, we use a Mahalanobis-distance head as in Bronskill et al. (2021), reporting results with a linear head in the appendix. We summarize the results in Figure 3 (left) and add a tabular breakdown in Appendix C.2. CaSE outperforms SE in all conditions, conﬁrming that a contextual adaptation mechanism is fundamental to transfer knowledge effectively across tasks. Comparing adaptation mechanisms We perform a comparison on VTAB+MD of CaSE against FiLM generators (Bronskill et al., 2021), and a baseline that uses a pretrained model but no adaptation of the body. Methods are compared in identical conditions, using a Mahalanobis-distance head, an EfﬁcientNetB0 backbone, and same training schedule. We show a summary of the results in 7Figure 3 (center) and provide a full breakdown in the appendix. CaSE is able to outperform FiLM generators in all conditions. In Appendix C.3 we report the results for CaSE with reduction 64 (0.4M parameters) showing that it is able to outperform FiLM generators (1.7M parameters) using a fraction of the parameters. The comparison with the baseline with no adaptation, shows that in all but one condition (VTAB specialized) adaptation is beneﬁcial. This is likely due to the strong domain shift introduced by some of the specialized datasets. Role of CaSE blocks To examine the role of CaSE blocks we analyze the aggregated activations at different stages of the body for 800 tasks sampled from the MDv2 test set using an EfﬁcientNetB0 trained with UpperCaSE on224×224 images. In Figure 3 (right) we report the aggregated distribution as boxplots, and in Appendix C.5 we provide a per-dataset breakdown. Overall the median is close to 1.0 (identity) which is the expected behavior as on average we aim at exploiting the underlying pretrained model. The variance is small at early stages, indicating that CaSE has learned to take advantage of general-purpose ﬁlters that are useful across all tasks. In deeper layers the variance increases, showing a task-speciﬁc modulation effect. In Appendix C.5 we also include a plot with per-channel activations for all datasets at different depths, showing that the modulation is similar across datasets at early stages and it diverges later on. An ablation study of different factors (e.g. reduction, number of hidden layers, activation functions) is reported in Appendix C.4. 5.2 Performance evaluation of UpperCaSE In this sub-section we analyze the performance of UpperCaSE in two settings: 1) comparison on the VTAB+MD benchmark against SOTA ﬁne-tuners and meta-learners, where we show that UpperCaSE is able to outperform all the meta-learners, narrowing the gap with Big Transfer (BiT) on VTAB;2) we show an application of UpperCaSE in a real-world personalization task on the challenging ORBIT dataset (Massiceti et al., 2021) for the cross-domain condition MDv2→ORBIT, where we achieve the best average-score in most metrics, although these improvements are within the error bars. Table 1: UpperCaSE outperforms ﬁne-tuners on MDv2 and narrows the gap on VTAB with the leading method (BiT) with a much lower adaptation cost. Average accuracy on the 26 datasets of VTAB+MD. RN=ResNet, EN=EfﬁcientNet. Img: image size. Param.: total parameters (no adapters) in millions. Cost: MACs to adapt on a task (10-shot, 100-way), in Teras. Best results in bold. Cost↓ MDv2↑ VTAB↑ Method Protocol Net Img Param. MACs all all natur. spec. struc. MD-Transfer ﬁne-tuning RN18 126 11.2 118.6 63.4 55.6 52.4 72.9 49.3 SUR ﬁne-tuning RN50 224 164.6 28.8 71.3 43.7 50.9 66.2 27.2 Big Transfer ﬁne-tuning RN50 224 23.5 526.3 73.3 65.4 69.4 81.0 54.5 UpperCaSE hybrid RN50 224 23.5 0.8 74.9 56.6 66.3 80.1 37.6 UpperCaSE hybrid ENB0 224 4.0 0.2 76.1 58.4 69.1 80.3 39.4 Table 2: UpperCaSE outperforms all meta-learning/hybrid methods and uses the lowest num- ber of parameters per adaptive blocks . Average accuracy on the 26 datasets of VTAB+MD. RN=ResNet, EN=EfﬁcientNet. Img: image size. Param.: total parameters (excluding adapters). Adapt.: total adaptive parameters in millions. Best results in bold. Adapt.↓ MDv2↑ VTAB↑ Method Protocol Net Img Param. count all all natur. spec. struc. ProtoMAML hybrid RN18 126 11.2 n/a 64.2 45.0 45.7 70.7 31.5 CTX meta-learning RN34 224 21.3 n/a 71.6 50.5 61.1 67.3 34.0 ProtoNet meta-learning ENB0 224 4.0 n/a 72.7 46.1 60.9 64.2 25.9 LITE meta-learning ENB0 224 4.0 1.7 73.8 51.4 65.2 71.9 30.8 UpperCaSE hybrid RN50 224 23.5 0.8 74.9 56.6 66.3 80.1 37.6 UpperCaSE hybrid ENB0 224 4.0 0.4 76.1 58.4 69.1 80.3 39.4 Comparison on VTAB+MDWe compare UpperCaSE against ﬁne-tuners, meta-learners, and hybrids on the 18 datasets of VTAB and the 8 datasets of MetaDataset-v2 (MDv2) and report the results 8Table 3: ORBIT: UpperCaSE obtains the best average-score in most metrics, being within error bars with leading methods. Average accuracy and 95% conﬁdence interval for frames, videos, and frames to recognition (FTR). Cost: average MACs over all tasks (Teras). Results and setup from Massiceti et al. (2021): meta-train on MetaDataset and test on ORBIT, image-size 84 ×84, ResNet18 backbone, 85 test tasks (17 test users, 5 tasks per user). Best results (within error bars) in bold. Cost Clean Video Evaluation (CLE-VE) Clutter Video Evaluation (CLU-VE) Method MACs ↓ frame acc.↑ FTR↓ video acc.↑ frame acc.↑ FTR↓ video acc.↑ ProtoNet 3.2 59.0 ±2.2 11.5 ±1.8 69.2 ±3.0 47.0 ±1.8 20.4 ±1.7 52.8 ±2.5 CNAPs 3.5 51.9 ±2.5 20.8 ±2.3 60.8 ±3.2 41.6 ±1.9 30.7 ±2.1 43.0 ±2.5 MAML 95.3 42.5 ±2.7 37.3 ±3.0 47.0 ±3.2 24.3 ±1.8 62.3 ±2.3 25.7 ±2.2 FineTuner 317.7 61.0±2.2 11.5 ±1.8 72.6 ±2.9 48.4 ±1.9 19.1 ±1.7 54.1 ±2.5 UpperCaSE 3.5 63.0±2.2 8.8 ±1.6 74.4 ±2.8 48.1 ±1.8 18.2 ±1.7 54.5 ±2.5 in Table 1 and Table 2. UpperCaSE outperforms all methods (including BiT) on MDv2 with an accuracy of 74.9% (ResNet50) and 76.1% (EfﬁcientNetB0). On VTAB, UpperCaSE outperforms most methods, narrowing the gap with BiT. A closer look at the differences in performance on VTAB between UpperCaSE and BiT (see Table 1) shows that the gap is narrower on the natural and specialized splits (+3.1% and +0.9%) but larger on structured (+16.9%). The breakdown by dataset reported in Appendix C.6 shows that the major performance drops are on tasks that require localization and counting (e.g. dSprites, SmallNORB). Similar issues are encountered by methods such as LITE (Bronskill et al., 2021) which are based on FiLM generators, suggesting that those tasks may introduce a strong domain shift w.r.t. the meta-training set that is difﬁcult to compensate without ﬁne-tuning the body. It is not clear whether transfer learning is beneﬁcial on these datasets in the ﬁrst place. The results in terms of adaptation cost (see Table 1) over a synthetic task (10-shot, 100 way) show that UpperCaSE is orders of magnitude more efﬁcient (0.2 ×1012 MACs) than all ﬁne-tuners, with BiT being the most expensive method overall (526.3 × 1012 MACs). The comparison against meta-learners in terms of number of adaptive parameters (see Table 2) shows that UpperCaSE requires a fraction of the parameters (0.4 vs 1.7 millions for an EfﬁcientNetB0) compared to LITE (Bronskill et al., 2021) which is based on FiLM generators. Comparison on ORBIT We compare UpperCaSE to other methods on ORBIT (Massiceti et al., 2021), a real-world dataset for teachable object recognizers. ORBIT consists of 3822 videos of 486 objects recorded by 77 blind/low-vision people on their mobile phones. The dataset is challenging because objects are poorly framed, occluded, blurred, and in a wide variation of backgrounds and lighting. The dataset includes two sets of target videos, one for clean video evaluation (CLE-VE) with well-centered objects, and another for clutter video evaluation (CLU-VE) with objects in complex, cluttered environments. We consider a hard transfer-learning condition where classiﬁers are meta-trained on MetaDataset and tested on ORBIT. Results are reported in Table 3. UpperCaSE outperforms all other methods (on average) on most metrics, being within error bars with the two leading methods. Comparing UpperCaSE with FineTuner, the gap in favor of UpperCaSE is marginal on CLU-VE but substantial on CLE-VE (frame accuracy +2%, video accuracy +1.8%, and FTR −2.7). Comparison in terms of adaptation cost (average MACs over all tasks) shows that UpperCaSE is orders of magnitude more efﬁcient than FineTuner and close to the leading method (ProtoNet). 6 Conclusions We have introduced a new adaptive block called CaSE, which is based on the popular Squeeze-and- Excitation (SE) block proposed by Hu et al. (2018). CaSE is effective at modulating a pretrained model in the few-shot setting, outperforming other adaptation mechanisms. Exploiting CaSE we have designed UpperCaSE, a hybrid method based on a Coordinate-Descent training protocol, that combines the performance of ﬁne-tuners with the low adaptation cost of meta-learners. UpperCaSE achieves SOTA accuracy w.r.t. meta-learners on the 26 datasets of VTAB+MD and it compares favorably with leading methods in the ORBIT personalization benchmark. 9Limitations There are two limitations that are worth mentioning: (i) UpperCaSE requires iterative gradient updates that are hardware-dependent and may be slow/unavailable in some portable devices; (ii) breakdown VTAB results per-dataset shows that the method falls short on structured datasets. This indicates that ﬁne-tuning the body may be necessary for high accuracy when the shift w.r.t. the meta-training set is large. Societal impact Applications based on CaSE and UpperCaSE could be deployed in few-shot classiﬁ- cation settings that can have a positive impact such as: medical diagnosis, recommendation systems, object detection, etc. The efﬁciency of our method can reduce energy consumption and beneﬁt the environment. Certain applications require careful consideration to avoid biases that can harm speciﬁc groups of people (e.g. surveillance, legal decision-making). Acknowledgments and Disclosure of Funding Funding in direct support of this work: Massimiliano Patacchiola, John Bronskill, Aliaksandra Shysheya, and Richard E. Turner are supported by an EPSRC Prosperity Partnership EP/T005386/1 between the EPSRC, Microsoft Research and the University of Cambridge. The authors would like to thank: anonymous reviewers for useful comments and suggestions; Aristeidis Panos, Daniela Massiceti, and Shoaib Ahmed Siddiqui for providing suggestions and feedback on the preliminary version of the manuscript. References Antoniou, A., Edwards, H., and Storkey, A. (2018). How to train your maml. arXiv preprint arXiv:1810.09502. Bateni, P., Goyal, R., Masrani, V ., Wood, F., and Sigal, L. (2020). Improved few-shot visual classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Bauer, M., Rojas-Carulla, M., ´Swi ˛ atkowski, J. B., Schölkopf, B., and Turner, R. E. (2017). Discrimi- native k-shot learning using probabilistic models. arXiv preprint arXiv:1706.00326. Bennett, J., Lanning, S., et al. (2007). The netﬂix prize. In Proceedings of KDD cup and workshop. Bertinetto, L., Henriques, J. F., Torr, P. H., and Vedaldi, A. (2018). Meta-learning with differentiable closed-form solvers. arXiv preprint arXiv:1805.08136. Bronskill, J., Massiceti, D., Patacchiola, M., Hofmann, K., Nowozin, S., and Turner, R. (2021). Memory efﬁcient meta-learning with large images. Advances in Neural Information Processing Systems. Chen, W., Tripp, A., and Hernández-Lobato, J. M. (2022). Meta-learning feature representations for adaptive gaussian processes via implicit differentiation. arXiv preprint arXiv:2205.02708. Chen, W.-Y ., Liu, Y .-C., Kira, Z., Wang, Y .-C. F., and Huang, J.-B. (2019). A closer look at few-shot classiﬁcation. arXiv preprint arXiv:1904.04232. Chen, Y ., Friesen, A. L., Behbahani, F., Doucet, A., Budden, D., Hoffman, M., and de Freitas, N. (2020). Modular meta-learning with shrinkage. Advances in Neural Information Processing Systems. Dumoulin, V ., Houlsby, N., Evci, U., Zhai, X., Goroshin, R., Gelly, S., and Larochelle, H. (2021). Comparing transfer and meta learning approaches on a uniﬁed few-shot classiﬁcation benchmark. arXiv preprint arXiv:2104.02638. Dumoulin, V ., Shlens, J., and Kudlur, M. (2016). A learned representation for artistic style.arXiv preprint arXiv:1610.07629. Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning. Garnelo, M., Schwarz, J., Rosenbaum, D., Viola, F., Rezende, D. J., Eslami, S., and Teh, Y . W. (2018). Neural processes. arXiv preprint arXiv:1807.01622. 10Gordon, J., Bronskill, J., Bauer, M., Nowozin, S., and Turner, R. E. (2018). Meta-learning probabilistic inference for prediction. arXiv preprint arXiv:1805.09921. Guo, Y ., Shi, H., Kumar, A., Grauman, K., Rosing, T., and Feris, R. (2019). Spottune: transfer learning through adaptive ﬁne-tuning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Han, Y ., Huang, G., Song, S., Yang, L., Wang, H., and Wang, Y . (2021). Dynamic neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence. Hospedales, T., Antoniou, A., Micaelli, P., and Storkey, A. (2020). Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439. Hu, J., Shen, L., and Sun, G. (2018). Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Huang, X. and Belongie, S. (2017). Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. (2020). Big transfer (bit): General visual representation learning. In European conference on computer vision. Lee, K., Maji, S., Ravichandran, A., and Soatto, S. (2019). Meta-learning with differentiable convex optimization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Li, W.-H., Liu, X., and Bilen, H. (2022). Cross-domain few-shot learning with task-speciﬁc adapters. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Massiceti, D., Zintgraf, L., Bronskill, J., Theodorou, L., Harris, M. T., Cutrell, E., Morrison, C., Hofmann, K., and Stumpf, S. (2021). Orbit: A real-world few-shot dataset for teachable object recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Nichol, A., Achiam, J., and Schulman, J. (2018). On ﬁrst-order meta-learning algorithms. arXiv preprint arXiv:1803.02999. Ochal, M., Patacchiola, M., Storkey, A., Vazquez, J., and Wang, S. (2021a). Few-shot learning with class imbalance. arXiv preprint arXiv:2101.02523. Ochal, M., Patacchiola, M., Storkey, A., Vazquez, J., and Wang, S. (2021b). How sensitive are meta-learners to dataset imbalance? arXiv preprint arXiv:2104.05344. Oreshkin, B., Rodríguez López, P., and Lacoste, A. (2018). Tadam: Task dependent adaptive metric for improved few-shot learning. Advances in neural information processing systems, 31. Patacchiola, M., Turner, J., Crowley, E. J., O’Boyle, M., and Storkey, A. J. (2020). Bayesian meta- learning for the few-shot setting via deep kernels. Advances in Neural Information Processing Systems. Perez, E., Strub, F., De Vries, H., Dumoulin, V ., and Courville, A. (2018). Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence. Raghu, A., Raghu, M., Bengio, S., and Vinyals, O. (2019). Rapid learning or feature reuse? towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157. Rajeswaran, A., Finn, C., Kakade, S. M., and Levine, S. (2019). Meta-learning with implicit gradients. Advances in Neural Information Processing Systems. Rebufﬁ, S.-A., Bilen, H., and Vedaldi, A. (2017). Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems. Rebufﬁ, S.-A., Bilen, H., and Vedaldi, A. (2018). Efﬁcient parametrization of multi-domain deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition. 11Requeima, J., Gordon, J., Bronskill, J., Nowozin, S., and Turner, R. E. (2019). Fast and ﬂexible multi- task classiﬁcation using conditional neural adaptive processes. Advances in Neural Information Processing Systems. Schmidhuber, J. (1987). Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universität München. Sendera, M., Tabor, J., Nowak, A., Bedychaj, A., Patacchiola, M., Trzcinski, T., Spurek, P., and Zieba, M. (2021). Non-gaussian gaussian processes for few-shot regression. Advances in Neural Information Processing Systems. Snell, J., Swersky, K., and Zemel, R. (2017). Prototypical networks for few-shot learning. Advances in Neural Information Processing Systems. Sun, Q., Liu, Y ., Chua, T.-S., and Schiele, B. (2019). Meta-transfer learning for few-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Tian, Y ., Wang, Y ., Krishnan, D., Tenenbaum, J. B., and Isola, P. (2020). Rethinking few-shot image classiﬁcation: a good embedding is all you need? In European Conference on Computer Vision. Triantaﬁllou, E., Zhu, T., Dumoulin, V ., Lamblin, P., Evci, U., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Manzagol, P.-A., et al. (2019). Meta-dataset: A dataset of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096. Veit, A. and Belongie, S. (2018). Convolutional networks with adaptive inference graphs. In Proceedings of the European Conference on Computer Vision (ECCV). Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. (2016). Matching networks for one shot learning. Advances in Neural Information Processing Systems. Wright, S. J. (2015). Coordinate descent algorithms. Mathematical Programming, 151. Wu, Z., Nagarajan, T., Kumar, A., Rennie, S., Davis, L. S., Grauman, K., and Feris, R. (2018). Blockdrop: Dynamic inference paths in residual networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Xiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., and Stolcke, A. (2018). The microsoft 2017 conversational speech recognition system. In IEEE international conference on acoustics, speech and signal processing (ICASSP). Zintgraf, L., Shiarli, K., Kurin, V ., Hofmann, K., and Whiteson, S. (2019). Fast context adaptation via meta-learning. In International Conference on Machine Learning. 12A CaSE: additional details A.1 CaSE implementation Standardization Empirically we have observed that standardizing the pooled representations before passing them to the MLP improves the training stability in CaSE (but not in SE). Standardization is performed by taking the pooled representation at layer las showed in Equation (3), that is ¯h(l) ∈RC, subtracting the mean and dividing by the standard deviation. Activation function for the output layer Standard SE blocks usually rely on a sigmoid function in the last layer of the MLPs. This works well when the adaptive block is trained in parallel with the underlying neural network. However, in our case we use a pretrained model and learning can be speeded up considerably by enforcing the identity function as output of the MLPs. We achieve this by multiplying the output of the sigmoid by a constant scalar c= 2which extends the range to [0,2], and then set to zero the weights and bias of the layer. This has the effect of enforcing the identity function at the beginning of the training. We have also used a linear activation function instead of a sigmoid, with good results. When using a linear output the identity can be enforced by setting the weights of the last layer to zero, and the bias to one. An ablation over the activation function of SE and CaSE is provided in Appendix C.4 (Table 6). CaSE location For the choice of CaSE location in the feature extractor, we followed the same principles used in Bronskill et al. (2021) for FiLM generators. In EfﬁcientNetB0 we place CaSE at the beginning of each hyperblock and the last layer (excluding the ﬁrst layer). Differently from FiLM (placed after the BatchNorm) we place CaSE after the non-linearity (as done in standard SE) and before the Squeeze-and-Excitation block (included by default in EfﬁcientNet): Conv2d→BatchNorm2d→SiLU→CaSE→SqueezeExcitation→Conv2d→BatchNorm2d This results in a total of 18 CaSE blocks for EfﬁcientNetB0. Increasing the number of blocks did not provide a signiﬁcant beneﬁt. In ResNet18 we place two CaSE blocks per each basic block as: Conv2d→BatchNorm2d→ReLU→CaSE→Conv2d→BatchNorm2d→ReLU→CaSE Similarly we place two CaSE blocks inside a bottleneck block in ResNet50. See the code for more details. Based on the qualitative analysis reported in Section 5 we hypothesize that adaptive blocks are not needed in the initial layers of the network, since at those stages their activity is minimal. Identifying which layer needs adapters and which layer does not, can reduce even more the parameter count of adaptive blocks. Additional work is needed to fully understand this factor. CaSE reduction The number of parameters allocated to the CaSE blocks is regulated by a divider r that is used to compute the number of hidden units in the MLPs. Given the input sizeC(corresponding to the number of channels in that layer) the number of hidden units is given by C/r. We also use a clipping factor rmin that prevents the number of units to fall under a given threshold. This prevents the allocation of a low number of units for layers with a small number of channels. A.2 Context pooling In this section we provide additional details about the context pooling operation performed in a CaSE adaptive block (described in Section 2). Similarities with other methods Context pooling is a way to summarize a task with a permutation- invariant aggregation of the embeddings. A similar mechanism has been exploited in various meta-learning methods. For instance, in ProtoNets (Snell et al., 2017) a prototype for a single class is computed by taking the average over all the context embeddings associated to the inputs for that class. The embeddings are generated in the last layer of the feature extractor. In Simple-CNAPs (Bateni et al., 2020) a prototype is estimated as in ProtoNets but it is used to deﬁne a Gaussian distribution instead of a mean vector. Neural latent variable models, such as those derived from the Neural Processes family (Garnelo et al., 2018) also rely on similar permutation-invariant aggregations to deﬁne distributions over functions. 13Global vs. local context-pooling Comparing CaSE with the FiLM generators of Bronskill et al. (2021) it is possible to distinguish between two types of context pooling: global and local. The FiLM generators of Bronskill et al. (2021) rely on a global pooling strategy, meaning that the aggregation is performed once-for-all by using a dedicated convolutional set encoder. More speciﬁcally, the encoder takes as input all the context images and produces embeddings for each one of them, followed by an average-pooling of those embeddings. The aggregated embedding is then passed to MLPs in each layer that generates a scale and shift parameter. Crucially, each MLP receives the same embedding. CaSE exploits a local context-pooling at the layer level. The convolutional set encoder is discarded, and the feature maps produces by the backbone itself at each stage are used as context embeddings. Therefore, the MLPs responsible for generating the scale parameters receive a unique embedding. As showed in the experimental section (Section 5), local pooling improves performances and uses less parameters, as no convolutional encoder is needed. Additional details about the differences between CaSE and FiLM generators is also provided in the paper (Section 4). 14A.3 Pytorch code for CaSE Implementation of a CaSE adaptive block in Pytorch. The script is also available as case.py at https://github.com/mpatacchiola/contextual-squeeze-and-excitation . import torch from torch import nn class CaSE (nn. Module ): def __init__ (self , cin , reduction =32 , min_units =32 , standardize =True , out_mul =2.0, device =None , dtype = None ): \"\"\" Initialize a CaSE adaptive block . Parameters : cin ( int ): number of input channels . reduction ( int ): divider for computing number of hidden units . min_units ( int ): clip hidden units to this value (if lower ). standardize ( bool ): standardize the input for the MLP . out_mul ( float ): multiply the MLP output by this value . \"\"\" factory_kwargs = {’ device ’: device , ’dtype ’: dtype } super (CaSE , self ). __init__ () self . cin = cin self . standardize = standardize self . out_mul = out_mul hidden = max ( min_units , cin // reduction ) self . gamma_generator = nn. Sequential ( nn. Linear (cin , hidden , bias =True , ** factory_kwargs ), nn. SiLU () , nn. Linear ( hidden , hidden , bias =True , ** factory_kwargs ), nn. SiLU () , nn. Linear ( hidden , cin , bias =True , ** factory_kwargs ), nn. Sigmoid () ) self . reset_parameters () def reset_parameters ( self ): nn. init . zeros_ ( self . gamma_generator [4]. weight ) nn. init . zeros_ ( self . gamma_generator [4]. bias ) self . gamma = torch . tensor ([1.0]) def forward (self , x): if( self . training ): # adaptive mode self . gamma = torch . mean (x, dim =[2,3]) # spatial pooling self . gamma = torch . mean ( self . gamma , dim =[0])# context pooling if( self . standardize ): self . gamma = ( self . gamma - torch . mean ( self . gamma )) / \\ torch . sqrt ( torch . var ( self . gamma , unbiased = False ) + 1e-5) self . gamma = self . gamma . unsqueeze (0) self . gamma = self . gamma_generator ( self . gamma ) * self . out_mul self . gamma = self . gamma . reshape ([1,-1,1,1]) return self . gamma * x else : # inference mode self . gamma = self . gamma .to(x. device ) return self . gamma * x def extra_repr ( self ): return ’cin ={}’. format ( self . cin ) 15B UppereCaSE: additional details B.1 Algorithm of UpperCaSE Algorithm 1 UpperCaSE: training function for the few-shot classiﬁcation setting. Require: D= {τ1,...,τ D}training dataset Require: bφ() pretrained feature extractor (body) with CaSE blocks parameterized by φ. Require: step(): gradient-step function; Lloss; α, β: step-size hyperparameters for the optimizer. 1: Set φto random values ⊿optional: set φto enforce identity in CaSE output 2: while not done do 3: Sample task τ = (C,T) ∼D 4: Forward pass over context set bφ(Cx) → z1,..., zN ⊿CaSE in adaptive mode 5: Store context embeddings and associated labels M= {(zn,yn)}N n=1 ⊿temporary memory buffer 6: Deﬁne a linear model for the head hψτ () and set ψτ to zero 7: for total inner-steps do ⊿loop to estimate head params 8: Sample (with replacement) mini-batch of training pairs B∼M 9: Update the head parameters ψτ ←step(α,L,B,hψτ ) 10: end for 11: Update the CaSE parameters φ←step(β,L,C,T,bφ,hψτ ) ⊿CaSE in adaptive mode 12: end while Algorithm 2 UpperCaSE: test function for the few-shot classiﬁcation setting. Require: τ∗ = (C∗,x∗) unseen test task with target input x∗ an context C∗. Require: bφ() pretrained feature extractor (body) with meta-learned CaSE blocks parameterized by φ. Require: step(): gradient-step function; Lloss; α: step-size hyperparameter for the optimizer. 1: Forward pass over context set bφ(Cx ∗ ) → z1,..., zN ⊿CaSE in adaptive mode 2: Store context embeddings and associated labels M∗ = {(zn,yn)}N n=1 ⊿temporary memory buffer 3: Deﬁne a linear model for the head hψτ∗ () and set ψτ∗ to zero 4: for total inner-steps do ⊿loop to estimate head params 5: Sample (with replacement) mini-batch of training pairs B∗ ∼M∗ 6: Update the head parameters ψτ∗ ←step(α,L,B∗,hψτ∗ ) 7: end for 8: Return Prediction ˆy∗ = hψτ∗ (bφ(x∗)) ⊿CaSE in inference mode C Additional experimental details and results C.1 Additional details MACs counting MACs are proportional to the size of the task, size of the images, and number of classes. We can count MACs using synthetic tasks. In our case we used a synthetic task of 100-way, 10-shot with input images of size 224 ×224 ×3 generated via Gaussian noise (µ= 0,σ = 1), and labels generated as random integers. We used a mini-batch of size 128 and 500 update steps for UpperCaSE and BiT with an EfﬁcientNetB0 backbone for the ﬁrst and a ResNet50-S for the second. For MD-Transfer we used the same parameters reported in Dumoulin et al. (2021) with images of size 126 ×126 ×3 and ResNet18 backbone. For the ORBIT experiments we counted MACs by using the code in the original repository 2 and reporting the average MACs over all test tasks for both CLE-VE and CLU-VE using a ResNet18 backbone. VTAB+MD trainingWe follow the protocol reported in the original papers (Triantaﬁllou et al., 2019; Dumoulin et al., 2021) training UpperCaSE for 10K tasks on the training datasets and evaluating on the MD test set and on the VTAB datasets. At evaluation time we sample 1200 tasks from the MD test set, and report the mean and conﬁdence intervals. On VTAB we report the results of a single run on the test data (data points are given in advance and do not change across seeds). In all experiments we used the MetaDataset-v2 (MDv2) which does not include ImageNet in the test set. We used a pretrained EfﬁcientNetB0 from the ofﬁcial Torchvision repository 3, and a pretrained ResNet50-S 2https://github.com/microsoft/ORBIT-Dataset 3https://pytorch.org/vision 16from the BiT repository 4. We normalized the inputs using the values reported in the Torchvision documentation (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), for ResNet50-S we use the BiT normalization values (mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]). ORBIT training For the ORBIT experiments we trained UpperCaSE on MDv2 using a pretrained ResNet18 taken from the ofﬁcial Torchvision repository. We normalized the inputs using the values reported in the Torchvision documentation (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]). For the evaluation phase we followed the instructions reported in Massiceti et al. (2021). C.2 CaSE vs SE Table 4: Comparing CaSE against standard Squeeze-and-Excitation (SE) on VTAB+MD using different adaptation heads. MD: Mahalanobis distance head (Bronskill et al., 2021). Linear: linear head trained with UpperCaSE. All adaptive blocks use a reduction of 32. Best results in bold. Model SE CaSE SE CaSE Contextual pooling No Yes No Yes Adaptation head MD MD Linear Linear Image size 84 84 224 224 MetaDataset (all) 67.8 69.6 74.6 76.2 VTAB (all) 43.6 45.3 56.6 58.2 VTAB (natural) 47.5 50.2 65.3 68.1 VTAB (specialized) 63.6 64.9 79.8 79.6 VTAB (structured) 30.6 31.8 38.6 40.1 C.3 CaSE vs other adapters Table 5: Comparing CaSE adaptive blocks (with reduction 64, 32, 16) on VTAB+MD against the FiLM generators used in Bronskill et al. (2021), and a baseline with no body adaptation. CaSE blocks are more efﬁcient in terms of adaptive and amortization parameters while providing higher classiﬁcation accuracy. All models have been trained and tested on 84 ×84 images, using a Mahalanobis distance head. Best results in bold. Adaptation type None FiLM CaSE64 CaSE32 CaSE16 Adaptive Params (M) n/a 0.02 0.01 0.01 0.01 Amortiz. Params (M) n/a 1.7 0.4 0.8 1.6 MetaDataset (all) 53.4 68.4 69.8 69.6 70.4 VTAB (all) 43.5 44.7 46.2 45.3 46.4 VTAB (natural) 45.4 49.5 52.1 50.2 52.6 VTAB (specialized) 69.4 63.8 66.3 64.9 65.5 VTAB (structured) 29.1 31.7 31.8 31.8 32.1 C.4 Ablation studies In this section we provide additional experimental results focusing on ablation studies of the CaSE adaptive block. The results can be summarized as follows: • Ablation of the activation function for the output layer for both CaSE and SE. We have tested three activation funcitons: linear, sigmoid, sigmoid with multiplier. The sigmoid with multiplier uses a constant value set to 2 to center the sigmoid at 1 (this enforces the identity function). The empirical results reported in Table 6 show that the sigmoid with multiplier and the linear layer provide the best results. 4https://github.com/google-research/big_transfer 17• Ablation of the number of hidden units in the hidden layers of CaSE. The number of hidden units is controlled by the reduction and min-units parameters in the code and it depends on the number of inputs. See the paper for more details. The results reported in Table 8 show that blocks with more units provide marginal gains or no gains at all. This is probably due to overﬁtting issues affecting the models with more units. • Ablation of the number of hidden layers of CaSE. The results reported in Table 7 show that the best performance is obtained with 1 and 2 layers. The performance worsen when there are 3 or more layers which is likely due to overﬁtting issues affecting the models with more parameters. • Ablation of the activation function for the hidden layers. Results reported in Table 9 show that CaSE is quite robust against this factor when activations like ReLU and SiLU are used but the performance worsen with Tanh. We have chosen SiLU for the experiments as this is the same activation typically used in Squeeze-and-Excitation layers (e.g. in EfﬁcientNet backbones). Table 6: Performance on VTAB+MD for various activation functions used in the last layer of SE and CaSE. Sigmoid-2 indicates that the output of a standard Sigmoid is multiplied by 2. Both SE and CaSE use a reduction factor of 32 with min-clipping of 32. All model have been trained using an EfﬁcientNetB0 backbone with a linear head on images of size 224 ×224. Results for SE with linear activation have not been reported because the training was unstable (loss rapidly diverging at the ﬁrst iterations). Best results in bold. Adaptive block SE SE CaSE CaSE CaSE Activation (output) Sigmoid Sigmoid-2 Linear Sigmoid Sigmoid-2 MetaDataset (all) 74.2 74.6 75.8 74.9 76.2 VTAB (all) 56.8 56.6 58.4 56.8 58.2 VTAB (natural) 67.0 65.3 68.3 67.1 68.1 VTAB (specialized) 81.1 79.8 79.5 80.8 79.6 VTAB (structured) 36.9 38.6 40.3 37.1 40.1 Table 7: Comparing CaSE adaptive blocks with different number of hidden layers on VTAB+MD. All models have been trained and tested on 224 ×224 images, using CaSE with reduction 64 and clip factor (min-units) 16, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. # Hidden layers 1 2 3 4 Amortiz. Params (M) 0.420 0.426 0.432 0.438 MetaDataset (all) 76.0 76.1 75.5 75.2 VTAB (all) 58.2 58.4 58.2 58.0 VTAB (natural) 68.3 69.1 68.0 67.4 VTAB (specialized) 79.7 80.3 80.5 80.3 VTAB (structured) 40.0 39.4 39.7 39.7 18Table 8: Comparing CaSE adaptive blocks with different number of hidden units on VTAB+MD. The number of hidden units depends on the input size and is deﬁned by the reduction and the clip factor (min-units). All models have been trained and tested on 224 ×224 images, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. Reduction factor 64 32 16 8 Clip factor 16 32 48 64 Amortiz. Params (M) 0.4 0.8 1.6 3.0 MetaDataset (all) 76.1 76.2 75.8 76.2 VTAB (all) 58.4 58.2 57.9 58.5 VTAB (natural) 69.1 68.1 67.9 68.3 VTAB (specialized) 80.3 79.6 79.4 79.0 VTAB (structured) 39.4 40.1 39.7 40.9 Table 9: Comparing CaSE adaptive blocks with different activation functions for the hidden layers on VTAB+MD. All models are based on a reduction factor of 64 and a clip factor of 16 (0.4M amortiza- tion parameters) and they have been trained and tested on 224 ×224 images, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. Activation (hidden) SiLU ReLU Tanh MetaDataset (all) 76.1 75.8 74.8 VTAB (all) 58.4 57.8 48.2 VTAB (natural) 69.1 69.8 67.0 VTAB (specialized) 80.3 79.7 80.8 VTAB (structured) 39.4 39.4 36.4 19C.5 Role of CaSE blocks Figure 4: Boxplots for all the MDv2 test datasets (100 tasks per dataset) reporting the CaSE activation (vertical axis) at different stages of an EfﬁcientNetB0 (horizontal axis, with early stages on the left). The box encloses ﬁrst to third quartile, with the median represented by the orange line. The whiskers extend from the box by 1.5 the inter-quartile range. Outlier (point past the end of the whiskers) are represented with black circles. 20Figure 5: CaSE activation values (vertical axis) for all channels (horizontal axis) at different stages (top plots are early stages) in EfﬁcientNetB0 for the MDv2 test dataset (one task per dataset). Values are similar and closer to one in the ﬁrst stages but diverge in the latest. The magnitude tends to increase with depth. 21C.6 UpperCaSE: results on VTAB+MD In this section we provide a full breakdown of the results for UpperCaSE vs. other methods on the VTAB+MD benchmark. Results for other methods are taken from Bronskill et al. (2021) and Dumoulin et al. (2021). UpperCaSE uses CaSE with reduction 64 (min-clip 16) for EfﬁcientNetB0 and reduction 32 (min-clip 32) for ResNet50-S. Results for UpperCaSE on MD are the average over 1200 test tasks. In Table 10 we report the results for UpperCaSE against ﬁne-tuning methods (BiT, MD-Trasnfer, SUR) and in Table 11 the results for UpperCaSE against meta-learning and hybrid methods (ProtoNet, ProtoMAML, Cross Transformer CTX, LITE). Overall UpperCaSE performs well on MD and the natural split of VTAB, this may be due to the fact that transfer learning is more beneﬁcial on those datasets as they are more similar to those used during meta-training. The largest difference in performance between UpperCaSE and ﬁne-tuning methods is on the structured split of VTAB, which includes tasks that require counting and pose estimation. This is likely due to the difference w.r.t. the meta-training set. In this case, ﬁne-tuning the entire network is more effective than body adaptation as the knowledge gap is wider and it requires more adjustments to the parameters. Table 10: Comparing UpperCaSE against ﬁne-tuning methods. Best result in bold. Model BiT MD-Transfer SUR UpperCaSE UpperCaSE Image Size 224 126 224 224 224 Network RN50-S RN18 RN50 ×7 ENB0 RN50-S Params (M) 23.5 11.2 164.5 4.0 23.5 Omniglot 68.0 ±4.5 82.0 ±1.3 92.8±0.5 90.7±0.4 89.1 ±0.5 Aircraft 77.4 ±3.5 76.8 ±1.2 84.4 ±0.6 89.4±0.4 87.5±0.4 Birds 90.8±1.5 61.2±1.3 75.8 ±1.0 90.4±0.4 89.6 ±0.4 DTD 85.0±2.5 66.0±1.1 74.3 ±0.7 83.4±0.4 84.8 ±0.5 QuickDraw 66.6 ±3.7 61.3 ±1.1 70.3 ±0.7 76.8±0.5 73.7±0.6 Fungi 59.4 ±4.2 35.5 ±1.1 81.7±0.6 59.3±0.8 56.8 ±0.8 Trafﬁc Sign 73.5 ±4.7 84.7±0.9 50.0±1.1 68.5 ±0.8 70.6 ±0.8 MSCOCO 65.7±2.7 39.6±1.0 49.4 ±1.1 50.8 ±0.7 46.7 ±0.8 Caltech101 87.2 70.6 82.3 88.3 86.2 CIFAR100 54.4 31.3 33.7 52.7 47.0 Flowers102 83.3 66.1 55.7 85.3 83.0 Pets 87.9 49.1 76.3 89.9 89.3 Sun397 33.3 13.9 27.5 35.8 32.5 SVHN 70.4 83.2 18.7 62.7 59.8 EuroSAT 94.4 88.7 78.9 92.2 91.6 Resics45 76.1 63.7 62.4 75.5 74.4 Patch Camelyon 83.1 81.5 75.6 79.3 80.9 Retinopathy 70.2 57.6 27.9 74.3 73.7 CLEVR-count 74.0 40.3 30.0 40.3 42.0 CLEVR-dist 51.5 52.9 37.1 38.9 37.3 dSprites-loc 82.7 85.9 30.0 45.3 38.1 dSprites-ori 55.1 46.4 19.8 42.5 41.4 SmallNORB-azi 17.8 36.5 12.9 15.7 15.1 SmallNORB-elev 32.1 31.2 18.1 22.7 21.0 DMLab 43.2 37.9 33.3 38.7 36.1 KITTI-dist 79.9 58.7 52.3 71.0 69.6 MetaDataset (all) 73.3 63.4 71.0 76.1 74.9 VTAB (all) 65.4 55.6 42.9 58.4 56.6 VTAB (natural) 69.4 52.4 49.0 69.1 66.3 VTAB (specialized) 81.0 72.9 61.2 80.3 80.1 VTAB (structured) 54.5 49.4 29.2 39.4 37.6 22Table 11: Comparing UpperCaSE against meta-learning and hybrid methods. Best result in bold. Model ProtoNet ProtoMAML CTX LITE UpperCaSE UpperCaSE Image Size 224 126 224 224 224 224 Network ENB0 RN18 RN34 ENB0 ENB0 RN50-S Params (M) 4.0 11.2 21.3 4.0 4.0 23.5 Omniglot 88.3 ±0.8 90.2±0.7 84.6±0.9 86.5 ±0.8 90.7±0.4 89.1±0.5 Aircraft 85.0 ±0.7 82.1 ±0.6 85.3 ±0.8 83.6 ±0.7 89.4±0.4 87.5±0.4 Birds 90.2±0.5 73.4±0.9 72.9 ±1.1 88.6 ±0.7 90.4±0.4 89.6±0.4 DTD 81.4 ±0.6 66.3 ±0.8 77.3 ±0.7 84.1±0.7 83.4±0.4 84.8±0.5 QuickDraw 76.0±0.7 66.4±1.0 73.3 ±0.8 75.7±0.8 59.3±0.8 56.8 ±0.8 Fungi 57.4 ±1.1 46.3 ±1.1 48.0 ±1.2 56.9 ±1.2 59.3±0.8 56.8±0.8 Trafﬁc Sign 53.5 ±1.1 50.3 ±1.1 80.1±1.0 65.8±1.1 68.5 ±0.8 70.6 ±0.8 MSCOCO 49.8±1.1 39.0±1.0 51.4±1.1 50.0 ±1.0 50.8 ±0.7 46.7±0.8 Caltech101 87.4 73.1 84.2 87.7 88.3 86.2 CIFAR100 43.1 29.7 37.5 48.8 52.7 47.0 Flowers102 78.2 60.2 81.8 83.5 85.3 83.0 Pets 88.6 56.6 70.9 89.3 89.9 89.3 Sun397 32.9 8.1 24.8 30.9 35.8 32.5 SVHN 35.2 46.8 67.2 51.0 62.7 59.8 EuroSAT 83.3 80.1 86.4 89.3 92.2 91.6 Resics45 68.8 53.5 67.7 76.4 75.5 74.4 Patch Camelyon 73.3 75.9 79.8 81.4 79.3 80.9 Retinopathy 31.3 73.2 35.5 40.3 74.3 73.7 CLEVR-count 27.2 32.7 27.9 31.4 40.3 42.0 CLEVR-dist 28.5 35.4 29.6 32.8 38.9 37.3 dSprites-loc 13.4 42.0 23.2 12.3 45.3 38.1 dSprites-ori 19.6 23.0 46.9 31.1 42.5 41.4 SmallNORB-azi 9.4 13.4 37.0 14.5 15.7 15.1 SmallNORB-elev 17.0 18.8 21.6 21.0 22.7 21.0 DMLab 35.8 32.5 31.9 39.4 38.7 36.1 KITTI-dist 56.5 54.4 54.3 63.9 71.0 69.6 MetaDataset (all) 72.7 64.2 71.6 73.9 76.1 74.9 VTAB (all) 46.1 45.0 50.5 51.4 58.4 56.6 VTAB (natural) 60.9 45.7 61.1 65.2 69.1 66.3 VTAB (specialized) 64.2 70.7 67.3 71.9 80.3 80.1 VTAB (structured) 25.9 31.5 34.1 30.8 39.4 37.6 23",
      "references": [
        "How to train your maml.",
        "Improved few-shot visual classification.",
        "Discriminative k-shot learning using probabilistic models.",
        "The Netflix Prize.",
        "Memory efficient meta-learning with large images.",
        "Meta-learning feature representations for adaptive gaussian processes via implicit differentiation.",
        "A closer look at few-shot classification.",
        "Modular meta-learning with shrinkage.",
        "Comparing transfer and meta learning approaches on a unified few-shot classification benchmark.",
        "A learned representation for artistic style.",
        "Model-agnostic meta-learning for fast adaptation of deep networks.",
        "Neural processes.",
        "Meta-learning probabilistic inference for prediction.",
        "Spottune: transfer learning through adaptive fine-tuning.",
        "Dynamic neural networks: A survey.",
        "Meta-learning in neural networks: A survey.",
        "Squeeze-and-excitation networks.",
        "Arbitrary style transfer in real-time with adaptive instance normalization.",
        "Big transfer (bit): General visual representation learning.",
        "Meta-learning with differentiable convex optimization.",
        "Cross-domain few-shot learning with task-specific adapters.",
        "Orbit: A real-world few-shot dataset for teachable object recognition.",
        "On first-order meta-learning algorithms.",
        "Few-shot learning with class imbalance.",
        "How sensitive are meta-learners to dataset imbalance?",
        "Tadam: Task dependent adaptive metric for improved few-shot learning.",
        "Bayesian meta-learning for the few-shot setting via deep kernels.",
        "FiLM: Visual reasoning with a general conditioning layer.",
        "Rapid learning or feature reuse? Towards understanding the effectiveness of maml.",
        "Meta-learning with implicit gradients.",
        "Learning multiple visual domains with residual adapters.",
        "Efficient parametrization of multi-domain deep neural networks.",
        "Fast and flexible multi-task classification using conditional neural adaptive processes.",
        "Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook.",
        "Non-gaussian gaussian processes for few-shot regression.",
        "Prototypical networks for few-shot learning.",
        "Meta-transfer learning for few-shot learning.",
        "Rethinking few-shot image classification: a good embedding is all you need?",
        "Meta-dataset: A dataset of datasets for learning to learn from few examples.",
        "Convolutional networks with adaptive inference graphs.",
        "Matching networks for one shot learning.",
        "Coordinate descent algorithms.",
        "Blockdrop: Dynamic inference paths in residual networks.",
        "The microsoft 2017 conversational speech recognition system.",
        "Fast context adaptation via meta-learning."
      ],
      "meta_data": {
        "arxiv_id": "2206.09843v3",
        "authors": [
          "Massimiliano Patacchiola",
          "John Bronskill",
          "Aliaksandra Shysheya",
          "Katja Hofmann",
          "Sebastian Nowozin",
          "Richard E. Turner"
        ],
        "published_date": "2022-06-20T15:25:08Z",
        "github_url": "https://github.com/mpatacchiola/contextual-squeeze-and-excitation"
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Contextual Squeeze-and-Excitation (CaSE), an adaptive block that modulates a pretrained backbone using task-context information to enable rapid adaptation in few-shot image classification. Proposes UpperCaSE, a hybrid meta-training + fine-tuning protocol that adapts the body of the network via CaSE in a single forward pass over the context and trains only a linear head, achieving state-of-the-art accuracy among meta-learners on VTAB+MD and strong results on ORBIT, while markedly reducing adaptation cost (parameters and MACs) compared to full fine-tuning and FiLM-based adapters.",
        "methodology": "CaSE extends Squeeze-and-Excitation by using a per-task contextual pooling to produce a single scale vector per layer, which is then transferred to target data during inference. CaSE parameters φ are meta-trained, while task-specific head parameters ψτ are obtained via an inner-loop gradient routine. UpperCaSE uses a Coordinate-Descent (CD) scheme: (i) adapt the body with CaSE on the task context (CaSE in adaptive mode) while keeping the head zeroed, (ii) solve for ψτ with a few gradient steps (inner loop), then (iii) update φ (outer loop) with respect to the loss. Inference on new tasks follows a three-step process: CaSE adaptation on the context, estimation of ψ* via inner loops, and prediction with hψ*(bφ(x*)). CaSE is placed in several blocks of the backbone (e.g., EfficientNetB0, ResNet50-S) with a design that uses 50% fewer parameters than FiLM (CaSE uses only scale, not shift). The approach is evaluated against meta-learners, fine-tuners, and hybrids on comprehensive benchmarks, with ablations on CaSE configuration (reduction, number of layers, activation).",
        "experimental_setup": "Datasets and benchmarks: VTAB+MD (26 datasets: 8 from MD MetaDataset-v2, 18 VTAB) and the ORBIT real-world personalization benchmark. Backbones and pretraining: pretrained ImageNet backbones (EfficientNetB0 and ResNet50-S). Training protocol: meta-training with Bronskill et al. (2021) style episodic tasks (10k training tasks), evaluation on 1200 MD tasks per dataset; no data augmentation. Evaluation metrics: average accuracy across datasets; adaptation cost measured by MACs for task adaptation. ORBIT evaluation follows the cross-domain protocol (MDv2 training, ORBIT testing) with frames and videos accuracy and MACs. Implementation details: Adam optimizer with decaying learning rate, CaSE reduction factors, and the use of a Mahalanobis distance head in many baselines; code released publicly.",
        "limitations": "Two main limitations: (i) UpperCaSE requires iterative gradient updates for the head, which are hardware-dependent and can be slow on portable devices; (ii) VTAB per-dataset analysis shows degradation on structured datasets (localization/counting tasks), suggesting that larger domain shifts may still require body fine-tuning for high accuracy. Potentially sensitive to domain shift magnitude and dataset imbalance in some settings.",
        "future_research_directions": "Explore adaptive placement, size, and number of CaSE blocks to further reduce adaptation cost; investigate global vs local context pooling trade-offs and potentially hybrid architectures; extend CaSE/UpperCaSE to other modalities and architectures; improve performance on structured/dense tasks by integrating selective full-body fine-tuning or alternative adapters; study dynamical gating strategies to decide which layers to adapt per task; optimize for mobile hardware with reduced memory footprint and latency; combine with other meta-learning paradigms and larger-scale pretraining to close remaining gaps with full fine-tuning.",
        "experimental_code": "import torch\nfrom torch import nn\nfrom collections import OrderedDict\n\nclass CaSE(nn.Module):\n  def __init__(self, cin, reduction=64, min_units=16, standardize=True, out_mul=2.0, device=None, dtype=None):\n      \"\"\"\n      Initialize a CaSE adaptive block.\n  \n      Parameters:\n      cin (int): number of input channels.\n      reduction (int): divider for computing number of hidden units.\n      min_units (int): clip hidden units to this value (if lower).\n      standardize (bool): standardize the input for the MLP.\n      out_mul (float): multiply the MLP output by this value.\n      \"\"\"\n      factory_kwargs = {'device': device, 'dtype': dtype}\n      super(CaSE, self).__init__()\n      self.cin = cin\n      self.standardize = standardize\n      self.out_mul = out_mul\n\n      # Gamma-generator\n      hidden_features = max(min_units, cin // reduction)\n      self.gamma_generator = nn.Sequential(OrderedDict([\n          ('gamma_lin1', nn.Linear(cin, hidden_features, bias=True, **factory_kwargs)),\n          ('gamma_silu1', nn.SiLU()),\n          ('gamma_lin2', nn.Linear(hidden_features, hidden_features, bias=True, **factory_kwargs)),\n          ('gamma_silu2', nn.SiLU()),\n          ('gamma_lin3', nn.Linear(hidden_features, cin, bias=True, **factory_kwargs)),\n          ('gamma_sigmoid', nn.Sigmoid()),\n        ]))\n\n      self.gamma = torch.tensor([1.0]) # Set to one for the moment\n      self.reset_parameters()\n\n  def reset_parameters(self):      \n      torch.nn.init.zeros_(self.gamma_generator.gamma_lin3.weight)\n      torch.nn.init.zeros_(self.gamma_generator.gamma_lin3.bias)\n\n  def forward(self, x):\n      # Adaptive mode\n      if(self.training):\n          self.gamma = torch.mean(x, dim=[0,2,3]) # spatial + context pooling\n          if(self.standardize):\n                  self.gamma = (self.gamma - torch.mean(self.gamma)) / torch.sqrt(torch.var(self.gamma, unbiased=False)+1e-5)\n          self.gamma = self.gamma.unsqueeze(0) #-> [1,channels]\n          self.gamma = self.gamma_generator(self.gamma) * self.out_mul\n          self.gamma = self.gamma.reshape([1,-1,1,1])\n          return self.gamma * x # Apply gamma to the input and return\n      # Inference Mode\n      else:\n          self.gamma = self.gamma.to(x.device)\n          return self.gamma * x # Use previous gamma\n\n  def extra_repr(self) -> str:\n        return 'cin={}'.format(self.cin)\n",
        "experimental_info": "Context: The CaSE module adapts a pretrained backbone by applying a per-channel scale gamma generated by a small MLP, conditioned on a per-task context. Key design choices include:\n- CaSE only modulates the scale (no shift) to reduce parameter count and computation (about 50% fewer parameters than FiLM).\n- Gamma generation: a small MLP with 3 linear layers, GEL-like activations (SiLU) between layers, and a final sigmoid to produce per-channel gamma in [0,1]. The last layer weights are initialized to zero for stable startup.\n- Adaptive mode for training: during forward when in training, CaSE pools over spatial and batch dimensions (mean over channels+spatial dims) to obtain a per-channel context vector, optionally standardizes it, passes it through the gamma_generator, scales by out_mul, and applies to the input feature map; in inference, uses a stored gamma.\n- Integration points: CaSE is designed to be inserted in multiple blocks of a backbone (e.g., EfficientNetB0, ResNet50-S) as adaptive_layer; the codebase demonstrates usage by passing CaSE as adaptive_layer to block constructors.\n- Inference and meta-learning usage: CaSE parameters phi are meta-trained (the gamma_generator) while task-specific head parameters psi_tau are learned in an inner loop. UpperCaSE executes a Coordinate-Descent (CD) scheme, alternating body adaptation via CaSE with inner-loop head optimization and an outer-loop phi update.\n- Implementation status in repo: CaSE is implemented in adapters/case.py, with a simple API and standard PyTorch modules; the forward path handles both adaptive (training) and inference modes, storing gamma for inference, and employing a per-channel gamma_generator to modulate the input.\n- Additional integration notes: The design is used in combination with a wrapper model (UpperCaSE) that coordinates CaSE-based adaptation on a per-task context and learns a linear head on top, while keeping the rest of the backbone fixed or adaptively modulated by CaSE.\n- Example usage snippet context: The repository includes example scripts (e.g., example.py) showing CaSE integrated with a pretrained EfficientNetB0, using a context/target batch scheme and a meta/adaptation pipeline."
      }
    },
    {
      "title": "Domain Generalization for Medical Imaging Classification with Linear-Dependency Regularization",
      "full_text": "Domain Generalization for Medical Imaging Classiﬁcation with Linear-Dependency Regularization Haoliang Li1 YuFei Wang1 Renjie Wan1 Shiqi Wang2 Tie-Qiang Li3,4 Alex C. Kot1 1Rapid-Rich Object Search Lab, Nanyang Technological University, Singapore 2Department of Computer Science, City University of Hong Kong, China 3Department of Clinical Science, Intervention, and Technology, Karolinska Institute, Sweden 4Department of Medical Radiation and Nuclear Medicine, Karolinska University Hospital, Sweden {lihaoliang,yufei001,rjwan,eackot}@ntu.edu.sg shiqwang@cityu.edu.hk tie-qiang.li@ki.se Abstract Recently, we have witnessed great progress in the ﬁeld of medical imaging classiﬁ- cation by adopting deep neural networks. However, the recent advanced models still require accessing sufﬁciently large and representative datasets for training, which is often unfeasible in clinically realistic environments. When trained on limited datasets, the deep neural network is lack of generalization capability, as the trained deep neural network on data within a certain distribution (e.g. the data captured by a certain device vendor or patient population) may not be able to gener- alize to the data with another distribution. In this paper, we introduce a simple but effective approach to improve the generalization capability of deep neural networks in the ﬁeld of medical imaging classiﬁcation. Motivated by the observation that the domain variability of the medical images is to some extent compact, we propose to learn a representative feature space through variational encoding with a novel linear-dependency regularization term to capture the shareable information among medical data collected from different domains. As a result, the trained neural network is expected to equip with better generalization capability to the “unseen\" medical data. Experimental results on two challenging medical imaging classiﬁca- tion tasks indicate that our method can achieve better cross-domain generalization capability compared with state-of-the-art baselines. 1 Introduction Due to the breakthrough in machine learning and deep learning, recent years have witnessed numerous signiﬁcant successes in various medical imaging tasks. However, one of the limitations of deep learning is that it lacks generalization capability when the number of training data is not sufﬁcient [44]. In practice, it is often the case that the testing data (a.k.a. target domain) can be dissimilar to the training data (a.k.a. source domain) in terms of many factors, such as imaging protocol, device vendors and patient populations. Such domain shift problem can lead to a signiﬁcantly negative impact on the performance of medical imaging classiﬁcation. To tackle such domain shift problem, domain adaptation [ 31] aims to transfer the knowledge from a source domain to a different but relevant target domain. Recently, many studies have been conducted to improve the transferable capability in the ﬁeld of medical imaging classiﬁcation with domain adaptation by assuming that target domain data are accessible [43, 8]. In many cases, requiring to access the target domain data in advance may not be feasible. For example, in the real-time clinical application scenario, it is difﬁcult to collect sufﬁcient target domain data to 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2009.12829v3  [cs.CV]  29 Oct 2020help with network training. For another example, it is also difﬁcult to access the target domain data as many medical data are protected by privacy regulation. Thus, it is natural to ask whether we can still learn a generalized deep neural network without any prior knowledge regarding the target domain. Domain generalization has been proposed to tackle this problem by assuming to have no access to the target information but utilizing multiple source domains’ information to better generalize to the “unseen\" new domain for testing. Generally speaking, current research regarding domain generalization in the ﬁeld of medical imaging classiﬁcation can be categorized into two streams. The ﬁrst stream aims at conducting data aug- mentation based on medical imaging data in terms of image quality, image appearance and spatial shape [42]. Although the variation of medical images turns out to be more compact as the capturing environment can be ﬁxed in advanced compared with the images captured in our daily life, it may be difﬁcult to choose suitable augmentation types and magnitudes for clinical deployment purposes in a certain environment. The other stream leverages the advantage of domain alignment or meta-learning methods for feature representation learning [39, 7]. However, the learned feature representation may still suffer from the overﬁtting problem, as the feature representations are only shareable among multiple source domains which may not be able to generalize to target. In this work, we propose to marriage the advantage of data augmentation and domain alignment to tackle the domain generalization problem for medical imaging classiﬁcation. Instead of directly conducting augmentation in the image domain through some linear transformations with pre-deﬁned parameters [42], we assume that there exists linear dependency in a latent space among various domains. To model such linear dependency, we propose to train a deep neural network with a novel rank regularization term on latent feature space by setting the rank of latent feature to be the number of categories. Meanwhile, we also propose to restrict the distribution of latent features to follow a pre- deﬁned prior distribution through variational encoding. We theoretically prove that an upper bound on the empirical risk of any “unseen\" but related target domain can be achieved under our formulation, such that the overﬁtting problem can be alleviated. Experimental results on two challenging medical imaging classiﬁcation tasks, including imbalanced-category based skin lesion classiﬁcation as well as spinal cord gray matter segmentation (which can be treated as pixel-wise classiﬁcation), indicate that our proposed method can achieve much better generalization capability compared with other state-of-the-art baselines. The code is available at https://github.com/wyf0912/LDDG. 2 Related Works Domain Adaptation and Generalization. To tackle the domain-shift problem between source and target domain data, traditional domain adaptation approaches focused on either subspace learning or instance re-weighting [16, 30, 41, 11]. Deep learning methods are also proved to be effective for domain adaptation task through either distribution alignment (e.g. Maximum Mean Discrepancy) [26] or adversarial learning through feature level [10, 35] or pixel level [3]. Recently, it has been shown that by considering pixel level adaptation and feature level adaptation together, better adaptation performance can be achieved [15, 23]. Compared with domain adaptation, domain generalization is much more challenging, as we assume that we have no access to the target domain. Instead, we aim to train a model that is expected to be generalized to the “unseen\" target by assuming that only multiple source domains are available. For example, Yang and Gao [38] proposed to leverage Canonical Correlation Analysis (CCA) to extract shareable information among domains. Muandet et al. [29] proposed a Domain Invariant Component Analysis (DICA) algorithm to learn an empirical mapping based on multiple source- domain data where the distribution mismatch across domains was minimized. This idea was further extended by [22, 24] in an autoencoder framework with distribution regularization on latent space. In [37, 20], the low-rank regularization based on classiﬁer and model parameters were explored to extract universal feature representation. Ghifary et al. [12] proposed a multi-task autoencoder to learn domain invariant features by reconstructing the latent representation of a given sample from one domain to another domain. Motiian et al. [28] proposed to minimize the semantic alignment loss as well as the separation loss based on deep learning models. Carlucci et al. [4] proposed to shufﬂe the image patch to learn generalized feature representation. Recently, Wang et al. [36] proposed to extend MixUp [40] to the settings of multiple domains for heterogeneous domain generalization task. As for meta-learning based techniques, Li et al. [21] proposed to transfer the idea in [9] to the “unseen\" target domain setting by randomly constructing meta-train and meta-test set, which was 2further extended by Balaji et al. [1] with a scheme to learn a regularization network to improve the scalability of domain generalization. Cross-Domain Medical Imaging Classiﬁcation. Due to the various imaging protocols, device vendors and patient populations, we may also encounter the problem of distribution shift in clinical practice. To tackle such domain shift problem, image synthesis can be adopted through Generative Adversarial Networks [13, 45] to mitigate the domain shift problem. For example, Zhang et al. [43] proposed to leverage CycleGAN for medical imaging problem to transfer the knowledge from CT images to X-ray images. Chen et al. [5] conducted domain translation from MR to CT domain for heart segmentation problem. With few label information available in target domain, Zhang et al. [44] proposed to conduct segmentation and data synthesis jointly to segment heart chambers in both CT and MR domain. Dou et al. [8] proposed a two parallel domain-speciﬁc encoders and a decoder where the weights are shared between domains to boost the performance of training on both single domain and cross-domain scenario. When target domain data are not available, Zhang et al. [42] proposed to conduct data augmentation on source domain to achieve better generalization capability for medical imaging classiﬁcation task. Yoon et al. [39] proposed to learn generalized feature representation through classiﬁcation and contrastive semantic alignment technique [28]. More recently, Dou et al. [7] proposed to conduct meta-learning with global class alignment as well as local sample clustering regularization for medical imaging classiﬁcation task. 3 Methodology Preliminary. We denote the training samples from multiple source domains on a joint space X×Y as D= {(xk i,yk i)}Nk i=1,k ∈{1,2,...,K }, where xk i denotes the ith training sample from the kth source domain, Nk is the number of samples in the kth domain, and yk i is the corresponding label groundtruth. The goal of domain generalization is that given a sample xT from an unseen domain, we aim to predict its output ˆyT through a trained classiﬁer. We provide a framework named Linear-Dependency Domain Generalization (LDDG) that improves the generalization capability of medical imaging classiﬁcation. By assuming that there exists linear dependency in the latent feature space among various domains based on a certain task, we propose to regularize the latent feature space by modeling intra-class variation among multiple source domains through rank constraint meanwhile matching the distribution of latent features extracted from multiple source domains to a pre-deﬁned distribution prior, such that the shareable information among domains can be learned. The details of our proposed method are introduced below. Linear-Dependency Modeling. Directly training a classiﬁcation network with a task-speciﬁc loss (e.g. cross-entropy loss) may not be feasible, as in the ﬁeld of medical imaging, it is difﬁcult to collect large and diverse datasets, which can lead to poor generalization on a new and “unseen\" domain. To improve the generalization capability of medical imaging classiﬁcation, in [42], based on the observation that medical image domain variability is more compact compared with other image data, a data augmentation approach was proposed based on three different aspects: image quality (e.g., blurriness), image appearance (e.g., brightness) and spatial shape (e.g., rotation, scaling), by assuming other characteristics are supposed to be more consistent. However, we empirically ﬁnd that it is challenging to choose a suitable augmentation type as well as its magnitude for a speciﬁc medical imaging classiﬁcation task. Inspired by the observation that most of the aforementioned augmentation processes can be con- ducted through linear transformation, we assume that there exists linear dependency on the latent feature space. To be more speciﬁc, by assuming that we have a medical image batch collected from K different domains with label cas {x1 i1,c,x2 i2,c,...,x K iK,c}, there exists a set of parameters {α1,α2,...,α K}such that the corresponding latent features {z1 i1,c,z2 i2,c,...,z K iK,c}hold the property that zj ij,c = α1z1 i1,c + α2z2 i2,c + ...+ αj−1zj−1 ij−1,c + αj+1zj+1 ij+1,c + ...+ αKzK iK,c for different j. In other words, there exists a dominant eigenvalue capturing the category information of the matrix [z1 i1,c,z2 i2,c,...,z K iK,c]. Therefore, given a sample mini-batch denoted by X= {xk i}, we can obtain the corresponding latent features as Zthrough a posterior q(z|x) parameterized by an encoder. By further conducting mode-1 ﬂattening Zas Z1, our proposed rank regularization can be given as rank(Z) =C, where Cis the number of categories of a speciﬁc task. 1We assume that the ﬁrst dimension is associated with sample index. 3Setting the rank of Z to Cis equivalent to minimize the (C+ 1)th singular value of Z. By denoting it as σC+1, we can reformulate the rank loss and compute its sub-gradient as Lrank = σC+1, ∂σC+1 ∂Z = U:,C+1V⊤ :,C+1, (1) where U and V are obtained through SVD as Z = UΣV⊤. Noted that it is quite common to impose low-rank regularization in the ﬁnal objective for domain generalization task [37, 20]. Our proposed method is different from these methods in two folds, 1) we impose rank regularization based on the latent feature space while the existing works imposed low-rank regularization on classiﬁer parameters, which are not computational efﬁciency; 2) we set the rank to be a speciﬁc number instead of simply conducting low-rank regularization, we show in the experimental section that it can lead to better performance. Distribution Alignment. In addition to modeling the linear dependency in latent feature space, we further propose to extract shareable information among multiple domains, such that a more transferable feature representation can be learned which can beneﬁt generalization capability of deep neural networks. Existing techniques aimed to either minimize domain variance through distribution alignment between domain pairs [29, 22] or conduct local sample clustering through contrastive loss or triplet loss [28, 7]. However, based on our empirical analysis, the aforementioned technique may suffer from overﬁtting problem to the source domains, which is not surprising as the distribution of “unseen\" target domain may not match the distribution of multiple source domains. Thus, simply minimizing domain variance or local sample cluttering on source domains may not be able to generalize well to the “unseen\" one. To this end, we propose to conduct variational encoding [18] by adopting Kullback-Leibler (KL) divergence, which aims to match the latent features from multiple source domains to a pre-deﬁned prior distribution. In our work, we adopt Gaussian distribution N∼ (0,1) as the prior distribution, which is computationally tractable through reparameterization trick [18]. The KL divergence can be formulated as KL(q(Z|X)||N∼ (0,1)), where Zis the latent features deﬁned in the previous section. We show in the next section that by jointly conducting linear-dependency modeling and distribution regularization through KL divergence can lead to an upper bound of empirical risk from any “unseen\" but related domains. Theoretical Analysis. In this section, we provide the theoretical analysis of our proposed framework. In particular, We show that our framework can lead to an upper bound of expected loss on “unseen\" but related target domain. We ﬁrst make the following assumptions: Assumption 1. For any latent feature belong to domain T with label c, it can be represented by data from other related domains, i.e., q(zT iT ,c|xT iT ,c) =∑K j=1 βjq(zj ij,c|xj ij,c), where βj >= 0and ∥β∥≤ M , {x1 i1,c,x2 i2,c,...,x K iK,c}belong to the same category as xT. Noted that Assumption 1 is a mild assumption in the ﬁeld of medical imaging classiﬁcation task and is also reasonable in our setting as we restrict the rank of latent features to be the number of category, such that there exists linear dependency based on the latent features belonging to the same category. We further make assumption on the loss function Lbased on the output of classiﬁer. Assumption 2. (1) L is non-negative and bounded. (2) L is convex: L(∑ jλjyj,y) ≤∑ jλjL(yj,y), where λj ≥0 and ∑ jλj = 1. Note that this assumption is easy to be satisﬁed for several standard loss functions (e.g. cross-entropy loss). Under these assumptions, we have the following theorems. Theorem 1. Given a sample xT iT ,c from target domain T where the distribution of its latent variable is represented as q(zT iT ,c|xT iT ,c) =∑K j=1 βjq(zj ij,c|xj ij,c), its latent variable is within the manifold of N∼ (0,1). 4Proof. For simplicity, we ﬁrst denote the distribution on latent variables asqi(z),i = {1,2,...,K,T } as well as the Gaussian prior N∼ (0,1) as q∗(z). We can obtain the following upper bound, KL(qT(z)||q∗(z)) = K∑ j=1 βj ∫ z qj(z) logqT(z) q∗(z) dz = K∑ j=1 βj ∫ z qj(z) logqj(z)[1 + (qT(z)/qj(z) −1)] q∗(z) dz≤ K∑ j=1 βjKL(qj(z)∥q∗(z)), (2) where we use log(1 +x) ≤xand ∫ qT(z)dz= ∫ qj(z)dz= 1. As KL(qj(z)∥q∗(z)) is minimized according to our proposed distribution alignment, KL(qT(z)||q∗(z)) is then minimized. This completes the proof. Theorem 1 shows that the latent feature of any unseen but related domain lies in the manifold of pre-deﬁned prior. With the help of Theorem 1, we can further derive the upperbound of empirical risk of target domain. Theorem 2. Given data from Ksource domains, where the empirical risk of domain jis given as L(ˆyj,y) =ϵj ≤ϵ, the expected loss L(ˆyT,y) is at most Mϵ + logC, where Cdenotes the number of category given a task, if the classiﬁcation layer is linear with softmax normalization trained by L which is a cross-entropy loss. Proof. Based on Theorem 1, we have qT(z) =q∗(z). Thus, we have the following upper bound, ∫ z L(ˆyT,y)qT(z)dz= ∫ z L( K∑ j=1 βjˆyj,y)q∗(z)dz= ∫ z L(∥β∥ K∑ j=1 βj ∥β∥ˆyj,y)q∗(z)dz ≤ K∑ j=1 βj ∥β∥ ∫ z L(∥β∥ˆyj,y)qj(z)dz≤ K∑ j=1 βj ∥β∥Mϵ + logC = Mϵ + logC, (3) where L(·) denotes the cross-entropy loss with softmax operation. Noted that we only adopt a linear layer for classiﬁer, thus ˆyT = ∑K j=1 βjˆyj holds based on Assumption 1. We also utilize the bounds of Log-Sum-Exp function f(a) ≤max{a1,a2,...,a n}+ logn, where f(a) = log∑n i=1 exp(ai). In our work, {a1,a2,...an}corresponds to the softmax output of ndifferent nodes. Thus, the second line of proof holds. This completes the proof. Theorem 2 shows that our proposed method has a good generalization capability. If the empirical risks on source domains are small, the empirical risk on target domain is also expected to be small. Model Training. Our proposed architecture consists of three part, a feature extractor Qθ, a variational encoding network Fω, and a classiﬁcation network Tφ. Regarding the classiﬁcation network, we only adopt a linear module (e.g. convolutional layer, linear layer) without any non-linear processing, such that our assumption can be satisﬁed. Images X= {xk i}are ﬁrst fed into the feature extractor Qθ to obtain the latent features, and then the latent features are resampled [ 18] through variational encoding network Fω, ﬁnally the classiﬁcation network Tφ outputs the corresponding prediction {ˆyk i}. A cross-entropy loss together with rank and distribution regularization to penalize the difference between {ˆyk i}and the groundtruth label {yk i}, the distribution difference between latent features and the Gaussian prior as well as the rank of the latent features. In summary, our model can be trained by minimizing the following objective as Lobj = ∑ i,k Lc(ˆyk i,yk i) +λ1Lrank + λ2KL(q(Z|X)||N∼ (0,1)), (4) where Lc(ˆyk i,yk i) denotes the cross-entropy loss with softmax operation, Lrank is the rank loss deﬁned in Equation 1. 5Table 1: Domain generalization results on the skin lesion classiﬁcation task. We repeat experiment for 5 times for each technique and report the mean value and standard deviation. Target DeepAll MASF [7] MLDG [21] CCSA [39] LDDG (Ours) DMF 0.2492±0.0127 0.2692±0.0146 0.2673±0.0452 0.2763±0.0263 0.2793±0.0244 D7P 0.5680±0.0181 0.5678±0.0361 0.5662±0.0212 0.5735±0.0227 0.6007±0.0208 MSK 0.6674±0.0083 0.6815±0.0122 0.6891±0.0167 0.6826±0.0131 0.6967±0.0193 PH2 0.8000±0.0167 0.7833±0.0101 0.8016±0.0096 0.7500±0.0419 0.8167±0.0096 SON 0.8613±0.0296 0.9204±0.0227 0.8817±0.0198 0.9045±0.0128 0.9272±0.0117 UDA 0.6264±0.0312 0.6538±0.0196 0.6319±0.0284 0.6758±0.0138 0.6978±0.0110 Avg 0.6287 0.6460 0.6396 0.6438 0.6697 4 Experiments In this section, we evaluate our proposed method based on two different medical imaging classiﬁcation tasks: skin lesion classiﬁcation task and gray matter segmentation task of spinal cord. The detail of architectures and experimental settings can be found in supplementary materials. 4.1 Skin lesion classiﬁcation We adopt seven public skin lesion datasets, including HAM10000 [34], Dermoﬁt (DMF) [2], Derm7pt (D7P) [17], MSK [6], PH2 [27], SONIC (SON) [6], and UDA [6], which contain skin lesion images collected from different equipments. We follow the protocol in [ 39] by choosing seven-category subset from these datasets, including melanoma (mel), melanocytic nevus (nv), dermatoﬁbroma (df), basal cell carcinoma (bcc), vascular lesion (vasc), benign keratosis (bkl), and actinic keratosis (akiec). Each dataset is randomly divided into 50% training set, 20% validation set and 30% testing set, where the relative class proportions are maintained across dataset partitions. As suggested in [39], for each setting, we use one dataset from DMF, D7P, MSK, PH2, SON and UDA as target domain and the remaining datasets together with HAM10000 as source domains. We use a ResNet18 model [ 14] pretrained on ImageNet as the backbone for our proposed method as well as other baselines. Results: We compare our method with state of the art domain generalization methods, including MASF [7], MLDG [ 21], and CCSA [ 39], which have shown the capability to generalize across domains in the ﬁeld of medical imaging. We report the baseline results by tuning the hyper-parameters in a wide range. We also adopt the baseline by directly training the model with the classiﬁcation loss, which is referred as “DeepAll\". The results are shown in Table 1. As we can see, all the domain generalization based techniques can outperform the DeepAll by directly training on source domains with classiﬁcation loss. Among the domain generalization methods, MASF can achieve relatively better performance compared with MLDG and CCSA, as it is built upon meta-learning mechanism by considering both global and local based domain alignment. Compared with all baselines, our proposed algorithm can achieve better performance in a clear margin, which is reasonable as our proposed training mechanism leverage the advantage of both data augmentation and domain alignment, which is less likely to suffer from overﬁtting problem. We can also observe that in some cases, all algorithms can perform relatively well, which we conjecture that the domain gap between source and target domain is relatively small. However, the performances are not desired in some cases (e.g. when using DMF as target domain), which may be due to the large domain gap between source and target domain. This observation is also consistent with the results reported in [39]. We also experiment using the data augmentation based technique BigAug [42] by considering a wide-range of augmentation spaces but ﬁnd it cannot yield competitive performance and the results are omitted here for brevity. We conjecture the reason that the augmentation types may not be suitable for skin lesion classiﬁcation task. In practice, it is also likely that we only have the data from one single domain during training. To further analyze the effectiveness of our proposed method under this scenario, we consider HAM10000 for training and the others for testing for skin lesion classiﬁcation task. Besides directly training with classiﬁcation loss (DeepAll), we also compare with CCSA [39] and MixUp [40]. Noted that other domain generalization baselines are not applicable in this case, as they require multiple domains available to simulate domain shift. The results are shown in Table 2. In most of the cases, our proposed method can outperform the DeepAll, CCSA as well as MixUp. For CCSA, as the contrastive loss is applied on only one source domain while target domain is unseen, it is likely to suffer from overﬁtting 6Table 2: Domain generalization results with HAM10000 as source domain. DMF D7P MSK PH2 SON UDA Average DeepAll 0.3003 0.4972 0.1667 0.4945 0.5025 0.4945 0.4093 CCSA [39] 0.2762 0.5082 0.4652 0.4667 0.5275 0.5055 0.4582 MixUp [40]0.3514 0.4029 0.3000 0.4333 0.6296 0.4615 0.4298 Ours 0.2943 0.5191 0.5087 0.5500 0.6949 0.5714 0.5231 Table 3: Domain generalization results on gray matter segmentation task. (a) DeepAll source target DSC CC JI TPR ASD 2,3,4 1 0.8560 65.34 0.7520 0.8746 0.08091,3,4 2 0.7323 26.21 0.5789 0.8109 0.09921,2,4 3 0.5041 -209 0.3504 0.4926 1.86611,2,3 4 0.8775 71.92 0.7827 0.8888 0.0599 Average 0.7425 -11.4 0.6160 0.7667 0.5265 (b) Probabilistic U-Net [19] source target DSC CC JI TPR ASD 2,3,4 1 0.8387 59.94 0.7276 0.8943 0.18201,3,4 2 0.8067 51.53 0.6778 0.7555 0.05801,2,4 3 0.5113 -188 0.3550 0.5638 2.08661,2,3 4 0.8782 72.18 0.7833 0.8910 0.2183 Average 0.7587 -1.09 0.6359 0.7762 0.6362 (c) MASF [7] source target DSC CC JI TPR ASD 2,3,4 1 0.8502 64.22 0.7415 0.8903 0.22741,3,4 2 0.8115 53.04 0.6844 0.8161 0.08261,2,4 3 0.5285 -99.3 0.3665 0.5155 1.85541,2,3 4 0.8938 76.14 0.8083 0.89910.0366 Average 0.7710 23.52 0.6502 0.7803 0.5505 (d) MLDG [21] source target DSC CC JI TPR ASD 2,3,4 1 0.8585 64.57 0.7489 0.8520 0.05731,3,4 2 0.8008 49.65 0.6696 0.7696 0.07451,2,4 3 0.5269 -108 0.3668 0.5066 1.77081,2,3 4 0.8837 73.60 0.7920 0.8637 0.0451 Average 0.7675 19.96 0.6443 0.7480 0.4869 (e) CCSA [39] source target DSC CC JI TPR ASD 2,3,4 1 0.8061 50.15 0.6801 0.8703 0.16781,3,4 2 0.8009 50.04 0.6687 0.8141 0.09391,2,4 3 0.5012 -112 0.3389 0.5444 1.54801,2,3 4 0.8686 69.61 0.7684 0.8926 0.0449 Average 0.7442 14.45 0.6140 0.7804 0.4637 (f) LDDG (Ours) source target DSC CC JI TPR ASD 2,3,4 1 0.8708 69.29 0.7753 0.8978 0.04111,3,4 2 0.8364 60.58 0.7199 0.8485 0.04161,2,4 3 0.5543 -71.6 0.3889 0.5923 1.51871,2,3 4 0.8910 75.46 0.8039 0.88440.0289 Average 0.7881 33.43 0.6720 0.8058 0.4076 problem. For MixUp, the combination is conducted only in a convex manner, which may not be able to generalize well to the out-of-distribution target domain. 4.2 Spinal cord gray matter segmentation We then consider the task of gray matter segmentation of spinal cord based on magnetic resonance imaging (MRI) to evaluate our proposed method. In particular, we adopt the data from spinal cord gray matter segmentation challenge [32], which are collected from four different medical centers with different MRI systems (Philips Achieva, Siemens Trio, Siemens Skyra). The voxel size resolutions are ranging from 0.25 ×0.25 ×2.5mm to 0.5 ×0.5 ×5mm. To evaluate the generalization capability of our proposed method, we consider the data collected from one medical center as a domain, which leads to four different domain, namely \"site1\", \"site2\", \"site3\" and \"site4\", where one domain is adopted as target domain and the remaining are considered as source domains. We adopt 2D-UNet [33] as the backbone network by considering the MRI axial slice as input2. Due to the imbalance of the number of voxels belonging to spinal cord gray matter and background in the MRI image, we follow [32] to consider a two-stage strategy in a coarse-to-ﬁne manner: 1) segment the spinal cord area (where the groundtruth of spinal cord is available), 2) segment the gray matter area from the output of 1) for our proposed method as well as baselines for comparison. Results: We compare our method with state of the art domain generalization methods, including MASF [7], MLDG [ 21], CCSA [ 39], by tuning the baseline hyper-parameters in a wide range. Moreover, we also compare with the Probabilistic U-Net [19] which automatically learned a prior for medical imaging segmentation task. For quantitative evaluation, we use a number of metrics to validate the effectiveness of our method. In particular, the metrics include three overlapping metrics: Dice Similarity Coefﬁcient (DSC), Jaccard Index (JI) and Conformity Coefﬁcient (CC); One 2Noted that we have tried 3D-UNet as suggested in [32] but ﬁnd the performances are similar to 2D-UNet in cross-domain scenario. 7Figure 1: Qualitative comparisons. Each row represents a sample from a speciﬁc domain. Each column denotes the input, ground truth (gt) or different methods including DeepAll, CCSA [ 39], MLDG [21], MASF [7], Probabilistic U-Net [19] (abbreviated as PROB here), respectively. As the area of interest in the original samples is very small, all the samples are center cropped for better visualization. statistical based metrics: Sensitivity (a.k.a. True Positive Rate (TPR))3; One distance based metric: Average surface distance (ASD), which are all performed in 3D. The results are shown in Table 5. As we can observe, our method achieve the best results when using “site1\", “site2\" and “site3\" as target domain based on all metrics, and achieve the best results in “site4\" under ASD, which shows the effectiveness of our proposed method. Among all other domain generalization based methods, MASF can achieve better performance compared with CCSA and MLDG. Such results are consistent with the performance for skin lesion classiﬁcation task. We also observe that probabilistic U-Net can achieve relatively better performance compared with the “DeepAll\" baseline by directly training on source domain with classiﬁcation loss. However, it may still suffer from overﬁtting problem as the learned prior may not generalize well to “unseen\" target domain. Again, we also evaluate the method proposed in [42] by considering a wide range of data augmentation parameters but ﬁnd the results are not desired. We conjecture that the default augmentation types are not suitable for this task. Some results of adopting [42] can be found in the supplementary materials. We further show some qualitative results in Figure 1. As we can see, while the “DeepAll\" baseline as well as other domain generalization based methods fail to segment the gray matter (e.g. when using “site2\" as target domain) or over-segment a large portion of gray matter by extending the segmentation maps to the white matter (e.g. when using “site3\" as target domain), our proposed method can generally achieve better performance compared with all the methods for comparison. 4.3 Ablation Study we ﬁrst conduct experiments on skin lesion classiﬁcation task to understand the impact of different components of our proposed algorithm by considering UDA as target domain. The results are shown in Table 4, where “Rank\" and “KL\" denote our proposed rank regularization and distribution alignment through KL divergence minimization, respectively. “LR\" denotes the rank regularization with nuclear norm minimization, which is a popular way to conduct low-rank constraint. We have the following observations: 1) both rank regularization and distribution alignment through KL can beneﬁt the generalization capability for medical imaging classiﬁcation task, which is reasonable as adopting these two terms jointly can theoretically lead to a empirical risk upper bound on target domain; 2) our proposed rank regularization can outperform the low-rank regularization by directly 3We omit True Negative Rate in our case due to the imbalance of the number of voxels belonging to spinal cord gray matter and background in the MRI image. 8Rank - LR LR - ✓ ✓KL - - ✓ ✓ - ✓ accuracy0.6264 0.6319 0.6703 0.6703 0.6813 0.6978 Table 4: Ablation study on key components of our method. We choose the skin lession classiﬁcation task and use UDA as the target domain. 1 3 5 7 9 11 13 15 rank(Z) 0.62 0.64 0.66 0.68 0.7 average accuracy Figure 2: The model performance with differ- ent rank(Z). UDA from skin lesion classiﬁ- cation task is selected as the target domain. minimizing the nuclear norm, which is reasonable as the discriminative category speciﬁc information can be explored by enforcing the value of rank to be the number of category. We then evaluate the effectiveness of our proposed rank regularization by varying the rank values of latent features by considering UDA as target domain. The results are shown in Figure 2. As we can observe, the average classiﬁcation accuracy has an ascending trend ﬁrst, and then drop. In particular, the accuracy reaches its peak when rank(Z) = 7, which is also the number of category in our task. However, we also observe that the performance drops when rank(Z) gets larger, which is reasonable as increasing the value of rank(Z) may leads to noise information which can have negative impact to the task. Noted that we can still achieve better performance compared with only using low-rank regularization with nuclear norm as shown in Table 4, which is reasonable as the nuclear norm does not take category information into consideration. Finally, we are interested in the singular values of our proposed method, which can be computed through SVD. In particular, we conduct experiments on segmentation task for stage 1 and stage 2 by considering “site1\" as the target domain. We show the convergence results of singular values in Figure 3. As we can see, our proposed method can converge in 100 epochs for both stage 1 and 2 despite the fact that deep neural networks are highly nonlinear. Regarding the singular value, we ﬁnd that the magnitude for σ1 and σ2 are relatively large while other values are much smaller, which is reasonable as we aim to explore the category speciﬁc information. Last but not the least, we ﬁnd that compared with σ2, the value of σ1 is much larger, which we conjecture the reason that the area of gray matter (or spinal cord) is much smaller than others. 50 100 150 200  epoch (a) 10 0 10 1 10 2 singular value  1 2 3 4 5 6 7 8 0 50 100 150 epoch  (b) 10 2 singular value  1 2 3 4 5 6 7 8 Figure 3: Analysis of singular values, (a) singular values in spinal cord segmentation stage (stage 1), (b) singular values of in gray matter segmentation stage (stage 2). 5 Conclusion In this paper, we tackle the generalization problem in medical imaging classiﬁcation task. Our proposed method takes the advantage of both linear-dependency modeling and domain alignment. In particular, we propose to learn a representative feature space for medical imaging classiﬁcation task through variational encoding with linear-dependency regularization with a novel rank regularization term. Our theoretical analysis shows that an empirical risk upper bound on target domain can be achieved under our formulation. Experimental results on skin lesion classiﬁcation task and spinal cord gray matter segmentation task show the effectiveness of our proposed method. 9Broader Impact. Our proposed method shows reasonable potential in the application of clinically realistic environments especially under the scenarios where only limited training samples are available and the capturing vendors and environments are diverse. In the short-term, the potential beneﬁciary of the proposed research lies in that it could signiﬁcantly alleviate the domain shift problem in medical image analysis, as evidenced in this paper. In the long term, it is expected that the principled methodology could offer new insights in intelligent medical diagnostic systems. One concrete example is that the medical imaging classiﬁcation functionality can be incorporated into different types of smartphones (with different capturing sensors, resolutions, etc.) to assess risk of skin disease (e.g. skin cancer in suspicious skin lesions) such that the terminal stage of skin cancer can be avoided. However, the medical data can be protected by privacy regulation such that the protected attributes (e.g. gender, ethnicity) may not be released publicly for training purpose. In this sense, the trained model may lack of fairness, or worse, may actively discriminate against a speciﬁc group of people (e.g. ethnicity with relatively small proportion of people). In the future, the proposed methodology can be feasibly extended to improve the algorithm fairness for numerous medical image analysis tasks and meanwhile guarantee the privacy of the protected attributes. Acknowledgement. The research work was done at the Rapid-Rich Object Search (ROSE) Lab, Nanyang Technological University. This research is supported in part by the Wallenberg-NTU Presidential Postdoctoral Fellowship, the NTU-PKU Joint Research Institute, a collaboration between the Nanyang Technological University and Peking University that is sponsored by a donation from the Ng Teng Fong Charitable Foundation, the Science and Technology Foundation of Guangzhou Huangpu Development District under Grant 2017GH22 and 201902010028, and Sino-Singapore International Joint Research Institute (Project No. 206-A017023 and 206-A018001). References [1] Y . Balaji, S. Sankaranarayanan, and R. Chellappa. Metareg: Towards domain generalization using meta-regularization. In Advances in Neural Information Processing Systems , pages 998–1008, 2018. [2] L. Ballerini, R. B. Fisher, B. Aldridge, and J. Rees. A color and texture based hierarchical k-nn approach to the classiﬁcation of non-melanoma skin lesions. In Color Medical Image Analysis, pages 63–86. Springer, 2013. [3] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, page 7, 2017. [4] F. M. Carlucci, A. D’Innocente, S. Bucci, B. Caputo, and T. Tommasi. Domain generalization by solving jigsaw puzzles. arXiv preprint arXiv:1903.06864, 2019. [5] C. Chen, Q. Dou, H. Chen, J. Qin, and P.-A. Heng. Synergistic image and feature adaptation: Towards cross-modality domain adaptation for medical image segmentation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 865–872, 2019. [6] N. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti, S. W. Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler, et al. Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic). In 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018), pages 168–172. IEEE, 2018. [7] Q. Dou, D. C. de Castro, K. Kamnitsas, and B. Glocker. Domain generalization via model- agnostic learning of semantic features. In Advances in Neural Information Processing Systems, pages 6447–6458, 2019. [8] Q. Dou, C. Ouyang, C. Chen, H. Chen, B. Glocker, X. Zhuang, and P.-A. Heng. Pnp-adanet: Plug-and-play adversarial domain adaptation network with a benchmark at cross-modality cardiac segmentation. arXiv preprint arXiv:1812.07907, 2018. [9] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, pages 1126–1135, 2017. [10] Y . Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V . Lempitsky. Domain-adversarial training of neural networks.Journal of Machine Learning Research, 17(59):1–35, 2016. 10[11] M. Ghifary, D. Balduzzi, W. B. Kleijn, and M. Zhang. Scatter component analysis: A uniﬁed framework for domain adaptation and domain generalization. IEEE Transactions on Pattern Analysis & Machine Intelligence, (1):1–1, 2017. [12] M. Ghifary, W. Bastiaan Kleijn, M. Zhang, and D. Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pages 2551–2559, 2015. [13] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio. Generative adversarial nets. InAdvances in neural information processing systems, pages 2672–2680, 2014. [14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778, 2016. [15] J. Hoffman, E. Tzeng, T. Park, J.-Y . Zhu, P. Isola, K. Saenko, A. A. Efros, and T. Darrell. Cycada: Cycle-consistent adversarial domain adaptation. ICML, 2018. [16] J. Huang, A. Gretton, K. M. Borgwardt, B. Schölkopf, and A. J. Smola. Correcting sample selection bias by unlabeled data. In Advances in Neural Information Processing Systems, 2006. [17] J. Kawahara, S. Daneshvar, G. Argenziano, and G. Hamarneh. Seven-point checklist and skin lesion classiﬁcation using multitask multimodal neural nets. IEEE journal of biomedical and health informatics, 23(2):538–546, 2018. [18] D. P. Kingma and M. Welling. Auto-encoding variational bayes.arXiv preprint arXiv:1312.6114, 2013. [19] S. Kohl, B. Romera-Paredes, C. Meyer, J. De Fauw, J. R. Ledsam, K. Maier-Hein, S. A. Eslami, D. J. Rezende, and O. Ronneberger. A probabilistic u-net for segmentation of ambiguous images. In Advances in Neural Information Processing Systems, pages 6965–6975, 2018. [20] D. Li, Y . Yang, Y .-Z. Song, and T. M. Hospedales. Deeper, broader and artier domain gener- alization. In Proceedings of the IEEE International Conference on Computer Vision, pages 5542–5550, 2017. [21] D. Li, Y . Yang, Y .-Z. Song, and T. M. Hospedales. Learning to generalize: Meta-learning for domain generalization. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [22] H. Li, S. Jialin Pan, S. Wang, and A. C. Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5400–5409, 2018. [23] H. Li, R. Wan, S. Wang, and A. C. Kot. Unsupervised domain adaptation in the wild via disentangling representation learning. International Journal of Computer Vision, pages 1–17, 2020. [24] H. Li, S. Wang, R. Wan, and A. K. Chichung. Gmfad: Towards generalized visual recognition via multi-layer feature alignment and disentanglement. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. [25] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Dollár. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980–2988, 2017. [26] M. Long, Y . Cao, J. Wang, and M. Jordan. Learning transferable features with deep adaptation networks. In ICML, 2015. [27] T. Mendonça, P. M. Ferreira, J. S. Marques, A. R. Marcal, and J. Rozeira. Ph 2-a dermoscopic image database for research and benchmarking. In 2013 35th annual international conference of the IEEE engineering in medicine and biology society (EMBC) , pages 5437–5440. IEEE, 2013. [28] S. Motiian, M. Piccirilli, D. A. Adjeroh, and G. Doretto. Uniﬁed deep supervised domain adaptation and generalization. In ICCV, 2017. [29] K. Muandet, D. Balduzzi, and B. Schölkopf. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pages 10–18, 2013. [30] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. IEEE Transactions on Neural Networks, 22(2):199–210, 2011. 11[31] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345–1359, 2010. [32] F. Prados, J. Ashburner, C. Blaiotta, T. Brosch, J. Carballido-Gamio, M. J. Cardoso, B. N. Conrad, E. Datta, G. Dávid, B. De Leener, et al. Spinal cord grey matter segmentation challenge. Neuroimage, 152:312–329, 2017. [33] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234–241. Springer, 2015. [34] P. Tschandl, C. Rosendahl, and H. Kittler. The ham10000 dataset, a large collection of multi- source dermatoscopic images of common pigmented skin lesions. Scientiﬁc data, 5:180161, 2018. [35] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7167–7176, 2017. [36] Y . Wang, H. Li, and A. C. Kot. Heterogeneous domain generalization via domain mixup. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3622–3626. IEEE, 2020. [37] Z. Xu, W. Li, L. Niu, and D. Xu. Exploiting low-rank structure from latent domains for domain generalization. In ECCV. 2014. [38] P. Yang and W. Gao. Multi-view discriminant transfer learning. In IJCAI, 2013. [39] C. Yoon, G. Hamarneh, and R. Garbi. Generalizable feature learning in the presence of data bias and domain class imbalance with application to skin lesion classiﬁcation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 365–373. Springer, 2019. [40] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk mini- mization. arXiv preprint arXiv:1710.09412, 2017. [41] K. Zhang, M. Gong, and B. Schölkopf. Multi-source domain adaptation: A causal view. In AAAI, volume 1, pages 3150–3157, 2015. [42] L. Zhang, X. Wang, D. Yang, T. Sanford, S. Harmon, B. Turkbey, H. Roth, A. Myronenko, D. Xu, and Z. Xu. Generalizing deep learning for medical image segmentation to unseen domains via deep stacked transformation. IEEE Transactions on Medical Imaging, 2020. [43] Y . Zhang, S. Miao, T. Mansi, and R. Liao. Task driven generative modeling for unsupervised domain adaptation: Application to x-ray image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 599–607. Springer, 2018. [44] Z. Zhang, L. Yang, and Y . Zheng. Translating and segmenting multimodal medical volumes with cycle-and shape-consistency generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern Recognition, pages 9242–9251, 2018. [45] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223–2232, 2017. A Detail of Architectures and Experimental Settings A.1 Experimental Setting for Skin Lesion Classiﬁcation Task We use a ResNet18 model [14] pretrained on ImageNet without the FC layer as the feature extractor Qθ with the input size 224 ×224 for our proposed method as well as other baselines. For our method, the network before average pooling is used as the feature extractor. We insert a variational encoding network between the feature extractor and the ﬁnal fully connected layer which acts as the classiﬁer Tφ. The variational encoding network Fω is implemented using two separated networks with the same architecture, including a fully connected layer with the output dimension as 512, a Relu activation layer and a fully connected layer with the output dimension as 80. We then conduct 12reparameterization trick to obtain the output of variational encoding network. The classiﬁcation network Tφ is a fully connected layer with the output size as 7. For the hyperparameters, we choose λ1 = 0.001 and λ2 = 0.4 for all settings. Due to the class imbalance within and across datasets, we adopt the focal loss [25] as the classiﬁcation objective for our proposed method as well as other baseline techniques. For implementation, the alternate form proposed in [25] is adopted as it is an extension of cross-entropy loss and is also bounded and convex, which satisﬁes our assumption in the manuscript. During training, the Adam optimizer is used with learning rate as 0.0001, weight decay as 0.001 and the size of minibatch as 32. We train the models for 200 epochs and the learning rate is decreased by a factor 10 after every 80 epochs. For evaluation on testing set, we use the best performing model on the validation set. A.2 Experimental Setting for Spinal Cord Gray Matter Segmentation Task We adopt 2D-UNet [33] (without the last 1 ×1 convolutional layer) as the backbone network by considering the MRI axial slice as input. The variational encoding network Fω is implemented using two separated network with an identical architecture which includes a latent layer using 1 ×1 convolutional layer with output channel as 64, a Relu activation layer and a 1 ×1 convolution layer to predict mean and standard deviation layers of the distribution with output channel as 8 . We then conduct reparameterization trick to obtain the output of variational encoding network. We further adopt a 1×1 convolution layer as the classiﬁcation network Tφ with the output channel size as 2 for segmentation purpose. For the hyperparameters, we use λ1 = 0.001 and λ2 = 0.01. We adopt the weighted binary cross- entropy loss for classiﬁcation, where the weight of a positive sample is set to the reciprocal of the positive sample ratio in the region of interest. We use Adam algorithm with learning rate as 1e-4, weight decay as 1e-8 and the batch size as 8 for each domain for training. We train the model for 200 epochs, where the learning rate is decreased every 80 epoch with a factor of 10. For data processing, the 3D MRI data is ﬁrst sliced into 2D in axial slice view and then center cropped to 160 ×160. We further conduct random cropping which leads to the size as 144 ×144 for training. B BigAug [42] for Segmentation We present here by considering the data-augmentation based domain generalizing method BigAug [42], which stacked different types of transformations, including sharpness, blurriness, noise, bright- ness, contrast, rotation, scaling, etc., by considering 2D-UNet for spinal cord gray matter segmentation task [32]. The results are shown in Table 1 (a). As we can observed, the performances are not desired by directly adopting the default parameters for augmentation in [42], which are even worse than the “DeepAll\" baseline in terms of DSC and JI. To understand the reason why BigAug [42] with default parameter setting leads to negative transfer, we visualize in Figure 4 some examples of the transformed input and groundtruth pairs by considering both the groundtruth of spinal cord and gray matter. As we can see, by conducting the augmentation with default parameters in [42], the quality of input deteriorates and the boundary can be oversmooth, which may further lead to more discrepancy between source and target domain. We further consider to adopt the same augmentation in [42] by tuning the parameters in a wide range to report the best segmentation performance for this task. The results are shown in Table 1 (b). We observe that there exists some improvement compared with “DeepAll\" baseline, but our proposed method can still outperform [42] with parameter tuning, which is reasonable as it is difﬁcult to choose a suitable augmentation type and magnitude for different medical imaging classiﬁcation tasks. 13Table 5: Domain generalization results on gray matter segmentation task using BigAug [42]. (a) Default Parameters [42] source target DSC CC JI TPR ASD 2,3,4 1 0.7675 38.47 0.6250 0.7798 0.12861,3,4 2 0.7542 34.50 0.6061 0.9187 0.10131,2,4 3 0.5468 -76.2 0.3809 0.6381 1.90131,2,3 4 0.8706 70.18 0.7712 0.9232 0.0437 Average 0.7348 16.74 0.5958 0.8150 0.5437 (b) Tuned Parameters source target DSC CC JI TPR ASD 2,3,4 1 0.8438 62.02 0.7334 0.8600 0.16131,3,4 2 0.7703 40.17 0.6269 0.8866 0.18021,2,4 3 0.5556 -73.7 0.3905 0.6282 1.55601,2,3 4 0.8891 74.94 0.8009 0.8827 0.0362 Average 0.7647 25.86 0.6379 0.8144 0.4834 Figure 4: The samples of input and ground truth pairs generated from BigAug using the default hyper parameters. The ﬁrst row shows the input, the second row shows the groundtruth of spinal cord, and the last row shows the groundtruth gray matters. 14",
      "references": [
        "Metareg: Towards domain generalization using meta-regularization",
        "A color and texture based hierarchical k-nn approach to the classification of non-melanoma skin lesions",
        "Unsupervised pixel-level domain adaptation with generative adversarial networks",
        "Domain generalization by solving jigsaw puzzles",
        "Synergistic image and feature adaptation: Towards cross-modality domain adaptation for medical image segmentation",
        "Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)",
        "Domain generalization via model- agnostic learning of semantic features",
        "Pnp-adanet: Plug-and-play adversarial domain adaptation network with a benchmark at cross-modality cardiac segmentation",
        "Model-agnostic meta-learning for fast adaptation of deep networks",
        "Domain-adversarial training of neural networks",
        "Scatter component analysis: A uniﬁed framework for domain adaptation and domain generalization",
        "Domain generalization for object recognition with multi-task autoencoders",
        "Generative adversarial nets",
        "Deep residual learning for image recognition",
        "Cycada: Cycle-consistent adversarial domain adaptation",
        "Correcting sample selection bias by unlabeled data",
        "Seven-point checklist and skin lesion classification using multitask multimodal neural nets",
        "Auto-encoding variational bayes",
        "A probabilistic u-net for segmentation of ambiguous images",
        "Deeper, broader and artier domain generalization",
        "Learning to generalize: Meta-learning for domain generalization",
        "Domain generalization with adversarial feature learning",
        "Unsupervised domain adaptation in the wild via disentangling representation learning",
        "GMFAD: Towards generalized visual recognition via multi-layer feature alignment and disentanglement",
        "Focal loss for dense object detection",
        "Learning transferable features with deep adaptation networks",
        "Ph 2-a dermoscopic image database for research and benchmarking",
        "Unified deep supervised domain adaptation and generalization",
        "Domain generalization via invariant feature representation",
        "Domain adaptation via transfer component analysis",
        "A survey on transfer learning",
        "Spinal cord grey matter segmentation challenge",
        "U-net: Convolutional networks for biomedical image segmentation",
        "The ham10000 dataset, a large collection of multi- source dermatoscopic images of common pigmented skin lesions",
        "Adversarial discriminative domain adaptation",
        "Heterogeneous domain generalization via domain mixup",
        "Exploiting low-rank structure from latent domains for domain generalization",
        "Multi-view discriminant transfer learning",
        "Generalizable feature learning in the presence of data bias and domain class imbalance with application to skin lesion classification",
        "mixup: Beyond empirical risk minimization",
        "Multi-source domain adaptation: A causal view",
        "Generalizing deep learning for medical image segmentation to unseen domains via deep stacked transformation",
        "Task driven generative modeling for unsupervised domain adaptation: Application to x-ray image segmentation",
        "Translating and segmenting multimodal medical volumes with cycle-and shape-consistency generative adversarial network",
        "Unpaired image-to-image translation using cycle-consistent adversarial networks"
      ],
      "meta_data": {
        "arxiv_id": "2009.12829v3",
        "authors": [
          "Haoliang Li",
          "YuFei Wang",
          "Renjie Wan",
          "Shiqi Wang",
          "Tie-Qiang Li",
          "Alex C. Kot"
        ],
        "published_date": "2020-09-27T12:30:30Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes Linear-Dependency Domain Generalization (LDDG) for medical imaging classification to improve cross-domain generalization when target-domain data are unavailable. Core idea: learn a representative latent feature space by enforcing latent features across multiple source domains to exhibit linear dependency (rank(Z)=C, where C is number of classes) and regularize the latent distribution to a Gaussian prior via variational encoding (KL divergence). The framework yields an empirical risk bound on unseen but related target domains and demonstrates improved cross-domain performance on skin-lesion classification and spinal cord gray-matter segmentation compared to state-of-the-art baselines.",
        "methodology": "Three-component model: feature extractor Q_theta, variational encoder F_omega, linear classifier T_phi. Latent features Z = q(z|x) through encoder; apply rank regularization L_rank that minimizes the (C+1)th singular value of Z, effectively enforcing rank(Z)=C. Apply distribution alignment via KL(q(Z|X) || N(0,1)). Train by L_obj = sum L_c(y_hat, y) + lambda1 L_rank + lambda2 KL(...). The rank loss uses SVD: L_rank = sigma_{C+1}. Theoretical results: Theorem 1 shows latent variable of an unseen target lies near Gaussian prior; Theorem 2 provides an upper bound on target empirical risk, showing generalization. Experimental backbone: ResNet18 for skin lesions; 2D-UNet for segmentation; learning rates etc described in supplementary; focusing on cross-domain generalization rather than domain adaptation.",
        "experimental_setup": "Two tasks and datasets: 1) Skin lesion classification using HAM10000, DMF, Derm7pt, MSK, PH2, SON, UDA; seven-category subset; training: one dataset as target; rest plus HAM10000 as sources; evaluation across 5 runs; backbone: ResNet18; loss: focal loss; hyperparameters lambda1=0.001, lambda2=0.4; training 200 epochs; evaluation: best validation model. Compared with DeepAll, MASF, MLDG, CCSA; results show LDDG yields higher average accuracy across target domains (with specific improvement margins described). 2) Spinal cord gray matter segmentation: four sites; 2D-UNet backbone; two-stage coarse-to-fine segmentation; variational encoder appended; metrics: DSC, JI, CC, TPR, ASD across site splits; comparison with MASF, MLDG, CCSA, Probabilistic U-Net; BigAug results discussed; overall improvements reported.",
        "limitations": "Assumptions include linear dependency of latent space across domains, rank(Z) equals number of classes, Gaussian prior for latent distribution. The approach relies on multiple source domains with shared category structure; sensitivity to choice of rank; potential over-regularization if rank too high; may not fully guard against large domain shift; in some settings domain gap remains large; BigAug results show augmentation strategies can be task-sensitive; privacy concerns and fairness issues discussed in Broad Impact; additional computational cost for SVD per batch, though framed as latent-space rather than classifier-parameter regularization.",
        "future_research_directions": "Explore adaptive rank selection and different latent-space priors; combine LDDG with other domain generalization strategies (e.g., meta-learning, adversarial alignment); extend to additional medical imaging tasks and modalities; investigate automatic fairness and privacy-preserving adaptations; analyze scalability to larger datasets and real-time clinical deployment; refine theoretical bounds and explore alternative low-rank penalties or structured regularizations.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Single-Photon Image Classification",
      "full_text": "Published as a conference paper at ICLR 2021 SINGLE -PHOTON IMAGE CLASSIFICATION Thomas Fischbacher Google Research tfish@google.com Luciano Sbaiz Google Research sbaiz@google.com ABSTRACT Quantum Computing based Machine Learning mainly focuses on quantum comput- ing hardware that is experimentally challenging to realize due to requiring quantum gates that operate at very low temperature. We demonstrate the existence of a “quantum computing toy model” that illustrates key aspects of quantum information processing while being experimentally accessible with room temperature optics. Pondering the question of the theoretical classiﬁcation accuracy performance limit for MNIST (respectively “Fashion-MNIST”) classiﬁers, subject to the constraint that a decision has to be made after detection of the very ﬁrst photon that passed through an image-ﬁlter, we show that a machine learning system that is permitted to use quantum interference on the photon’s state can substantially outperform any machine learning system that can not. Speciﬁcally, we prove that a “classical” MNIST (respectively “Fashion-MNIST”) classiﬁer cannot achieve an accuracy of better than 22.96% (respectively 21.38% for “Fashion-MNIST”) if it must make a decision after seeing a single photon falling on one of the 28 ×28 image pixels of a detector array. We further demonstrate that a classiﬁer that is permitted to employ quantum interference by optically transforming the photon state prior to detection can achieve a classiﬁcation accuracy of at least 41.27% for MNIST (respectively 36.14% for “Fashion-MNIST”). We show in detail how to train the corresponding quantum state transformation with TensorFlow and also explain how this example can serve as a teaching tool for the measurement process in quantum mechanics. 1 I NTRODUCTION Both quantum mechanics and machine learning play a major role in modern technology, and the emerging ﬁeld of AI applications of quantum computing may well enable major breakthroughs across many scientiﬁc disciplines. Yet, as the majority of current machine learning practitioners do not have a thorough understanding of quantum mechanics, while the majority of quantum physicists only have an equally limited understanding of machine learning, it is interesting to look for “Rosetta Stone” problems where simple and widely understood machine learning ideas meet simple and widely understood quantum mechanics ideas. It is the intent of this article to present a setting in which textbook quantum mechanics sheds a new light on a textbook machine learning problem, and vice versa, conceptually somewhat along the lines of Google’s TensorFlow Playground (Smilkov et al. (2017),) which was introduced as a teaching device to illustrate key concepts from Deep Learning to a wider audience. Speciﬁcally, we want to consider the question what the maximal achievable accuracy on common one-out-of-many image classiﬁcation tasks is if one must make a decision after the detection of the very ﬁrst quantum of light (i.e. photon) that passed a ﬁlter showing an example image from the test set. In this setting, we do not have a one-to-one correspondence between example images from the training (respectively test) set and classiﬁcation problems. Instead, every example image deﬁnes a probability distribution for the (x,y) detector pixel location on which the ﬁrst photon passing an image ﬁlter lands, the per-pixel probability being the pixel’s brightness relative to the accumulated (across all pixels) image brightness. So, from every (28 ×28 pixels) example image, we can sample arbitrarily many photon-detection-event classiﬁer examples, where the features are a pair of integer pixel coordinates, and the label is the digit class. On the MNIST handwritten digit dataset (LeCun and Cortes (2010)), any machine learning sys- tem that only gets to see a single such “photon detected at coordinates (x,y)” event as its in- 1 arXiv:2008.05859v2  [cs.LG]  12 Mar 2021Published as a conference paper at ICLR 2021 put features, of the pixel that ﬂashed up are the only input features, is limited in accuracy by the maximum likelihood estimate, since we have: P(Image class C|Photon detected at (x,y)) =∑ E P(Image class C|Example E)P(Example E|Photon detected at (x,y)). On photon detection events generated each by ﬁrst randomly picking an example image, and then randomly picking a brightness-weighted pixel from that, we cannot do any better than predicting the most likely digit class given these input features – the two pixel coordinates. As performance is measured on the test set, no classiﬁer could possibly ever outperform one that is built to achieve maximal performance on the test set. This is obtained by determining, for each pixel, what the most likely class is, where examples from the test set are weighted by the fraction of total example-image brightness that comes from the pixel in question. Figure 2(b) shows the most likely image-class per pixel. (For MNIST, some pixels are dark in every test set example.) No classiﬁer can outperform one that simply looks up the pixel-coordinates at which a photon was detected in Figure 2(b) and returns the corresponding class, and this optimal classiﬁer’s accuracy is22.96% for the the MNIST dataset – substantially higher than random guessing (10%). Appendix A.2 provides a detailed (but mostly straightforward) optimality proof of this accuracy threshold. We cannot, for example, outperform it by redistributing light intensity between pixels, since any such redistribution could only destroy some of the available useful information, not magically create extra useful information. An entirely different situation arises when we allow quantum mechanics to enter the stage: For a single photon passing through a coherently illuminated image ﬁlter, with all pixels at the same optical phase on the incoming wave, we can imagine putting some precision optical device between the image ﬁlter and the detector array that redistributes not the probabilities (which correspond to light intensity when aggregating over many photons), but the amplitudes that make up the spatial part of the photon wave-function. Illuminating such a set-up with many photons would show a hologram-like interference pattern on the detector array. This transformation of the (single-)photon wave function by linear optical elements then has tuneable parameters which we can adjust to improve classiﬁer accuracy. Quantum mechanics tells us that every (lossless) linear optical device can be represented by a linear unitary transform on the photon state: The action of any complex optical device consisting of (potentially very many) components which transforms a N-component photon state (in our case, N = 282 amplitudes in the spatial part of the photon wave function) can be described by an element of the N2-dimensional unitary matrix Lie group U(N). Vice versa, Reck et al. (1994) describes a constructive algorithm by which any U(N) transformation matrix can be translated back to a network of optical beam splitters and phase shifters. 1.1 R ELATED WORK Conceptually, exploiting interference to enhance the probability of a quantum experiment producing the sought outcome is the essential idea underlying all quantum computing. The main difference between this problem and modern quantum computing is that the latter tries to perform calculations by manipulating quantum states of multiple “entangled” constituents, typically coupled two-state quantum systems called “qubits,” via “quantum gates” that are controlled by parts of the total quantum system’s quantum state. Building a many-qubit quantum computer hence requires delicate control over the interactions between constituent qubits. This usually requires eliminating thermal noise by going to millikelvin temperatures. For the problem studied here, the quantum state can be transformed with conventional optics at room temperature: the energy of a green photon is 2.5 eV , way above the typical room temperature thermal radiation energy of kT ≃25 meV . The price to pay is that it is challenging to build a device that allows multiple photons to interact in the way needed to build a many-qubit quantum computer. Nevertheless, Knill, Laﬂamme, and Milburn (Knill et al. (2001)) devised a protocol to make this feasible in principle, avoiding the need for coherency-preserving nonlinear optics (which may well be impossible to realize experimentally) by clever exploitation of ancillary photon qubits, boson statistics, and the measurement process. In all such applications, the basic idea is to employ coherent multiphoton quantum states to do computations with multiple qubits. In the problem studied here, there is only a single photon, the only relevant information that gets processed is encoded in the spatial part of its wave function (i.e. polarization is irrelevant), so the current work resembles the “optical simulation of quantum logic” proposed by Cerf et al. (1998) where a N-qubit system is represented by 2N spatial modes of a single photon. Related work studied similar “optical simulations of quantum computing” for implementing various algorithms, in particular (small) integer factorization (Clauser and Dowling (1996); Summhammer (1997)), but to the best of the present authors’ knowledge did not consider machine learning problems. 2Published as a conference paper at ICLR 2021 This work can be described as belonging to the category of machine learning methods on quantum non-scalable architectures. Alternatively, one can regard it as a quantum analogue of recent work that demonstrated digital circuit free MNIST digit classiﬁcation via classical nonlinear optics, for instance via saturable absorbers (Khoram et al. (2019).) Apart from providing an accessible and commonly understandable toy problem for both quantum and ML research communities, this simple- quantum/simple-ML corner also may be of interest for teaching the physics of the measurement process (“the collapse of the wave function”) in a more accessible setting. Whereas explanations of the measurement process are forced to remain vague where they try to model “the quantum states of the observer” (typically unfathomably many states that one would never hope to be able to model in terms of actual numbers), using machine learning as a sort-of cartoon substitute for high level mental processes actually allows us to come up with fully concrete toy models of the measurement process on low-dimensional (such as: D< 1000) Hilbert spaces that nevertheless capture many of the essential aspects – to the extent that “ML classiﬁes the measurement as showing the image of a shoe” can be regarded as a crude approximation to “observer sees a shoe”. Looking closer at the relation between the present article and Khoram et al. (2019), both articles study the general feasibility of realizing Machine Learning classiﬁers in the form of an analog optical computer at the theoretical level, using numerical optimization to produce a blueprint of a device that can perform inference for a speciﬁc problem. In both articles, the primary problem under study is MNIST handwritten digit classiﬁcation, the input is encoded as spatial dependency of a (monochromatic) laser beam’s light intensity, and classiﬁcation happens by using interference to funnel optical energy onto a detector array. In both cases, stochastic gradient descent is used to shape how this funneling of optical energy happens. Indeed, even the loss function used for training (cross entropy) is essentially equivalent. The key differences are that Khoram et al. (2019) only considers the many-photon limit of classical wave optics, which allows one the luxury of using non-linear optical components, speciﬁcally saturable absorbers, to implement non-linearities. This has no analog for the single photon case. Also, having many photons available allows identifying the target class that receives most laser light and calling this the prediction of the model. This is clearly not possible when a decision has to be made after seeing only a single photon. If one sent many photons through an interference device as described in this article and picked the target class with the highest photon count, one would observe classiﬁcation accuracies of about 90% rather than the claimed about-40% for a single photon. This is considerably higher than the accuracies of about 80% presented inKhoram et al. (2019) as the focus of that article is on manufacturability, running gradient backpropagation directly on a Finite Difference Frequency Domain PDE simulation of Maxwell’s equations and taking materials engineering constraints into account, whereas our work focuses on upper and lower bounds for achievable accuracy, exploiting the one-to-one equivalence between linear optical devices and unitary transforms. Our work directly trains the parameters of the unitary transform, which only afterwards get mapped to a blueprint for an experimental device realization. Speculatively, if a device were built experimentally that was designed by the methods inKhoram et al. (2019), subject to the extra constraint that no non-linear elements can be used, and then deployed in a low-light-intensity single-photon setting, using a suitable detector such as a SPAD array, it may manage to realize better-than-classically-achievable classiﬁer performance, for reasons explained in the current work. 1.2 T HE MEASUREMENT PROCESS How well one can one solve a mental processing task, such as identifying a handwritten digit, if one is permitted to only measure a single quantum? This question leads to a Hilbert space basis factorization that parallels the factorization needed to study the quantum mechanical measurement process. Let us consider a gedankenexperiment where our quantum system (see Feynman et al. (2010); Landau and Lifshitz (1981) for an introduction to quantum mechanics) is a single atom that has two experiment-relevant quantum states, ‘spin-up’ and ‘spin-down’, |ψAtom⟩= c0|ψAtom=↑⟩+ c1|ψAtom=↓⟩. (1) This atom undergoes a measurement by interacting, over a limited time period, with an apparatus. The measurement process may involve for instance an atom emitting a photon that is detected by a camera, and it may include a human observing the result. We describe a quantum state in the potentially enormous Hilbert space of apparatus states with the vector |ψApparatus⟩.If, in this gedankenexperiment, we actually assume that we have maximal information about the (quantum) 3Published as a conference paper at ICLR 2021 state of the measurement apparatus (which, however, in practical terms would be unfathomably complicated) at the beginning of the experiment, then the full quantum state of the initial system is the tensor product |ψSystem, initial⟩= |ψAtom, initial⟩⊗|ψApparatus, initial⟩. (2) This factorization implies that atom and apparatus states are independent before the interaction. Without interaction between the apparatus and the atom, the time-evolution of the total system factorizes. A measurement requires an interaction between the apparatus and the atom, the solution of the Schrödinger equation is equivalent to the application of a unitary operator U to the state |ψSystem, initial⟩. This has the effect of combining the state components of the atom and the apparatus and, as a consequence, the joint time evolution no longer can be factorized. The overall state is |ψSystem, ﬁnal⟩= U|ψSystem, initial⟩and can be always decomposed in the sum: |ψSystem, ﬁnal⟩= α|ψAtom=↑⟩⊗|ψApparatus, ﬁnal=↑⟩+ β|ψAtom=↓⟩⊗|ψApparatus, ﬁnal=↓⟩, (3) where the apparatus states |ψApparatus, ﬁnal=↑⟩and |ψApparatus, ﬁnal=↓⟩represent the state of the apparatus after the measurement for the two basis states of the atom. Therefore, the apparatus is in a different state for the two cases, which leads to the apparent “collapse” of the wave function. The apparatus in the state |ψApparatus, ﬁnal=↑⟩perceives the “collapse” because the atom seems to have taken the state |ψAtom=↑⟩.The state of the apparatus includes also the representation of the thought process of a possible human observer, for instance asking herself at what instant the atom took a well determined state. This thought process disregards the superposed state |ψApparatus, ﬁnal=↓⟩which represents the alternative reality, where the apparatus observed a different outcome. Considering that a mental process could be seen as a measurement on the environment, one would naturally be inclined to think that high level mental concepts never would naturally lend themselves to a description in terms of some Hilbert space basis that has tensor product structure|ψgeneral concept⟩⊗ |ψdetails⟩.Machine learning is now making the question to what extent this may nevertheless work quantitatively testable for some simple cases, if we consider it as providing reasonably good (for this purpose) models for mental concepts. Let us consider the spatial part of a single photon’s quantum state as it traveled through a mask that has the shape of a complicated object. For instance, let’s assume that the mask is obtained from a random sample of the “Fashion-MNIST” dataset, Xiao et al. (2017), where each sample represents an object such as a shirt, a trouser, etc. One would generally expect that any sort of transformation that connects a highly regular and mathematically simple description of such a quantum system, such as in terms of per-picture-cell (“pixel”) amplitudes, with a description in human-interpretable terms, such as “the overall intensity pattern resembles a shirt,” would unavoidably involve very complicated entanglement, and one should not even remotely hope to be able to even only approximately express such photon states in terms of some factorization |ψphoton⟩≈ ∑ shape classC∈{shirt,trouser,...} ∑ style S cCS |ψshape class C⟩⊗|ψstyle S⟩, (4) since one would not expect the existence of a basis of orthonormal quantum states that can be (approxi- mately) labeled |ψshirt⟩, |ψshoe⟩, etc. Using machine learning, we can quantitatively demonstrate that, at least for some simple examples, precisely such a factorization does indeed work remarkably well, at least if we content ourselves with the concept of a “shirt shape” being that of a one-out-of-many machine learning classiﬁer, so not quite that of a human. In any case, it is reassuring to see that even near-future few-qbits quantum computers might be able to model high level concepts rather well. 2 S INGLE -QUANTUM OBJECT CLASSIFICATION Our gedankenexperiment starts with a single photon passing from faraway through a programmable LCD screen, which we here consider to consist of N ×N pixels and show an image, where for both the MNIST handwritten digit dataset of LeCun and Cortes (2010) and the “Fashion-MNIST” dataset of Xiao et al. (2017), we have N = 28. The size of the screen shall be sufﬁciently small for the photon’s quantum state to be at the same phase as it reaches each individual pixel. This does not mean that the screen has to be small in comparison to the wavelength. Rather, the light source must provide highly collimated illumination. 4Published as a conference paper at ICLR 2021 The relevant spatial part of the photon’s quantum state is described by an element of a N ×N- dimensional complex vector space. We can choose a basis for this Hilbert space such that the quantum state of a photon that managed to pass through the screen (rather than getting absorbed) has the form |ψPhoton⟩= ∑ row j, column k cjk|ψjk⟩ (5) where the |ψjk⟩basis functions correspond to a photon that went through pixel (j,k), and the coefﬁ- cients cjk are real, non-negative, proportional to the square roots of the image’s pixel-brightnesses, and are normalized according to ∑ j,k |cj,k|2 = 1. As we want to perform a rotation on this Hilbert space that maximizes alignment with a tensor product Hilbert space where one factor describes an image class, we pad this N2-dimensional Hilbert space into a larger Hilbert space with dimensionality M divisible by the number of object classes C, i.e. M = C ·S. This amounts to adding always-dark pixels (that may not form a complete row) to the image. The problem then amounts to engineering, for a problem P such as handwritten digit recognition, a single problem-speciﬁc unitary transform UP of the photon state, |ψPhoton⟩→ UP |ψPhoton⟩, such that we can meaningfully claim: UP |ψPhoton⟩= |ψPhoton∗ ⟩≈ ∑ example classc ∑ style s ccs|ψclass isc⟩⊗|ψstyle variant iss⟩ (6) Speciﬁcally, for each individual example image E, we would like to have UP |ψPhoton,E⟩≈| ψC(E)⟩⊗ ∑ style s cs|ψstyle variant iss(E)⟩, (7) where C(E) is the ground truth label of the example in a supervised learning setting. Using the method described in Reck et al. (1994), this trained matrix then can be translated to an optical network blueprint. The transformed quantum state at the output side of the network of beam splitters and phase shifters then gets measured by a detector array that can discriminate M = C·S quantum states which are labeled |ψdigit is a 0⟩⊗|ψstyle variant 1⟩, |ψdigit is a 0⟩⊗|ψstyle variant 2⟩, . . . , |ψdigit is a 3⟩⊗| ψstyle variant 57⟩, . . . ,|ψdigit is a 9⟩⊗| ψstyle variantSmax ⟩. If we detect the photon in any of the |ψdigit is a 7⟩⊗... cells, the classiﬁer output is a “7”, and likewise for the other digits. From a machine learning perspective, the trainable parameters hence are the complex entries of the matrix UP , which according to quantum mechanics have to satisfy an unitarity constraint,UP U† P = I, and this search space automatically covers all experimentally realizable linear optical devices. For MNIST, where examples have 28 ×28 pixels, the most obvious choice is padding to a M = 790-dimensional input vector. While one could implement the unitarity constraint in terms of a (regularizer) loss-function contribution that measures the degree of violation of unitarity, it here makes more sense to instead use a parametrization of UP that automatically guarantees unitarity, using Lie group theory. If WP is a 790 ×790 matrix of trainable (real) weights, then the hermitean matrix HP = −i(WP −WT P ) + (WP + WT P ) parametrizes the Lie algebra u(790), and UP = exp(iHP ) covers all of the (compact) unitary group U(790). This approach slightly over-parametrizes the problem, since, in the tensor-product basis that we are transforming to, we can freely re-deﬁne the basis on each of the ten 790/10 = 79-dimensional style subspaces. This means that 10% of the parameters are redundant. Overall, with all the trainable weights being provided by the matrix WP , and the brightness of the pixel at coordinates (y,x) for example Ebeing bE;yx, we have this formula for the probability of a photon travelling through an optical device that was designed by training weights and landing on a detector cell that predicts class c: p(c|E) = ∑ s ⏐⏐⏐⏐⏐⏐ ∑ j,k,y,x expm ( WP −WT P + i(WP + WT P ) ) kj √ bE;yx∑ ˜y,˜x bE;˜y˜x δN·y+x,jδj,c·S+s ⏐⏐⏐⏐⏐⏐ 2 . (8) Here, yand xare image row- and column-indices (for MNIST, running from 0 to 27), j,k are matrix row- and column-indices (in our example, running from 0 to 789, inclusive) for the exponentiated unitary matrix UP = expm(···), sis a style-index (here, running from 0 to S−1 = 78), the term 5Published as a conference paper at ICLR 2021 c = 0  c = 1 (a) c = 0  c = 1 (b) Figure 1: (a) The two shapes of the toy example. The four gray pixels correspond to a photon arrival probability of 1/4, i.e. a probability amplitude of 1/2. (b) The per-pixel photon arrival probability after the orthogonal transformation is applied. The dark gray pixels correspond to a probability of 1/8 and the light gray pixels to 1/2. under the square root is the relative contribution of the (y,x)-pixel to the total brightness of example image E, and the δ-factors are used for translating a pair of row,column image-indices to a linear pixel index, respectively an index on the UP -rotated quantum state vector to a pair of (class, style)- indices. Technically speaking, from the viewpoint of mapping an optical amplitude that describes light intensity passing through the image-ﬁlter to the quantum amplitude of a particular (class, style)- combination, this is simply a linear model (since quantum mechanics is linear), whose linear weights are however speciﬁed in a slightly unusual way, underneath a complex matrix exponential (since quantum mechanics is unitary, i.e. probability-preserving). The probability to predict a given class c is then obtained by summing over the probabilities associated with the given class (but different style-index). Model accuracy has to be evaluated with caution: as we need to make a prediction after detection of a single photon, accuracy is the quantum probability of the correct label, averaged over all examples. Naturally, we can not determine which of the Coutput classes would receive the most photons (= has highest probability) if all we have is a single photon detection event. This accuracy, about40% for the problems considered here, differs substantially from the accuracy that would be obtainable by looking at many photons coming from the same example image, which here typically exceeds 90%, roughly in alignment with the expected achievable performance of a linear model on MNIST. In other words, probabilities are uncalibrated, and the (non-linear “deep learning”) transformation that would be required to calibrate them cannot be expressed as a unitary operator. Let us consider a radically simpliﬁed example that illustrates why this method works. We want to discriminate between only two different shapes (with no further shape variation) on a 2 ×4 pixel screen where each pixel is either “on” or “off”, using only one photon. Speciﬁcally, let us consider the two Tetris “T” shapes represented in ﬁgure 1(a). For both shapes, the probability that the single photon arrives on one of the “on” pixels is 1/4; therefore, taking into account that for two pixels the correct shape is identiﬁed exactly and for two with 50% probability, we conclude that the baseline accuracy is 1/2 + 1/4 = 75%.Instead, we can apply a unitary transformation to reshape the probability amplitudes. Let us now consider the simple but not optimal transformation of the photon amplitude that replaces the pair of amplitudes (a,b) in each 2-pixel column with ((a−b)/ √ 2,(a+ b)/ √ 2), i.e. creates destructive interference in the top row and constructive interference in the bottom row. This gives the detection probability patterns shown in ﬁgure 1 (b). Maximum likelihood estimation here gives an accuracy of 1/2 + 3/8 = 87.5%. Obtaining the maximum achievable accuracy will here require a more complicated all-pixel amplitude transformation, obtained as follows: The quantum amplitude transformation is angle-preserving, and the angle αbetween the two amplitude quantum states q1, q2 is given by cos α = ⟨q1|q2⟩= 0.5. Hence, we can rotate these two states to lie in the plane of the ﬁrst two Cartesian coordinate axes of the Hilbert space, and at the same angle from their bisector. Identifying these coordinate axes with the correct labels, the accuracy is the cosine-squared of the angle between the transformed state and the corresponding axis, i.e. cos2(π/4 −α/2) = ( √ 3 + 2)/4 ≈93.30%. 6Published as a conference paper at ICLR 2021 Table 1: Results for the Fashion-MNIST and MNIST datasets. The “classic” accuracy and information refer to the observation of a single photon, while the “quantum” quantities are obtained after applying the quantum transformation. Dataset Entropy [bits] Accuracy Bound (classic) Information (classic) [bits] Accuracy (quantum) Information (quantum) [bits] Fashion-MNIST 3.32 21.38% 1.10 36.14% 1.85 MNIST 3.32 22.96% 1.20 41.27% 2.04 While the performance measure that we care about here is the probability for a correct classiﬁcation, one observes that model training is nevertheless more effective when one instead minimizes cross- entropy, as one would when training a conventional machine learning model. Intuitively, this seems to make sense, as a gradient computed on cross-entropy loss is expected to transport more information about the particular way in which a classiﬁcation is off than a gradient that is based only on maximizing the correct classiﬁcation probability. Overall, this task is somewhat unusual as a machine learning problem for three reasons: First, it involves complex intermediate quantities, and gradient backpropagation has to correctly handle the transitioning from real to complex derivatives where the loss function is the magnitude-squared of a complex quantity. TensorFlow is at the time of this writing the only widely used machine learning framework that can handle this aspect nicely. Appendix A.3 provides details on numerical aspects. Second, (as explained above), we cannot simply pick the class for which the predicted probability is highest as the predicted class. Rather, the probability for the single-photon measurement to produce the ground truth label sets the accuracy. Third, while most machine learning architectures roughly follow a logistic regression architecture and accumulate per-class evidence which gets mapped to a vector of per-class probabilities, we here have the probabilities as the more readily available data, so the computation of cross-entropy loss will have to infer logits from probabilities. Due to this need to compute logarithms of probabilities, it is very important that the training process does not intermediately see invalid probabilities outside the range (0 ... 1), and this is ensured by parametrizing unitary transforms as matrix exponentials of anti-hermitean matrices. TensorFlow code to both train such a model and also evaluate its performance is included in the supplementary material. 3 R ESULTS Figure 2 shows the most probable image class for each pixel, for the “Fashion-MNIST” and MNIST datasets. A classiﬁer that looks up and predicts the most likely class in this table achieves maximal accuracy among all single photon classiﬁers that do not employ quantum interference. This includes classiﬁers that have had access to the test set during training. This accuracy is reported in the third column of table 1. (We note that, as pointed out by Sun et al. (2007), the “Fashion-MNIST” dataset contains many mislabeled instances, which affects both classical and quantum results.) We can compute the amount of information provided by the photon by computing the difference between the class entropy, i.e. −log2(0.1) = 3.32,since there are 10 classes, and the entropy associated to the classiﬁcation errors, i.e. the accuracy. The mutual information for the classical classiﬁer is given in the fourth column of table 1. Training a unitary U(790) quantum transformation that gets applied after the photon passed the image ﬁlter and before it hits a bank of 790 detectors allows boosting accuracy for both the “Fashion-MNIST” and MNIST datasets, as reported in the ﬁfth column of table 1. The observation of the photon after the transformation provides a higher amount of mutual information with respect to the classical case. The values of mutual information in this case are given in the last column of table 1. Explicit matrices to perform the transformation for the two data sets have been made available with the supplementary material. The quantum transformation UP allows us to deﬁne the pixel-space projection operators: Pclass C := U−1 P (|ψC⟩⟨ψC|⊗Istyle) UP (9) 7Published as a conference paper at ICLR 2021 4 8 0 0 6 6 6 6 6 1 1 1 1 1 1 1 1 1 1 1 0 9 5 5 9 9 5 9 8 8 8 0 6 6 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 9 9 9 9 9 9 8 8 8 8 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 9 9 9 9 9 8 8 8 8 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 9 9 9 9 8 8 8 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 5 9 9 9 8 8 8 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 9 9 9 9 8 8 8 8 0 0 0 2 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 9 9 9 9 9 8 8 8 8 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 9 9 9 9 9 9 9 9 8 8 8 8 8 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 9 9 9 9 9 9 9 9 8 8 8 8 8 0 0 2 0 4 1 1 1 1 1 3 1 1 1 1 9 9 9 9 9 9 9 5 5 8 8 8 8 8 2 2 6 4 1 1 1 1 1 7 1 7 7 7 9 9 9 9 9 9 7 7 5 8 8 8 8 8 2 2 2 4 1 1 1 1 7 7 7 7 7 7 7 5 9 5 5 7 7 7 5 8 8 8 8 8 2 2 4 4 0 1 7 1 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 8 8 8 8 8 8 2 4 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 5 5 5 7 7 7 7 7 7 7 1 5 5 5 5 5 5 5 5 5 5 5 7 7 7 7 7 5 5 5 5 5 1 1 1 3 3 3 1 1 9 9 9 5 5 5 5 5 5 7 9 9 9 9 5 5 5 5 5 5 1 1 1 3 3 3 1 1 9 9 9 9 9 9 9 5 9 7 9 9 9 9 9 9 9 9 9 9 1 1 1 3 3 3 1 1 1 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 1 1 1 3 3 3 1 1 3 8 9 9 9 9 9 9 9 9 9 9 9 9 9 2 2 9 0 3 1 1 1 3 3 3 1 1 3 0 8 9 9 9 9 9 9 9 5 5 9 9 2 2 2 8 0 3 3 1 1 3 3 3 1 1 3 0 0 4 2 2 9 9 9 9 5 5 9 8 2 2 2 8 0 3 3 1 1 3 3 3 1 1 3 0 0 4 2 2 9 5 9 9 5 5 5 8 2 2 2 8 0 3 3 1 1 3 3 3 1 1 3 0 0 2 2 2 2 5 9 9 5 5 8 2 2 2 2 0 0 0 3 1 1 3 3 3 1 1 3 0 0 2 2 2 2 5 5 9 8 8 8 2 2 2 2 8 0 0 3 1 1 3 3 3 1 1 0 0 0 2 2 2 2 5 5 9 (a) 2 2 2 2 6 6 6 6 6 6 6 6 6 6 6 6 2 2 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 5 3 2 2 2 2 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 2 2 2 2 2 2 2 2 2 2 2 2 6 6 6 5 5 5 5 5 5 3 3 4 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 5 5 5 5 5 5 5 7 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 5 5 5 5 5 5 5 5 8 7 7 7 3 3 7 7 3 3 7 7 9 9 1 1 1 1 1 0 5 5 5 5 5 5 5 8 7 7 7 7 7 7 7 7 7 7 7 7 7 1 1 1 7 7 7 0 0 5 5 5 5 5 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 1 1 1 7 7 7 0 0 0 5 5 5 5 8 7 7 7 7 7 7 7 7 7 9 9 9 7 1 1 1 1 7 7 7 0 0 0 0 0 5 5 5 7 7 7 7 7 7 7 7 9 9 9 5 5 1 1 1 1 7 7 7 0 0 0 0 0 5 5 8 4 7 7 7 7 4 4 9 9 5 5 3 1 1 1 1 7 7 7 0 0 0 0 0 0 5 7 7 7 4 0 0 4 4 4 5 5 3 1 1 1 9 9 7 7 0 0 0 0 0 6 6 7 4 4 0 0 4 4 4 4 4 8 1 1 1 9 9 4 6 0 0 0 0 0 6 7 5 4 0 0 0 4 4 4 4 4 4 1 1 1 4 4 4 6 0 0 0 0 0 2 7 4 0 3 2 0 0 0 0 4 4 4 4 1 1 1 1 7 4 4 6 6 0 0 0 0 2 2 2 3 2 0 0 0 0 0 6 6 2 1 1 1 7 7 4 3 6 6 0 0 0 2 2 2 2 2 3 3 2 0 0 0 0 6 6 1 1 1 1 7 7 6 6 6 6 0 2 2 2 2 2 2 2 3 3 2 0 0 0 0 2 6 1 1 1 1 7 6 6 6 6 2 2 2 2 2 2 2 2 2 3 3 2 2 2 0 2 2 2 1 1 1 1 6 6 3 3 2 2 2 2 2 2 2 2 3 3 3 2 2 2 2 2 2 1 1 1 1 1 3 3 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 8 9 9 4 4 2 2 2 2 2 2 3 3 3 3 7 7 7 7 7 7 7 7 9 9 9 9 9 9 9 9 9 9 2 3 7 7 7 7 7 7 7 7 7 7 7 7 9 9 9 9 9 9 9 9 4 9 7 7 7 7 7 7 7 7 7 7 7 7 7 7 9 9 9 9 9 9 9 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 9 (b) Figure 2: (a) Fashion-MNIST most likely class given detection of a single photon at the corresponding pixel coordinates. Here, the classes are: 0=T-shirt/top, 1=Trouser, 2=Pullover, 3=Dress, 4=Coat, 5=Sandal, 6=Shirt, 7=Sneaker, 8=Bag, 9=Ankle Boot. (b) Most likely digit-class given detection of a single photon for MNIST. A non-quantum classiﬁer cannot outperform one that looks up its answer on the corresponding table. 0 1 2 3 4 5 6 7 8 9 (a) 0 1 2 3 4 5 6 7 8 9 True Predicted 0.000 0.012 0.024 0.036 0.048 0.060 0 1 2 3 4 5 6 7 8 9 (b) 0 1 2 3 4 5 6 7 8 9 True Predicted 0.000 0.012 0.024 0.036 0.048 0.060 0 1 2 3 4 5 6 7 8 9 (c) 0 1 2 3 4 5 6 7 8 9 True Predicted 0.000 0.012 0.024 0.036 0.048 0.060 0 1 2 3 4 5 6 7 8 9 (d) 0 1 2 3 4 5 6 7 8 9 True Predicted 0.000 0.012 0.024 0.036 0.048 0.060 Figure 3: The confusion matrices for the “Fashion-MNIST” and MNIST datasets when classic and quantum classiﬁers are used: (a) “Fashion-MNIST”/classic, (b) MNIST/classic, (c) “Fashion- MNIST”/quantum, (d) MNIST/quantum. with which we can decompose any example into contributions that are attributable to the different classes. Here, one must keep in mind that such separation is done at the level of probability amplitudes, so while we can compute intensities/probabilities from these components, which are mutually orthogonal as quantum states, summing these per-component per-pixel intensities will not reproduce the example’s per-pixel intensities. This shows most clearly when considering the decomposition of an example “Trouser” from the “Fashion-MNIST” dataset’s test set with the model we trained for this task, as shown in ﬁgure 4(a). The dark vertical line between the legs in the original image mostly comes from destructive interference between a bright line from the “Trousers” component and a matching bright line from the “Dress” component. Due to the intrinsic quantum nature of this set-up, care must be taken when interpreting confusion matrices. Naturally, we never can claim of any single-photon classiﬁer that it would ‘classify a particular image example correctly’, since re-running the experiment on the same example will not see the photon always being counted by the same detector! So, strictly speaking, for any single-photon classiﬁer realized as a device, the “confusion matrix” could be determined experimentally only in the statistical sense, leaving uncertainty in the entries that decreases with the number of passes over the test set. Confusion matrices are shown in ﬁgure 3. 8Published as a conference paper at ICLR 2021 Sample c = 0 c = 1 c = 2 c = 3 c = 4 c = 5 c = 6 c = 7 c = 8 c = 9  −π 0 π −π 0 π (a) Sample c = 0 c = 1 c = 2 c = 3 c = 4 c = 5 c = 6 c = 7 c = 8 c = 9  −π 0 π −π 0 π (b) Figure 4: Projection of probability amplitudes for some samples of the “Fashion-MNIST” (a) and the MNIST (b) datasets. The ﬁrst image shows the original sample probability and the following images show the probability amplitudes for each class. We visualize the complex amplitude by using brightness to represent magnitude and hue for phase (the colormap for the phase is shown on the right of each row.) Our factorization ansatz appears to contain a hidden constraint: we are forcing each image class to use the same number of style-states. One could imagine, for instance, that a classiﬁer might achieve even higher accuracy by treating image classes unevenly. Any such model that allows more style-space dimesions for some classes can always be embedded into a model that allows more style-space dimensions for all classes, so this question can be answered by padding to a larger Hilbert space. Numerical experiments, e.g. padding to 1000 rather than 790 dimensions, suggest that this has no appreciable impact on classiﬁcation accuracy. 4 D ISCUSSION In summary, we demonstrated that, at least for the considered datasets, the space of the single observed photon state can be factorized remarkably well by a product of the example class space and a space collecting the remaining variables, such as the style. This factorization can be obtained easily by the proposed method and is experimentally realizable with optical elements placed in front of the sensor. The supplementary material contains a blueprint for an example circuit outperforming the classical limit (at 36.05% accuracy) on 10 ×10 downsampled MNIST. An experimental implementation of the proposed system would be a demonstration of a high- temperature and low- effective-qubit quantum ML device. With respect to other experimental approaches to quantum computing, such a device would have the limitation that it is built for the speciﬁc classiﬁcation problem and cannot be reconﬁgured easily. It would be interesting to see whether an advanced quantum protocol along the lines of Knill et al. (2001) might enable the realization of more sophisticated intermediate-scale high temperature quantum machine learning in a way that mostly (like here) bypasses the need for quantum logic built from common quantum gates. 5 T ENSOR FLOW CODE TensorFlow2 code to reproduce the experiments of this work and all the ﬁgures is provided in the ancillary ﬁles together with the computed unitary transformations for MNIST and “Fashion-MNIST”. REFERENCES Daniel Smilkov, Shan Carter, D. Sculley, Fernanda B. Viégas, and Martin Wattenberg. Direct-manipulation visualization of deep networks. CoRR, abs/1708.03788, 2017. URL http://arxiv.org/abs/1708. 03788. 9Published as a conference paper at ICLR 2021 Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun. com/exdb/mnist/. E. Knill, Laﬂamme, R., and G. Milburn. A scheme for efﬁcient quantum computation with linear optics. Nature, 409:46–52, 2001. URL https://doi.org/10.1038/35051009. N. J. Cerf, C. Adami, and P. G. Kwiat. Optical simulation of quantum logic. Phys. Rev. A , 57:R1477– R1480, Mar 1998. doi: 10.1103/PhysRevA.57.R1477. URL https://link.aps.org/doi/10. 1103/PhysRevA.57.R1477. John F. Clauser and Jonathan P. Dowling. Factoring integers with young’s n-slit interferometer.Phys. Rev. A, 53:4587–4590, Jun 1996. doi: 10.1103/PhysRevA.53.4587. URL https://link.aps.org/doi/10. 1103/PhysRevA.53.4587. Johann Summhammer. Factoring and fourier transformation with a mach-zehnder interferometer. Phys. Rev. A, 56:4324–4326, Nov 1997. doi: 10.1103/PhysRevA.56.4324. URL https://link.aps.org/doi/10. 1103/PhysRevA.56.4324. Erfan Khoram, Ang Chen, Dianjing Liu, Lei Ying, Qiqi Wang, Ming Yuan, and Zongfu Yu. Nanophotonic media for artiﬁcial neural inference. Photon. Res., 7(8):823–827, Aug 2019. doi: 10.1364/PRJ.7.000823. URL http://www.osapublishing.org/prj/abstract.cfm?URI=prj-7-8-823 . Richard Phillips Feynman, Robert Benjamin Leighton, and Matthew Sands. The Feynman lectures on physics; New millennium ed. Basic Books, New York, NY , 2010. URL https://cds.cern.ch/record/ 1494701. Originally published 1963-1965. L. D. Landau and L. M. Lifshitz. Quantum Mechanics Non-Relativistic Theory, Third Edition: Volume 3 . Butterworth-Heinemann, 3 edition, January 1981. ISBN 0750635398. URL http://www.worldcat. org/isbn/0750635398. Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv, 2017. Michael Reck, Anton Zeilinger, Herbert J. Bernstein, and Philip Bertani. Experimental realization of any discrete unitary operator. Phys. Rev. Lett., 73:58–61, Jul 1994. doi: 10.1103/PhysRevLett.73.58. URL https://link.aps.org/doi/10.1103/PhysRevLett.73.58. J. Sun, F. Zhao, C. Wang, and S. Chen. Identifying and correcting mislabeled training instances. In Future Generation Communication and Networking (FGCN 2007), volume 1, pages 244–250, 2007. A A PPENDIX A.1 E XPERIMENT SCHEMATICS A        B         C          D Figure 5: Schematics of the experimental set-up. Top: Classical Baseline, Bottom: Quantum Set-Up. Figure 5 shows the schematics of an exper- imental set-up: The lens (A) stylizes the last optical component of the monochro- matic, coherent, linear-polarized (i.e. laser) light source that emits photons en- tering from the left and traveling to the right. Light intensity is controlled (e.g. by means of an absorbing ﬁlter, not shown) to be so low that photons travel through the apparatus individually. Any interference effects are hence due to self-interference of a single photon’s wave function (just as in the double slit gedankenexperiment). The laser photons then hit a coherently il- luminated N ×N screen (B) (e.g. a LCD screen, in this diagram 10 ×10) which allows light to pass through a given pixel with coordinates (y,x) with a probability that is proportional to the ink density on the example image. The photon arrives at each pixel with the same optical phase (i.e. 10Published as a conference paper at ICLR 2021 having traveled the same (fractional) number of wavelengths as seen from the laser). The diagram shows an ex- emplary raster image of a digit zero with maximal brightness (maximal ink density on the digitized ML example) on 14 pixels (with zero-based row/column coordinates (y,x) = (2,4),(2,5),(2,6),(3,2),..., (5,7)), with 75% brightness on three pixels (coordinates (3,3),(6,3),(6,5)), and 50% brightness on one pixel (coordinates (7,6)). The ‘Classical Baseline’ set-up does not use interference and would work just as well in a world where photons are ‘Particles of Light’ that cannot self-interfere (as envisioned by Newton). The dimensions of the apparatus need to be such that, when the image-ﬁlter is brightly illuminated, it casts a sharp shadow on the detector-array. For the classical case, use of a coherent source of light is not necessary. At very low light levels, photons coming from the light source (A) will keep hitting the screen (B), frequently getting absorbed by a dark pixel. At some point, a photon will (by chance) manage to hit a non-dark pixel and not get absorbed (the more likely the brighter the speciﬁc pixel) and travel on to the single-photon detector array (D) (such as: a SPAD array) and be detected as having passed through a speciﬁc pixel. Ignoring experimental imperfections that could in principle be made small such as optical losses, the only possible transform on the photon state one could perform with a passive-linear optical device at (C) would be equivalent to coupling the photon into an array of optical ﬁbers, routing each ﬁber from one pixel to some other pixel, and coupling out the photon at the other side of the device. This is equivalent to re-shufﬂing the pixels, which can always be un-done by re-shufﬂing the addresses of the cells of the detector array (D) and so does not affect classiﬁcation accuracy – the diagram hence omits such transformations that cannot affect performance. The quantum set-up (bottom) is perhaps easiest to analyze via Feynman’s path integral interpretation of Quantum Mechanics: There are different ‘histories’ which lead from the same initial point (a photon coming from the laser) to the same ﬁnal result (the photon being detected at a speciﬁc pixel, such as: ‘at coordinates (4, 5)’), and the prescription is that we have to attribute a complex quantum amplitude to each such ‘history’, summing over all amplitudes that connect the same initial and ﬁnal state to get the resultant amplitude, and obtaining the associated probability as the magnitude-square of the (complex) resultant probability. We can also consider the resultant per-pixel quantum amplitudes for a photon having traveled from the light source (A) not all the way to the detector but to some intermediate point, such as just after passing the screen (B). These are described by vectors with N2 entries, one per pixel, whose absolute-magnitude-squares sum to 1. In the example, the photon-state-vector on any ﬂat, screen-parallel surface between B and C, |ψBC ⟩, has zero entries for all but the 14+3+1=18 non-dark pixels. The entries ψBC [24],ψBC [25],... that correspond the 14 maximally-bright pixels at (2,4),(2,5),... are identical, as the photon reached each pixel at the same optical phase. Calling this amplitude cB (B for ‘bright’), the amplitudes for the three moderately-bright pixels cM , and the single dim-but-not-dark pixel’s amplitudecD, the amplitude magnitude-squares must be proportional to pixel brightness (probability for a photon to pass through the image) and sum to 1 (total probability for the photon that passed through the screen to have passed through a pixel). As the complex quantum amplitude phase matches the optical phase, these constraints ﬁx cB = u· √ 1/Z≈0.244, cM = u· √ 0.75/Z≈0.212, cD = u· √ 0.5/Z≈0.173, with Z being the overall normalization factor that makes the sum of magnitude-squares of all amplitudes 1, i.e. Z = 14·1 + 3·0.75 + 1·0.5, and ubeing some complex number of magnitude 1, the non-observable overall quantum phase factor. In the ‘Quantum’ set-up, we can employ a linear optical device, built from many beam-splitters and phase shifters (= ‘delay lines’), to adjust self-interference of the photon wave function. The diagram shows two example paths out of the total 10 ×10 different paths that the photon can take before it reaches the detector array. Quantum Mechanics tells us that ‘the single photon(!) travels along all these paths simultaneously’ – this is just Young’s double slit experiment in a slightly more complicated setting. There is a 1-to-1 correspondence between physically realizable linear optical devices and probability-preserving (generalized) rotations of the quantum state vector. The (in this downsampled example: 100 ×100) components of such a transformation form a unitary matrix. If we used a linear optical device that implemented a random such transformation, and sent many photons through the apparatus, they collectively would produce an image conceptually resembling the interference pattern on photo ﬁlm that codiﬁes a hologram. Using a basic Machine Learning procedure, stochastic gradient descent, we can train the parameters of the transform such that photons coming from an image that shows a digit ‘0’ preferentially land on the 0-th row of the detector array, photons coming from an image that shows a digit ‘4’ preferentially land on the 4-th row, etc. The supplementary material describes a speciﬁc set-up in terms of optical components that reaches >36% accuracy for MNIST downsampled to 10 ×10: If this were manufactured from ideal-quality optical components, the probability for a photon that passed through the screen (B) to land on the detector row matching its digit-class is better than 36%. A.2 C LASSICAL BASELINE ACCURACY THRESHOLD : M AXIMALITY PROOF Elementary statistical considerations allow us to obtain a stringent upper bound for the maximum accuracy that cannot be exceeded by any classiﬁer which satisﬁes these two properties: • P1: The classiﬁer must make a prediction using as its input only the index of the one detector in the detector-array that received the ﬁrst photon. It can not use any additional information about the 11Published as a conference paper at ICLR 2021 example (but may have been trained with arbitrary information about the example set, even including full knowledge of the training and test set). • P2: There is a one-to-one correspondence between image pixels and detector-cells: For each image- pixel, there is exactly one detector-cell such that when the example image Eis presented, then the probability for the ﬁrst photon to land on the detector cell kis proportional to the brightness of the associated pixel in image E. This bound is what we call the ‘classical accuracy bound’. The ‘quantum’ classiﬁer violates P2 by employing photon self-interference: The probability for the k-th detector to observe the ﬁrst photon depends on collective information which the photon ‘holographically’ transports about the input image once it passed the image-ﬁlter, rather than on a single pixel. The protocol for evaluating classiﬁer accuracy is as follows: We pick a random example from the dataset’s test set, set up the device to present this example as a problem, send light towards the ﬁlter-screen, and look at the ﬁrst photon that managed to pass the ﬁlter-screen and get counted by the detector array, then map the index of the detector that counted the photon to a predicted class. We register ‘successful prediction’ if the predicted class matches the example’s label, otherwise we register an ‘unsuccessful prediction’. The accuracy is the probability of the prediction to be successful. Somewhat unusually, this means that there is no such thing as ‘the predicted class of a given image’, as one normally would have it in a Machine Learning problem. This is due to the inherent randomness of quantum mechanics: Even repeating classiﬁcation for the same image multiple times, we will see photons land on detectors that correspond to different classiﬁcations. If we were to experimentally determine accuracy, this would then suffer from the usual problems of determining a probability via a statistical experiments: one can make the likelihood to be way off arbitrarily small, but never shrink it to zero. However, the aforementioned protocol makes it possible to directly compute the maximal achievable probability for any classiﬁer, without resorting to a statistical experiment. The gist of our argument parallels the reasoning behind the claim that we can show with simple statistics that no ML classiﬁer can possibly outperform an accuracy threshold of 29/36 for predicting from the eye total of rolling two dice whether any of the two dice showed a six. Here, the reason is that we can get maximal accuracy by looking look at all the possible realizations of any given eye total and make the best possible guess given the situation. For eye totals 2 −6 (1 + 2 + 3 + 4 + 5 = 15of 36 cases), we would predict ‘No’ and always be correct. For eye totals 11 and 12 (2 + 1additional cases), we would predict ‘Yes’ and also always be correct. For each other eye total, there are two realizations where one die shows a ‘six’, so we would want to predict ‘Yes’ for eye totals having at most four realizations, i.e. where we have at least a 50% chance of being correct, and ‘No’ otherwise. Using this approach, we would incorrectly classify three cases as ‘Yes’ (5 + 5,4 + 5,5 + 4), and incorrectly classify four cases as ‘No’ (6 + 1,1 + 6,6 + 2,2 + 6). Except for these 7/36 cases, we would make a correct prediction, so optimal accuracy is 29/36. Given the perhaps somewhat unfamiliar ‘quantum ML’ setting, and the need to rigorously justify the optimality claim, we prove it below. The only material difference to the dice-sum example is that relative weights of realizations are not determined by counting, but by looking at pixel brightnesses. In analogy to the the dice example, the key observation is that the classiﬁer’s input is a single pixel-index, and its output is an image-class. So, we can completely specify any (deterministic or not) classiﬁer’s action by tabulating, per-pixel-index, what the probability is for this classiﬁer to map the given input pixel-index kto each of the possible output classes c. The resulting matrix Kkc would, for a deterministic classiﬁer, simply be a matrix with one-hot encoded image class, one row per pixel-index. The classiﬁer’s accuracy is then given by Accuracy = P(Classiﬁcation is correct) =∑ E P(E) ∑ k ·P(γk|E) ·P(yE = C(γk)) = = ∑ E,k P(E) ·P(γk|E) ·Kk,c=yE. (10) Here, P(E) is the probability to pick example E from the test set (i.e. 1/{test set size}), P(γk|E) is the probability to detect the photon in the detector cell with index k, given the example E, and P(yE = C(γk)) is the probability that example E’s labelyE matches the classiﬁer’s output on the input “the photon was detected in cell k”. The probability for detecting a photon in cell kwhen randomly drawing an example image from the test set is P(γk) =∑ E P(E) ·P(γk|E). The probability for a fairly drawn example’s label to beyc when a photon was detected at cell kis P(yc|γk) =∑ E P(E) ·P(yE = yc) ·P(γk|E). Let us tabulate these P(yc|γk) in the {#pixels}×{#classes}matrix Rkc := P(yc|γk). We then have: Accuracy = ∑ Detector cellk ∑ Class c P(γk) ·P(yc|γk) ·Kkc = ∑ k,c P(γk)RkcKkc. (11) In words: We can compute accuracy by looking at each detector cell k and each class c, determining the probability P(γk) that, when fairly drawing examples from the test set, a photon gets detected at cell k, and 12Published as a conference paper at ICLR 2021 splitting up this probability into contributions from examples where the target class was 0, 1, 2, etc. These contributions are P(γk) ·P(yc|γk). We make a correct classiﬁcation when the classiﬁer also predicts class c given the input k. The classiﬁer’s behavior when given the inputkis speciﬁed by row kof the K-matrix, so this probability is Kkc. Here, P(γk) and Rkc are determined by the test set. Each admissible matrix Kkc that has ∑ c Kkc = 1speciﬁes a different classiﬁer, and the accuracy is a function of this matrix Konly. The question is now which admissible matrix Kmaximizes accuracy. Total classiﬁcation performance (accuracy) is a weighted sum over per-detector- cell performances (the weights being the probabilities to observe a photon in cell k when doing detection experiments on samples drawn fairly from the test set). Let K1 be a matrix that maximizes accuracy, and K2 be a matrix obtained by picking, for each cell-index k, a probability row-vector that maximizes ∑ c RkcK2,kc. We have Accuracy(K1) ≥Accuracy(K2) (since K1 is optimal), and also ∑ c Rkc(K2,kc −K1,kc) ≥0 (since K2 maximizes this value on each row k), so, taking a weighted sum with weights P(γk) ≥0, we ﬁnd ∑ k P(γk) ∑ c Rkc(K2,kc −K1,kc) ≥0, i.e. Accuracy(K2) ≥Accuracy(K1), hence Accuracy(K1) = Accuracy(K2). In words, we achieve maximal accuracy if we individually look at each “photon detected in cell k” case and make the optimal prediction there. Now, for a ﬁxed cell-index k, ∑ c RkcK2,kc is maximal if the matrix-row K2,kc has an entry 1 for the index cfor which Rkc is maximal, and is zero otherwise. To see this, let us assume K2,kc >0 for some index cfor which there is another class index dwith Rkd >Rkc. Then, incrementing K2,kd by K2,kc and subsequently setting K2,kc to zero increases ∑ c RkcK2,kc, which is a contradiction. So, optimal choices of K2,kc are zero for classes cfor which Rkc is not maximal. Also, we always attain the maximum when choosing each row-vector of Kto be one-hot and have its 1-entry in a place that maximizes Rkc, i.e. maximal achievable accuracy is obtained by a classiﬁer which, for every cell-index k, predicts the most likely digit-class subject to the constraint that a randomly drawn example from the test set had its ﬁrst photon-detection occur at detector cell k. This theoretical upper bound on classiﬁer accuracy hence is given by: Accuracy ≤ ∑ k P(γk)maxcRkc. (12) For the MNIST dataset, this is found to be 22.957% (rounded up to 22.96%), while for Fashion-“MNIST”, we get 21.375% (rounded up to 21.38%). Code that implements this calculation is available in the supplementary material. We should emphasize that the constructive procedure described here that yields a classiﬁer attaining this stringent upper bound does inspect the test set, and relevant deviations in statistical properties between training and test set would manifest in the form of lowering attainable accuracy for a classiﬁer that is trained on the training set only. A.3 B ACKPROPAGATION WITH COMPLEX INTERMEDIATE QUANTITIES As explained in the main text, the per-class probabilities deﬁned by Eq. (8), when used as input to a conventional softmax loss function, make training the real weight-parameters matrixWP in terms of which the unitary rotation is expressed a straightforward procedure. Nevertheless, this approach utilizes some capabilities which at the time of this writing are likely TensorFlow- speciﬁc. It hence may make sense to describe the training procedure in sufﬁcient detail to allow straightforward re-implementation on top of some other Machine Learning framework, or perhaps even directly without use of any such library. The loss function is deﬁned in terms of the magnitude-squared of a complex intermediate quantity, which here is the vector of complex quantum amplitudes, one entry per class/style combination. In this appendix, we henceforth consider the simpliﬁed 10 ×10 problem described in detail in appendix A.1. We can perform the calculation entirely in terms of real quantities by replacing every complex number C+ iD by a real 2 ×2 matrix block of the form C+ iD→ ( C −D D C ) . (13) This means in particular that a 100-dimensional (complex) amplitude-vector aj gets replaced by a 200 ×2- matrix Amn. If we interpret the a-index j as encoding class c and style s, i.e. j = c·S + s, the total probability for class c is p(c) = ∑ s |ac·S+s|2 = (Re ac·S+s)2 + (Imac·S+s)2, and this gets replaced by p(c) =∑ s ( A2 (c·S+s)·2,0 + A2 (c·S+s)·2,1 ) (reading off the real and imaginary part from the 1st column of the 2 ×2 block that represents aj). As the square root of the relative per-pixel intensity is real, the input-image amplitudes in this approach likewise get represented by a 200 ×2-matrix B. Speciﬁcally, if e.g. Q2,5 is the contribution of pixel (y= 2,x = 5)’s 13Published as a conference paper at ICLR 2021 brightness to the total image-brightness, this gets represented as: ( B(2·10+5)·2,0 B(2·10+5)·2+1,0 B(2·10+5)·2,1 B(2·10+5)·2+1,1 ) = ( B50,0 B51,0 B50,1 B51,1 ) = ( √ Q2,5 0 0 √ Q2,5 ) . (14) The off-diagonal part, which would correspond to the imaginary part of the amplitude, is zero here. The matrix that gets exponentiated is a real200×200 matrix, and its exponential, which also is a real200×200 matrix, gets multiplied from the right with the 200 ×2 matrix of input-image amplitudes and gives the real200 ×2-matrix A from above that contains the real and imaginary parts of class- and style-amplitudes. The real 200 ×200 matrix under the exponential only depends on 100 ×100 real parameters WP . Calling the 200 ×200-matrix M, the “2 ×2-blocking” prescription to obtain its entries from WP is: ( Mi·2 ,j·2 Mi·2 ,j·2+1 Mi·2+1,j·2 Mi·2+1,j·2+1 ) = ( (Wij −Wji) −(Wij + Wji) (Wij + Wji) ( Wij −Wji) ) . (15) Finally, we need a backpropagation-friendly prescription for computing a good approximation to the matrix exponential. The theory of compact Lie groups tells us that we can reach every ‘generalized’ (since complex) rotation matrix by exponentiating matrices where each entry is from some not too large interval. For matrices with small entries only, we can use a truncated Taylor polynomial to get a good numerical approximation of its exponential, using expm(M) ≈I+ M + 1 2M ·M + 1 6M ·M ·M + ..., (16) and we can reduce the problem of ﬁnding the matrix exponential of a matrix where this series requires many terms to give a good approximation by repeated halving and squaring, repeatedly using the property expm(M) = expm(M/2)2 = expm(M/2) ·expm(M/2). For the problem discussed here, the angle-ranges for rotations that need to be considered are limited, and this makes it feasible to in-advance pick both a number of squarings (such as: 8) and a maximal term in the Taylor expansion (such as: 10th power), and get very good results. The numerical computation implemented in the supplementary material, being based on TensorFlow, deviates from the procedure described here in two relevant ways. First, while TensorFlow’s differentiable matrix exponentiation algorithm employs repeated halving/squaring, it uses a Padé rather than Taylor approximation to compute the matrix exponential of a matrix with small entries. Second, TensorFlow can directly backpropagate through complex intermediate quantities, and handles the transition between real and complex gradients in just the way that one also obtains when expanding complex numbers to real 2 ×2 blocks as described above. It can however avoid the inefﬁciency associated with using actual real2 ×2 matrix blocks that make every real and every imaginary part show up in memory not once, but twice. 14",
      "references": [
        "Direct-manipulation visualization of deep networks.",
        "MNIST handwritten digit database.",
        "A scheme for efﬁcient quantum computation with linear optics.",
        "Optical simulation of quantum logic.",
        "Factoring integers with young’s n-slit interferometer.",
        "Factoring and fourier transformation with a mach-zehnder interferometer.",
        "Nanophotonic media for artiﬁcial neural inference.",
        "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.",
        "Experimental realization of any discrete unitary operator.",
        "Identifying and correcting mislabeled training instances."
      ],
      "meta_data": {
        "arxiv_id": "2008.05859v2",
        "authors": [
          "Thomas Fischbacher",
          "Luciano Sbaiz"
        ],
        "published_date": "2020-08-13T12:37:21Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates the best possible accuracy for one-shot image classification (MNIST and Fashion-MNIST) when a decision must be made after detecting only the first photon that passes through an image filter. It demonstrates a strict classical probabilistic bound: any classical single-photon classifier without quantum interference cannot exceed 22.96% (MNIST) or 21.38% (Fashion-MNIST) accuracy. By enabling a unitary optical transformation that leverages quantum interference on the photon’s spatial state, the authors show a substantial improvement to at least 41.27% (MNIST) and 36.14% (Fashion-MNIST), respectively. They provide a concrete, room-temperature optical toy-model of quantum information processing, show how to train the state-transform using TensorFlow, and discuss educational value for teaching quantum measurement concepts.",
        "methodology": "The model uses a single photon whose spatial wavefunction spans an N^2=28x28 pixel basis. The input image is encoded as amplitudes c_jk proportional to the square root of pixel brightness and normalized. The method pads the space to M = C*S and trains a unitary UP (UP U†P = I) realized as UP = exp(iHP) with HP built from a trainable WP, a real matrix. The unitary transformation is mapped to a linear optical network via Reck et al. 1994 decomposition. The probability p(c|E) of predicting class c for image E is computed by summing over class/style channels after applying UP to the photon state. Training minimizes cross-entropy loss computed from complex amplitudes, with backpropagation handled by representing complex numbers as 2x2 real blocks to keep probabilities valid. The approach also demonstrates a toy 2x4 pixel example illustrating interference-based gains and provides an upper-bound classical accuracy calculation (Accuracy ≤ sum_k P(γ_k) max_c R_kc).",
        "experimental_setup": "Datasets: MNIST and Fashion-MNIST (28x28 images). Input dimension padded to M=790 (10 classes × 79-style channels). The model trains a unitary UP via WP parameters and maps the learned transform to an optical circuit (beam splitters and phase shifters). Evaluation uses a single-photon detection event: the class is predicted by the detector index mapping to (class, style). Reported results include classical bound accuracies (22.96% and 21.38%) and quantum accuracies (41.27% and 36.14%), along with mutual information gains. The paper provides confusion matrices, per-pixel class likelihood maps, and supplementary TensorFlow code and optical blueprint for the 10×10 downsampled MNIST experiment. An Appendix discusses experimental schematics and the relationship between classical and quantum setups.",
        "limitations": "Limitations include reliance on a single-photon, lossless linear optical transforms which cannot be easily scaled to multi-photon quantum computation; the device is a problem-specific blueprint with limited reconfigurability. The unitarity constraint and padding introduce redundancy; the assumption of uniform phase across pixels and ideal optical components is idealized. The reported gains assume perfect optical components and do not account for real-world losses or detector inefficiencies. The probabilistic outputs are uncalibrated, and training must operate in the complex domain, which raises practical numerical challenges. Fashion-MNIST mislabeling affects both classical and quantum results; the bound is computed using the test set, which can overestimate real-world performance if training/test discrepancy exists.",
        "future_research_directions": "Extensions include scaling to larger Hilbert spaces and more qubits via multi-photon or multi-mode optical networks, exploring Knill–Laflamme–Milburn style linear-optical quantum computation without nonlinear optics, improving experimental realizations with real hardware (SPAD arrays, loss modelling), exploring uneven style-space allocation per class, applying the approach to other datasets beyond MNIST, developing more efficient training for complex-valued networks, and investigating calibration, error models, and reconfigurability to move toward reusable quantum/classical hybrid optical classifiers. The work also suggests educational and pedagogical directions for teaching quantum measurement processes using ML-inspired tooling.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Vocabulary-free Image Classification",
      "full_text": "Vocabulary-free Image Classification Alessandro Conti1 Enrico Fini1 Massimiliano Mancini1 Paolo Rota1 Yiming Wang2 Elisa Ricci1,2 1University of Trento 2Fondazione Bruno Kessler (FBK) Abstract Recent advances in large vision-language models have revolutionized the image classification paradigm. Despite showing impressive zero-shot capabilities, a pre-defined set of categories, a.k.a. the vocabulary, is assumed at test time for composing the textual prompts. However, such assumption can be impractical when the semantic context is unknown and evolving. We thus formalize a novel task, termed as V ocabulary-free Image Classification (VIC), where we aim to assign to an input image a class that resides in an unconstrained language-induced semantic space, without the prerequisite of a known vocabulary. VIC is a challenging task as the semantic space is extremely large, containing millions of concepts, with hard- to-discriminate fine-grained categories. In this work, we first empirically verify that representing this semantic space by means of an external vision-language database is the most effective way to obtain semantically relevant content for classifying the image. We then propose Category Search from External Databases (CaSED), a method that exploits a pre-trained vision-language model and an external vision- language database to address VIC in a training-free manner. CaSED first extracts a set of candidate categories from captions retrieved from the database based on their semantic similarity to the image, and then assigns to the image the best matching candidate category according to the same vision-language model. Experiments on benchmark datasets validate that CaSED outperforms other complex vision- language frameworks, while being efficient with much fewer parameters, paving the way for future research in this direction1. 1 Introduction Large-scale Vision-Language Models (VLMs) [47, 62, 34] enabled astonishing progress in computer vision by aligning multimodal semantics in a shared embedding space. This paper focuses on their use for image classification, where models such as CLIP [ 47] demonstrated strength in zero-shot transfer. While we witnessed advances in VLM-based classification in many directions, e.g. prompt learning [53, 66], scaling up to larger models and datasets [ 25, 45, 6], or by jointly considering captioning task [62, 34], they all assume a finite set of target categories, i.e. the vocabulary, to be pre-defined and static (as shown in Fig. 1a). However, this assumption is fragile, as it is often violated in practical applications (e.g. robotics, autonomous driving) where semantic categories can either differ from the development/training to the deployment/testing or evolve dynamically over time. In this work, we remove this assumption and study the new task of V ocabulary-free Image Classifica- tion (VIC). The objective of VIC is to assign an image to a class that belongs to an unconstrained language-induced semantic space at test time, without a vocabulary, i.e. without a pre-defined set of categories (as shown in Fig. 1b). The unconstrained nature of the semantic space makes VIC a challenging problem. First, the search space is extremely large, with a cardinality on the order 1Code and demo is available at https://github.com/altndrr/vic 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2306.00917v3  [cs.CV]  12 Jan 2024Blue Jay Cassowary Flamingo Kiwi Pelican Cassowary candidate estimation Cassowary Emu Peacock Rhea Kiwi Cassowary Unconstrained  language-induced   semantic space (a) VLM-based classification (b) V ocabulary-free Image Classification Figure 1: Vision-Language Model (VLM)-based classification (a) assumes a pre-defined set of target categories, i.e. the vocabulary, while our novel task (b) lifts this assumption by directly operating on the unconstrained language-induced semantic space, without a known vocabulary. fv VLM and ft VLM denote the pre-trained vision and text models of a VLM, respectively. of millions of semantic concepts 2, way larger than any existing image classification benchmark (e.g. ImageNet-21k [10]). Second, it also includes very fine-grained concepts that might be hard to discriminate by the model. Third, as the categories encountered at test time are undefined beforehand, VIC calls for classification methods that do not rely on any vocabulary-aware supervision. Recent web-scale Vision-Language Databases (VLDs) [51, 54], offer a unique opportunity to address VIC, as they cover a wide set of semantic concepts, ranging from general to highly specific ones. We empirically show that such external databases allow identifying semantic content that is more relevant to the target category than the captioning-enhanced VLMs supporting visual queries (e.g. BLIP-2 [33]). Motivated by this observation, we propose a training-free method for VIC, Category Search from External Databases (CaSED), which jointly leverages the discriminative multimodal representations derived from CLIP [47] and the information provided by recent VLDs ( e.g. PMD [54]). CaSED operates in two steps: it first coarsely estimates a set of candidate categories for the test image, then it predicts the final category via multimodal matching. Specifically, we first retrieve the captions from the database that are semantically closer to the input image, from which we extract candidate categories via text parsing and filtering. We then estimate the similarity score between the input image and each candidate category via CLIP, using both visual and textual information, predicting as output the best matching candidate. CaSED exploits the pre-trained CLIP without further training, thus being flexible and computationally efficient. We experiment on several datasets, considering both coarse- (e.g. Caltech-101 [14], UCF101 [55]) and fine-grained (e.g. FGVC-Aircraft [40], Flowers-102 [43]) classification tasks. To quantitatively assess the performance of methods addressing VIC, we also propose a set of metrics to measure how well the predicted class matches the semantics of the ground-truth label. Across all tasks and metrics, CaSED consistently outperforms all baselines, including VLMs as complex as BLIP-2 [33]. We believe that thanks to its simplicity and effectiveness, our training-free CaSED can serve as a competitive baseline for future works aiming to address the challenging VIC task. To summarize, this work provides the following contributions: • We explore the task of V ocabulary-free Image Classification where the goal is to assign a class to an image over an unconstrained set of semantic concepts, overcoming the fundamental assumption of existing VLM-based methods for image classification. We formalize this task and suggest specific evaluation metrics that can be used as a reference for future works. • We propose CaSED, the first method to address VIC thanks to the adoption of large captioning databases. Notably, CaSED is training-free, not requiring any additional parameter nor finetuning of the network’s textual and visual encoders. • Our large-scale evaluation demonstrates that CaSED consistently outperforms a more complex VLM such as BLIP-2 [33] on VIC, while requiring much fewer parameters. 2We estimate the number of concepts using BabelNet [42], which is close to 4 million for English. 22 Related work Vision-Language Models. Leveraging large-scale datasets with image-text pairs [51, 50, 54], recent works train models by mapping the two modalities into a shared representation space [ 28, 18, 11, 47, 26, 35, 15]. A notable example is CLIP [ 47] which, using modality-specific encoders and a contrastive objective to align their output representations, showed remarkable performance on zero-shot classification. Subsequent works improved CLIP by e.g. connecting the two modalities via cross-modal attention [ 35], multi-object representation alignment [ 64], learning from weak- supervision [60] or unaligned data [54]. Another line of works improved vision-language pre-training for complex vision-language tasks, such as image captioning and visual question answering (VQA) [62, 22, 34, 1]. In this context, BLIP [34] exploits web data and generated captions to supervise pre-training of a multimodal architecture, outperforming existing VLMs on both captioning and VQA. The current state-of-the-art method BLIP-2 [33] trains a module connecting the two modalities on top of a frozen visual encoder and a frozen large-language model, enabling instructed zero-shot image-to-text generation. In this work, we challenge a fundamental assumption of zero-shot classification with VLMs: the set of target classes is known a priori. We propose a new task, VIC, which sidesteps this assumption, performing classification in a language-induced open-ended space of semantic categories. We show that even BLIP-2 struggles in this scenario while external multimodal databases provide valuable priors for inferring the semantic category of an image. As a final note, VIC differs from open- vocabulary recognition (e.g. [63, 17]) since the latter assumes that the list of target classes is known and available to the model during inference. Retrieval augmented models. In natural language processing, multiple works showed the benefit of retrieving information from external databases, improving the performance of large language models [19, 32, 3]. In computer vision, such a paradigm has been used mostly to deal with the imbalanced distribution of classes. Examples are [38, 39], addressing long-tail recognition by learning to retrieve training samples [38] or image-text pairs from an external database [39]. Similarly, [58] retrieves images from a given dataset to learn fine-grained visual representations. More recently, retrieval-augmentation has been extended to various types of sources for visual question answering [23], as well as to condition the generative process in diffusion models [2], or image captioning [48]. Our work is close in spirit to [ 39], as we exploit an external database. However, [ 39] assumes a pre-defined set of classes (and data) available for training a retrieval module, something we cannot have for the extremely large semantic space of VIC. In CaSED, retrieval is leveraged to first create a set of candidate classes, and to then perform the final class prediction. Moreover, we assume the database to contain only captions, and not necessarily paired image-text data, thus being less memory-demanding. Finally, the performance of retrieval models is largely affected by the employed database. In the context of VLMs, researchers collected multiple open datasets to study vision-language pre- training. Two notable examples are LAION-5B [51], collected by filtering Common Crawl [8] via CLIP, and the Public Multimodal Datasets (PMD) [54], collecting image-text pairs from different public datasets, such as Conceptual Captions [52, 5], YFCC100M [57], Wikipedia Image Text [56], and Redcaps [12]. In our experiments, we use a subset of PMD as database and investigate how classification performance varies based on the size and quality of the database. 3 Vocabulary-free Image Classification Preliminaries. Given the image spaceX and a set of class labelsC, a classification modelf : X →C is a function mapping an imagex ∈ Xto its corresponding class c ∈ C. While C may be represented by indices that refer to semantic classes, here we focus on cases where C consists of a set of concept names that correspond to real entities. Note that C is a finite set whose elements belong to the semantic space S, i.e. C ⊂ S. In standard image classification, C is given a priori, and f is learned on a dataset of image-label pairs. Recently, the introduction of contrastive-based VLMs [47, 25], which learn a cross-modal aligned feature space, revised this paradigm by defining a function fVLM that infers the similarity between an 3C101 DTD ESAT Airc. Flwr Food Pets SUN Cars UCF Average 0 20 40 60 80 100Accuracy (%) Figure 2: Results of our preliminary study, showing the top-1 accuracy when matching semantic descriptions to ground-truth class names in ten different datasets. We compare BLIP-2 (VQA) and BLIP-2 (Captioning) with Closest Caption and Captions Centroid , i.e.the average represen- tation of the retrieved captions. We additionally highlight the Upper bound for zero-shot CLIP. Representing the large semantic space as VLDs and retrieving captions from it produces semantically more similar outputs to ground-truth labels w.r.t. querying outputs from VQA-enabled VLMs, while requiring 10 times fewer parameters compared to the latter. image and a textual description t ∈ T, i.e. fVLM : X × T →R, with T being the language space. Given this function, classification can be performed with: f(x) = arg max c∈C fVLM(x, ϕ(c)) (1) where ϕ(c) is a string concatenation operation, combining a fixed text template, i.e. a prompt, with a class name. This definition allows for zero-shot transfer, i.e. performing arbitrary classification tasks by re-defining the set C at test time, without re-training the model. However, such zero-shot transfer setup still assumes that the setC is provided. In this work, V ocabulary-free Image Classification (VIC) is formulated to surpass this assumption. Task definition. VIC aims to assign a class c to an image x without prior knowledge on C, thus operating on the semantic class space S that contains all the possible concepts. Formally, we want to produce a function f mapping an image to a semantic label in S, i.e. f : X → S. Our task definition implies that at test time, the function f has only access to an input image x and a large source of semantic concepts that approximates S. VIC is a challenging classification task by definition due to the extremely large cardinality of the semantic classes in S. As an example, ImageNet- 21k [10], one of the largest classification benchmarks, is 200 times smaller than the semantic classes in BabelNet [42]. This large search space poses a prime challenge for distinguishing fine-grained concepts across multiple domains as well as ones that naturally follow a long-tailed distribution. Semantic space representation. As the main challenge of VIC, how to represent the large semantic space plays a fundamental role in the method design. We can either model the multimodal semantic space directly with a VLM equipped with an autoregressive language decoder [ 34] or via image- text retrieval from VLDs. Consequently, we can approach VIC either via VQA-enabled VLMs by querying for the candidate class given the input image, or by retrieving and processing data from an external VLD to obtain the candidate class. To investigate the two potential strategies, we perform a preliminary experimental analysis to under- stand how well the output of a method semantically captures the image category, or in other words, to assess the alignment of class boundaries in the visual and textual representations. Specifically, we compare the semantic accuracy of querying VQA VLMs and of retrieving from VLDs w.r.t. the ground-truth class labels. We consider the output of a method as correct if its closest textual embed- ding among the target classes of the dataset corresponds to the ground-truth class of the test sample3. We exploit the text encoder of CLIP (ViT-L) [47] to obtain textual embeddings. 3Note that this metric is not the standard accuracy in image classification as we use distances in the embedding space to ground predictions from the unconstrained semantic space to the set of classes in a specific dataset. 4Regarding experimented methods, we select BLIP-2 [33] to represent VQA-enabled VLMs for its state-of-the-art performance in VQA benchmarks, while we use a subset of PMD [54] as the VLD. In particular, we compare the following methods: i) BLIP-2 VQA, which directly queries BLIP-2 for the image category; ii) BLIP-2 Captioning, which queries BLIP-2 for the image caption; iii) Closest Caption, which is the closest caption to the image, as retrieved from the database; iv) Caption Centroid, which averages the textual embeddings of the 10 most similar captions to the input image. As we use CLIP embeddings, if visual and textual representations perfectly align, the performance would be the same as zero-shot CLIP with given target classes. We thus report zero-shot CLIP to serve as the upper bound for retrieval accuracy. We experiment on a variety of test datasets for both coarse- and fine-grained classification (see details in Sec. 5), and report the results in Fig. 2. The average textual embedding of the retrieved captions (i.e. Caption Centroid) achieves the best semantic accuracy for 9 datasets out of 10, consistently surpassing methods based on BLIP-2. On average, the accuracy achieved by Caption Centroid is 60.47%, which is +17.36% higher than the one achieved by BLIP-2 Captioning (43.11%). Moreover, Captions Centroid achieves results much closer to the CLIP upper bound (67.17%) than the other approaches. Notably, such VLD-based retrieval is also computationally more efficient, faster (~ 4 second for a batch size of 64 on a single A6000 GPU), and requires fewer parameters (approximately 10 times less) than BLIP-2 (see Tab. 7 in Appendix B). The results of this preliminary study clearly suggest that representing the large semantic space with VLDs can produce (via retrieval) semantically more relevant content to the input image, in comparison to querying VQA-enabled VLMs, while being computationally efficient. Based on this conclusion, we develop an approach, Category Search from External Databases (CaSED), that searches for the semantic class from the large semantic space represented in the captions of VLDs. 4 CaSED: Category Search from External Databases Our proposed method CaSED finds the best matching category within the unconstrained semantic space by multimodal data from large VLDs. Fig. 3 provides an overview of our proposed method. We first retrieve the semantically most similar captions from a database, from which we extract a set of candidate categories by applying text parsing and filtering techniques. We further score the candidates using the multimodal aligned representation of the large pre-trained VLM, i.e. CLIP [47], to obtain the best-matching category. We describe in detail each process in the following. 4.1 Generating candidate categories We first restrict the extremely large classification space to a few most probable candidate classes. Let fVLM be the pre-trained VLM and D be the external database of image captions. Given an input x, we retrieve the set Dx ⊂ D of K closest captions to the input image via Dx = top-k d∈D fVLM(x, d) = top-k d∈D ⟨fv VLM(x), ft VLM(d)⟩, (2) where fv VLM : X → Zis the visual encoder of the VLM,ft VLM : T → Zis the textual encoder, andZ is their shared embedding space. The operation ⟨·, ·⟩ indicates the computation of the cosine similarity. Note that our approach is agnostic to the particular form of D, and that it can accommodate a flexible database size by including captions from additional resources. From the set Dx, we then extract a finite set of candidate classes Cx by performing simple text parsing and filtering techniques, e.g. stop-words removal, POS tagging. Details on the filtering procedure can be found in Appendix A. 4.2 Multimodal candidate scoring Among the small set Cx, we score each candidate by accounting for both visual and textual semantic similarities using the VLM encoders, in order to select the best-matching class for the input image. Image-to-text score. As the image is the main driver for the semantic class we aim to recognize, we use the visual information to score the candidate categories. We denote sv c as the visual score of each 5a v g Check out this ﬁerce fowl -  the cassowary means  business! Check out this prehistoric-  looking bird! This colorful cassowary is  a true hidden gem of the  animal kingdom. Animal Gem Bird Cassowary Fowl Animal Bird Cassowary Fowl Gem Retrieval from  text database Check out this ﬁerce fowl -  the cassowary means  business! Check out this prehistoric-  looking bird! This colorful cassowary is  a true hidden gem of the  animal kingdom. … Cassowary Gem Cassowary … Cassowary Gem Bird Bird Figure 3: Overview of CaSED. Given an input image, CaSED retrieves the most relevant captions from an external database filtering them to extract candidate categories. We classify image -to- text and text -to- text , using the retrieved captions centroid as the textual counterpart of the input image. candidate category c and compute it as the similarity between the visual representation of the input image and the textual representation of the candidate name: sv c = ⟨fv VLM(x), ft VLM(c)⟩. (3) The higher value of sv c indicates a closer alignment between the target image and the candidate class. Text-to-text score. While the image-to-text score sv c is effective, there exists a well-known modality gap in the space Z, harming the performance of zero-shot models [ 36]. As suggested by Fig. 2 in Sec. 3, the semantic relevance of the retrieved captions, and their centroid in particular, is high w.r.t. the underlying ground-truth label. We are therefore motivated to exploit such attributes and introduce a unimodal text-to-text scoring to mitigate the modality gap of cross-modal scoring. Formally, we define the centroid ¯dx of the retrieved captions as: ¯dx = 1 K X d∈Dx ft VLM(d), (4) where K is the number of retrieved captions. We then define the text-based matching score st c as the similarity between the centroid and the candidate category: st c = ⟨ ¯dx, ft VLM(c)⟩. (5) A higher value of st c means a higher alignment between the caption centroid and the candidate embedding. Note that the semantic relevance of the caption centroid is an inherent property of the text encoder of VLMs (i.e. CLIP). Since CLIP is trained with an image-text alignment loss, its text encoder focuses on the visual elements of the caption, discarding parts that are either non-visual or non-relevant to the visual content. This improves the model’s robustness to noisy candidates. Final predicted candidate. To predict the final candidate, we merge the two scores, obtaining the final score sc for each candidate c as: sc = α σ(sv c) + (1 − α) σ(st c) (6) where σ(·) is the softmax operation on the two scores of each candidate class, and α is a hyperpa- rameter regulating the contribution of the two modalities. Finally we obtain the output category as f(x) = arg maxc∈Cx sc. Notably, CaSED respects the VIC task definition, performing classification without known class priors, while being training-free with the use of a pre-trained and frozen VLM. This makes the approach flexible and applicable to a variety of architectures and databases. 65 Experiments We evaluate CaSED in comparison to other VLM-based methods on the novel task V ocabulary-free Image Classification with extensive benchmark datasets covering both coarse-grained and fine-grained classification. We first describe the experimental protocol in terms of the datasets, the proposed evaluation metrics, and the baselines applicable to this task (Sec. 5.1). We then discuss the quantitative results regarding the comparison between our method and baselines (Sec. 5.2). Finally, we present a thorough ablation to justify the design choices of CaSED (Sec. 5.3). In addition, we provide a cost comparison between the baseline methods and CaSED in Appendix B. We also offer further ablation of our method in Appendix C regarding architecture and database, and showcase qualitative results of predicted categories from multiple datasets in Appendix D. 5.1 Experimental protocol Datasets. We follow existing works [53, 66] and use ten datasets that feature both coarse-grained and fine-grained classification in different domains: Caltech-101 (C101) [ 14], DTD [ 7], Eu- roSAT (ESAT) [21], FGVC-Aircraft (Airc.) [ 40], Flowers-102 (Flwr) [43], Food-101 (Food) [ 4], Oxford Pets (Pets), Stanford Cars (Cars) [ 29], SUN397 (SUN) [ 61], and UCF101 (UCF) [ 55]. Additionally, we used ImageNet [10] for hyperparameters tuning. Evaluation metrics. Due to the unconstrained nature of the semantic space, evaluating the effective- ness of methods for VIC is not trivial. In this paper, we propose to use two main criteria, namely semantic relevance, i.e. the similarity of the predicted class w.r.t. the ground-truth label, andimage grouping, i.e. the quality of the predicted classes for organizing images into clusters. For semantic, we consider i) Semantic Similarity, i.e. the similarity of predicted/ground-truth labels in a semantic space, and ii) Semantic IoU, i.e. the overlap of words between the prediction and the true label. More formally, given an input x with ground-truth label y and prediction ˆc = f(x), we compute the Semantic Similarityas ⟨g(ˆc), g(y)⟩, where g : T → Yis a function mapping text to an embedding space Y. Since we want to model free-form text, we use Sentence-BERT [ 49] as g. For Semantic IoU, given a predicted label c, and assuming c being a set of words, we compute the Semantic IoU as |c ∩ y|/|c ∪ y|, where y is the set of words in the ground-truth label. To assess grouping, we measure the classic Cluster Accuracyby first clustering images according to their predicted label, and then assigning each cluster to a ground-truth label with Hungarian matching. Sometimes, this mapping is resolved with a many-to-one match, where a predicted cluster is assigned to the most present ground-truth label. This evaluation draws inspiration from the protocols used for deep visual clustering [59, 24, 20]. Baselines. We consider three main groups of baselines for our comparisons. The most straightforward baselines consist of using CLIP with large vocabularies, such as WordNet [ 41] (117k names) or the English Words (234k names [16]). As an upper bound, we also consider CLIP with the perfect vocabulary, i.e. the ground-truth names of the target dataset (CLIP upper bound). Due to lack of space, we only report results for CLIP with ViT-L [13], while results with other architectures are reported in Appendix C. The second group of baselines consists of captioning methods, as captions can well describe the semantic content of images. We consider two options: captions retrieved from a database and captions generated by a pre-trained image captioning model. For the former we exploit a large collection of textual descriptions, retrieving the most fitting caption for the given image. For the latter, we exploit BLIP-2 [ 33] — a VLM with remarkable performance on a variety of tasks, including image captioning — to provide a description for the image. The last group of baselines consists of using a VQA model to directly predict the class name associated with the image. Again, we consider BLIP-2 [33], since being highly effective also in VQA. We evaluate BLIP-2 over both ViT-L and ViT-g throughout the experiments4. Implementation details. Our experiments were conducted using NVIDIA A6000 GPUs with mixed- bit precision. As database, we use a subset of PMD [ 54], containing five of its largest datasets: Conceptual Captions (CC3M) [52], Conceptual Captions 12M (CC12M) [5], Wikipedia Image Text (WIT) [56], Redcaps [12], and a subset of [57] used for PMD (YFCC100M*). Further details on the 4Following the BLIP-2 [33] demo, for captioning, we used the prompt “Question: what’s in the image? Answer:”. For VQA, we used “Question: what’s the name of the object in the image? Answer: a”. 7Method Cluster Accuracy (%)↑ C101 DTD ESAT Airc. Flwr Food Pets SUN Cars UCF Avg. CLIP WordNet 34.0 20.1 16.7 16.7 58.3 40.9 52.0 29.4 18.6 39.5 32.6 English Words 29.1 19.6 22.1 15.9 64.0 30.9 44.4 24.2 19.3 34.5 30.4 Caption Closest Caption 12.8 8.9 16.7 13.3 28.5 13.1 15.0 8.6 20.0 17.8 15.5 BLIP-2 (ViT-L) 26.5 11.7 23.3 5.4 23.6 12.4 11.6 19.5 14.8 25.7 17.4 BLIP-2 (ViT-g) 37.4 13.0 25.2 10.0 29.5 19.9 15.5 21.5 27.9 32.7 23.3 VQA BLIP-2 (ViT-L) 60.4 20.4 21.4 8.1 36.7 21.3 14.0 32.6 28.8 44.3 28.8 BLIP-2 (ViT-g) 62.2 23.8 22.0 15.9 57.8 33.4 23.4 36.4 57.2 55.4 38.7 CaSED 51.5 29.1 23.8 22.8 68.7 58.8 60.4 37.4 31.3 47.7 43.1 CLIP upper bound 87.6 52.9 47.4 31.8 78.0 89.9 88.0 65.3 76.5 72.5 69.0 Table 1: Cluster Accuracy on the ten datasets. Green is our method, gray shows the upper bound. Method Semantic Similarity (x100)↑ C101 DTD ESAT Airc. Flwr Food Pets SUN Cars UCF Avg. CLIP WordNet 48.6 32.7 24.4 18.9 55.9 49.6 53.7 44.9 28.8 44.2 40.2 English Words 39.3 31.6 19.1 18.6 43.4 38.0 44.2 36.0 19.9 34.7 32.5 Caption Closest Caption 42.1 23.9 23.4 29.2 40.0 46.9 40.2 39.8 49.2 40.3 37.5 BLIP-2 (ViT-L) 57.8 31.4 39.9 24.4 36.1 44.6 29.0 45.3 46.4 38.0 39.3 BLIP-2 (ViT-g) 63.0 33.1 36.2 24.3 45.2 51.6 31.6 48.3 61.0 44.6 43.9 VQA BLIP-2 (ViT-L) 70.5 34.9 29.7 29.1 48.8 42.0 40.0 50.6 52.4 48.6 44.7 BLIP-2 (ViT-g) 73.5 36.5 31.4 30.8 59.9 52.1 43.9 53.3 65.1 55.1 50.1 CaSED 65.7 40.0 32.0 30.3 55.5 64.5 62.5 52.5 47.4 54.1 50.4 CLIP upper bound 90.8 69.8 67.7 66.7 83.4 93.7 91.8 80.5 92.3 83.3 82.0 Table 2: Semantic Similarity on the ten datasets. Values are multiplied by x100 for readability. Green highlights our method and gray indicates the upper bound. selection are left for Appendix C. We speed up the retrieval process by embedding the database via the text encoder ft VLM and using fast indexing technique, i.e. FAISS [27]. We tuned the α hyperparameter of Eq. (6) and the number of retrieved captions K of our method on the ImageNet dataset, finding that α = 0.7 and K = 10 led to the best results. We use these values across all experiments. 5.2 Quantitative results The results of CaSED and the baselines are presented in Table 1, Table 2, and Table 3 for Cluster Accuracy ( %), Semantic Similarity, and Semantic IoU ( %), respectively. CaSED consistently outperforms all baselines in all metrics on average and in most of the datasets. Notably, CaSED surpasses BLIP-2 (VQA) over ViT-g by+4.4% in Cluster Accuracy and +1.7% on Semantic IoU, while using much fewer parameters (i.e. 102M vs 4.1B). The gap is even larger over the same visual backbone, with CaSED outperforming BLIP-2 on ViT-L (VQA) by+14.3% on Cluster Accuracy, +5.7 in Semantic Similarity, and +6.8% on Semantic IoU. These results highlight the effectiveness of CaSED, achieving the best performance both in terms of semantic relevance and image grouping. An interesting observation from the tables is that simply applying CLIP on top of a pre-defined, large vocabulary, is not effective in VIC. This is due to the challenge of classifying over a huge search space, where class boundaries are hard to model. This is confirmed by the results of CLIP with English Words having a larger search space but performing consistently worse than CLIP with WordNet across all metrics (e.g. −7.7 on Semantic Similarity, -3.2% on Semantic IoU). A final observation relates to the captioning models. Despite their ability to capture the image semantic even in challenging settings ( e.g. 39.3 Semantic Similarity of BLIP-2 ViT-L on ESAT), captions exhibit high variability across images of the same category. This causes the worst performance on Clustering and Semantic IoU across all approaches (e.g. almost −20% less than CaSED on average in terms of Cluster Accuracy), thus demonstrating that while captions can effectively describe the content of an image, they do not necessarily imply better categorization for VIC. 8Method Semantic IoU (%)↑ C101 DTD ESAT Airc. Flwr Food Pets SUN Cars UCF Avg. CLIP WordNet 15.0 3.0 1.3 0.5 31.3 7.8 14.7 9.0 4.8 3.8 9.1 English Words 8.0 2.0 0.0 1.1 16.4 2.0 17.2 8.1 2.7 1.8 5.9 Caption Closest Caption 4.5 0.8 1.3 1.9 5.9 3.1 3.0 2.3 11.4 1.0 3.5 BLIP-2 (ViT-L) 13.4 1.4 4.8 0.0 7.5 4.7 1.7 4.7 11.6 1.1 5.1 BLIP-2 (ViT-g) 16.8 1.8 4.1 0.1 13.9 7.9 2.9 5.7 24.7 1.9 8.0 VQA BLIP-2 (ViT-L) 36.1 1.8 7.0 0.1 21.5 3.7 5.7 11.5 18.9 2.5 10.9 BLIP-2 (ViT-g) 41.5 2.4 7.5 2.0 38.0 8.6 10.2 13.8 33.2 2.8 16.0 CaSED 35.4 5.1 2.3 4.8 33.1 19.4 35.1 17.2 16.2 8.4 17.7 CLIP upper bound 86.0 52.2 51.5 28.6 75.7 89.9 88.0 66.6 84.5 71.3 69.4 Table 3: Semantic IoU on the ten datasets. Green is our method, gray shows the upper bound. Candidates Scoring CA S-Sim. S-IoU Generation Vis. Lang. Generative [33]✓ 23.3 47.1 11.9 ✓ 41.7 49.3 17.0 Retrieval ✓ 42.7 50.3 17.0 ✓ ✓ 43.1 50.4 17.7 (a) Ablation on candidate generation and scoring. Database Size CA S-Sim. S-IoU CC3M 2.8M 34.2 47.9 13.1 WIT 4.8M 34.6 42.9 12.1 Redcaps 7.9M 42.0 49.5 17.2 CC12M 10.3M 44.0 51.3 18.3 YFCC100M* 29.9M 40.7 48.8 17.1 All 54.8M 43.1 50.4 17.7 (b) Ablation on the database. Table 4: Ablation studies. Metrics are averaged across the ten datasets. Green highlights our setting. Bold represents best, underline indicates second best. 5.3 Ablation studies In this section, we present the ablation study associated with different components of our approach. We first analyze the impact of our retrieval-based candidate selection and multimodal scoring. We then show the results of CaSED for different databases, and how the number of retrieved captions impacts the performance. Candidates generation. We consider two options to generate the set of candidate classes. The first uses BLIP-2 (ViT-g), asking for multiple candidate labels for the input image. The second is our caption retrieval and filtering strategy. Table 4a shows the results where, for fairness, we score both sets with the same VLM (i.e. CLIP ViT-L). Our approach consistently outperforms BLIP-2 across all metrics (e.g. 41.7 vs 23.3 for Cluster Accuracy). This confirms the preliminary results of our study in Fig. 2, with retrieved captions providing better semantic priors than directly using a powerful VLM. Multimodal scoring. The second ablation studies the effect of our proposed multimodal scoring vs its unimodal counterparts. As Table 4a shows, multimodal candidate scoring provides the best results across all metrics, with clear improvements over the visual modality alone ( i.e. +1.4% of Cluster Accuracy). Notably, scoring via language only partially fills the gap between visual and multimodal scoring (e.g. +1% on Cluster Accuracy and +1 on Semantic Similarity), confirming that caption centroids contain discriminative semantic information. Retrieval database. We analyze the impact of retrieving captions from different databases, using five public ones, i.e. CC3M, WIT, Redcaps, CC12M, and a subset of YFCC100M. The databases have different sizes (e.g. from 2.8M captions of CC3M to 29.9M captions of the YFCC100M subset), and different levels of noise. As shown in Table 4b, the results tend to improve as the size of the database increases (e.g. +8.9% on Cluster Accuracy over CC3M). However, the quality of the captions influences the performance, with CC12M and Redcaps alone achieving either comparable or slightly better results than the full database. These results suggest that while performance improves with the size of the database, the quality of the captions has a higher impact than the mere quantity. 91 2 5 10 20 Number of retrieved captions 10 20 30 40 50Metric value Figure 4: Ablation on the number of re- trieved captions. We report Cluster accuracy (%) , Semantic similarity , and Semantic IoU (%) . Number of captions. Finally, we check how performance varies w.r.t. the number of retrieved captions. As shown in Fig. 4, all metrics con- sistently improve as the number of captions in- creases from 1 to 10. After that, performance tends to saturate, with a slight decrease in terms of Semantic Similarity for K = 20. These re- sults suggest that while a minimum number of captions is needed to fully capture the semantics of the image, the possible interference of less- related (or even noisy) captions may impact the final performance. Future research may further improve the performance on VIC by focusing on how to deal with noisy retrieval results. 6 Discussion and conclusions In this work, we proposed a new task, VIC, which operates on an unconstrained semantic space, without assuming a pre-defined set of classes, a brittle assumption of VLM-based classification. We experimentally verified that multimodal databases provide good semantic priors to restrict the large search space, and developed CaSED, an efficient training-free approach that retrieves the closest captions to the input image to extract candidate categories and scores them in a multimodal fashion. On different benchmarks, CaSED consistently achieved better results than more complex VLMs. Limitations and future works. The performance of CaSED strongly depends on the choice of the retrieval database, with potential issues in retrieving concepts that are not well represented in the latter. Moreover, if the domain of application contains fine-grained concepts, a generic database might not be suitable. Without any prior information on the test labels, it is hard to predict the performance of a database a priori. On the other hand, CaSED can flexibly mitigate this issue by incrementally including new concepts in the database (even domain-specific ones) from textual corpora, without retraining. Future research may explore strategies to automatically select/extend a database based on test samples and/or pre-computing the best database to use in the absence of test label information. Additionally, as CaSED lacks control over the classes contained in the database, its predictions might reflect potential biases. Improvements in mitigating biases and data quality control would reduce this issue. Another limitation is that CaSED does not keep track of its output history. This may lead to inconsistent predictions, i.e. assigning slightly different labels to images of the same semantic concept (e.g. cassowary vs Casuarius). Equipping CaSED with a memory storing the predicted labels may address this issue. Finally, CaSED does not deal with different class granularities: e.g. an image of a cassowary can be as well predicted as a bird. Future works may disambiguate such cases by explicitly integrating the user needs within VIC models. Broader impact. CaSED addresses VIC in a scalable and training-free manner. We believe that our new problem formulation, metrics, and the effectiveness of CaSED will encourage future research in this topic, overcoming the limiting assumption of VLM-based classification and allowing the power of VLMs to benefit dynamic and unconstrained scenarios. Acknowledgements This work was supported by the MUR PNRR project FAIR - Future AI Research (PE00000013) and ICSC National Research Centre for High Performance Computing, Big Data and Quantum Computing (CN00000013), funded by the NextGenerationEU. E.R. is partially supported by the PRECRISIS, funded by the EU Internal Security Fund (ISFP-2022-TFI-AG-PROTECT-02-101100539), the EU project SPRING (No. 871245), and by the PRIN project LEGO-AI (Prot. 2020TA3K9N). The work was carried out in the Vision and Learning joint laboratory of FBK and UNITN. We also thank the Deep Learning Lab of the ProM Facility for the GPU time. 10References [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. NeurIPS, 2022. [2] Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, and Björn Ommer. Retrieval-augmented diffusion models. NeurIPS, 2022. [3] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In ICML, 2022. [4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative components with random forests. In ECCV 2014, 2014. [5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. [6] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In CVPR, 2023. [7] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, 2014. [8] Common Crawl. Common crawl, 2023. Accessed May 17, 2023. https://commoncrawl.org/. [9] Yufeng Cui, Lichen Zhao, Feng Liang, Yangguang Li, and Jing Shao. Democratizing contrastive language- image pre-training: A clip benchmark of data, model, and supervision. arXiv, 2022. [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. [11] Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In CVPR, 2021. [12] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people. arXiv, 2021. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [14] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR Workshop, 2004. [15] Enrico Fini, Pietro Astolfi, Adriana Romero-Soriano, Jakob Verbeek, and Michal Drozdzal. Improved baselines for vision-language pre-training, 2023. [16] FreeBSD. Web2 dictionary (revision 326913), 2023. Accessed May 17, 2023.https://svnweb.freebsd. org/base/head/share/dict/web2?view=markup&pathrev=326913. [17] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In ECCV, 2022. [18] Lluis Gomez, Yash Patel, Marçal Rusinol, Dimosthenis Karatzas, and CV Jawahar. Self-supervised learning of visual features through embedding images into text topic spaces. In CVPR, 2017. [19] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In ICML, 2020. [20] Kai Han, Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, Andrea Vedaldi, and Andrew Zisserman. Automat- ically discovering and learning new visual categories with ranking statistics. In ICLR, 2019. [21] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Selected Topics in Applied Earth Observations and Remote Sensing, 2019. [22] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In CVPR, 2022. [23] Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A Ross, and Alireza Fathi. Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory. In CVPR, 2023. [24] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised image classification and segmentation. In ICCV, 2019. [25] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. [26] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. [27] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. Transactions on Big Data, 2019. 11[28] Armand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from large weakly supervised data. In ECCV, 2016. [29] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV Workshops, 2013. [30] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017. [31] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision, 2020. [32] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge- intensive nlp tasks. NeurIPS, 2020. [33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv, 2023. [34] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022. [35] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. NeurIPS, 2021. [36] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. NeurIPS, 2022. [37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. [38] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-tailed recognition in an open world. In CVPR, 2019. [39] Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg, Alan Blair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented classification for long-tail visual recognition. In CVPR, 2022. [40] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv, 2013. [41] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 1995. [42] Roberto Navigli and Simone Paolo Ponzetto. Babelnet: Building a very large multilingual semantic network. In ACL, 2010. [43] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics & Image Processing, 2008. [44] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. NeurIPS, 2011. [45] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for zero-shot transfer learning. arXiv, 2021. [46] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In ECCV, 2020. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [48] Rita Ramos, Desmond Elliott, and Bruno Martins. Retrieval-augmented image captioning. In EACL, 2023. [49] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In EMNLP-IJCNLP, 2019. [50] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. [51] Christoph Schuhmann, Robert Kaczmarczyk, Aran Komatsuzaki, Aarush Katta, Richard Vencu, Romain Beaumont, Jenia Jitsev, Theo Coombes, and Clayton Mullis. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. In NeurIPS Workshop Datacentric AI, 2021. [52] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Annual Meeting of the Association for Computational Linguistics, 2018. [53] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. arXiv, 2022. [54] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In CVPR, 2022. 12[55] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv, 2012. [56] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipedia- based image text dataset for multimodal multilingual machine learning. In Research and Development in Information Retrieval, 2021. [57] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 2016. [58] Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze, Matthieu Cord, and Hervé Jégou. Grafit: Learning fine-grained image representations with coarse labels. In ICCV, 2021. [59] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. Scan: Learning to classify images without labels. In ECCV, 2020. [60] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In ICLR, 2022. [61] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large- scale scene recognition from abbey to zoo. In CVPR, 2010. [62] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv, 2022. [63] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection using captions. In CVPR, 2021. [64] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning texts with visual concepts. In ICML, 2022. [65] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017. [66] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. IJCV, 2022. 13Appendix A Candidates filtering With the closest captions retrieved from the external database given the input image (please refer to Sec. 4 of the main manuscript for further details), we post-process them to filter out a set of candidate category names. We first create a set of all words that are contained in the captions. Then we apply sequentially three different groups of operations to the set to (i) remove noisy candidates, (ii) standardize their format, and (iii) filter them. With the first group of operations, we remove all the irrelevant textual contents, such as tokens (i.e. \"<PERSON>\"), URLs, or file extensions. Note that, for the file extensions, we remove the extension but retain the file name as it might contain candidate class names. We also remove all the words that are shorter than three characters and split compound words by underscores or dashes. Finally, we remove all those terms containing symbols or numbers and meta words that are irrelevant to the classification task, such as \"image\", \"photo\", or \"thumbnail\". As shown in Table 5, when compared to having no operation for candidate filtering (first row), this set of operations removes inappropriate content and increases the accuracy of clusters by +6.4% and improves the semantic IoU by +0.3. However, we can observe a drop in semantic similarity by −1.5. This might be due to the removal of unnatural words that could still describe well the content of the image, i.e. underline- or dash-separated words, or URLs since they are longer w.r.t. natural words. The second group of operations standardize the candidate names by aligning words that refer to the same semantic class to a standard format, reducing class redundancy. For example, \"cassowary\" and \"Cassowary\" will be considered as a single class instead of two. To this end, we perform two operations—lowercase conversion and singular form conversion. With such standardizing conversions, we observe a sizeable boost in terms of performances when compared to the results obtained by applying only removal-related operations. As shown in Table 5, we achieve higher results across all three metrics, leading to a relative improvement of +7.2%, +0.7, and +1.2 in terms of cluster accuracy, semantic similarity, and semantic IoU, respectively. The last group of operations considers two forms of filtering, where the first aims to filter out entire categories of words via Part-Of-Speech (POS) tag and the second aims to filter out rare and noisy contents based on the word occurrences. We select these two operations since common dataset class names do not contain terms that carry no semantics e.g. articles and pronouns, and since [9] showed that CLIP performs better when exposed to a smaller amount of unique tokens. The POS tagging5 categorizes words into groups, such as adjectives, articles, nouns, or verbs, enabling us to filter all the terms that are not semantically meaningful for a classification task. Regarding the occurrence filtering, we first count how often a word appears in the retrieved captions and then we remove words that appear only once to make the candidate list less noisy. We can see from Table 5 that the inclusion of this final set of operations scores the best among all three metrics when compared to the results obtained when only the previous two groups of operations are applied. Number of captions vs number of selected candidates. To complement the previous analysis, in Table 6, we report the number of unique candidates extracted by the candidate filtering procedure, averaged over the ten datasets, and with an increasing number of retrieved captions, i.e., 1, 2, 5, 10, and 20. In the table, we show both the number of candidates extracted and the number of selected words. As the number of retrieved captions increases, the unique number of candidate words also increases, i.e., from 3849 with 1 caption to 28781 with 20. However, the number of selected words stabilizes around 800 as soon as we retrieve more than 1 caption. Having more captions reduces the noises in the selected words, something that might be present when relying on a single caption. B Computational cost We analyze the computational efficiency of CaSED versus BLIP-2 performing VQA and captioning, and report their respective number of parameters and inference time in Table 7. Notably, the methods 5We use the NLP library flair (https://github.com/flairNLP/flair). 14Operations CA S-Sim. S-IoU Remove Standardize Filter 27.7 48.8 15.0 ✓ 34.1 47.3 15.3 ✓ ✓ 41.3 48.0 16.5 ✓ ✓ ✓ 43.1 50.4 17.7 Table 5: Ablation on the candidate filtering operations. Metrics are averaged across the ten datasets. Num. captionsCA S-Sim. S-IoU Candidates Selected 1 30.9 43.2 12.4 3849 1047 2 35.0 46.4 14.6 6867 854 5 42.3 50.6 17.5 14548 775 10 43.1 50.4 17.7 22475 794 20 42.9 49.3 17.1 28781 802 Table 6: Extended ablation on the number retrieved captions. Green represents our selected configuration. We expand the results of Fig. 4 in the main manuscript to show the number of unique words extracted from captions (i.e., “candidates”) and the number of words selected by CaSED for classification (i.e., “selected”). Results are averaged across the ten datasets. using external databases are consistently faster than BLIP-2. For instance, CaSED achieves a speed- up of 1.5x with respect to both the largest captioning model and the largest VQA model, while also achieving better performance. Overall, the fastest method is Closest Caption, which exploits the external database to retrieve a single caption and does not consider any candidate extraction pipeline. Conversely, our method retrieves the ten most similar captions and post-processes them to extract class name, resulting in a increase in inference time of approximately 3 times. Compared with the CLIP upper bound, our method considers multiple additional steps, each adding extra inference time. First, our method retrieves candidates from the external database to then extract the class names. Second, we have to forward the class names through the text encoder for each sample, while CLIP can forward them once and cache the features for later reuse. When using an external database, note that an increase in database size implies a minimal variation in retrieval time. This is demonstrated by the computational cost required by retrieving from the large LAION-400M [51] database. As the results show, inference time is comparable between CaSED and CaSED (LAION-400M) despite the latter being approximately 8 times larger. C Additional ablation study In this section, we show the performance of our model with different backbones, and other commonly used databases for retrieval. Backbone architecture. To answer this natural question about whether the outcome of our model depends on the backbone architecture, we further extend our main results with a CLIP model with a ResNet50 architecture. We report this additional ablation in Table 8, Table 9, and Table 10 for the cluster accuracy, the semantic IoU, and the semantic similarity, respectively. We can see that the performance with the CLIP ResNet50 is lower across all the metrics compared to CLIP ViT- L/14. This is expected since ResNet50 is a a smaller architecture, thus with a reduced capacity for semantic representation learning as compared to ViT-L/14. Nevertheless, our method with ResNet50 is still competitive against BLIP-2 models while using 40x fewer parameters (note that our ViT-L implementation uses 10x fewer parameters). Retrieval database. We analyze the impact of retrieving captions from databases of different scales, expanding our main results with the four unused databases of PMD, including COCO [ 37], SBU Captions [44], Localized Narratives [ 46], and Visual Genome [ 30]. Moreover, we evaluate our method on other three databases, namely Ade20K [65], OpenImages [31], and LAION-400M [51]. These databases further extend the range of database sizes, with Ade20k containing only 0.07M captions and LAION-400M having 413M. We report the results in Table 11, ordering the rows by the database size. Differently from the tables reported in the main results, the results in Table 11 15Method Num. Params. Inference time (ms)↓ Caption Closest Caption 0.43B 1390 ± 10 BLIP-2 (ViT-L) 3.46B 5710 ± 153 BLIP-2 (ViT-g) 4.37B 6870 ± 177 VQA BLIP-2 (ViT-L) 3.46B 5670 ± 135 BLIP-2 (ViT-g) 4.37B 6650 ± 117 CaSED 0.43B 4370 ± 13 CaSED (LAION-400M) 0.43B 4350 ± 16 CLIP upper bound 0.43B 645 ± 77 Table 7: Computational cost of different methods. Green is our method, gray shows the upper bound. Inference time is reported on batches of size 64, as the average over multiple runs. Method Cluster Accuracy (%)↑ C101 DTD ESAT Airc. Flwr Food Pets SUN Cars UCF Avg. CLIP RN50 CLIP WordNet 30.3 18.3 22.5 13.2 47.8 31.4 45.2 26.0 14.2 31.2 28.0 English Words 24.8 17.5 18.5 13.4 49.5 23.1 36.6 22.2 15.5 27.1 24.8 Caption Closest Caption 9.7 7.1 13.3 8.4 21.2 6.2 8.7 6.5 12.8 14.9 10.9 CaSED 44.6 23.9 12.5 15.3 58.8 48.7 50.1 32.8 24.6 33.9 34.5 CLIP upper bound 82.1 41.5 33.5 19.6 63.1 74.6 78.9 55.6 54.9 58.4 56.2 CLIP ViT-L/14 CLIP WordNet 34.0 20.1 16.7 16.7 58.3 40.9 52.0 29.4 18.6 39.5 32.6 English Words 29.1 19.6 22.1 15.9 64.0 30.9 44.4 24.2 19.3 34.5 30.4 Caption Closest Caption 12.8 8.9 16.7 13.3 28.5 13.1 15.0 8.6 20.0 17.8 15.5 CaSED 51.5 29.1 23.8 22.8 68.7 58.8 60.4 37.4 31.3 47.7 43.1 CLIP upper bound 87.6 52.9 47.4 31.8 78.0 89.9 88.0 65.3 76.5 72.5 69.0 Caption BLIP-2 (ViT-L) 26.5 11.7 23.3 5.4 23.6 12.4 11.6 19.5 14.8 25.7 17.4 BLIP-2 (ViT-g) 37.4 13.0 25.2 10.0 29.5 19.9 15.5 21.5 27.9 32.7 23.3 VQA BLIP-2 (ViT-L) 60.4 20.4 21.4 8.1 36.7 21.3 14.0 32.6 28.8 44.3 28.8 BLIP-2 (ViT-g) 62.2 23.8 22.0 15.9 57.8 33.4 23.4 36.4 57.2 55.4 38.7 Table 8: Cluster Accuracy on the ten datasets. Green is our method, gray shows the upper bound. Bold represents best, underline indicates best considering also image captioning and VQA models. are obtained on ImageNet. We first discuss the databases belonging to the PMD superset, and then analyze the results obtained with Ade20K, OpenImages, and LAION-400M. In the table, we also show the POS tag distribution (e.g. nouns, adjectives, verbs) of each dataset. Moreover, we report a metric, the semantic similarity to the closest caption in the dataset of each image (C.C. S-Sim) to check how close a database is to the target one (i.e. ImageNet). By comparing the nine databases of PMD, we can perceive a non-uniform variation in performance across databases, with a remarkable gap between the top-5 (highlighted in blue ) and the rest. The performance of the fifth-performing (i.e. WIT) and the sixth database (i.e. SBU Captions) differs by 12.8% on cluster accuracy, while the difference between the best-performing (i.e. CC12M) and the fifth is 10.9%. This observation motivates us to use the top-5 databases from PMD instead of the full superset to be efficient. We expand the best database with the other top-5 to ensure better coverage of class names in the textual descriptions. With the additional databases, i.e. Ade20K, OpenImages, and LAION-400M, we further notice that there is a correlation between the size of a database and its performance. This was also noted in our results reported in the main manuscript. Ade20K, the smallest dataset, comprises only about 0.07M captions and obtains the lowest performance among all metrics. As the size of databases increase, the performance generally improves. However, it is important to note that dataset size alone is not the only requirement for good results. LAION-400M is of a much larger size than our split of PMD, yet the performance with LAION-400M is worse, which we attributes to the impact of much noisier retrieval from database of such scale. Instead, datasets such as CC12M and Redcaps perform better than LAION-400M despite being approximately 40x smaller. This finding suggests that the quality of the databases could be of a higher importance than its size. 16Method Semantic IoU (%)↑ C101 DTD ESAT Airc. Flwr Food Pets SUN Cars UCF Avg. CLIP RN50 CLIP WordNet 9.8 1.9 0.9 0.0 21.2 5.5 14.1 7.3 3.5 2.6 6.7 English Words 5.1 0.8 0.0 0.1 12.4 1.8 13.2 7.8 2.5 1.3 4.5 Caption Closest Caption 3.4 0.5 0.1 1.5 6.6 2.2 2.2 2.1 9.1 0.7 2.8 CaSED 31.1 2.9 0.08 2.8 29.7 15.1 27.6 15.0 13.4 5.2 14.3 CLIP upper bound 81.5 41.4 33.9 16.5 60.2 74.6 78.9 56.9 66.5 57.1 56.8 CLIP ViT-L/14 CLIP WordNet 15.0 3.0 1.3 0.5 31.3 7.8 14.7 9.0 4.8 3.8 9.1 English Words 8.0 2.0 0.0 1.1 16.4 2.0 17.2 8.1 2.7 1.8 5.9 Caption Closest Caption 4.5 0.8 1.3 1.9 5.9 3.1 3.0 2.3 11.4 1.0 3.5 CaSED 35.4 5.1 2.3 4.8 33.1 19.4 35.1 17.2 16.2 8.4 17.7 CLIP upper bound 86.0 52.2 51.5 28.6 75.7 89.9 88.0 66.6 84.5 71.3 69.4 Caption BLIP-2 (ViT-L) 13.4 1.4 4.8 0.0 7.5 4.7 1.7 4.7 11.6 1.1 5.1 BLIP-2 (ViT-g) 16.8 1.8 4.1 0.1 13.9 7.9 2.9 5.7 24.7 1.9 8.0 VQA BLIP-2 (ViT-L) 36.1 1.8 7.0 0.1 21.5 3.7 5.7 11.5 18.9 2.5 10.9 BLIP-2 (ViT-g) 41.5 2.4 7.5 2.0 38.0 8.6 10.2 13.8 33.2 2.8 16.0 Table 9: Semantic IoU on the ten datasets. Green is our method, gray shows the upper bound. Bold represents best, underline indicates best considering also image captioning and VQA models. Method Semantic Similarity (x100)↑ C101 DTD ESAT Airc. Flwr Food Pets SUN Cars UCF Avg. CLIP RN50 CLIP WordNet 43.2 29.0 18.5 21.6 46.7 44.6 50.3 42.8 26.4 40.0 36.3 English Words 36.0 29.5 14.9 20.0 38.1 34.2 40.7 35.4 18.3 32.4 29.9 Caption Closest Caption 37.2 22.8 14.2 26.8 38.9 41.2 32.6 37.4 44.3 32.4 32.8 CaSED 62.3 36.4 22.6 28.7 52.8 59.0 57.0 50.2 42.9 46.2 45.8 CLIP upper bound 88.1 62.4 52.8 53.9 72.0 83.7 85.8 73.9 81.0 73.9 72.7 CLIP ViT-L/14 CLIP WordNet 48.6 32.7 24.4 18.9 55.9 49.6 53.7 44.9 28.8 44.2 40.2 English Words 39.3 31.6 19.1 18.6 43.4 38.0 44.2 36.0 19.9 34.7 32.5 Caption Closest Caption 42.1 23.9 23.4 29.2 40.0 46.9 40.2 39.8 49.2 40.3 37.5 CaSED 65.7 40.0 32.0 30.3 55.5 64.5 62.5 52.5 47.4 54.1 50.4 CLIP upper bound 90.8 69.8 67.7 66.7 83.4 93.7 91.8 80.5 92.3 83.3 82.0 Caption BLIP-2 (ViT-L) 57.8 31.4 39.9 24.4 36.1 44.6 29.0 45.3 46.4 38.0 39.3 BLIP-2 (ViT-g) 63.0 33.1 36.2 24.3 45.2 51.6 31.6 48.3 61.0 44.6 43.9 VQA BLIP-2 (ViT-L) 70.5 34.9 29.7 29.1 48.8 42.0 40.0 50.6 52.4 48.6 44.7 BLIP-2 (ViT-g) 73.5 36.5 31.4 30.8 59.9 52.1 43.9 53.3 65.1 55.1 50.1 Table 10: Semantic similarity on the ten datasets. Green is ours, gray shows the upper bound. Bold represents best, underline indicates best considering also image captioning and VQA models. Another observation pertains to the differences between captioning and visual question answer- ing (VQA) datasets (e.g. COCO, Localized Narratives, and Visual Genome) w.r.t. image-text datasets collected from the web (e.g. CC12M, Redcaps, and LAION-400M). In general, it appears that textual descriptions from captioning and VQA datasets perform worse than datasets scraped from the web. This difference in performance may be due to the type of description provided, as the former often describes specific regions of the image, while the latter provides a general description of the image as a whole. Since our prime objective is to understand the class name of the subject in the image (whether it be a location, a pet, or a car model), captions describing secondary objects can lead to sub-optimal results. Interestingly, we found that the average performance on the 10 datasets of a chosen textual database generally correlates with the accuracy on ImageNet and with the closest caption semantic similarity (C.C. S-Sim in Table 11). Examples are the subset of PMD and CC12M, achieving the best average results on the 10 datasets according to all metrics (Tab. 4b of the main paper) while being the best on the ImageNet validation set w.r.t. both the closest caption semantic similarity and the ViC metrics. The ImageNet validation set can thus be used as a proxy to pick the textual database in case of a lack of priors on the test set. 17Database Size CA S-Sim. S-IoU Nouns(%) Adjs(%) Verbs(%) Concepts C.C. S-Sim. Ade20K 0.07M 11.0 31.8 1.7 82.7 10.6 6.7 3.1K 19.0 COCO 0.9M 13.8 35.6 2.6 86.1 9.5 4.4 14.6K 22.6 SBU Captions 1.0M 20.1 40.8 5.7 98.0 1.0 0.9 29.3K 26.9 OpenImages 1.4M 16.8 37.8 4.2 85.4 10.0 4.5 21.4K 24.8 Loc. Narr. 1.9M 15.7 37.1 4.1 86.9 9.8 3.2 30.9K 24.9 CC3M 2.8M 37.9 53.9 16.2 57.9 23.6 18.5 28.4K 35.2 WIT 4.8M 32.9 47.9 14.5 99.8 0.05 0.07 163.7K 30.0 Visual Genome 5.4M 14.6 35.2 3.8 75.1 15.6 9.2 21.1K 26.7 Redcaps 7.9M 41.1 54.7 19.6 98.6 0.7 0.6 50.5K 37.5 CC12M 10.3M 43.8 57.4 21.2 96.6 1.8 1.5 36.7K 38.7 YFCC100M* 29.9M 39.1 53.5 18.9 99.7 0.2 0.1 179.6K 37.8 Ours 54.8M 41.5 55.7 20.4 99.8 0.05 0.05 334.1K 38.7 LAION-400M 413.8M37.7 52.7 18.7 99.4 0.5 0.1 1.68M 37.3 Table 11: Ablation on the databases on ImageNet. Green is our database, Blue shows the top-5 databases of PMD, which we used to create Ours. We also evaluate the semantic similarity of the closest caption to each image in the dataset (i.e., C.C. S-Sim.). Bold represents best, while underline second best. YFCC100M* indicates we use the subset of the dataset used in PMD. For what concerns the POS tag distribution, it is hard to extract any pattern that justifies the perfor- mance of different databases. For instance, CC3M is the only database without an extreme imbalance for nouns (i.e., 57.9% compared to >96% for the other in the top-5) while still achieving good performance. Moreover, while the number of concepts depends on the size of the database, there is no clear correlation between this value and the achieved scores. As an example, Localized Narratives has a comparable amount of concepts w.r.t. CC3M but a gap of >20% in cluster accuracy, and of 10 points in semantic similarity and IoU. Finally, it is worth noting that most of the datasets have an average of 12.6 words per caption, while Visual Genome has an average of only 5.1 words per caption. This characteristic may explain why Visual Genome behaves differently in terms of the database scaling rule of increasing performance with the database size. D Qualitative results Last, we report some qualitative results of our method applied on three different datasets, namely Caltech-101 (Fig. 5), Food101 (Fig. 6), and SUN397 (Fig. 7), where the first is coarse, and the last two are fine-grained, focusing on food plates and places respectively. For each, we present a batch of five images, where the first three represent success cases and the last two show interesting failure cases. Each sample shows the image we input to our method with the top-5 candidate classes. From the results, we can see that for many success cases, our method not only generates the correct class name and selects it as the best matching label, but it also provides valid alternatives for classification. For example, the third image in Fig 5 or the second image in Fig 6, where CaSED provides the names \"dessert\" for the cheesecake and the label \"bird\" for the ibis. This phenomenon also happens in failure cases, where e.g. the last sample in Fig 6 provides both the name \"pizza\" and the name \"margherita\" for the dish, despite selecting the wrong name from the set. Another interesting observation is that our method provides names for different objects in the same scene. For instance, the third and fourth samples in Fig 6 contain labels for both \"guacamole\" and \"tortillas\" for the first, and for \"mozzarella\", \"insalata\", and \"balsamic\" for the second. A further detail on the latter case is the ability of CLIP to reason in multiple languages since \"insalata\" translates to \"salad\" from Italian to English. Regarding failure cases, it is interesting to note that the candidate names and the predicted label often describe well the input image despite being wrong w.r.t the dataset label. For instance, the two failure cases in Fig. 7 select \"stadium\" and \"dumpsite\" when the ground-truth class names are \"football\" and \"garbage site\". In addition, for the first case, the exact name \"football\" is still available among the best candidate names, but our method considers \"stadium\" as a better fit. Another example is the last failure case in Fig 5, where the model assigns the name \"nokia\" to a Nokia 3310 cellphone, while the ground-truth class name is \"cellphone\". Also in this case, the ground-truth label is present in the candidate list but our method considers \"nokia\" a more fitting class. 18Figure 5: Qualitative results on Caltech-101. The first three samples represent success cases, the last two shows failure cases. Figure 6: Qualitative results on Food101. The first three samples represent success cases, the last two shows failure cases. Figure 7: Qualitative results on SUN397. The first three samples represent success cases, the last two shows failure cases. Finally, we notice the discovery of correlations between terms in the reasoning of our model. In the provided examples, it happens multiple times that the candidate class names do not describe objects in the scene but rather a correlated concept to the image. For instance, the third example in Fig 5 shows a Dalmatian, and among the candidate names there is \"cruella\", which is the name of the villain of the movie \"The Hundred and One Dalmatians\". Another instance of this appears in the first example of Fig 6, where the model correctly associates the \"bibimbap\" dish to its place of origin, Korea, with the candidate name \"korean\". 19",
      "references": [
        "Flamingo: a visual language model for few-shot learning.",
        "Retrieval-augmented diffusion models.",
        "Improving language models by retrieving from trillions of tokens.",
        "Food-101–mining discriminative components with random forests.",
        "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.",
        "Reproducible scaling laws for contrastive language-image learning.",
        "Describing textures in the wild.",
        "Democratizing contrastive language- image pre-training: A clip benchmark of data, model, and supervision.",
        "Imagenet: A large-scale hierarchical image database.",
        "Virtex: Learning visual representations from textual annotations.",
        "Redcaps: Web-curated image-text data created by the people, for the people.",
        "An image is worth 16x16 words: Transformers for image recognition at scale.",
        "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories.",
        "Combined scaling for zero-shot transfer learning.",
        "Scaling open-vocabulary image segmentation with image-level labels.",
        "Self-supervised learning of visual features through embedding images into text topic spaces.",
        "Retrieval augmented language model pre-training.",
        "Automatically discovering and learning new visual categories with ranking statistics.",
        "Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.",
        "Scaling up vision-language pre-training for image captioning.",
        "Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory.",
        "Invariant information clustering for unsupervised image classification and segmentation.",
        "Scaling up visual and vision-language representation learning with noisy text supervision.",
        "Billion-scale similarity search with GPUs.",
        "Learning visual features from large weakly supervised data.",
        "3d object representations for fine-grained categorization.",
        "Visual genome: Connecting language and vision using crowdsourced dense image annotations.",
        "The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.",
        "Retrieval-augmented generation for knowledge- intensive nlp tasks.",
        "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
        "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.",
        "Align before fuse: Vision and language representation learning with momentum distillation.",
        "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning.",
        "Microsoft coco: Common objects in context.",
        "Large-scale long-tailed recognition in an open world.",
        "Retrieval augmented classification for long-tail visual recognition.",
        "Fine-grained visual classification of aircraft.",
        "Wordnet: a lexical database for english.",
        "Babelnet: Building a very large multilingual semantic network.",
        "Automated flower classification over a large number of classes.",
        "Im2text: Describing images using 1 million captioned photographs.",
        "Connecting vision and language with localized narratives.",
        "Learning transferable visual models from natural language supervision.",
        "Retrieval-augmented image captioning.",
        "Sentence-bert: Sentence embeddings using siamese bert-networks.",
        "Laion-5b: An open large-scale dataset for training next generation image-text models.",
        "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.",
        "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.",
        "Test-time prompt tuning for zero-shot generalization in vision-language models.",
        "Flava: A foundational language and vision alignment model.",
        "Ucf101: A dataset of 101 human actions classes from videos in the wild.",
        "Wit: Wikipedia- based image text dataset for multimodal multilingual machine learning.",
        "Yfcc100m: The new data in multimedia research.",
        "Grafit: Learning fine-grained image representations with coarse labels.",
        "Scan: Learning to classify images without labels.",
        "Simvlm: Simple visual language model pretraining with weak supervision.",
        "Sun database: Large- scale scene recognition from abbey to zoo.",
        "Coca: Contrastive captioners are image-text foundation models.",
        "Open-vocabulary object detection using captions.",
        "Multi-grained vision language pre-training: Aligning texts with visual concepts.",
        "Scene parsing through ade20k dataset.",
        "Learning to prompt for vision-language models."
      ],
      "meta_data": {
        "arxiv_id": "2306.00917v3",
        "authors": [
          "Alessandro Conti",
          "Enrico Fini",
          "Massimiliano Mancini",
          "Paolo Rota",
          "Yiming Wang",
          "Elisa Ricci"
        ],
        "published_date": "2023-06-01T17:19:43Z",
        "github_url": "https://github.com/flairNLP/flair"
      },
      "llm_extracted_info": {
        "main_contributions": "We formalize Vocabulary-free Image Classification (VIC), a setting where test-time categories lie in an unconstrained language-induced semantic space and no predefined vocabulary is available. We introduce CaSED (Category Search from External Databases), a training-free method that achieves VIC by leveraging large vision-language databases (VLDs) to retrieve caption captions semantically aligned with the input image, selecting the best matching candidate via multimodal matching using a frozen Vision-Language Model (CLIP). Our experiments show CaSED consistently outperforms complex VLMs (e.g., BLIP-2) across coarse- and fine-grained datasets, while using far fewer parameters and requiring no training. We also propose evaluation metrics tailored to VIC (semantic similarity, semantic IoU, cluster accuracy) and demonstrate that external caption databases provide semantically relevant priors for open-ended classification tasks. ",
        "methodology": "CaSED operates in two stages without any training: (1) Candidate generation by retrieving the K most similar captions D_x from an external VLD D using the image encoder of a pre-trained VLM (e.g., CLIP) and computing cosine similarity between image and caption embeddings; parsing/filtering retrieved captions to produce a finite candidate set C_x of potential class names. (2) Multimodal scoring of candidates by combining (i) image-to-text similarity sv_c = <fv_VLM(x), ft_VLM(c)> and (ii) text-to-text similarity st_c between the caption centroid bar{d} and the candidate embedding ft_VLM(c), with final score sc = alpha * softmax(sv_c) + (1 - alpha) * softmax(st_c). The predicted class f(x) = argmax_c in C_x sc. The approach uses a frozen CLIP model, no fine-tuning, and uses FAISS for fast retrieval. Text filtering includes stop-word removal, POS tagging, normalization (lowercasing, singularization), and occurrence-based pruning to produce robust candidate sets. ",
        "experimental_setup": "Datasets span coarse- and fine-grained categories: Caltech-101, DTD, Eurosat (ESAT), FGVC-Aircraft, Flowers-102, Food-101, Oxford Pets, Stanford Cars, SUN397, and UCF101; ImageNet is used for hyperparameter tuning. Baselines include CLIP with large vocabularies (WordNet, English Words), caption-based retrieval (Closest Caption, Caption Centroid) and captioning/VQA models (BLIP-2 with ViT-L/g, VQA BLIP-2) to address VIC. External databases come from the Public Multimodal Datasets (PMD) subset (CC3M, CC12M, WIT, Redcaps, YFCC100M*), with ablations using additional sources (OpenImages, Ade20K, LAION-400M). Evaluation metrics include semantic similarity (Sentence-BERT g), Semantic IoU, and Cluster Accuracy obtained by clustering predicted labels, with CLIP upper bound as an oracle. Inference is performed on NVIDIA A6000 GPUs; K=10 retrieved captions, alpha=0.7, FAISS indexing, and no training required. ",
        "limitations": "CaSED’s performance depends on the quality and coverage of the external caption database; domain-specific or ultra-fine-grained vocabularies not well represented in the database may degrade results. Lack of output history can cause inconsistent predictions across similar images; biases in the database can propagate to predictions; no explicit mechanism for multi-granularity disambiguation between coarse and fine concepts. Also, retrieval noise can hurt when using large, noisy databases; selecting and extending databases automatically remains challenging. ",
        "future_research_directions": "Explore automatic, test-time database selection or domain-adaptive database expansion to cover new concepts without retraining; add memory to CaSED to track past predictions and improve consistency; incorporate user-specified granularity or task constraints to disambiguate similar concepts; develop bias-detection and data-quality controls for retrieved captions; investigate hybrid approaches combining retrieval priors with lightweight captioning or VQA modules to handle domain-specific or ultra-fine semantics; study strategies for robust retrieval in dynamic semantic spaces and open-ended deployment scenarios.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Soft Augmentation for Image Classification",
      "full_text": "Soft Augmentation for Image Classification Yang Liu, Shen Yan, Laura Leal-Taixé, James Hays, Deva Ramanan Argo AI youngleoel@gmail.com, shenyan@google.com, leal.taixe@tum.de, hays@gatech.edu, deva@cs.cmu.edu Abstract Modern neural networks are over-parameterized and thus rely on strong regularization such as data augmenta- tion and weight decay to reduce overfitting and improve generalization. The dominant form of data augmentation applies invariant transforms, where the learning target of a sample is invariant to the transform applied to that sam- ple. We draw inspiration from human visual classifica- tion studies and propose generalizing augmentation with invariant transforms to soft augmentation where the learn- ing target softens non-linearly as a function of the de- gree of the transformapplied to the sample: e.g., more ag- gressive image crop augmentations produce less confident learning targets. We demonstrate that soft targets allow for more aggressive data augmentation, offer more robust performance boosts, work with other augmentation poli- cies, and interestingly, produce better calibrated models (since they are trained to be less confident on aggressively cropped/occluded examples). Combined with existing ag- gressive augmentation strategies, soft targets 1) double the top-1 accuracy boost across Cifar-10, Cifar-100, ImageNet- 1K, and ImageNet-V2, 2) improve model occlusion perfor- mance by up to 4×, and 3) half the expected calibration error (ECE). Finally, we show that soft augmentation gen- eralizes to self-supervised classification tasks. Code avail- able at https://github.com/youngleox/soft_ augmentation 1. Introduction Deep neural networks have enjoyed great success in the past decade in domains such as visual understanding [42], natural language processing [5], and protein structure pre- diction [41]. However, modern deep learning models are often over-parameterized and prone to overfitting. In addi- tion to designing models with better inductive biases, strong regularization techniques such as weight decay and data augmentation are often necessary for neural networks to achieve ideal performance. Data augmentation is often a computationally cheap and effective way to regularize mod- els and mitigate overfitting. The dominant form of data aug- mentation modifies training samples with invariant trans- forms – transformations of the data where it is assumed that the identity of the sample is invariant to the transforms. Indeed, the notion of visual invariance is supported by evidence found from biological visual systems [54]. The robustness of human visual recognition has long been docu- mented and inspired many learning methods including data augmentation and architectural improvement [19, 47]. This paper focuses on the counterpart of human visual robust- ness, namely how our vision fails. Instead of maintaining perfect invariance, human visual confidence degrades non- linearly as a function of the degree of transforms such as occlusion, likely as a result of information loss [44]. We propose modeling the transform-induced information loss for learned image classifiers and summarize the contribu- tions as follows: • We propose Soft Augmentation as a generalization of data augmentation with invariant transforms. With Soft Aug- mentation, the learning target of a transformed training sample softens. We empirically compare several soften- ing strategies and prescribe a robust non-linear softening formula. • With a frozen softening strategy, we show that replac- ing standard crop augmentation with soft crop augmenta- tion allows for more aggressive augmentation, and dou- bles the top-1 accuracy boost of RandAugment [8] across Cifar-10, Cifar-100, ImageNet-1K, and ImageNet-V2. • Soft Augmentation improves model occlusion robustness by achieving up to more than 4× Top-1 accuracy boost on heavily occluded images. • Combined with TrivialAugment [37], Soft Augmentation further reduces top-1 error and improves model calibra- tion by reducing expected calibration error by more than half, outperforming 5-ensemble methods [25]. • In addition to supervised image classification models, Soft Augmentation also boosts the performance of self- supervised models, demonstrating its generalizability. arXiv:2211.04625v2  [cs.CV]  23 Jan 2024-32 -24 -16 -8 0 8 16 24 32 tx -32 -24 -16 -8 0 8 16 24 32 ty Top-1 Error: 20.80 Standard Hard Crop -32 -24 -16 -8 0 8 16 24 32 tx -32 -24 -16 -8 0 8 16 24 32 ty 22.99(+2.19) Aggressive Hard Crop -32 -24 -16 -8 0 8 16 24 32 tx -32 -24 -16 -8 0 8 16 24 32 ty 18.31(−2.49) Soft Augmentation 0 1 0 1 0 1 Target Confidence (p) original image 77% visible  38% visible  22% visible Figure 1. Traditional augmentation encourages invariance by requiring augmented samples to produce the same target label; we visualize the translational offset range (tx, ty) of Standard Hard Crop augmentations for 32 × 32 images from Cifar-100 on the left, reporting the top-1 error of a baseline ResNet-18. Naively increasing the augmentation range without reducing target confidence increases error (middle), but softening the target label by reducing the target confidence for extreme augmentations reduces the error ( right), allowing for training with even more aggressive augmentations that may even produce blank images. Our work also shows that soft augmentations produce models that are more robust to occlusions (since they encounter larger occlusions during training) and models that are better calibrated (since they are trained to be less-confident on such occluded examples). 2. Related Work 2.1. Neural Networks for Vision Since the seminal work from Krizhevskyet al. [24], neu- ral networks have been the dominant class of high per- forming visual classifiers. Convolutional Neural Networks (CNNs) are a popular family of high performing neural models which borrows a simple idea of spatially local com- putations from biological vision [12, 18, 26]. With the help of architectural improvements [15], auxiliary loss [42], and improved computational power [13], deeper, larger, and more efficient neural nets have been developed in the past decade. 2.2. Data Augmentation Data augmentation has been an essential regularizer for high performing neural networks in many domains includ- ing visual recognition. While many other regularization techniques such as weight decay [32] and batch normal- ization [4] are shown to be optional, we are aware of no competitive vision models that omit data augmentation. Accompanying the influential AlexNet model, Krizhevsky et al . [24] proposed horizontal flipping and random cropping transforms which became the back- bone of image data augmentation. Since the repertoire of invariant transformations has grown significantly in the past decade [42], choosing which subset to use and then finding the optimal hyperparameters for each transform has become computationally burdensome. This sparked a line of research [7, 28] which investigates optimal policies for data augmentation such as RandAugment [8] and TrivialAugment [37]. 2.3. Learning from Soft Targets While minimizing the cross entropy loss between model logits and hard one-hot targets remains the go-to recipe for supervised classification training, learning with soft targets has emerged in many lines of research. Label Smooth- ing [36, 43] is a straightforward method which applies a fixed smoothing (softening) factor α to the hard one-hot classification target. The motivation is that label smoothing prevents the model from becoming over-confident. Müller et al. [36] shows that label smoothing is related to knowl- edge distillation [17], where a student model learns the soft distribution of a (typically) larger teacher model. A related line of research [49,53] focuses on regularizing how a model interpolates between samples by linearly mix- ing two or more samples and linearly softening the result- ing learning targets. Mixing can be in the form of per-pixel blending [53] or patch-level recombination [49]. 2.4. Robustness of Human Vision Human visual classification is known to be robust against perturbations such as occlusion. In computer vision re- search, the robustness of human vision is often regarded as the gold standard for designing computer vision mod- els [34, 54]. These findings indeed inspire development of robust vision models, such as compositional, recurrent, and occlusion aware models [22,46,47]. In addition to specialty models, much of the idea of using invariant transforms toaugment training samples come from the intuition and ob- servation that human vision are robust against these trans- forms such as object translation, scaling, occlusion, photo- metric distortions, etc. Recent studies such as Tang et al. [44] indeed confirm the robustness of human visual recognition against mild to moderate perturbations. In a 5-class visual classifica- tion task, human subjects maintain high accuracy when up to approximately half of an object is occluded. However, the more interesting observation is that human performance starts to degenerate rapidly as occlusion increases and falls to chance level when the object is fully occluded (see Figure 2 right k = 2, 3, 4 for qualitative curves). 3. Soft Augmentation In a typical supervised image classification setting, each training image xi has a ground truth learning target yi asso- ciated to it thus forming tuples: (xi, yi), (1) where xi ∈ RC×W×H denotes the image and yi ∈ [0, 1]N denotes a N-dimensional one-hot vector representing the target label (Figure 2 left, “Hard Target”). As modern neural models have the capacity to memorize even large datasets [1], data augmentation mitigates the issue by hal- lucinating data points through transformations of existing training samples. (Hard) data augmentation relies on the key underlying assumption that the augmented variant of xi should main- tain the original target label yi: (xi, yi) ⇒ (tϕ∼S(xi), yi) , Hard Augmentation (2) where tϕ∼S(xi) denotes the image transform applied to sample xi, ϕ is a random sample from the fixed transform range S. Examples of image transforms include transla- tion, rotation, crop, noise injection, etc. As shown by Tang et al. [44], transforms of xi such as occlusion are approxi- mately perceptually invariant only whenϕ is mild. Hence S often has to be carefully tuned in practice, since naively in- creasing it can lead to degraded performance (Figure 1). In the extreme case of 100% occlusion, total information loss occurs, making it detrimental for learning. Label Smoothing applies a smoothing function g to the target label yi parameterized by a handcrafted, fixed smoothing factor α. Specifically, label smoothing replaces the indicator value ‘1’ (for the ground-truth class label) with p = 1 − α, distributing the remaining α probability mass across all other class labels (Figure 2 left, “Soft Target”). One can interpret label smoothing as accounting for the average loss of information resulting from averaging over transforms from the range S. From this perspective, the smoothing factor α can be written as a function of the fixed transform range S: (xi, yi) ⇒ \u0000 tϕ∼S(xi), gα(S)(yi) \u0001 , Label Smoothing (3) Soft Augmentation, our proposed approach, can now be described succinctly as follows: replace the fixed smoothing factor α(S) with an adaptive smoothing factor α(ϕ), that depends on the degree of thespecific sampled augmentation ϕ applied to xi: (xi, yi) ⇒ \u0000 tϕ∼S(xi), gα(ϕ)(yi) \u0001 , Soft Augmentation (Target) (4) Crucially, conditioning on the information loss from a par- ticular ϕ allows one to define far larger augmentation ranges S. We will show that such a strategy consistently produces robust performance improvements with little tun- ing across a variety of datasets, models, and augmentation strategies. Extensions to Soft Augmentation may be proposed by also considering loss reweighting [40, 48], which is an al- ternative approach for softening the impact of an augmented example by down-weighting its contribution to the loss. To formalize this, let us write the training samples of a super- vised dataset as triples including a weight factor wi (that is typically initialized to all ‘1’s). One can then re-purpose our smoothing function g to modify the weight instead of (or in addition to) the target label (Figure 2 left): (xi, yi, wi) ⇒ \u0000 tϕ∼S(xi), yi, gα(ϕ)(wi) \u0001 , Soft Augmentation (Weight) (5) (xi, yi, wi) ⇒ \u0000 tϕ∼S(xi), gα(ϕ)(yi), gα(ϕ)(wi) \u0001 . Soft Augmentation (Target & Weight) (6) Finally, one may wish to soften targets by exploit- ing class-specific confusions when applying α(ϕ); the smoothed target label of a highly-occluded truck example could place more probability mass on other vehicle classes, as opposed to distributing the remaining probability equally across all other classes. Such extensions are discussed in Section 5. 4. Experiments 4.1. Soft Augmentation with Crop As a concrete example of the proposed Soft Augmenta- tion, we consider the crop transform t(tx,ty,w,h)(x). In the case of 32×32 pixel Cifar images [23], the cropped images typically have a constant sizew = h = 32, and t(x) is fully parameterized by tx and ty, which are translational offsets1 0 0 1 0 0 0.1 0.1 0.6 0.1 0.1 0.1 0.1 0.6 0.1 0.1 0.6 x 0 0 1 0 0 0.6 x Hard Target Soft Weight Soft Target Soft Target & Weight 1.0 x 1.0 x Weight One-Hot Target  Variants of Soft Augmentation Variant 0.0 0.5 1.0 Proportion Visible (v) 0.0 pmin 1 - α 1.0 Target Confidence (p) Softening Curves chance k=1 k=2 k=3 k=4 LS Figure 2. Variants of Soft Augmentation as prescribed by Equations 4 (Soft Target), 5 (Soft Weight), 6 (Soft Target & Weight) with example target confidence p = 0.6 (left). Soft Augmentation applies non-linear (k = 2, 3, 4, ...) softening to learning targets based on the specific degree of occlusion of a cropped image (Equation 7), which qualitatively captures the degradation of human visual recognition under occlusion [44]. Label Smoothing applies a fixed softening factor α to the one-hot classification target. between the cropped and the original image. As shown in Figure 1 (left), the standard hard crop augmentation for the Cifar-10/100 classification tasks drawstx, tyindependently from a uniform distribution of a modest range U(−4, 4). Under this distribution, the minimal visibility of an image is 77% and ResNet-18 models trained on the Cifar-100 task achieve mean top-1 validation error of 20.80% across three independent runs (Figure 1 left). Naively applying aggres- sive hard crop augmentation drawn from a more aggressive range U(−16, 16) increases top-1 error by 2.19% (Figure 1 middle). We make two changes to the standard crop aug- mentation. We first propose drawing tx, tyindependently from a scaled normal distribution S∗ ∼N(0, σL) (with clipping such that |tx| < L,|ty| < L), where L is the length of the longer edge of the image ( L = 32 for Cifar). The distri- bution has zero mean and σ controls the relative spread of the distribution hence the mean occlusion level. Following the 3σ rule of normal distribution, an intuitive tuning-free choice is to set σ ≈ 0.3, where ∼ 99% of cropped samples have visibility ≥ 0. Figure 3 (left, α = 0) shows that chang- ing the distribution alone without target softening provides a moderate ∼ 0.4% performance boost across crop strength σ. Directly borrowing the findings from human vision re- seach [44], one can define an adaptive softeningα(tx, ty, k) that softens the ground truth learning target. Similar to La- bel Smoothing [43], a hard target can be softened to confi- dence p ∈ [0, 1]. Instead of a fixed α, consider a family of power functions that produces target hardness p given crop parameters tx, tyand curve shape k: p = 1 −α(tx, ty, k) = 1 −(1 −pmin)(1 −v(tx,ty))k, (7) where v(tx,ty) ∈ [0, 1] is the image visibility which is a function of tx and ty. The power function family is a simple one-parameter formulation that allows us to test both linear (k = 1) and non-linear (k ̸= 1) softening: higher k provides flatter plateaus in high visibility regime (see Figure 2 right). 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Top-1 Error Reduction (%) Label Smoothing baseline α=0 α=0.001 α=0.01 α=0.1 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Soft Target baseline k=1 k=2 k=3 k=4 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Soft Weight baseline k=1 k=2 k=3 k=4 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Soft Target & Weight baseline k=1 k=2 k=3 k=4 Figure 3. Soft Augmentation reduces the top-1 validation error of ResNet-18 on Cifar-100 by up to 2.5% via combining both target and weight softening (Equation 6). Applying target softening alone (Equation 4) can boost performance by ∼ 2%. Crop parameters tx, ty are independently drawn from N(0, σL) (L = 32). Higher error reductions indicate better performance over baseline. All results are the means and standard errors across 3 independent runs.As seen in Label Smoothing, p can be interpreted as ground truth class probability of the one-hot learning target. pmin is the chance probability depending on the task prior. For example, for Cifar-100, pmin = 1 #classes = 0.01. Equation 7 has three assumptions: 1) the information loss is a function of image visibility and all information is lost only when the image is fully occluded, 2) the original label of a training image has a confidence of 100%, which suggests that there is no uncertainty in the class of the label, and 3) the information loss of all images can be approx- imated by a single confidence-visibility curve. While the first assumption is supported by observations of human vi- sual classification research [44], and empirical evidence in Sections 4.2 and 4.3 suggests that the second and the third assumptions approximately hold, the limitations to these as- sumptions will be discussed in Section 5. 4.2. How to Soften Targets As prescribed by Equations 4, 5, and 6, three versions of Soft Augmentation are compared with Label Smoothing across a range of crop strength σ. The popular ResNet-18 models [16] are trained on the 100-class classification Cifar- 100 training set. Top-1 error reductions on the validation set are reported (details in Appendix B). Consistent with prior studies, label smoothing can boost model performance by ∼ 1.3% when the smoothing factor α is properly tuned (Figure 3 left). Combining both target and weight softening (Equation 6) with k = 2 and σ ≈ 0.3 boosts model performance by 2.5% (Figure 3 right). Note that k = 2 qualitatively re- sembles the shape of the curve of human visual confidence degradation under occlusion reported by Tang et al. [44]. Interestingly, the optimal σ ≈ 0.3 fits the intuitive 3-σ rule. In the next section we freeze k = 2 and σ = 0 .3 and show robust improvements that generalize to Cifar-10 [23], ImageNet-1K [9], and ImageNet-V2 tasks [39]. 4.3. Supervised Classification 4.3.1 Comparison with Related Methods As mentioned in Section 2, many approaches similar to soft augmentation have demonstrated empirical perfor- mance gains, including additional data augmentation trans- forms [10], learning augmentation policies [8], softening learning targets [43], and modifying loss functions [29]. However, as training recipes continued to evolve over the past decade, baseline model performance has improved ac- cordingly. As seen in Table 1 (Baseline), our baseline ResNet-18 models with a 500-epoch schedule and cosine learning rate decay [33] not only outperform many recent baseline models of the same architecture, but also beat var- ious published results of Mixup and Cutout. To ensure fair comparisons, we reproduce 6 popular methods: Mixup, Table 1. Soft augmentation outperforms related methods. Optimal hyperparameters for Mixup [53], Cutout [10], and Online Label Smoothing [52] were applied. α of Focal Loss is tuned as [29] did not prescribe an optimal α for Cifar classification. It is worth not- ing that our baseline model (20.80%) not only outperforms other published baseline models by 1.5% to 4.8%, but also beat various implementations of Mixup and Cutout. Top-1 errors of ResNet-18 on Cifar-100 are reported. ResNet-18 Top-1 Error Baseline Zhanget al. [53] 25.6 DeVries and Taylor [10]22.46±0.31 Kimet al. [20] 23.59 Ours 20.80±0.11 Mixup Zhanget al. [53] 21.1 Kimet al. [20] 22.43 Ours 19.88±0.38 Cutout DeVries and Taylor [10]21.96±0.24 Ours 20.51±0.02 Label Smoothing Ours 19.47±0.18 Online Label Smoothing 20.12±0.05 Focal Loss (α= 1) 20.45±0.08 Focal Loss (α= 2) 20.38±0.08 Focal Loss (α= 5) 20.69±0.17 RandAugment 20.99±0.11 Soft Augmentation 18.31±0.17 Cutout, Label Smoothing, Online Label Smoothing, Focal Loss, and RandAugment, and report the Top-1 Error on Cifar-100 in Table 1. Additional comparisons with the self- reported results are available in Appendix Table 5. Table 1 shows that Soft Augmentation outperforms all other single methods. It is worth noting that although focal loss [29] proposed for detection tasks, it can be tuned to slightly improve classification model performance. 4.3.2 Soft Augmentation Compliments RandAugment This section investigates the robustness of Soft Augmen- tation across models and tasks, and how well it compares or complements more sophisticated augmentation policies such as RandAugment [8]. The ImageNet-1K dataset [9] has larger and variable-sized images compared to the Ci- far [23] datasets. In contrast with the fixed-sized crop augmentation for Cifar, a crop-and-resize augmentation t(tx,ty,w,h)(x) with random location tx, tyand random size w, his standard for ImageNet training recipes [7,8,42]. The resizing step is necessary to produce fixed-sized training images (e.g. 224 × 224). We follow the same σ = 0 .3 principle for drawing tx, tyand w, h(details in Appendix B). Comparing single methods, Soft Augmentation with crop only consistently outperforms the more sophisticated RandAugment with 14 transforms (Table 2). The small ResNet-18 models trained with SA on Cifar-10/100 even outperforms much larger baseline ResNet-50 [39] andTable 2. Soft Augmentation (SA) with a fixed softening curve of k = 2 doubles the top-1 error reduction of RandAugment (RA) across datasets and models. Note that the ResNet-18 models trained with SA on Cifar-10/100 even outperform larger baseline ResNet-50 and WideResNet-28 models. All results are mean ± standard error of top-1 validation error in percentage. Best results are shown in bold, runners-up are underlined, and results in parentheses indicate improvement over baseline. Statistics are computed from three runs. Dataset Model Baseline SA RA SA+RA Cifar100 EfficientNet-B0 49.70±1.55 42.13±0.45(−7.57) 46.68±1.52(−3.02) 38.72±0.71(−10.98) ResNet-18 20.80±0.11 18.31±0.17(−2.49) 20.99±0.11(+0.19) 18.10±0.20(−2.70) ResNet-50 20.18±0.30 18.06±0.24(−2.12) 18.57±0.09(−1.61) 16.72±0.06(−3.46) WideResNet-28 18.60±0.19 16.47±0.18(−2.13) 17.65±0.14(−0.95) 15.37±0.17(−3.23) PyramidNet + ShakeDrop15.77±0.17 14.03±0.05(−1.75) 14 .02±0.28(−1.76) 12.78±0.16(−2.99) Cifar10 EfficientNet-B0 17.73±0.69 12.21±0.22(−5.52) 14.54±0.47(−3.19) 11.67±0.26(−6.06) ResNet-18 4.38±0.05 3.51±0.08(−0.87) 3.89±0.06(−0.49) 3.27±0.08(−1.11) ResNet-50 4.34±0.14 3.67±0.08(−0.67) 3.91±0.14(−0.43) 3.01±0.02(−1.33) WideResNet-28 3.67±0.08 2.85±0.02(−0.82) 3.26±0.04(−0.41) 2.45±0.03(−1.20) PyramidNet + ShakeDrop 2.86±0.03 2.26±0.02(−0.60) 2.32±0.08(−0.54) 2.02±0.01(−0.84) ImageNet-1K ResNet-50 22.62±<0.01 21.66±0.02(−0.96) 22.02±0.02(−0.60) 21.27±0.05(−1.35) ResNet-101 20.91±0.04 20.63±0.03(−0.28) 20 .39±0.07(−0.52) 19.86±0.03(−1.05) ImageNet-V2 ResNet-50 34.97±0.03 33.32±0.10(−1.65) 34.16±0.21(−0.81) 32.38±0.16(−2.59) ResNet-101 32.68±0.04 31.81±0.16(−0.87) 32.08±0.19(−0.60) 31.26±0.12(−1.42) WideResNet-28 [50] models. Because RandAugment is a searched policy that is orig- inally prescribed to be applied in addition to the standard crop augmentation [8], one can easily replace the standard crop with soft crop and combine Soft Augmentation and RandAugment. As shown in Table 2, Soft Augmentation complements RandAugment by doubling its top-1 error re- duction across tasks and models. Note that for the small ResNet-18 model trained on Cifar-100, the fixed RandAugment method slightly de- grades its performance. Consistent with observations from Cubuk et al. [8], the optimal hyperparameters for RandAug- ment depend on the combination of model capacity and task complexity. Despite the loss of performance of applying RandAugment alone, adding Soft Augmentation reverses the effect and boosts performance by 2.7%. For the preceding experiments, a fixed k = 2 is used for Soft Augmentation and the official PyTorch RandomAug- ment [38] is implemented to ensure a fair comparison and to evaluate robustness. It is possible to fine-tune the hyperpa- rameters for each model and task to achieve better empirical performance. 4.3.3 Occlusion Robustness As discussed in Section 2, occlusion robustness in both hu- man vision [34,44,54] and computer vision [22,46,47] have been an important property for real world applications of vi- sion models as objects. To assess the effect of soft augmen- tation on occlusion robustness of computer vision models, ResNet-50 models are tested with occluded ImageNet vali- dation images (Figure 4 and Appendix Figure 7).224×224 validation images of ImageNet are occluded with randomly placed square patches that cover λ of the image area. λ is set to {0%, 20%, 40%, 60%, 80%} to create a range of oc- clusion levels. As shown in Figure 5, both RandAugment (RA) and Soft Augmentation (SA) improve occlusion robustness indepen- dently across occlusion levels. Combining RA with SA re- duces Top-1 error by up to 17%. At 80% occlusion level, SA+RA achieves more than 4× accuracy improvement over the baseline (18.98% vs 3.42%). 4.3.4 Confidence Calibration In addition to top-1 errors, reliability is yet another impor- tant aspect of model performance. It measures how close a model’s predicted probability (confidence) tracks the true correctness likelihood (accuracy). Expected Calibration Er- ror (ECE) is a popular metric [14, 25, 35] to measure con- fidence calibration by dividing model predictions into M confidence bins (Bm) and compute a weighted average er- ror between accuracy and confidence: ECE = MX m=1 |Bm| n |acc(Bm) − conf(Bm)|, (8) where n is the number of samples, acc(Bm) denotes the accuracy of bin m, and conf(Bm) denotes mean model confidence of bin m. Consistent with Guo et al. [14], we set M = 10 and compute ECE for Cifar-10 and Cifar-100 tasks. As shown in Table 3, many methods [25,30,35,45] have been proposed to improve confidence calibration, some- times at the cost of drastically increased computational overhead [25], or degraded raw performance [30]. We show in Table 3 (and Appendix Table 7) that it is possible to fur- ther reduce model top-1 error and expected calibration error simultaneously.Occlusion: 0% wreckBaselineSA+RA Class Prediction Probability wreck 1.00 wreck 0.98 Class Prediction Probability liner 0.00 liner 0.00 Class Prediction Probability dock 0.00 dock 0.00 Class Prediction Probability pirate 0.00 submarine 0.00 Class Prediction Probability submarine 0.00 pirate 0.00 Occlusion: 20% Class Prediction Probability wreck 0.95 wreck 0.95 Class Prediction Probability steel_arch_bridge 0.01 dock 0.01 Class Prediction Probability pier 0.01 submarine 0.01 Class Prediction Probability liner 0.01 liner 0.00 Class Prediction Probability crane 0.00 paddlewheel 0.00 Occlusion: 40% Class Prediction Probability crane 0.28 dock 0.20 Class Prediction Probability web_site 0.18 boathouse 0.10 Class Prediction Probability beacon 0.07 paddlewheel 0.08 Class Prediction Probability pier 0.06 pier 0.08 Class Prediction Probability envelope 0.04 wreck 0.07 Occlusion: 60% Class Prediction Probability web_site 0.78 dock 0.18 Class Prediction Probability crane 0.06 liner 0.16 Class Prediction Probability beacon 0.05 submarine 0.05 Class Prediction Probability seashore 0.01 crane 0.04 Class Prediction Probability envelope 0.01 container_ship 0.03 Occlusion: 80% Class Prediction Probability beacon 0.34 liner 0.07 Class Prediction Probability web_site 0.22 dock 0.03 Class Prediction Probability crane 0.07 aircraft_carrier 0.03 Class Prediction Probability seashore 0.06 container_ship 0.03 Class Prediction Probability liner 0.02 schooner 0.02 Figure 4. Examples of occluded ImageNet validation images and model predictions of ResNet-50. 224 × 224 validation images of ImageNet are occluded with randomly placed square patches that cover λ of the image area. λ is set to {0%, 20%, 40%, 60%, 80%} to create a range of occlusion levels. 0 20 40 60 80 Occlusion Level (%) 0 20 40 60 80Top-1 Accuracy (%) baseline RA SA RA+SA 0 20 40 60 80 Occlusion Level (%) 0 20 40 60 80Top-5 Accuracy (%) baseline RA SA RA+SA Figure 5. Soft Augmentation improves occlusion robustness of ResNet-50 on ImageNet. Both RandAugment (RA) and Soft Augmentation (SA) improve occlusion robustness independently. Combining RA with SA reduces Top-1 error by up to 17%. At 80% occlusion level, compared with baseline accuracy (3.42%), SA+RA achieves more than 4× accuracy (18.98%). Compared to previous single-model methods, our strong baseline WideResNet-28 models achieves lower top-1 er- ror at the cost of higher ECE. Combining Soft Augmen- tation with more recently developed augmentation policies such as TrivialAugment [37] (SA+TA) reduces top-1 error by 4.36% and reduces ECE by more than half on Cifar-100, outperforming the 4× more computationally expensive 5- ensemble model [25]. To the best of our knowledge, this is state of the art ECE performance for WideResNet-28 on Cifar without post-hoc calibration. 4.4. Soft Augmentation Boosts Self-Supervised Learning In contrast with supervised classification tasks where the learning target yi is usually a one-hot vector, many self-supervised methods such as SimSiam [6] and Barlow Twins [51] learn visual feature representations without class labels by encouraging augmentation invariant feature rep- resentations. This section investigates whether Soft Aug- Table 3. Soft Augmentation improves both accuracy and calibra- tion. We report mean and standard error of three WideResNet-28 runs per configuration (bottom two rows). On the more challeng- ing Cifar-100 benchmark, our Baseline already outperforms much of prior work in terms of Top-1 error, but has worse calibration er- ror (ECE). Applying Soft Augment + Trivial Augment (SA+TA) reduces Top-1 error by 4.36% and reduces ECE by more than half, outperforming even compute-heavy models such as the 5- Ensemble [25]. Similar trends hold for Cifar-10. Method Cifar-100 Cifar-10 Top-1 Error ECE Top-1 Error ECE Energy-based [31] 19.74 4.62 4.02 0.85 DUQ [45] – – 5.40 1.55 SNGP [30] 20.00 4.33 3.96 1.80 DDU [35] 19.02 4.10 4.03 0.85 5-Ensemble [25] 17.21 3.32 3.41 0.76 Our Baseline 18.60±0.16 4.86±0.10 3.67±0.07 2.22±0.03 SA+TA 14.24±0.11 1.76±0.15 2.23±0.06 0.61±0.10 mentation generalizes to learning settings where no one-hot style labeling is provided. In a typical setting, two random crops of the same image are fed into a pair of identical twin networks (e.g., ResNet- 18) with shared weights and architecture. The learning tar- get can be the maximization of similarity between the fea- ture representations of the two crops [6], or minimization of redundancy [51]. By default, all randomly cropped pairs have equal weights. We propose and test two alternative hypotheses for weight softening with SimSiam. To accom- modate self-supervised learning, Equation 7 is modified by replacing visibility vtx,ty with intersection over union IoU of two crops of an image: p = 1 −α(ϕ1, ϕ2, k) = 1 −(1 −pmin)(1 −IoUϕ1,ϕ2 )k, SA#1 (9) where ϕ1 = (tx1, ty1, w1, h1) and ϕ2 = (tx2, ty2, w2, h2) are crop parameters for the first and second sample in a pair.Table 4. Soft Augmentation (SA#1) improves self supervised learning with SimSiam (ResNet-18) on Cifar-100 by down- weighting sample pairs with small intersection over union (IoU), outperforming the opposite hypothesis (SA#2) of down-weighting pairs with large IoU. For each configuration, we report means and standard errors of 3 runs with best learning rates (LR) found for Cifar-100. The effect of SA#1 (with a fixed k = 4) generalizes to Cifar-10 without re-tuning. Task LR Baseline LR SA#1 LR SA#2 Cifar100 0.2 37.64±0.06 0.2 36.61±0.05 0.1 37.39±0.06 Cifar10 0.2 9.87±0.03 0.2 9.31±0.01 - - p is used to soften weights only as no one-hot classification vector is available in this learning setting. With this hypoth- esis (SA#1), “hard\" sample pairs with low IoUs are assigned low weights. Alternatively, one can assign lower weights to “easy\" sample pairs with higher IoUs (SA#2), as prescribed by Equation 10: p = 1 − α(ϕ1, ϕ2, k) = 1 − (1 − pmin)(IoUϕ1,ϕ2 )k. SA#2 (10) We first test all three hypotheses (baseline, SA#1, and SA#2) on Cifar-100 with the SimSiam-ResNet-18 models. Table 4 (top) shows that SA#1 outperform both baseline and SA#2 (details in Appendix B.4). Additional experiments show that models trained with the same SA#1 configuration also generalize to Cifar-10 (Table 4 bottom). 5. Discussion Other augmentations. While we focus on crop aug- mentations as an illustrative example, Soft Augmentation can be easily extended to a larger repertoire of transforms such as affine transforms and photometric distortions, as seen in the more sophisticated augmentation policies such as RandAugment. As the formulation of Equation 7 (and Figure 2 right) is directly inspired by the qualitative shape of human vision experiments from Tanget al. [44], optimal softening curves for other transforms may be discovered by similar human experiments. However, results with a sec- ond transform in Appendix Table 6 suggest that Equation 7 generalizes to additive noise augmentation as well. A po- tential challenge is determining the optimal softening strat- egy when a combination of several transforms are applied to an image since the cost of a naive grid search increases ex- ponentially with the number of hyperparameters. Perhaps reinforcement learning methods as seen in RandAugment can be used to speed up the search. Other tasks. While we limit the scope of Soft Augmen- tation to image classification as it is directly inspired by hu- man visual classification research, the idea can be general- ized to other types of tasks such as natural language mod- eling and object detection. Recent studies have shown that detection models benefit from soft learning targets in the fi- nal stages [3,27], Soft Augment has the potential to comple- ment these methods by modeling information loss of image transform in the models’ input stage. Class-dependant augmentations. As pointed out by Balestriero et al. [2], the effects of data augmentation are class-dependent. Thus assumption 3 of Equation 7 does not exactly hold. One can loosen it by adaptively determining the range of transform and softening curve on a per class or per sample basis. As shown in Equation 11, (xi, yi) ⇒ \u0000 tϕ∼S(xi,yi)(xi), gα(ϕ,xi,yi)(yi) \u0001 , (11) two adaptive improvements can be made: 1) the transforma- tion range S where ϕ is drawn from can be made a function of sample (xi, yi), 2) the softening factorα can also adapt to (xi, yi). Intuitively, the formulation recognizes the hetero- geneity of training samples of images at two levels. Firstly, the object of interest can occupy different proportions of an image. For instance, a high-resolution training image with a small object located at the center can allow more ag- gressive crop transforms without losing its class invariance. Secondly, texture and shape may contribute differently de- pending on the visual class. A heavily occluded tiger may be recognized solely by its distinctive stripes; in contrast, a minimally visible cloak can be mistaken as almost any clothing. 6. Conclusion In summary, we draw inspiration from human vision re- search, specifically how human visual classification perfor- mance degrades non-linearly as a function of image occlu- sion. We propose generalizing data augmentation with in- variant transforms to Soft Augmentation where the learning target (e.g. one-hot vector and/or sample weight) softens non-linearly as a function of the degree of the transform ap- plied to the sample. Using cropping transformations as an example, we em- pirically show that Soft Augmentation offers robust top-1 error reduction across Cifar-10, Cifar-100, ImageNet-1K, and ImageNet-V2. With a fixed softening curve, Soft Aug- mentation doubles the top-1 accuracy boost of the popular RandAugment method across models and datasets, and im- proves performance under occlusion by up to 4×. Combin- ing Soft Augment with the more recently developed Triv- ialAugment further improves model accuracy and calibra- tion simultaneously, outperforming even compute-heavy 5- ensemble models. Finally, self-supervised learning exper- iments demonstrate that Soft Augmentation also general- izes beyond the popular supervised one-hot classification setting.References [1] Devansh Arpit, Stanisław Jastrz˛ ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International conference on machine learning , pages 233– 242. PMLR, 2017. 3 [2] Randall Balestriero, Leon Bottou, and Yann LeCun. The effects of regularization and data augmentation are class de- pendent. arXiv preprint arXiv:2204.03632, 2022. 8 [3] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S Davis. Soft-nms–improving object detection with one line of code. In Proceedings of the IEEE international conference on computer vision, pages 5561–5569, 2017. 8 [4] Andy Brock, Soham De, Samuel L Smith, and Karen Si- monyan. High-performance large-scale image recognition without normalization. In International Conference on Ma- chine Learning, pages 1059–1071. PMLR, 2021. 2 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural in- formation processing systems, 33:1877–1901, 2020. 1 [6] Xinlei Chen and Kaiming He. Exploring simple siamese rep- resentation learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 15750–15758, 2021. 7, 14 [7] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude- van, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 113–123, 2019. 2, 5 [8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmenta- tion with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702–703, 2020. 1, 2, 5, 6, 12 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 5 [10] Terrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 5 [11] Terrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 12 [12] Kunihiko Fukushima and Sei Miyake. Neocognitron: A self- organizing neural network model for a mechanism of visual pattern recognition. In Competition and cooperation in neu- ral nets, pages 267–285. Springer, 1982. 2 [13] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noord- huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large mini- batch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 2 [14] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pages 1321–1330. PMLR, 2017. 6 [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 2 [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision , pages 630–645. Springer, 2016. 5, 11 [17] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015. 2 [18] David H Hubel and Torsten N Wiesel. Receptive fields of single neurones in the cat’s striate cortex. The Journal of physiology, 148(3):574, 1959. 2 [19] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. Advances in neural informa- tion processing systems, 28, 2015. 1 [20] Jang-Hyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint mixup with super- modular diversity. arXiv preprint arXiv:2102.03065, 2021. 5, 12 [21] Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puz- zle mix: Exploiting saliency and local statistics for optimal mixup. In International Conference on Machine Learning , pages 5275–5285. PMLR, 2020. 12 [22] Adam Kortylewski, Ju He, Qing Liu, and Alan L Yuille. Compositional convolutional neural networks: A deep archi- tecture with innate robustness to partial occlusion. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8940–8949, 2020. 2, 6 [23] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 3, 5 [24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. Advances in neural information processing systems , 25, 2012. 2 [25] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estima- tion using deep ensembles. Advances in neural information processing systems, 30, 2017. 1, 6, 7 [26] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015. 2 [27] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection. Advances in Neural Information Processing Systems, 33:21002–21012, 2020. 8 [28] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. Advances in Neural Information Processing Systems, 32, 2019. 2 [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Pro- ceedings of the IEEE international conference on computer vision, pages 2980–2988, 2017. 5, 12[30] Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. Advances in Neural Infor- mation Processing Systems, 33:7498–7512, 2020. 6, 7 [31] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems , 33:21464–21475, 2020. 7 [32] Yang Liu, Jeremy Bernstein, Markus Meister, and Yisong Yue. Learning by turning: Neural architecture aware optimi- sation. In International Conference on Machine Learning , pages 6748–6758. PMLR, 2021. 2 [33] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas- tic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 5 [34] David G Lowe. Object recognition from local scale-invariant features. In Proceedings of the seventh IEEE international conference on computer vision, volume 2, pages 1150–1157. Ieee, 1999. 2, 6 [35] Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip H. S. Torr, and Yarin Gal. Deep deterministic uncer- tainty: A simple baseline, 2021. 6, 7 [36] Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Advances in neural in- formation processing systems, 32, 2019. 2 [37] Samuel G Müller and Frank Hutter. Trivialaugment: Tuning- free yet state-of-the-art data augmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 774–782, 2021. 1, 2, 7 [38] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai- son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im- perative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Informa- tion Processing Systems 32, pages 8024–8035. Curran Asso- ciates, Inc., 2019. 6, 12 [39] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to im- agenet? In International Conference on Machine Learning, pages 5389–5400. PMLR, 2019. 5, 12 [40] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urta- sun. Learning to reweight examples for robust deep learn- ing. In International conference on machine learning, pages 4334–4343. PMLR, 2018. 3 [41] Andrew W Senior, Richard Evans, John Jumper, James Kirk- patrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander WR Nelson, Alex Bridgland, et al. Im- proved protein structure prediction using potentials from deep learning. Nature, 577(7792):706–710, 2020. 1 [42] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 1–9, 2015. 1, 2, 5 [43] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception archi- tecture for computer vision. In Proceedings of the IEEE con- ference on computer vision and pattern recognition , pages 2818–2826, 2016. 2, 4, 5 [44] Hanlin Tang, Martin Schrimpf, William Lotter, Charlotte Moerman, Ana Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, and Gabriel Kreiman. Recurrent computations for visual pattern completion. Proceedings of the National Academy of Sciences, 115(35):8835–8840, 2018. 1, 3, 4, 5, 6, 8 [45] Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a single deep de- terministic neural network. In International conference on machine learning, pages 9690–9700. PMLR, 2020. 6, 7 [46] Angtian Wang, Yihong Sun, Adam Kortylewski, and Alan L Yuille. Robust object detection under occlusion with context- aware compositionalnets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12645–12654, 2020. 2, 6 [47] Dean Wyatte, Tim Curran, and Randall O’Reilly. The limits of feedforward vision: Recurrent processing promotes robust object recognition when objects are degraded. Journal of Cognitive Neuroscience, 24(11):2248–2261, 2012. 1, 2, 6 [48] Mingyang Yi, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Zhi-Ming Ma. Reweighting augmented samples by minimizing the maximal expected loss. arXiv preprint arXiv:2103.08933, 2021. 3 [49] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu- larization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international con- ference on computer vision, pages 6023–6032, 2019. 2 [50] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. arXiv preprint arXiv:1605.07146, 2016. 6, 11 [51] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Ma- chine Learning, pages 12310–12320. PMLR, 2021. 7 [52] Chang-Bin Zhang, Peng-Tao Jiang, Qibin Hou, Yunchao Wei, Qi Han, Zhen Li, and Ming-Ming Cheng. Delving deep into label smoothing. IEEE Transactions on Image Process- ing, 30:5984–5996, 2021. 5, 12 [53] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017. 2, 5, 12 [54] Hongru Zhu, Peng Tang, Jeongho Park, Soojin Park, and Alan Yuille. Robustness of object recognition under ex- treme occlusion in humans and computational models.arXiv preprint arXiv:1905.04598, 2019. 1, 2, 6Appendix A. Implementation 1import torch 2 3class SoftCropAugmentation: 4def __init__(self, n_class, sigma=0.3, k=2): 5self.chance = 1/n_class 6self.sigma = sigma 7self.k = k 8 9def draw_offset(self, limit, sigma=0.3, n =100): 10# draw an integer from a (clipped) Gaussian 11for d in range(n): 12x = torch.randn((1))*sigma 13if abs(x) <= limit: 14return int(x) 15return int(0) 16 17def __call__(self, image, label): 18# typically, dim1 = dim2 = 32 for Cifar 19dim1, dim2 = image.size(1), image.size(2) 20# pad image 21image_padded = torch.zeros((3, dim1 * 3, dim2 * 3)) 22image_padded[:, dim1:2*dim1, dim2:2*dim2] = image 23# draw tx, ty 24tx = self.draw_offset(dim1, self. sigma_crop * dim1) 25ty = self.draw_offset(dim2, self. sigma_crop * dim2) 26# crop image 27left, right = tx + dim1, tx + dim1 * 2 28top, bottom = ty + dim2, ty + dim2 * 2 29new_image = image_padded[:, left: right, top: bottom] 30# compute transformed image visibility and confidence 31v = (dim1 - abs(tx)) * (dim2 - abs(ty)) / (dim1 * dim2) 32confidence = 1 - (1 - self.chance) * (1 - v) ** self.k 33return new_image, label, confidence Listing 1. Pytorch implementation of Soft Crop Augmentation for Cifar. 1import torch 2import torch.nn.functional as F 3 4def soft_target(pred, label, confidence): 5log_prob = F.log_softmax(pred, dim=1) 6n_class = pred.size(1) 7# make soft one-hot target 8one_hot = torch.ones_like(pred) * (1 - confidence) / (n_class - 1) 9one_hot.scatter_(dim=1, index=label, src= confidence) 10# compute weighted KL loss 11kl = confidence * F.kl_div(input=log_prob, 12target=one_hot, 13reduction=’none’). sum(-1) 14return kl.mean() Listing 2. Pytorch implementation of Soft Target loss function. Appendix B. Experiment Details Appendix B.1. Supervised Cifar-10/100 For Cifar-100 experiments, we train all ResNet-like models with a batch size 128 on a single Nvidia V100 16GB GPU on Amazon Web Services (AWS) and with an intial learning rate 0.1 with cosine learning rate decay over 500 epochs. EfficientNet-B0 is trained with an initial learning rate of 0.025, PyramidNet-272 is trained with 2 GPUs. We use the Conv-BatchNorm-ReLU configuration of ResNet models [16] and WideResNet-28 with a widening factor of 10 [50]. Horizontal flip is used in all experiments as it is considered a lossless transformation in the context of Ci- far images. We find decaying crop aggressiveness towards the end of training (e.g., last 20 epochs) by a large fac- tor (e.g., reducing σ by 1000×) marginally improve per- formance on Cifar-100, but slightly hurts performance on Cifar-10. Accordingly, we only apply σ decay for all Cifar- 100 experiments. A single run of ResNet-18, ResNet-50, and WideResNet-28 takes ∼ 2.5, ∼ 7, ∼ 9 GPU hours on Cifar-10/100, respectively. BL beaver 0.30 SA+TA seal 0.44 BL pear 0.43 SA+TA apple 0.79 BL television 0.12 SA+TA dinosaur 0.37 BL skyscraper 0.18 SA+TA castle 0.44 BL kangaroo 0.35 SA+TA rabbit 0.79 BL poppy 0.83 SA+TA beetle 0.27 BL poppy 0.74 SA+TA sunflower 0.89 BL clock 0.48 SA+TA ray 0.74 BL girl 0.51 SA+TA boy 0.83 BL crab 0.52 SA+TA crocodile 0.95 BL rabbit 0.56 SA+TA mouse 0.68 BL butterfly 0.28 SA+TA skunk 0.88 BL beetle 0.12 SA+TA flatfish 0.16 BL mouse 0.17 SA+TA lizard 0.98 BL forest 0.37 SA+TA pine_tree 0.56 BL skunk 0.68 SA+TA elephant 0.78 BL hamster 0.35 SA+TA raccoon 0.17 BL bee 0.84 SA+TA caterpillar 0.95 BL cattle 0.10 SA+TA kangaroo 0.31 BL cattle 0.24 SA+TA lion 0.89 Figure 6. Example images of the Cifar-100 validation set and pre- dictions of WideResNet-28. Predicted classes and confidence lev- els of models trained with Soft Augmentation + Trivial Augment (SA+TA) and baseline (BL) augmentation are reported. In many cases, SA+TA not only corrects the class prediction, but also im- proves the model confidence. For instance, BL mistakes “seal” for “beaver” (top-left, both classes belong to the same “aquatic mammal” superclass), and SA+TA makes a correct class predic- tion with higher confidence.Appendix B.2. Additional Results Table 5. Comparing SA with other methods. Recommended hyperparameters for Mixup [53], Cutout [11], and Online Label Smoothing [52]. α of Focal Loss is tuned as Lin et al. [29] did not prescribe an optimal α for Cifar classification. Top-1 errors of ResNet-18 on Cifar-100 are reported. ResNet-18 Top-1 Error Zhanget al. [53] Baseline 25.6 Mixup 21.1 Kimet al. [21] Baseline 23.67 Mixup 23.16 Manifold Mixup 20.98 Puzzle Mix 19.62 Kimet al. [20] Baseline 23.59 Mixup 22.43 Manifold Mixup 21.64 Puzzle Mix 20.62 Co-Mixup 19.87 Our Baseline 20.80±0.11 Label Smoothing 19.47±0.18 Online Label Smoothing20.12±0.05 Focal Loss (α= 1) 20.45±0.08 Focal Loss (α= 2) 20.38±0.08 Focal Loss (α= 5) 20.69±0.17 Mixup (α= 1.0) 19.88±0.38 Cutout (L= 8) 20.51±0.02 SA 18.31±0.17 RA 20.99±0.11 SA + RA 18.10±0.20 Table 6. Soft Augmentation with additive noise improves ResNet- 18 performance on Cifar-100. Given an image X and a random noise pattern Xnoise, and augmented image is given by Xaug = X + αXnoise, where α is drawn from N(0, 0.1) and pixel values of Xnoise are also independently drawn from N(0, 0.1). Apply- ing Soft Augmentation to additive noise boost performance over baseline as well as Soft Augmentation Crop + RandAugment. ResNet-18 Top-1 Error Baseline 20.80±0.11 RA 20.99±0.11 Hard Crop 20.26±0.12 SA-Crop (k=2) 18.31±0.17 Hard Noise 20.68±0.05 SA-Noise (k=1) 19.20±0.20 SA-Crop (k=2) + RA 18.10±0.20 SA-Noise (k=1) + SA-Crop (k=2) + RA17.87±0.17 Table 7. Soft Augmentation reduces expected calibration error (ECE) of ResNet-50 on ImageNet. Dataset Baseline RA SA RA+SA ImageNet-1K 5.11 4.09 3.17 2.78 ImageNet-V2 9.91 8.84 3.24 3.18 Appendix B.3. ImageNet All ImageNet-1k experiments are conducted with a batch size of 256 distributed across 4 Nvidia V100 16GB GPUs on AWS. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 dataset (BSD 3- Clause License) is downloaded from the official website (https://www.image-net.org/). Horizontal flip is used in all experiments as an additional lossless base augmentation. The base learning rate is set to 0.1 with a 5-epoch linear warmup and cosine decay over 270 epochs. A single run of ResNet-50 training takes ∼ 4 × 4 = 16 GPU days and ImageNet experiments take a total of 600 GPU days. We use the official PyTorch [38] implementation of Ran- dAugment and ResNet-50/101 (BSD-style license) and run all experiments with the standard square input Linput = W = H = 224. Note that the original RandAugment [8] uses a larger input size of H = 224 , W = 244 , but our re-implemention improved top-1 error (22.02 vs 22.4) of ResNet-50 despite using a smaller input size. ImageNet-V2 is a validation set proposed by He et al. [39]. For training, the standard crop transform has 4 hy- perparameters: (scalemin, scalemax) define the range of the relative size of a cropped image to the original one, (ratiomin, ratiomax) determine lower and upper bound of the aspect ratio of the cropped patch before the final resize step. In practice, a scale is drawn from a uni- form distribution U(scalemin, scalemax), then the loga- rithm of the aspect ratio is drawn from a uniform dis- tribution U(log(ratiomin), log(ratiomax)). Default val- ues are scalemin = 0 .08, scalemax = 1 .0, ratiomin = 3/4, ratiomax = 4/3. Similar to our Cifar crop augmentation, we propose a simplified ImageNet crop augmentation with only 2 hy- perparameters σ, Lmin. First, we draw ∆w, ∆h from a clipped rectified normal distribution NR(0, σ(L − Lmin)) and get w = W − ∆w, h= H − ∆h Lmin is the mini- mum resolution of a cropped image and set to half of input resolution 224. tx, tyare then independently drawn from N(0, σ(W +w)), N(0, σ(H +h)). Note that we use a fixed set of intuitive values σ = 0.3, Lmin = 1/2Linput = 112 for all the experiments. For model validation, standard augmentation practice first resizes an image so that its short edge has length Linput = 256 , then a center 224 × 224 crop is applied. Note that Linput is an additional hyperparameter introduced by the test augmentation. In contrast, we simplify this by setting Linput to the final input size 224 and use this con- figuration for all ImageNet model evaluation.Occlusion: 0% Boston_bullBaselineSA+RA Class Prediction Probability Boston_bull 0.82 Boston_bull 0.65 Class Prediction Probability French_bulldog 0.09 French_bulldog 0.23 Class Prediction Probability toy_terrier 0.03 toy_terrier 0.01 Class Prediction Probability tennis_ball 0.01 Chihuahua 0.00 Class Prediction Probability Chihuahua 0.00 pug 0.00 Occlusion: 20% Class Prediction Probability Boston_bull 0.95 Boston_bull 0.86 Class Prediction Probability French_bulldog 0.05 French_bulldog 0.13 Class Prediction Probability toy_terrier 0.00 toy_terrier 0.00 Class Prediction Probability Chihuahua 0.00 Chihuahua 0.00 Class Prediction Probability tennis_ball 0.00 pug 0.00 Occlusion: 40% Class Prediction Probability French_bulldog 0.83 Boston_bull 0.96 Class Prediction Probability Boston_bull 0.17 French_bulldog 0.04 Class Prediction Probability Staffordshire_bullterrier 0.00 toy_terrier 0.00 Class Prediction Probability American_Staffordshire_terrier0.00 Italian_greyhound 0.00 Class Prediction Probability Great_Dane 0.00 Chihuahua 0.00 Occlusion: 60% Class Prediction Probability Boston_bull 0.97 Boston_bull 0.80 Class Prediction Probability French_bulldog 0.01 French_bulldog 0.02 Class Prediction Probability Chihuahua 0.01 Italian_greyhound 0.02 Class Prediction Probability whippet 0.00 toy_terrier 0.02 Class Prediction Probability toy_terrier 0.00 Chihuahua 0.01 Occlusion: 80% Class Prediction Probability French_bulldog 0.88 Boston_bull 0.13 Class Prediction Probability Boston_bull 0.06 French_bulldog 0.06 Class Prediction Probability Chihuahua 0.01 Italian_greyhound 0.03 Class Prediction Probability pug 0.01 miniature_pinscher 0.02 Class Prediction Probability Staffordshire_bullterrier 0.01 Staffordshire_bullterrier 0.01 Occlusion: 0% papillonBaselineSA+RA Class Prediction Probability papillon 1.00 papillon 0.97 Class Prediction Probability Chihuahua 0.00 Chihuahua 0.00 Class Prediction Probability Japanese_spaniel 0.00 Japanese_spaniel 0.00 Class Prediction Probability toy_terrier 0.00 toy_terrier 0.00 Class Prediction Probability Pomeranian 0.00 Pomeranian 0.00 Occlusion: 20% Class Prediction Probability papillon 0.74 papillon 0.76 Class Prediction Probability Blenheim_spaniel 0.09 Blenheim_spaniel 0.03 Class Prediction Probability clumber 0.06 Japanese_spaniel 0.03 Class Prediction Probability Japanese_spaniel 0.05 Chihuahua 0.02 Class Prediction Probability Welsh_springer_spaniel 0.02 Pomeranian 0.01 Occlusion: 40% Class Prediction Probability papillon 0.29 Blenheim_spaniel 0.15 Class Prediction Probability Blenheim_spaniel 0.19 papillon 0.13 Class Prediction Probability Welsh_springer_spaniel 0.14 Welsh_springer_spaniel 0.13 Class Prediction Probability collie 0.11 Brittany_spaniel 0.05 Class Prediction Probability Brittany_spaniel 0.05 hare 0.02 Occlusion: 60% Class Prediction Probability envelope 0.09 bustard 0.04 Class Prediction Probability Indian_cobra 0.03 hare 0.02 Class Prediction Probability hognose_snake 0.02 golf_ball 0.02 Class Prediction Probability web_site 0.02 partridge 0.02 Class Prediction Probability dhole 0.01 kit_fox 0.02 Occlusion: 80% Class Prediction Probability envelope 0.15 kit_fox 0.01 Class Prediction Probability web_site 0.08 hare 0.01 Class Prediction Probability dhole 0.04 bustard 0.01 Class Prediction Probability red_fox 0.01 partridge 0.01 Class Prediction Probability honeycomb 0.01 bittern 0.01 Occlusion: 0% vending_machineBaselineSA+RA Class Prediction Probability vending_machine 0.65 vending_machine 0.98 Class Prediction Probability streetcar 0.04 streetcar 0.00 Class Prediction Probability refrigerator 0.02 refrigerator 0.00 Class Prediction Probability minibus 0.01 pop_bottle 0.00 Class Prediction Probability grocery_store 0.01 ambulance 0.00 Occlusion: 20% Class Prediction Probability vending_machine 0.39 vending_machine 0.83 Class Prediction Probability moving_van 0.12 streetcar 0.04 Class Prediction Probability refrigerator 0.12 moving_van 0.01 Class Prediction Probability web_site 0.05 refrigerator 0.01 Class Prediction Probability monitor 0.04 trolleybus 0.01 Occlusion: 40% Class Prediction Probability vending_machine 0.88 vending_machine 0.93 Class Prediction Probability refrigerator 0.03 refrigerator 0.01 Class Prediction Probability web_site 0.01 pay-phone 0.00 Class Prediction Probability desktop_computer 0.00 slot 0.00 Class Prediction Probability monitor 0.00 streetcar 0.00 Occlusion: 60% Class Prediction Probability screen 0.26 streetcar 0.03 Class Prediction Probability monitor 0.18 garbage_truck 0.02 Class Prediction Probability home_theater 0.14 parking_meter 0.01 Class Prediction Probability television 0.04 cab 0.01 Class Prediction Probability desktop_computer 0.04 trolleybus 0.01 Occlusion: 80% Class Prediction Probability home_theater 0.59 sliding_door 0.04 Class Prediction Probability monitor 0.13 refrigerator 0.02 Class Prediction Probability entertainment_center 0.05 vending_machine 0.02 Class Prediction Probability television 0.05 pay-phone 0.01 Class Prediction Probability screen 0.04 barbershop 0.01 Occlusion: 0% pirateBaselineSA+RA Class Prediction Probability pirate 1.00 pirate 0.93 Class Prediction Probability schooner 0.00 fireboat 0.01 Class Prediction Probability dock 0.00 dock 0.01 Class Prediction Probability fireboat 0.00 schooner 0.01 Class Prediction Probability crane 0.00 amphibian 0.00 Occlusion: 20% Class Prediction Probability pirate 0.98 pirate 0.76 Class Prediction Probability schooner 0.02 schooner 0.03 Class Prediction Probability dock 0.00 cannon 0.03 Class Prediction Probability stupa 0.00 dock 0.01 Class Prediction Probability crane 0.00 fireboat 0.01 Occlusion: 40% Class Prediction Probability crane 0.28 pirate 0.72 Class Prediction Probability pirate 0.22 schooner 0.04 Class Prediction Probability moving_van 0.12 fireboat 0.03 Class Prediction Probability schooner 0.05 dock 0.01 Class Prediction Probability scoreboard 0.03 drilling_platform 0.01 Occlusion: 60% Class Prediction Probability crane 0.43 pirate 0.26 Class Prediction Probability bookshop 0.08 schooner 0.03 Class Prediction Probability scoreboard 0.07 toyshop 0.03 Class Prediction Probability schooner 0.05 suspension_bridge 0.02 Class Prediction Probability drilling_platform 0.03 bookshop 0.02 Occlusion: 80% Class Prediction Probability pole 0.10 toyshop 0.03 Class Prediction Probability book_jacket 0.09 carousel 0.02 Class Prediction Probability comic_book 0.09 shoe_shop 0.02 Class Prediction Probability envelope 0.07 bookshop 0.01 Class Prediction Probability binder 0.06 totem_pole 0.01 Occlusion: 0% military_uniformBaselineSA+RA Class Prediction Probability military_uniform 0.74 military_uniform 0.70 Class Prediction Probability mortarboard 0.07 suit 0.06 Class Prediction Probability suit 0.05 bow_tie 0.02 Class Prediction Probability academic_gown 0.03 crutch 0.02 Class Prediction Probability cornet 0.03 groom 0.01 Occlusion: 20% Class Prediction Probability suit 0.19 crutch 0.23 Class Prediction Probability mortarboard 0.09 suit 0.14 Class Prediction Probability military_uniform 0.06 groom 0.09 Class Prediction Probability notebook 0.04 military_uniform 0.04 Class Prediction Probability lab_coat 0.04 turnstile 0.02 Occlusion: 40% Class Prediction Probability military_uniform 0.22 military_uniform 0.22 Class Prediction Probability lab_coat 0.07 projectile 0.04 Class Prediction Probability file 0.07 warplane 0.02 Class Prediction Probability bearskin 0.05 missile 0.02 Class Prediction Probability suit 0.05 grand_piano 0.01 Occlusion: 60% Class Prediction Probability military_uniform 0.11 military_uniform 0.14 Class Prediction Probability suit 0.10 lab_coat 0.02 Class Prediction Probability envelope 0.06 cowboy_hat 0.02 Class Prediction Probability kimono 0.06 rifle 0.02 Class Prediction Probability abaya 0.05 trombone 0.02 Occlusion: 80% Class Prediction Probability abaya 0.15 crutch 0.04 Class Prediction Probability space_heater 0.13 mortarboard 0.01 Class Prediction Probability web_site 0.13 academic_gown 0.01 Class Prediction Probability window_shade 0.12 trombone 0.01 Class Prediction Probability shower_curtain 0.04 shoe_shop 0.01 Figure 7. Examples of occluded ImageNet validation images and model predictions of ResNet-50.Appendix B.4. Self-Supervised Cifar-10/100 Self-supervised SimSiam experiments are run on a sin- gle Nvidia A6000 GPU. We follow the standard two-step training recipe [6]. 1) We first train the Siamese network in a self-supervised manner to learn visual features for 500 epochs with a cosine decay schedule and a batch size of 512. We apply Soft Augmentation only during this step. 2) The linear layer is then tuned with ground-truth labels for 100 epochs with an initial learning rate of 10 and 10× de- cay at epochs 60 and 80. Following [6], we set scalemin = 0.2, scalemax = 1 .0, ratiomin = 3 /4, ratiomax = 4 /3. Since down-weighting training samples in a batch effec- tively reduces learning rate and SimSiam is sensitive to it, we normalized the weight in a batch so that the mean re- mains 1 and re-tuned the learning rate (Table 8). Table 8. Soft Augmentation improve self supervised learning with SimSiam. Mean ± standard error of top-1 validation errors of three runs of ResNet-18 are reported. Task lr baseline SA#1 ∆#1 SA#2 ∆#2 Cifar100 0.1 39.50±0.13 40.21±0.03 +0.71 37 .39±0.06 −2.11 0.2 37.64±0.06 36.61±0.05 −1.03 39 .20±0.42 +1.56 0.4 40.28±2.49 37.68±0.06 −2.60 Diverged - 0.5 43.26±3.03 41.94±0.04 −1.32 Diverged - 0.8 78.88±9.05 55.44±4.15 −23.44 Diverged - Cifar10 0.2 9.87±0.03 9.31±0.01 −0.56 - - Table 9. SimSiam k tuning on Cifar-100 (single run) learning rate k Top-1 Error 0.2 1 37.78 2 37.27 3 36.34 4 36.31 Appendix C. Effects of Target Smoothing and Loss Reweighting on Loss Func- tions Consider the KL divergence loss of a single learning sample with a one-hot ground truth vector ytrue, and the softmax prediction vector of a model is denoted by ypred: L(ypred, ytrue) = w ∗ DKL(ytrue||ypred) =w ∗ NX n=1 ytrue n ∗ log(ytrue n ypred n ), (12) let n∗ be the ground truth class of an N-class classifica- tion task, Equation 12 can be re-written as: L(ypred, ytrue) = −w ∗ ytrue n∗ ∗ log(ypred n∗ ) + w ∗  ytrue n∗ ∗ log(ytrue n∗ ) + X n̸=n∗ ytrue n ∗ log(ytrue n ypred n )  . (13) In the case of hard one-hot ground truth target where ytrue n∗ = 1 and ytrue n = 0, n̸= n∗, with the default weight w = 1 it degenerates to cross entropy loss: L(ypred, ytrue) = −log(ypred n∗ ), (14) Now we apply label smoothing style softening to the one-hot target ytrue so that ytrue n∗ = p and ytrue n = (1 − p)/(N − 1) = q, n̸= n∗: L(ypred, ytrue) = −p ∗ log(ypred n∗ ) +  p ∗ log(p) + X n̸=n∗ q ∗ log( q ypred n )  . (15) If q is not distributed, and ytrue n = 0, n̸= n∗ (This con- figuration does not correspond to any of our experiments): L(ypred, ytrue) = −p ∗ log(ypred n∗ ) + p ∗ log(p), (16) When only weight w is softened to w = p: L(ypred, ytrue) = −p ∗ log(ypred n∗ ). (17) Note that p is not a function of model weights, so when we take the derivative w.r.t. model weights to compute gra- dient, Equations 16 and 17 yield the same gradient. When both the one-hot label and weight are softened with p: L(ypred, ytrue) = −p2 ∗ log(ypred n∗ ) + p ∗  p ∗ log(p) + X n̸=n∗ q ∗ log( q ypred n )  . (18) The three types of softening in Section 4 are unique as suggested by Equations 15, 17, and 18.",
      "references": [
        "A closer look at memorization in deep networks.",
        "The effects of regularization and data augmentation are class dependent.",
        "Soft-nms–improving object detection with one line of code.",
        "High-performance large-scale image recognition without normalization.",
        "Language models are few-shot learners.",
        "Exploring simple siamese representation learning.",
        "Autoaugment: Learning augmentation strategies from data.",
        "Randaugment: Practical automated data augmentation with a reduced search space.",
        "Imagenet: A large-scale hierarchical image database.",
        "Improved regular- ization of convolutional neural networks with cutout.",
        "Neocognitron: A self- organizing neural network model for a mechanism of visual pattern recognition.",
        "Accurate, large mini-batch sgd: Training imagenet in 1 hour.",
        "On calibration of modern neural networks.",
        "Deep residual learning for image recognition.",
        "Identity mappings in deep residual networks.",
        "Distilling the knowledge in a neural network.",
        "Receptive fields of single neurones in the cat’s striate cortex.",
        "Spatial transformer networks.",
        "Co-mixup: Saliency guided joint mixup with super- modular diversity.",
        "Puzzle mix: Exploiting saliency and local statistics for optimal mixup.",
        "Compositional convolutional neural networks: A deep archi- tecture with innate robustness to partial occlusion.",
        "Learning multiple layers of features from tiny images.",
        "Imagenet classification with deep convolutional neural networks.",
        "Simple and scalable predictive uncertainty estimation using deep ensembles.",
        "Deep learning.",
        "Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection.",
        "Fast autoaugment.",
        "Focal loss for dense object detection.",
        "Simple and principled uncertainty estimation with deterministic deep learning via distance awareness.",
        "Energy-based out-of-distribution detection.",
        "Learning by turning: Neural architecture aware optimi- sation.",
        "SGDR: Stochastic gradient descent with warm restarts.",
        "Object recognition from local scale-invariant features.",
        "Deep deterministic uncertainty: A simple baseline.",
        "When does label smoothing help?",
        "Trivialaugment: Tuning-free yet state-of-the-art data augmentation.",
        "Pytorch: An imperative style, high-performance deep learning library.",
        "Do imagenet classifiers generalize to imagenet?",
        "Learning to reweight examples for robust deep learning.",
        "Improved protein structure prediction using potentials from deep learning.",
        "Going deeper with convolutions.",
        "Rethinking the inception architecture for computer vision.",
        "Recurrent computations for visual pattern completion.",
        "Uncertainty estimation using a single deep deterministic neural network.",
        "Robust object detection under occlusion with context-aware compositionalnets.",
        "The limits of feedforward vision: Recurrent processing promotes robust object recognition when objects are degraded.",
        "Reweighting augmented samples by minimizing the maximal expected loss.",
        "Cutmix: Regularization strategy to train strong classifiers with localizable features.",
        "Wide residual networks.",
        "Barlow twins: Self-supervised learning via redundancy reduction.",
        "Delving deep into label smoothing.",
        "mixup: Beyond empirical risk minimization.",
        "Robustness of object recognition under extreme occlusion in humans and computational models."
      ],
      "meta_data": {
        "arxiv_id": "2211.04625v2",
        "authors": [
          "Yang Liu",
          "Shen Yan",
          "Laura Leal-Taixé",
          "James Hays",
          "Deva Ramanan"
        ],
        "published_date": "2022-11-09T01:04:06Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Soft Augmentation generalizes data augmentation by softening the learning target as a function of the transform's degree, modeling transform-induced information loss inspired by human vision. It enables more aggressive augmentations, yields stronger accuracy gains, improves occlusion robustness, calibrates better, and extends to self-supervised learning.",
        "methodology": "Introduce adaptive target softening gα(ϕ) with α(ϕ) determined by image visibility v and a non-linear curve parameter k. For cropping, v(tx,ty) computed from crop geometry; p = 1 − (1 − pmin)(1 − v)^k. Variants include Soft Target, Soft Weight, and Soft Target & Weight. Also explore SA#1 and SA#2 for self-supervised SimSiam using IoU between crops. Evaluate in CIFAR-10/100 and ImageNet; combine with RandAugment and TrivialAugment; compare against Mixup, Cutout, Online Label Smoothing, Focal Loss; show occlusion and calibration improvements.",
        "experimental_setup": "Datasets: CIFAR-10/100, ImageNet-1K, ImageNet-V2; Models: ResNet-18/50, WideResNet-28, EfficientNet-B0, PyramidNet; Training: CIFAR-100 with 500 epochs, cosine LR, batch 128; CIFAR-10 similarly; ImageNet-1K with batch 256 on 4x V100 GPUs, LR 0.1 with 5-epoch warmup, cosine decay over 270 epochs; occlusion tests on ImageNet validation with 0–80% occlusion; evaluation metrics: Top-1/Top-5 error, calibration error (ECE); self-supervised: SimSiam with two crops; training details; Ablations for k=2, σ=0.3; results summarized: SA doubles RandAugment gains; SA+RA achieves large occlusion gains; SA reduces ECE when combined with TA; etc.",
        "limitations": "Assumptions underlying Eq.7: information loss is a function of visibility; original label has 100% confidence; a single confidence-visibility curve approximates all images. Limitations include potential class-dependent augmentation effects, need for hyperparameter tuning, complexity when combining multiple transforms, and uncertain generalization to non-vision tasks; possible mismatch with per-sample or per-class augmentations.",
        "future_research_directions": "Extend soft augmentation to other transforms and tasks; develop per-class or per-sample adaptive softening; automate softening curve search with reinforcement learning or policy search; apply to object detection, NLP; explore combining with loss reweighting; investigate theoretical links to calibration; optimize computational efficiency; design diagnostics using human occlusion data.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Frozen Feature Augmentation for Few-Shot Image Classification",
      "full_text": "Frozen Feature Augmentation for Few-Shot Image Classification Andreas B¨ar1 2 * Neil Houlsby1 Mostafa Dehghani1 Manoj Kumar1 † 1Google DeepMind 2Technische Universit¨at Braunschweig andreas.baer@tu-braunschweig.de {neilhoulsby, dehghani, mechcoder}@google Abstract Vision foundation models are currently one of the main driving forces in computer vision research. Simply training a linear classifier or a lightweight model on top of model outputs or so-called ‘frozen features’ leads to impressive performance on a number of tasks. Currently, frozen fea- tures are not modified during training of such lightweight models. On the other hand, when networks are trained di- rectly on images, data augmentation is a standard recipe that improves performance with no additional overhead. In this paper, we conduct an extensive pilot study that ex- plores applying data augmentations in the frozen feature space for few-shot image classification. We dub this type of augmentation ‘frozen feature augmentation (FroFA)’. Our study demonstrates that adopting deceptively simple point- wise FroFAs, such as brightness, can improve few-shot per- formance consistently across three network architectures, three large pretraining datasets, and eight transfer datasets. 1. Introduction A prevalent trend now is to pretrain vision models on large datasets and adapt them downstream [5, 41, 56]. Notable, even training a simple linear layer or a light-weight model on top of vision transformer (ViT) outputs, also known as frozen features, can yield remarkable performance across a number of diverse downstream tasks [13, 19, 43]. However, there is still an interest in training ViTs to achieve good performance on ImageNet-sized [36, 52] or smaller [31, 34] datasets. In this setting, a crucial ingre- dient is data augmentation — a predefined set of simple, stochastic input transformations. Simple but effective ex- amples for image augmentations include random cropping which extracts a fixed-sized region from an image of ar- bitrary resolution, or pixel-wise modifications that change brightness, saturation, or contrast. These are complemented by more advanced augmentation strategies such as mixup [58] or RandAugment [10]. *Work conducted as Research Intern at Google DeepMind. †Project lead. 1 5 10 25 shots 0.0 2.5 5.0 top-1 acc. (abs. gains) JFT-3B 1 5 10 25 shots WebLI + SigLIP MAPwd linear probe Figure 1. Few-shot results averaged across eight test sets, in- cluding ILSVRC-2012 [14, 44]. We use cached features from an L/16 model [16] pretrained on JFT-3B [56] (left) or WebLI [5] following a sigmoid language-image pretraining (SigLIP) [57] (right). Our method, i.e., a multi-head attention pooling [30] head trained with weight decay (MAPwd) and frozen feature augmenta- tion (FroFA), shows significant gains across all shots with respect to a weight-decayed MAP, i.e., MAPwd, or an L2-regularized lin- ear probe baseline, both without FroFA. In this paper, we revisit standard image augmentation techniques in a data-constrained, few-shot frozen feature setting. In particular, we first stochastically transform frozen features and then train a lightweight model on top. Our only modification before applying image augmenta- tions on top of frozen features is a point-wise scaling such that each feature value lies in [0, 1] or [0, 255]. We investigate eighteen augmentations applied to frozen features extracted from vision transformers pretrained on JFT-3B [56], ImageNet-21k [14, 44], or WebLI [5]. We train a small lightweight multi-head attention pooling (MAP) [30, 56] head using these augmented inputs and evaluate its performance across eight downstream image classification datasets, where we on average achieve signif- icant gains (see Fig. 1). Our major insights are as follows: 1. Geometric augmentations that modify the shape and structure of two-dimensional frozen features always lead to worse performance on ImageNet. On the other hand, simple stylistic (point-wise) augmentations, such as brightness, contrast, and posterize, give steady im- 1 arXiv:2403.10519v2  [cs.CV]  26 Jul 2024provements on 1-, 5-, and 10-shot settings. 2. Unlike traditional image augmentations that apply a sin- gle randomly sampled value across the entire image, we introduce per-channel stochasticity by sampling inde- pendent random values for each channel. For example, on the 5-shot setting, we improve accuracy over a well- tuned MAP and linear probe baseline by 0.5% absolute and 0.8% absolute, respectively. 3. While FroFA provides modest but significant improve- ments on ImageNet, it excels on smaller transfer datasets. Across seven downstream datasets, FroFA out- performs the mean accuracy of the MAP baseline in the 5 shot setting by 3.2% absolute and the linear probe base- line by 4.2% absolute. 2. Related Works Transfer learning on few-shot data : State-of-the-art vi- sion models [5, 13, 16, 56] are typically pretrained on large-scale datasets, e.g., ImageNet-21k [14, 44] or ver- sions of JFT [21, 56], before transferred to other middle- scale to small-scale ones,e.g., CIFAR10 [1], ILSVRC-2012 [14, 44], or SUN397 [53, 54]. Depending on the model size, efficient transfer learning becomes a challenge. Many meth- ods have been proposed for large language models (LLMs), e.g., adapters [22], low-rank adaptation (LoRA) [23], or prompt tuning [32], of which some have been successfully adapted to computer vision [4, 17, 24, 59]. CLIP-Adapter [17] builds on the power of contrastive language-image pre- training (CLIP) [43] and combines it with adapters [22]. A follow-up work [59] proposes TiP-Adapter which uses a query-key cache model [18, 42] instead of a gradient de- scent approach. Inspired by the success of prompt tuning in LLMs [32], Jia et al. propose visual prompt tuning at the model input [24]. On the other hand, AdaptFormer [4] uses additional intermediate trainable layers to finetune a frozen vision transformer [16]. In contrast, we do not introduce additional prompts [24] or intermediate parameters [4, 17] that require backprop- agating through the network. Instead, we train a small network on top of frozen features coming from a vision transformer. This aligns with linear probing [43] which is typically used to transfer vision models to other tasks [13, 19, 56] — our objective. In addition, we focus our experiments around transfer learning on few-shot data [29, 51]. Although not surprising, few-shot results obtained by Dehghani et al . [13] clearly show significant gaps between linear probing and full fine- tuning. We take these results as an incentive to improve upon linear probing. Data augmentation: One go-to method to improve per- formance while training in a low-data regime is data aug- mentation [46]. Some prominent candidates in computer vision are AutoAugment [9], AugMix [20], RandAugment [9], and TrivialAugment [39]. These methods typically combine low-level image augmentations together to aug- ment the input. Although some works propose augmen- tations in feature space [15, 28, 33, 37, 50], a large-scale empirical study on frozen features of single-modal vision models does not exist. To this end, we investigate frozen feature augmentation (FroFA) by reformulating eighteen image augmentations. In particular, we consider a subset used in AutoAugment [9], inception crop [48], mixup [50, 58], and the recently introduced patch dropout [35]. 3. Framework Overview In this section, we give an overview of our framework. 3.1. Notation Let x ∈ IH×W×3 be an RGB image of height H, width W, and I = [0, 1]. A classification model processes x and outputs class scores y ∈ [0, 1]S for each class in a pre- defined set of classes S, with S = |S|. Let L and D be the number of intermediate layers and the number of fea- tures of a multi-layer classification model, respectively. We describe the intermediate feature representations of x as f = f(ℓ) = (f(ℓ) d ) ∈ RD, with layer index ℓ ∈ {1, ..., L} and feature index d ∈ {1, ..., D}. In the vision trans- former [26] architecture, f = f(ℓ) = (f(ℓ) n,c) ∈ RN×C is a two-dimensional entity, where N and C are the number of patches and number of per-patch channels, respectively. In addition, we introduce the patch index n ∈ {1, ..., N} and the per-patch channel index c ∈ {1, ..., C}. 3.2. Training on Cached Features We investigate pretrained vision transformers [26] with L transformer blocks (TBs) followed by a multi-head atten- tion pooling (MAP) [30] and a classification layer (CL). Fig. 2a presents a simplified illustration. For simplicity, we neglect all operations before the first transformer block (e.g., patchifying, positional embedding, etc.). To cache intermediate feature representations, we pro- cess each image x from an image dataset Dx through the network up until transformer blockL. Next, we store the re- sulting features f. After processing Dx we obtain a (frozen) feature dataset Df , with f ∈ Df (Fig. 2b). Finally, we train a lightweight model using the cached (frozen) features. Fig. 2c shows an example where a single MAP layer followed by a classification layer is trained using the feature dataset Df . Since our focus is fast training, we defer a detailed analysis on larger models to future work. 3.3. Frozen Feature Augmentation (FroFA) Data augmentation is a common tool to improve general- ization and is typically applied on the input, or in our case: 2(Frozen) Pretrained Model TB TB TB MAP CL (a) Step 1: Select a (frozen) pretrained model and a layer for caching. (Frozen) Pretrained Model image dataset (frozen) feature dataset TB TB TB (b) Step 2: Process an image dataset and cache the (frozen) features. Lightweight Model (frozen) feature dataset MAP CL frozen feature augmentation (FroFA)  (c) Step 3: Train on (augmented) frozen features. Figure 2. Pipeline for caching and training on (frozen) fea- tures. (2a): Given a (frozen) pretrained vision transformer, withL Transformer blocks (TBs), a multi-head attention pooling (MAP) layer, and a classification layer (CL), we select its L-th Trans- former block for caching. (2b): Next, we feed images x ∈ Dx to cache (frozen) features f ∈ Df . (2c): Finally, we use Df to train a lightweight model on top. We investigate frozen feature augmentation (FroFA) af ∈ Af in this scenario. images. A natural question arises: How to map such image augmentations to intermediate feature representations? Recall that the feature representation f = (fn,c) ∈ RN×C (layer index ℓ omitted) is two-dimensional. We first reshape it to a three-dimensional representation, i.e., f∗ = (f∗ n1,n2,c) ∈ R √ N× √ N×C. (1) We further define f∗ c = f∗ :,:,c ∈ R √ N× √ N×1 (2) as a two-dimensional representation of the c-th channel. Images and feature representations differ in two funda- mental aspects: channel dimensionality and value range. Before adapting image augmentations to the feature space, it is crucial to handle these differences. Channel dimensionality: RGB images have just three channels while intermediate representations possess an ar- bitrary number of channels. To address this, we ignore im- age augmentations that rely on three color channels, e.g., color jitter, and consider augmentations which can have an arbitrary number of channels instead, denoted asCa, cover- ing a majority of commonly applied image augmentations. Value range: RGB values lie within a specific range I, e.g., I = [0, 1] or I = {0, ...,255} ⊂N0, while in theory features have no such constraints. Assuming H = √ N and W = √ N, we define an image augmentation as ax : I √ N× √ N×Ca → I √ N× √ N×Ca , ax ∈ Ax, (3) where Ax is the set of image augmentations andCa = C is an arbitrary number of channels. To also address the value range mismatch, we introduce a deterministic feature-to- image mapping tf→x : R √ N× √ N×Ct → I √ N× √ N×Ct (4) that maps each element of f∗ (1) from R to I. In our exper- iments, we use xf = tf→x(f∗) = f∗ − fmin fmax − fmin , (5) where fmin and fmax are the minimum and maximum value of f∗, respectively, with elements of xf now in I = [0, 1]. We further define an image-to-feature mapping tf←x : I √ N× √ N×Ct → R √ N× √ N×Ct (6) that maps xf back to the original feature value range, with Ct = C by default. In this case, we simply invert (4) and use f∗ = tf←x(xf ) =xf · (fmax − fmin) +fmin. (7) Combining (3), (4), and (6), we obtain a generic (frozen) feature augmentation (FroFA) as a function composition af = tf←x ◦ ax ◦ tf→x. (8) We use three variations of af : 1. (Default) FroFA: We applyaf (8) once across the entire feature representation. We set Ca = Ct = C and com- pute fmin and fmax in (5), (7) across all elements of f∗. Further, as normally done in pixel space,ax (3) samples a random augmentation value and changes all elements of xf using the same value. For example, employing random contrast in a FroFA fashion scales each element of xf by the exact same randomly sampled factor. 2. Channel FroFA (cFroFA) : For each channel in the mapped features xf (5), ax (3) samples a random aug- mentation value per channel and applies that value to all elements in that channel. By using cFroFA for our ran- dom contrast example, we obtain C independently sam- pled scaling factors, one for each channel. 3. Channel2 FroFA (c2FroFA): In addition to applying augmentations per channel as done in cFroFA,tf→x (4) and tx←f (6) also operate per channel. In this case,fmin and fmax are the per-channel maximum and minimum, respectively. In contrast, FroFA and cFroFA use the maximum and minimum across the entire feature. We 3denote this variant as c 2FroFA since both the mappings (4), (6) and the augmentation (3) are applied on a per- channel basis. Although not adding additional stochas- ticity, we found that for random brightness this variant gives more stable results across a range of augmentation hyper parameters. While an element-wise FroFA might seem like a natural next step, our initial experiments lead to significantly worse results. We hypothesize that per-element augmentations might lead to substantial changes in the feature appearance. 4. Experimental Setup In this section, we introduce our experimental setup. 4.1. Network Architectures We employ the following pretrained vision transformers from prior work: Ti/16 [49], B/16 [16], and L/16 [16]. Fur- ther, we follow [56] and employ a lightweight multi-head attention pooling (MAP) layer [30] before the final classifi- cation layer on top of the frozen features (cf . Sec. 3.3). 4.2. Datasets Pretraining: We consider three datasets: JFT-3B, ImageNet-21k, and WebLI. First introduced by Hinton et al. [21], JFT is now a widely used proprietary, large-scale dataset [5, 7, 11, 16, 26, 27, 47, 56]. For our investigations we use the JFT-3B version following Zhai et al . [56]. It consists of nearly 3 billion multi-labeled images following a class-hierarchy of 29,593 labels. We further use ImageNet- 21k [14, 44] which consists of 14,197,122 (multi)-labeled images and 21,841 distinct labels. We equally split the first 51,200 images into a validation and test set and use the remaining 14,145,922 images for training. As a third dataset, we use WebLI [5] which is a recently introduced web-scale multilingual image-text dataset. Please refer to the Appendix, Sec. A3.1, for more details. Few-shot transfer : After pretraining we use eight datasets for few-shot transfer: ILSVRC-2012 [14, 44], CI- FAR10 [1], CIFAR100 [1], DMLab [2, 55], DTD [8], Re- sisc45 [6], SUN397 [53, 54], and SVHN [40]. ILSVRC-2012, also known as ImageNet-1k, is a slimmed version of ImageNet-21k and contains 1,281,167 training images of 1,000 classes. We use it as our main few-shot benchmark throughout the paper. We randomly sample 1-shot, 5-shot, 10-shot, and 25-shot versions from the first 10% of the training set. We further create addi- tional disjoint sets by using the next four 10% fractions of the training set. In addition, we follow previous works [3] and create a ‘minival’ set using the last 1% (12,811 images) of the ILSVRC-2012 training set. The ‘minival’ set is used for hyper parameter tuning and design decisions while the official ILSVRC-2012 validation set is used as a test set. In summary, our setup consists of 1,000, 5,000, 10,000, or 25,000 training images, 12,811 validation images (‘mini- val’), and 50,000 test images (‘validation’). For the other seven datasets, we also select a training, validation, and test split and create few-shot versions. More details on how these splits are created can be found in the Appendix, Sec. A3.1. We follow a similar procedure as with ILSVRC-2012 and use 10% of the training images to cre- ate 1-shot, 5-shot, 10-shot, and 25-shot versions of each dataset. We further use each validation set for hyper pa- rameter tuning and report final results on the respective test set. 4.3. Data Augmentation We reuse the set of augmentations first defined in AutoAug- ment [9] and adopted in later works, such as RandAugment [10] and TrivialAugment [39]. In addition, we also consider a few other image augmentations [35, 48, 58]. We select five geometric augmentations, i.e., rotate, shear-x, shear-y, translate-x, and translate-y; four crop & drop augmenta- tions, i.e., crop, resized crop, inception crop [48], and patch dropout [35]; seven stylistic augmentations, i.e., brightness, contrast, equalize, invert, posterize, sharpness, and solarize; and two other augmentations, i.e., JPEG and mixup [58]. In total, we end up with eighteen distinct augmentations . Note that all data augmentations incorporate random oper- ations, e.g., a random shift in x- and y-direction (translate- x and translate-y, respectively), a randomly selected set of patches (patch dropout), a random additive value to each feature (brightness), or a random mix of two features and their respective classes (mixup). Please refer to the Ap- pendix, Sec. A3.2, for more details. We focus on the following set of experiments: 1. We investigate FroFA for all eighteen augmentations. 2. For our top-performing FroFAs, namely, brightness, contrast, and posterize, we incorporate additional stochasticity using cFroFA and c 2FroFA variants ( cf . Sec. 3.3). 3. We investigate a sequential protocol where two of the best three (c/c 2)FroFAs are arranged sequentially, namely, brightness c 2FroFA, contrast FroFA, and pos- terize cFroFA. We test all six possible combinations. 4. Finally, we also apply variations of RandAugment [10] and TrivialAugment [39] directly on top of cached frozen features. More details and results can be found in the Appendix, Secs. A3.2 and A4, respectively. 4.4. Training & Evaluation Details We describe some base settings for pretraining, few- shot learning, and evaluation. Please refer to Appendix, Sec. A3.3 for more training details. Pretraining: We use the Big Vision code base for https://github.com/google-research/big_vision 4pretraining. We take the Ti/16, B/16, and L/16 models pre- trained on JFT-3B from Zhai et al. [56]. In addition, we pretrain Ti/16, B/16 and L/16 on ImageNet-21k following the settings of Steiner et al. [46]. To further explore trans- fer capabilities we also use an L/16 model with sigmoid language-image pretraining (SigLIP) [57] on WebLI [5]. Few-shot learning: We use the Scenic code base [12] for few-shot learning. We train the lightweight MAP-based head by sweeping across five batch sizes (32, 64, 128, 256, and 512), four learning rates (0.01, 0.03, 0.06, and 0.1), and five training step sizes (1,000; 2000; 4,000; 8,000; and 16,000), yielding 100 configurations for each shot. We use the respective validation set for early stopping and to find the best sweep setting. Our cached-feature setup fits on a single-host TPUv2 platform where our experiments run in the order of minutes. Evaluation: We report the top-1 accuracy across all our few-shot datasets. On ILSVRC-2012, we tune few-shot models exclusively on our validation set (our ILSVRC-2012 ‘minival’, cf . Sec. 4.2) and report results on our test set (of- ficial ILSVRC-2012 ‘validation’ set, cf . Sec. 4.2). 4.5. Baseline Models We establish two baselines: MAP and linear probe. MAP: We first cache theN×C-shaped (frozen) features from the last transformer block. Afterwards, we train a lightweight MAP head from scratch using the cached fea- tures followed by the final classification layer ( cf . Fig. 2). For simplicity, the MAP head follows the same architectural design as the underlying pretrained model. In some exper- iments, we additionally apply weight decay (wd), denoted as MAPwd. We sweep across [ADD V ALUES] and use the respective validation set for early stopping and to find the best sweep setting. Linear probe: We use cached1×C-shaped outputs from the pretrained MAP head to solve an L2-regularized regres- sion problem with a closed-form solution [56]. We sweep the L2 decay factor using exponents of 2 ranging from -20 up to 10. This setting is our auxiliary baseline. 5. Finding the Optimal FroFA Setup We focus our first investigations on an L/16 model pre- trained on JFT-3B, i.e., our largest model and largest im- age classification pretraining dataset, followed by few-shot learning on subsets of ILSVRC-2012 training set, i.e., our largest few-shot dataset. We will refer to this setup as our L/16 JFT-3B base setup. 5.1. Baseline Performance We first report the baseline performance in Tab. 1. We ob- serve a large gap between MAP and linear probe in the 1- https://github.com/google-research/scenic Method 1-shot 5-shot 10-shot 25-shot MAP 57.9 78.8 80.9 83.2 Linear probe 66.5 79.6 81.5 82.4 Table 1. Average top-1 accuracy for baseline settings on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup ( cf . Sec. 5) and follow the respective baseline setting ( cf . Sec. 4.5). The best setting for each baseline is found using our ILSVRC- 2012 validation set. Further, each shot is sampled five times. The best result per shot is boldfaced. shot setting (-8.6% absolute) which significantly decreases in the 5-, 10-, and 25-shot settings to -0.8%, -0.6%, and +0.8% absolute, respectively. In the following, our main point of comparison is the MAP baseline. This might be counter-intuitive since the performance is worse than linear probe in most cases. How- ever, the higher input dimensionality in the MAP-based set- ting (cf . Sec. 4.5) gives us the option to reshape the input to three dimensions ( cf . Sec. 3.3) which opens up more room and variety for frozen feature augmentations (Fro- FAs). Later in Sec. 6.4, we compare the performance of our best augmentations to the linear probe baseline. 5.2. Default FroFA As a next step, we investigate the effect of adding a single FroFA to the MAP baseline setting. We first focus on the default FroFA formulation which uses a single randomly sampled value per input ( cf . Sec. 3.3). Results are shown in Tab. 2 where we report gains with respect to the MAP baseline using eighteen distinct FroFAs categorized into ge- ometric, crop & drop, stylistic, and other. Geometric: Interestingly, all geometric augmentations consistently lead to worse performance across all settings. Crop & drop: A simple crop or a resized crop yield a significant performance boost in the 1-shot setting of +3.0% and +1.9% absolute, respectively. Further, patch dropout provides modest gains in the 1-shot regime. Dropping patches is related to training efficiency, so we investigate this further. Fig. 3a shows the top-1 accuracy on 1- and 25- shot as a function of number of patches. More results can be found in Appendix, Sec. A4.1. Similar to observations by Liu et al. [35] we can randomly drop a large fraction of patches (>50%) without loosing performance. A key dif- ference is that Liu et al. only investigated the effect in the image space, while we provide evidence that patch dropout also transfers to the feature space. Finally, inception crop does not improve performance. Stylistic: The largest gains can be observed when em- ploying a stylistic FroFA, in particular brightness, contrast, and posterize. We identified brightness as the best perform- ing FroFA with absolute gains of 4.8% on 1-shot, 1.1% on 5-shot, and up to 0.6% on 10-shot. 5Geometric Crop & drop Stylistic Other Shots MAP rotate shear-x shear-y translate-x translate-y crop res. crop incept. crop patch drop. brightness contrast equalize invert posterize sharpness* solarize* JPEG* mixup 1 57.9 −1.3 −0.6 −0.8 −1.2 −1.4 +3.0 +1.9 +0.0 +0.4 +4.8 +2.8 +1.0 +2.7 +3.7 −0.1 +1.0 −0.1 −1.4 5 78.8 −0.3 −0.2 −0.2 −0.3 −0.3 +0.0 −0.2 +0.0 +0.0 +1.1 +0.8 +0.5 −0.3 +0.8 +0.1 −0.1 −0.3 −0.3 10 80.9 −0.2 −0.1 −0.1 −0.2 −0.2 +0.0 −0.2 +0.0 +0.0 +0.6 +0.6 +0.4 +0.0 +0.6 +0.1 +0.0 −0.1 +0.2 25 83.2 −0.2 −0.1 −0.2 −0.1 −0.2 +0.0 −0.1 −0.1 +0.0 +0.1 +0.1 +0.0 −0.2 +0.0 +0.0 +0.0 +0.0 +0.1 Table 2. (Average) top-1 accuracy for default FroFA on our ILSVRC-2012 test set. Absolute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). In total, we investigate eighteen FroFAs, categorized intogeometric, crop & drop, stylistic, and other. We sweep across a base sweep ( cf . Sec. 4.4) and the respective augmentation sweep (cf . Appendix, Sec. A3.2) to first find the best setting on our ILSVRC-2012 validation set. Each shot is sampled five times, except for JPEG, sharpness, and solarize (marked with ‘*’). We highlight deterioration by shades of red and improvement by shades of green . Best three FroFAs are boldfaced. 1 50 100 150 number of patches 52 54 56 58top-1 accuracy 1-shot 1 50 100 150 number of patches 80 81 82 83 25-shot MAP + patch dropout FroFA (a) Patch dropout FroFA 0.1 0.3 0.5 0.7 0.9 brightness level 50 55 60 65top-1 accuracy 1-shot 0.1 0.3 0.5 0.7 0.9 brightness level 81.0 81.5 82.0 82.5 83.0 83.5 25-shot + brightness cFroFA + brightness c2FroFA (b) Channel variants (c/c2) of brightness FroFA Figure 3. Average top-1 accuracy for FroFA variantson our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each FroFA operation point (cf . Appendix, Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times. Brightness Contrast Posterize Shots MAP c c 2 c c 1 57.9 +4.8 +5.9 +6.1 +2.8 +2.5 +3.7 +5.9 5 78.8 +1.1 +1.5 +1.6 +0.8 +0.0 +0.8 +0.8 10 80.9 +0.6 +1.1 +0.9 +0.6 +0.0 +0.6 +0.5 25 83.2 +0.1 +0.4 +0.3 +0.1 −0.1 +0.0 +0.0 Table 3. Average top-1 accuracy for a selection of default ( ) and channel (c/c 2) FroFA on our ILSVRC-2012 test set. Ab- solute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep (cf . Sec. 4.4) and the respective augmentation sweep ( cf . Appendix, Sec. A3.2) to first find the best setting on our ILSVRC-2012 val- idation set. Each shot is sampled five times. The best results per shot and FroFA are boldfaced (multiple ones if close, i.e., ±0.2). Other: Neither JPEG nor mixup yield performance gains but rather more or less worsen the performance. 5.3. Channel FroFA Next, we investigate channel FroFA (cFroFA) for bright- ness, contrast, and posterize. Results are shown in Tab. 3, where we report absolute gains with respect to the MAP baseline. First, contrast cFroFA worsens performance across all shots. Second, posterize cFroFA improves perfor- mance on 1-shot from +3.7% to +5.9% while maintaining performance on all other shots. Lastly, brightness cFroFA significantly improves performance across all shots, i.e., from +4.8% to +5.9% on 1-shot, from +1.1% to +1.5% on 5-shot, from +0.6% to +1.1% on 10-shot, and from +0.1% to +0.4% on 25-shot. Giving the strong improvements for brightness cFroFA, we further test brightness c 2FroFA (see Tab. 3). On a first look, both variants perform equally well. In Fig. 3b, we further report the top-1 accuracy on 1-shot and 25-shot as a function of the brightness augmentation level. Results across other shots are similar and can be found in Appendix, Sec. A4.1. We clearly observe that brightness cFroFA is much more sensitive to the brightness level than brightness c2FroFA. Aross all shots, brightness cFroFA only works well for small brightness levels (0.1 to 0.5), while the c2FroFA variant performs better than the MAP baseline across the board. We attribute the better sensitivity prop- 6erties of brightness c2FroFA to the channel-wise mappings (5), (7) since this is the only change between cFroFA and c2FroFA. We did not a observe similar effect when switch- ing from cFroFA posterize to c2FroFA posterize. 5.4. Sequential FroFA Finally, out of our best three augmentations, i.e., bright- ness c 2FroFA (B-c 2), contrast FroFA (C), and posterize cFroFA (P-c), we combine two of them sequentially. We end up with a total of six combinations. Tab. 4 compares the performance of these six combinations against our prior best (B-c 2). On 1-shot, (B-c 2→P-c) significantly outper- forms (B-c2), improving absolute gains from 6.1% to 7.7%, while maintaining performance on other shots. We con- clude that advanced FroFA protocols may further improve performance. As an initial investigation, we applied varia- tions of RandAugment and TrivialAugment using our best three FroFAs ( cf . Tab. 3), however, with limited success. We include results in the Appendix, Sec. A4.2, and leave a deeper investigation to future works. 6. FroFA on More Datasets and Architectures How well does our best non-sequential augmentation strat- egy (brightness c 2FroFA) transfer across multiple dataset and architectures settings? In Secs. 6.1 to 6.3, we report results on seven other downstream few-shot datasets, two additional architectures, and two additional pretraining se- tups, respectively. This time, however, we also incorpo- rate weight decay in all MAP-based models . Further, in Secs. 6.2 and 6.3, we solely focus on the improvements over the MAP baseline and include a discussion on the improve- ments over the linear probe baseline in Secs. 6.1 and 6.4. 6.1. Transfer to Other Downstream Datasets In Tab. 5, we report results on seven additional transfer datasets, i.e., CIFAR10, CIFAR100, DMLab, DTD, Re- sisc45, SUN397, and SVHN. We compare the weight- decayed MAP and L2-regularized linear probe baseline to our approach, i.e., weight-decayed MAP combined with brightness c2FroFA (MAPwd + FroFA). We observe that across almost all shots and transfer datasets, MAP wd + FroFA shows the best results. Moreover, MAP wd + FroFA outperforms L2-regularized linear probe with only one exception, i.e., SUN397 (1-shot). With respect to the mean across all seven datasets, MAP wd + FroFA is signifi- cantly better than MAPwd, with improvements ranging from +4.4% absolute on 1-shot to +1.0% absolute on 25-shot. Fig. 1, left, displays the absolute accuracy gains averaged across all eight transfer datasets, including ILSVRC-2012. As before, our approach, i.e., MAPwd + FroFA, yields the best results across all shots. We further observe that the gains decrease with higher shots which aligns with our pre- vious observations. Shots MAP B-c 2 B-c2→C C→ B-c2 B-c2→P-c P-c→ B-c2 C→P-c P-c→C 1 57.9 +6.1 +4.0 +2.7 +7.7 +5.2 +5.0 +3.1 5 78.8 +1.6 +1.5 +0.2 +1.5 +0.4 +1.3 +0.0 10 80.9 +0.9 +1.2 +0.1 +1.0 +0.1 +0.9 +0.3 25 83.2 +0.3 +0.4 −0.7 +0.2 −0.5 +0.2 −0.4 Table 4. Average top-1 accuracy for a sequential FroFA pro- tocol on our ILSVRC-2012 test set. Absolute gains to the MAP baseline are reported. We use the L/16 JFT-3B base setup ( cf . Sec. 5). We combine the best settings of brightness c 2FroFA (B- c2), contrast FroFA (C), and posterize cFroFA (P-c) sequentially (two at a time, order indicated by ‘ ↑’). We sweep across a base sweep (cf . Sec. 4.4) to first find the best setting on our ILSVRC- 2012 validation set. Each shot is sampled five times. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Trans. dataset Method 1-shot 5-shot 10-shot 25-shot CIFAR10 MAPwd 85.1 96.7 97.1 97.5 Linear probe 80.9 94.1 96.7 97.3 MAPwd + FroFA 93.8 97.6 97.8 97.8 CIFAR100 MAPwd 63.1 82.7 85.5 86.8 Linear probe 58.4 80.9 83.8 85.1 MAPwd + FroFA 67.8 84.0 86.2 87.1 DMLab MAPwd 24.4 30.3 30.2 36.5 Linear probe 24.0 26.3 25.6 30.9 MAPwd + FroFA 27.1 29.4 30.3 36.8 DTD MAPwd 49.2 68.2 74.1 80.8 Linear probe 46.9 65.9 71.3 77.3 MAPwd + FroFA 53.5 70.7 76.1 82.2 Resisc45 MAPwd 63.2 86.9 89.8 90.7 Linear probe 67.1 85.6 88.2 91.0 MAPwd + FroFA 67.6 87.2 89.7 91.5 SUN397 MAPwd 51.3 73.5 77.7 80.3 Linear probe 56.7 70.9 75.6 78.6 MAPwd + FroFA 56.2 75.9 78.9 81.2 SVHN MAPwd 20.7 23.9 30.2 47.4 Linear probe 11.8 15.0 18.7 21.5 MAPwd + FroFA 21.8 31.0 43.5 50.3 Mean MAPwd 51.0 66.0 69.2 74.3 Linear probe 49.1 62.7 65.7 68.8 MAPwd + FroFA 55.4 68.0 71.8 75.3 Table 5. Top-1 accuracy of our best FroFA for additional transfer datasets using a JFT-3B L/16 model. Results are re- ported on the respective test set ( cf . Sec. A3.1). We compare results to a weight-decayed MAP baseline, i.e., MAP wd, and an L2-regularized linear probe. Depending on the setting, we sweep across a base,cf . Sec. 4.4, a weight decay or L2 decay,cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on the respective validation set. Per shot and dataset, the best result is boldfaced while the second-best result is underlined (multiple ones if close, i.e., ±0.2). 7Ti/16 B/16 L/16 model −10 −5 0 top-1 acc. (abs. gains) 1-shot Ti/16 B/16 L/16 model −0.5 0.0 0.5 5-shot Ti/16 B/16 L/16 model 0.00 0.25 0.50 0.75 1.00 1.25 10-shot Ti/16 B/16 L/16 model 0 1 2 3 4 5 25-shot MAPwd linear probe (a) JFT-3B Ti/16 B/16 L/16 model −20 −15 −10 −5 0 top-1 acc. (abs. gains) 1-shot Ti/16 B/16 L/16 model 0.0 0.5 1.0 1.5 2.0 5-shot Ti/16 B/16 L/16 model 0.0 0.5 1.0 1.5 2.0 10-shot Ti/16 B/16 L/16 model 0 1 2 3 4 25-shot (b) ImageNet21k Figure 4. Average top-1 accuracy of brightness c2FroFA for JFT-3B (a) and ImageNet-21k (b) models on our ILSVRC-2012 test set trained on few-shotted ILSVRC-2012 training sets. Absolute gains to the weight-decayed MAP, i.e. MAPwd, and L2-regularized linear probe baseline are reported. Depending on the setting, we sweep across a base, cf . Sec. 4.4, a weight decay or L2 decay, cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. 6.2. Transfer to Other Architectures We employ brightness c2FroFA on two other JFT-pretrained models, namely Ti/16 and B/16. In Fig. 4a, we report im- provements in top-1 accuracy with respect to the weight- decayed MAP baseline. Across all shots and model archi- tectures, incorporating FroFA either maintains or improves performance, except for B/16, 25-shot. Given that larger models tend to be more prone to overfitting in the 1-shot setting, we observe increasing improvements from FroFA when scaling the architecture. With a higher number of shots, the observed improvements over the baseline model become smaller. We attribute this to the strong baseline per- formance leaving lesser headroom for improvements. We refer to the Appendix, Sec. A4.3, for the exact values. 6.3. Transfer to Other Pretraining Setups ImageNet-21k: In Fig. 4b, we report improvements in top- 1 accuracy with respect to the weight-decayed MAP base- line for ImageNet-21k-pretrained Ti/16, B/16, and L/16. Consistent with our JFT-3B observations, across all shots and model architectures, incorporating FroFA either main- tains or improves performance. The improvements dimin- ish as the number of shots increases. This trend is likely due to the higher baseline accuracies at higher shot counts. We again refer to the Appendix, Sec. A4.3, for the exact values. WebLI and SigLIP : We also tested an L/16 model with sigmoid language-image pretraining (SigLIP), follow- ing [57]. We report the absolute accuracy gains averaged across eight datasets. The results are shown in Fig. 1, right. From the results we can conclude that our FroFA setting also transfers to language-image pretrained models further emphasizing its generalizability. 6.4. Linear Probe Comparison on ILSVRC-2012 We will now look at Figs. 4a and 4b, but discuss gains with respect to the L2-regularized linear probe baseline. We start with models pretrained on JFT-3B (cf . Fig. 4a). On 1-shot, we observe that we lack behind linear probe but can close the gap by scaling up the model size. On 5- to 25-shot, with the exception of Ti/16 on 5-shot, brightness c 2FroFA significantly outperforms the linear probe baseline. On ImageNet-21k (cf . Fig. 4b), we observe even larger gaps to linear probe on 1-shot (up to -20% absolute). How- ever, similar to results on JFT-3B, performance on 5- to 25-shot improves significantly over linear probe or at worst stays the same. 7. Conclusions We investigated eighteen frozen feature augmentations (FroFAs) along three axes: model size, pretraining and transfer few-shot dataset. We show that a training with Fro- FAs, in particular stylistic ones, gives large improvements upon a representative baseline across all shots. In addition, per-channel variants further improve performance, e.g., by 1.6% absolute in the ILSVRC-2012 5-shot setting. Finally, we were able to show that our results transfer. Averaged results across seven downstream tasks show that using a variant of brightness FroFA improves by 4.4% absolute upon the same representative baseline in the 1-shot setting. 8References [1] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images, 2009. 2, 4, 12 [2] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K ¨uttler, Andrew Lefrancq, Simon Green, V ´ıctor Vald ´es, Amir Sadik, Julian Schrit- twieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hass- abis, Shane Legg, and Stig Petersen. DeepMind Lab. arXiv, 1612.03801:1–11, 2016. 4, 12 [3] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Bet- ter Plain ViT Baselines for ImageNet-1k.arXiv, 2205.01580: 1–3, 2022. 4 [4] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. AdaptFormer: Adapting Vision Transformers for Scalable Visual Recogni- tion. In Proc. of NeurIPS, pages 16664–16678, New Orleans, LA, USA, 2022. 2 [5] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Brad- bury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. PaLI: A Jointly-Scaled Multilingual Language- Image Model. InProc. of ICLR, pages 1–33, Kigali, Rwanda, 2023. 1, 2, 4, 5, 12 [6] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote Sens- ing Image Scene Classification: Benchmark and State of the Art. Proc. IEEE, 105(10):1865–1883, 2017. 4, 12 [7] Franc ¸ois Chollet. Xception: Deep Learning With Depthwise Separable Convolutions. In Proc. of CVPR , pages 1063– 6919, Honolulu, HI, USA, 2017. 4 [8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing Textures in the Wild. In Proc. of CVPR, pages 3606–3613, Columbus, OH, USA, 2014. 4, 12 [9] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Va- sudevan, and Quoc V . Le. AutoAugment: Learning Aug- mentation Strategies From Data. In Proc. of CVPR , pages 113–123, Long Beach, CA, USA, 2019. 2, 4 [10] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. RandAugment: Practical Automated Data Augmenta- tion with a Reduced Search Space. In Proc. of NeurIPS , pages 18613–18624, virtual, 2020. 1, 4, 13 [11] Zihang Dai, Hanxiao Liu, Quoc V . Le, and Mingxing Tan. CoAtNet: Marrying Convolution and Attention for All Data Sizes. In Proc. of NeurIPS, pages 3965–3977, virtual, 2021. 4 [12] Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and Yi Tay. Scenic: A JAX Library for Computer Vision Research and Beyond. In Proc. of CVPR, pages 21393–21398, New Orleans, LA, USA, 2022. 5 [13] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul- mohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschan- nen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Ku- mar, Sjoerd Van Steenkiste, Gamaleldin Fathy Elsayed, Ar- avindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vigh- nesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling Vision Transformers to 22 Billion Parameters. In Proc. of ICML , pages 7480–7512, Honolulu, HI, USA, 2023. 1, 2, 12 [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In Proc. of CVPR , pages 248–255, Miami, FL, USA, 2009. 1, 2, 4, 12 [15] Terrance DeVries and Graham W. Taylor. Dataset Augmen- tation in Feature Space. In Proc. of ICLR - Workshops, pages 1–12, Toulon, France, 2017. 2 [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In Proc. of ICLR, pages 1–21, virtual, 2021. 1, 2, 4 [17] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. CLIP- Adapter: Better Vision-Language Models with Feature Adapters. Int. J. Comput. Vis., pages 1–15, 2023. 2 [18] Edouard Grave, Armand Joulin, and Nicolas Usunier. Im- proving Neural Language Models with a Continuous Cache. In Proc. of ICLR, pages 1–9, Toulon, France, 2017. 2 [19] Xuehai He, Chuanyuan Li, Pengchuan Zhang, Jianwei Yang, and Xin Eric Wang. Parameter-Efficient Model Adaptation for Vision Transformers. In Proc. of AAAI, pages 817–825, Washington, DC, USA, 2023. 1, 2 [20] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty. In Proc. of ICLR, pages 1–15, Virtual, 2020. 2 [21] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling Knowledge in a Neural Network. In proc. of NIPS - Work- shops, pages 1–9, Montr´eal, QC, Canada, 2014. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 2, 4 [22] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP. In Proc. of ICML , pages 2790–2799, Long Beach, CA, USA, 2019. 2 [23] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In Proc. of ICLR, pages 1–13, virtual, 2022. 2 [24] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi- 9sual Prompt Tuning. In Proc. of ECCV, pages 709–727, Tel Aviv, Israel, 2022. 2 [25] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proc. of ICLR, pages 1–15, San Diego, CA, USA, 2015. 13 [26] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big Transfer (BiT): General Visual Representation Learning. In Proc. of ECCV, pages 491–507, virtual, 2020. 2, 4 [27] Jannik Kossen, Mark Collier, Basil Mustafa, Xiao Wang, Xiaohua Zhai, Lucas Beyer, Andreas Steiner, Jesse Berent, Rodolphe Jenatton, and Efi Kokiopoulou. Three Towers: Flexible Contrastive Learning with Pretrained Image Mod- els. arXiv, 2112.13492:1–32, 2023. 4 [28] Varun Kumar, Hadrien Glaude, Cyprien de Lichy, and Wl- liam Campbell. A Closer Look At Feature Space Data Aug- mentation For Few-Shot Intent Classification. In Proc. of EMNLP - Workshops, pages 1–10, Hong Kong, China, 2019. 2 [29] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. The Omniglot Challenge: a 3-year Progress Re- port. Curr. Opin. Behav. Sci., 29:97–104, 2019. 2 [30] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se- ungjin Choi, and Yee Whye Teh. Set Transformer: A Frame- work for Attention-based Permutation-Invariant Neural Net- works. In Proc. of ICML , pages 3744–3753, Long Beach, CA, USA, 2019. 1, 2, 4 [31] Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song. Vision Transformer for Small-size Datasets. arXiv, 2112.13492:1–11, 2021. 1 [32] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient Prompt Tuning. In Proc. of EMNLP, pages 3045–3059, virtual, 2021. 2 [33] Xiaofeng Liu, Yang Zou, Lingsheng Kong, Zhihui Diao, Junliang Yan, Jun Wang, Site Li, Ping Jia, and Jane You. Data Augmentation via Latent Space Interpolation for Image Classification. In Proc. of ICPR , pages 728–733, Beijing, China, 2018. 2 [34] Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri, and Marco De Nadai. Efficient Training of Visual Transformers With Small Datasets. In Proc. of NeurIPS , pages 1–13, virtual, 2021. 1 [35] Yue Liu, Christos Matsoukas, Fredrik Strand, Hossein Az- izpour, and Kevin Smith. PatchDropout: Economizing Vi- sion Transformers Using Patch Dropout. In Proc. of WACV, pages 3942–3951, Waikoloa, HI, USA, 2023. 2, 4, 5 [36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows. In Proc. of ICCV, pages 10012–10022, virtual, 2021. 1 [37] Zichang Liu, Zhiqiang Tang, Xingjian Shi, Aston Zhang, Mu Li, Anshumali Shrivastava, and Andrew Gordon Wilson. Learning Multimodal Data Augmentation in Feature Space. In Proc. of ICLR, pages 1–15, Kigali, Rwanda, 2023. 2 [38] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In Proc. of ICLR, pages 1–18, New Orleans, LA, USA, 2019. 13 [39] Samuel G. M ¨uller and Frank Hutter. TrivialAugment: Tuning-Free Yet State-of-the-Art Data Augmentation. In Proc. of ICCV, pages 774–782, virtual, 2021. 2, 4, 13 [40] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis- sacco, Bo Wu, and Andrew Y . Ng. Reading Digits in Nat- ural Images with Unsupervised Feature Learning. In proc. of NIPS - Workshops , pages 1–9, Granada, Spain, 2011. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 4, 12 [41] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah- moud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv ´e Je- gou, Julien Mairal, Patrick Labatut, Armand Joulin, and Pi- otr Bojanowski. Dinov2: Learning Robust Visual Features Without Supervision. arXiv, 2304.07193:1–31, 2023. 1 [42] Emin Orhan. A Simple Cache Model for Image Recognition. In Proc. of NeurIPS, pages 10128–10137, Montr´eal, Canada, 2018. 2 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In Proc. of ICML, pages 8748–8763, virtual, 2021. 1, 2 [44] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal- lenge. Int. J. Comput. Vis., 115(3):211–252, 2015. 1, 2, 4, 12 [45] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. In Proc. of ICML, pages 4596–4604, Stockholm, Sweden, 2018. 13 [46] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to Train Your ViT? Data, Augmentation, and Regularization in Vision Transformers. Trans. Mach. Learn. Res., pages 1–16, 2022. 2, 5, 13 [47] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi- nav Gupta. Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. In Proc. of ICCV , pages 843–852, Venice, Italy, 2017. 4 [48] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the Inception Ar- chitecture for Computer Vision. In Proc. of CVPR , pages 2818–2826, Las Vegas, NV , USA, 2016. 2, 4 [49] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training Data-Efficient Image Transformers & Distillation Through Attention. In Proc. of ICML , pages 10347–10357, virtual, 2021. 4 [50] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Na- jafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Ben- gio. Manifold Mixup: Better Representations by Interpo- lating Hidden States. In Proc. of ICML, pages 6438–6447, Long Beach, CA, USA, 2019. 2 10[51] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching Networks for One Shot Learning. In Proc. of NIPS , pages 3637–3645, Barcelona, Spain, 2016. (‘NIPS’ was renamed to ‘NeurIPS’ after 2018). 2 [52] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra- mid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. In Proc. of ICCV , pages 548–558, virtual, 2021. 1 [53] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN Database: Large-Scale Scene Recognition from Abbey to Zoo. In Proc. of CVPR, pages 3485–3492, San Francisco, CA, USA, 2010. 2, 4, 12 [54] Jianxiong Xiao, Krista A. Ehinger, James Hays, Antonio Torralba, and Aude Oliva. SUN Database: Exploring a Large Collection of Scene Categories. Int. J. Comput. Vis., 119(1): 3–22, 2016. 2, 4, 12 [55] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djo- longa, Andr´e Susano Pinto, Maxim Neumann, Alexey Doso- vitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A Large-scale Study of Representation Learn- ing with the Visual Task Adaptation Benchmark. arXiv, 1910.04867:1–33, 2020. 4, 12 [56] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu- cas Beyer. Scaling Vision Transformers. In Proc. of CVPR, pages 12104–12113, New Orleans, LA, USA, 2022. 1, 2, 4, 5, 12, 13 [57] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid Loss for Language Image Pre- Training. In Proc. of ICCV , pages 11975–11986, Paris, France, 2023. 1, 5, 8, 13 [58] Hongyi Zhang, Moustapha Ciss ´e, Yann N. Dauphin, and David Lopez-Paz. Mixup: Beyond Empirical Risk Mini- mization. In Proc. of ICLR , pages 1–13, Vancouver, BC, Canada, 2018. 1, 2, 4 [59] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kun- chang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip- Adapter: Training-Free Adaption of CLIP for Few-Shot Classification. In Proc. of ECCV, pages 493–510, Tel Aviv, Israel, 2022. 2 11Frozen Feature Augmentation for Few-Shot Image Classification Supplementary Material A1. Introduction We give additional details and results to complement the main paper. All included citations refer to the main paper’s references. A2. Brightness We provide the code snippet for brightness c2 FroFa def transform_aug_reverse( x, augment, aug_min_val=0, aug_max_val=1.0, x_min_val=None, x_max_val=None, clip=True): \"\"\"Transform to (low, high)-space, perform augmentation, transform back.\"\"\" l = x_min_val if x_min_val is None: l = tf.reduce_min(x) h = x_max_val if x_max_val is None: h = tf.reduce_max(x) # [l, h] --> [0, 1] x = (x - l) / (h - l + 1e-8) # [0, 1] --> [low, high] x = x * (aug_max_val - aug_min_val) x = x + aug_min_val x = tf.cast(augment(x), tf.float32) if clip: tf.clip_by_value(x, aug_min_val, aug_max_val) # [low, high] --> [0, 1] x = (x - aug_min_val) x = x / (aug_max_val - aug_min_val) x = x * (h - l + 1e-8) + l # [0, 1] --> [l, h] return x def get_random_brightness(max_delta=0.1, clip=False): # A random value in [-max_delta, +max_delta] # is added to the image values. # Small max_delta <1.0 assumes that the # image values are within [0, 1]. def _random_brightness(image): return tf.image.random_brightness( image, max_delta) def tar(x): return transform_aug_reverse( x, augment=_random_brightness, aug_min_val=0, aug_max_val=1.0, clip=clip) return tar def get_random_brightness_per_channel_v2( max_delta=0.1, clip=True): \"\"\"Applies channel-wise random brightness transformations.\"\"\" # A random value in [-max_delta, +max_delta] is added to the image values. # Small max_delta <1.0 assumes that the # image values are within [0, 1]. random_brightness = get_random_brightness( max_delta, clip) def _random_brightness_pc(x): x = tf.expand_dims(x, axis=2) # (H, W, 1, C) x = tf.unstack(x, axis=-1) # C x (H, W, 1) x = [random_brightness( {\"image\": x_i})[\"image\"] for x_i in x] return tf.concat(x, axis=-1) return _random_brightness_pc A3. Detailed Experimental Setup In the following, we provide additional details to our exper- imental setup. A3.1. Datasets In this section, we focus on details regarding our pretraining and few-shot datasets. Pretraining: As stated in the main paper, Sec. 4.2, we pretrain our models by either using JFT-3B [56], ImageNet- 21k [14, 44], or WebLI [5]. In JFT-3B, the images are annotated with noisy labels by using a semi-automated pipeline. We follow common practice [13, 56] and ignore the hierarchical aspect of the labels. ImageNet-21k is a superset of the well known ILSVRC-2012 dataset, also known as “ImageNet-1k” or just “ImageNet”. WebLI is a recently introduced image- and-language dataset. It contains 10 billion images and tens of billions image-text pairs with over 100 languages. Few-shot transfer: As stated in the main paper, Sec. 4.2, our experiments concentrate around few-shot transfer on ILSVRC-2012 [14, 44]. We also provide results on CI- FAR10 [1], CIFAR100 [1], DMLab [2, 55], DTD [8], Re- sisc45 [6], SUN397 [53, 54], and SVHN [40]. When official test and validation splits are available, we use them for eval- uation across all datasets. In general, we use the versions in TensorFlow Datasets. CIFAR10 contains 60,000 images of 10 equally dis- tributed classes split into 50,000 training images and 10,000 test images. We further split the official training dataset into 45,000 training images and 5,000 validation images. CIFAR100 is a superset of CIFAR10 with 100 equally distributed classes and 60,000 images. Similar to CIFAR10, we use 45,000 images for training, 5,000 images for valida- tion and 10,000 images for test. DMLab consists of frames collected from the DeepMind Lab environment. Each frame is annotated with one out https://www.tensorflow.org/datasets 12of six classes. We use 65,550 images for training, 22,628 images for validation, and 22,735 for test. DTD is a collection of 5,640 textural images categorized into 47 distinct classes. Each of the three splits, i.e., train- ing, validation, and test, has exactly 1,880 images. Resisc45 is a benchmark with 31,500 images for image scene classification in remote sensing scenarios. In total, 47 different catogries for scenes are defined. We use the first 23,000 images for training, the subsequent 2,000 images for validation and the last 6,300 images for test. SUN397 is a 397-category database of 108,753 images for scene understanding. We use 76,128 images for training, 10,875 images for validation, and 21,750 images for test. SVHN is a Google Street View dataset with a large col- lection of house number images. In total, 10 distinct classes exist. We use the cropped version with 73,257 images for training and 26,032 images for test. Further, we create a val- idation subset by only using the first 70,000 out of 73,257 training images for actual training and the remaining 3,257 images for validation. A3.2. Data Augmentation In this section, we provide additional details on the used data augmentation techniques and protocols. (c/c2)FroFA: In Tab. 6, we give detailed descriptions of each FroFA, cFroFA, and c 2FroFA setting. We mostly build upon an AutoAugment implementation from Big Vision. To keep it simple, we use v or v1, v2 as sweep parameter(s) for all augmentations. By default, we first re- shape the two-dimensional features f to three-dimensional features f∗ (1) of shape √ N × √ N × C, with N = 196 and C ∈ {192, 768, 1024} in all our experiments. Note that the value of C depends on the architecture. We further want to point out, while some augmentations heavily rely on the three-dimensional representation, e.g., all geometric ones, some others are also transferable to a two-dimensional rep- resentation, e.g., brightness or contrast. As pointed out in the main paper, Tab. 3, brightness c2FroFA, contrast FroFA, and posterize cFroFA are our best FroFAs. For all three, we list the best sweep settings in Tab. 7. Advanced protocols: As mentioned in the main paper, Sec. 4.3, besides our fixed sequential protocol ( cf . Tab. 4) we also tested variations of RandAugment [10] and Triv- ialAugment [39]. In all protocols, we sample from the best settings of brightness c2FroFA, contrast FroFA, and poster- ize cFroFA. In particular, we use v = 1.0 for brightness c2FroFA, v = 6.0 for contrast FroFA, and v1 = 1, v2 = 8 for posterize cFroFA ( cf . Tab. 6). We re-use the abbrevi- ations from Tab. 4 in the following, i.e., B-c 2, C, and P- c, respectively. For the RandAugment and TrivialAugment https://github.com/google- research/big_vision/ blob/main/big_vision/pp/autoaugment.py variations, we uniformly sample from either the best three FroFAs, i.e., Atop3 = {B-c2, C, P-c}, or the best two Fro- FAs, i.e., Atop2 = A3 \\ {C}. Further, our RandAugment variation randomly constructs a sequence of augmentations by uniformly sampling the integer sequence length from 1 to |A|, with A ∈ {Atop2, Atop3} depending on whether Atop2 or Atop3 is used. A3.3. Training Details Pretraining: In the JFT-3B setup, we use pretrained mod- els from Zhai et al. [56]. The models are pretrained using a sigmoid cross-entropy loss. The weights are optimized by Adafactor [45] in half-precision mode, β1 = 0.9, and β2 = 0.999. Further, (decoupled) weight decay [38] is applied with 3.0 on the head and 0.03 for the rest of the network weights. The learning rate is adapted by a recip- rocal square-root schedule for 4,000,000 steps with a lin- ear warm-up phase of 10,000 steps and a linear cool-down phase of 50,000 steps. The starting learning rate is 0.01 for Ti/16 and L/16 and 0.03 for B/16. The images are prepro- cessed by an224×224 inception crop and a random horizon- tal flip. We set the batch size to 4,096. To stabilize training, a global norm clipping of 1.0 is used. In the ImageNet-21k setup, we follow settings from Steiner et al. [46] and use a sigmoid cross-entropy loss for multi-label pretraining. We use the Adam optimizer [25] in half-precision mode and set β1 = 0.9 and β2 = 0.999. Fur- ther, we apply (decoupled) weight decay with either 0.03 for Ti/16 or 0.1 for B/16 and L/16. We adapt the learning rate using a cosine schedule for roughly 930,000 steps (300 epochs) with a linear warm-up phase of 10,000 steps. We set the starting learning rate to 0.001 for all models. During preprocessing, we crop the images to 224×224 following an inception-style crop and a random horizontal flip. While we don’t use any additional augmentation for Ti/16, we fol- low suggestions by Steiner et al. [46] and use the ‘light1’ and ‘medium2’ augmentation settings for B/16 and L/16, respectively. Finally, we use a batch size of 4,096 and sta- bilize training by using a global norm clipping of 1.0. In the WebLI setup, we take an L/16 model from [57]. In particular, we use [ADD DETAILS]. Few-shot learning: We first cache each few-shot dataset by processing each of them through a pretrained model and store the extracted features (cf . Fig. 2). We resize each im- age to 224×224 before feeding it to the model. We follow up with a training where we mostly use trans- fer learning settings from Steiner et al. [46]. We use a sig- moid cross-entropy loss. This might be non-intuitive given that all of our few-shot datasets are not multi-labeled. How- ever, we didn’t really observe any performance drops com- pared to using the more common softmax cross-entropy loss, so we stick to the sigmoid cross-entropy loss. We use stochastic gradient descent with momentum of 0.9. Simi- 13Augmentation Description Geometric rotate We rotate each of the C feature channels fc (2) by z ∼ U(−v, v). We sweep across v ∈ {15, 30, 45, 60, 75, 90} representing the maximum positive and negative rotation angle in degrees. shear-{x,y} We (horizontally/vertically) shear each of the C feature channels fc (2) by z ∼ U(0, v). We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7} representing the maximum level of horizontal or vertical shearing. translate-{x,y} We (horizontally/vertically) translate each of theC feature channels fc (2) by uniformly samplingz from {0, 1, ..., v}. We sweep across integer values 1 ≤ v ≤ 7 representing the maximum horizontal or vertical translation. Crop & drop crop We randomly crop each of the C feature channels fc (2) to v×v at the same spatial position. We sweep across integer values 1 ≤ v ≤ 13 representing the square crop size. resized crop We resize each of the C feature channels fc (2) to v × v and then randomly crop each to 14 × 14 at the same spatial position. We sweep across v ∈ {16, 18, 20, 22, 24, 26, 28, 35, 42} representing the resized squared spatial resolution. inception crop We apply an inception crop with probability v. We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. patch dropout We randomly keep v out of N patches of f having shape N × C. Note that the patch ordering is also randomized. We sweep across v ∈ {1, 2, 4, 12, 20, 28, 36, 44, 52, 60, 68, 76, 84, 92, 100, 116, 132, 148, 164, 180}. Stylistic brightness We randomly add a value z ∼ U(−v, v) to each of the C feature channels fc (2). We sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. We test this method using all FroFA variants. In the default FroFA and the cFroFA variants, the features are scaled by (5) taking the minimumfmin and maximum fmax across all chan- nels into account. In the c 2FroFA variant, each channel fc (2) is shifted individually and uses the channel minimum and maximum instead. Further, in the cFroFA and c2FroFA variants we sample C values of z, one for each channel. contrast We randomly scale each of the C feature channels fc (2) by z ∼ U( 1 v , v). We sweep across v ∈ {1.25, 1.5, 2, 3, 4, 5, 6, 7, 9, 10}. We test this method using the default FroFA as well as cFroFA. Note that in the cFroFA variant we sample C values of z, one for each channel. equalize We first map the features from value range R to the integer subset I = {0, 1, ...,195}, i.e., executing (5) followed up by a discretization step. We choose this value range as preliminary results mapping from R to the more commonly used I = {0, 1, ...,255} instead didn’t show any effects. We continue by equalizing 196 bins and then transforming the results back to the original space using (7). We apply equalize with probability v. In particular, we sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. invert We change the sign of features f∗ with probability v. We sweep acrossv ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. posterize We first map the features from value range R to the integer subset I = {0, 1, ...,255}, i.e., executing (5) followed up by a discretization step. In other words, we use an 8-bit representation for features f∗. Posterize performs a quantization by a bit-wise left and right shift. We uniformly sample the shift value z between integer values v1 and v2. In our sweep, we test a subset of all possible combinations. In particular, we first set v2 = 8and reduce v1 from 7 to 1. We then fix v1 = 1and increase v2 from 2 to 7 again. We test this method using the default FroFA as well as cFroFA. Note that in the cFroFA variant we sampleC values of z, one for each channel. sharpness We first apply a two-dimensional convolution on f∗ (1) using a 3×3 smoothing filter. Next, we mix the original features with the resulting “smoothed” features using a randomly sampled blending factor z ∼ U(0, v). We sweep across v ∈ {0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0, 3.0}. solarize We do not map features from R to I = [0, 1], but stay in R. We compute the minimum fmin and maximum fmax across features f∗. We conditionally subtract all values smaller than0.5·fmin from fmin or larger than0.5·fmax from fmax. We apply this method with a probabilityv and sweep across v ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. Other JPEG We first map the features from value range R to the integer subset I = {0, 1, ...,255}, i.e., executing (5) followed up by a discretization step. We then perform a JPEG compression of each channel by randomly sampling a JPEG quality z ∼ U(v1, v2). We sweep across combinations of v1 ∈ {10, 25, 50, 75} and v2 ∈ {25, 50, 75, 100}, with v2 > v1. mixup We do not map features from R to [0, 1], but stay in R. We mix two features f∗ i , f∗ j according to z ·f∗ i + (1−z) ·f∗ j by sampling a random value z ∼ B(α, α), with Beta distribution B(α, α) parameterized by α = v. The labels are mixed using the same procedure. We sweep across v ∈ {0.025, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. Table 6. Details on our used set of augmentations. For simplicity, instead of introducing a new hyper parameter for each data augmenta- tion, we re-use v as a sweep parameter that is set during a sweep and differs for each augmentation. If not stated otherwise, each method is only applied as default FroFA and we first map featuresf (two-dimensional representation) or f∗ (three-dimensional representation) from value range R to I = [0, 1] using (5). By default, we assume a three-dimensional representation f∗ although some augmentations would work also in the two-dimensional representation f, i.e., a reshaping is not necessary. lar to the pretraining setup, we also store the internal state in half-precision. We do not apply any weight decay. The learning rate is adapted following a cosine schedule with a linear warm-up phase of 500 steps. In addition, we stabilize 14FroFA Shots Base learning rate Batch size Training steps v or v1, v2 B-c2 1 0.01 512 4,000 1.0 10 0.01 64 16,000 1.0 15 0.01 256 8,000 0.9 25 0.01 512 8,000 0.8 C 1 0.01 32 16,000 6.0 10 0.01 128 8,000 6.0 15 0.01 512 2,000 6.0 25 0.01 256 4,000 7.0 P-c 1 0.01 512 8,000 1, 8 10 0.03 512 8,000 1, 8 15 0.03 512 16,000 1, 8 25 0.03 64 16,000 2, 8 Table 7. Our best sweep settings for our best three FroFAs , namely, brightness cFroFA (B-c 2), contrast (C), and posterize cFroFA (P-c). We list the shots, base learning rate, batch size, number of training steps, and the augmentation parameter, denoted as v or v1, v2 (see Tab. 6 for a detailed explanation ofv and v1, v2). The best sweep settings are found using our ILSVRC-2012 vali- dation set. RA∗ TA∗ Shots MAP B-c 2 Atop2 Atop3 Atop2 Atop3 1 58.4 +6.0 +3.9 +2.4 +4.8 +4.3 5 79.1 +1.5 +1.0 +0.4 +1.4 +1.2 10 80.7 +1.3 +1.0 +0.6 +1.4 +1.4 25 83.0 +0.6 +0.4 +0.0 +0.5 +0.4 Table 8. Top-1 accuracy for advanced FroFA protocols on our ILSVRC-2012 test set. Absolute gains to the MAP baseline (ref- erence run) are reported. We use the L/16 JFT-3B base setup (cf . Sec. 5). We compare brightness c 2FroFA (B-c2) with our variations of RandAugment (RA∗) and TrivialAugment (TA∗), cf . Sec. A3.2. For the latter, we either use the top-2 ( Atop2) or top- 3 ( Atop3) augmentations. We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 val- idation set. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). training by using a global norm clipping of 1.0. Further, we sweep across batch size, learning rate and number of steps yielding 100 combinations (cf . Sec. 4.4) for each shot. A4. Additional Experimental Results In this section, we show additional experimental results. A4.1. Patch Dropout and Brightness In Fig. 3, we only report results for 1-shot and 25- shot settings using patch dropout FroFA and brightness (c/c2)FroFA. We extend this by also reporting results for 5-shot and 10-shot settings in Figs. 5 and 6. We observe the same effects in the other settings as well. A4.2. Advanced FroFA Protocols In Tab. 8, we report results for our RandAugment (RA ∗) and TrivialAugment (TA∗) variations. We did not average across five runs and thus only report absolute gains with re- spect to a reference run. Therefore, numbers which are also reported in the main paper, e.g., Tab. 4, are slightly differ- ent. All in all, we observe that both RA ∗ and TA∗ do not improve upon the best single augmentation, i.e., brightness c2FroFA (B-c2). We also observe that increasing the set of augmentations from Atop2 to Atop3 rather worsens the per- formance for both RA∗ and TA∗. A4.3. Detailed FroFA Transfer Results In Tab. 9, we report exact numbers for Fig. 4, i.e., Ti/16, B/16, and L/16 pretrained on either ImageNet-21k or JFT- 3B and subsequently finetuned on few-shotted ILSVRC- 2012 training sets. Numbers for the two baselines, i.e., MAP ( with weight decay) and linear probe, and our best method, i.e., MAP ( with weight decay) combined with brightness c2FroFA (MAP + FroFA), are reported. In addi- tion, we report numbers, where we use MAPwithout weight decay in Tab. 10. As before, we observe that our method performs worse on all 1-shot settings, but is on par or sig- nificantly better than MAP and/or linear probe on most 5- to 25-shot settings. 151 50 100 150 number of patches 52 54 56 58top-1 accuracy 1-shot 1 50 100 150 number of patches 72 74 76 78 5-shot 1 50 100 150 number of patches 76 77 78 79 80 81 10-shot 1 50 100 150 number of patches 80 81 82 83 25-shot MAP + patch dropout FroFA Figure 5. Average top-1 accuracy for patch dropout FroFA on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup ( cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each number of patches (cf . Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times. 0.1 0.3 0.5 0.7 0.9 brightness level 50 55 60 65top-1 accuracy 1-shot 0.1 0.3 0.5 0.7 0.9 brightness level 74 76 78 80 5-shot 0.1 0.3 0.5 0.7 0.9 brightness level 79 80 81 82 10-shot 0.1 0.3 0.5 0.7 0.9 brightness level 81.0 81.5 82.0 82.5 83.0 83.5 25-shot MAP + brightness cFroFA + brightness c2FroFA Figure 6. Top-1 accuracy for channel variants (c/c2) of brightness FroFA on our ILSVRC-2012 test set. We use the L/16 JFT-3B base setup (cf . Sec. 5). We sweep across a base sweep ( cf . Sec. 4.4) to first find the best setting on our ILSVRC-2012 validation set for each brightness level (cf . Sec. A3.2). Shaded areas indicate standard errors collected via sampling each shot five times ImageNet-21k JFT-3B Model Method 1-shot 5-shot 10-shot 25-shot 1-shot 5-shot 10-shot 25-shot Ti/16 MAPwd 20.5 53.6 59.7 64.9 19.1 46.4 53.6 60.2 Linear probe 36.8 53.7 58.0 61.1 33.0 48.0 52.2 55.4 MAPwd + FroFA 20.6 54.5 60.1 65.2 19.6 47.2 53.6 60.3 B/16 MAPwd 30.5 71.7 75.3 78.0 51.3 74.8 77.5 79.8 Linear probe 52.2 72.9 76.0 77.9 59.6 74.5 76.9 78.3 MAPwd + FroFA 30.6 73.3 76.0 78.1 52.5 75.1 77.6 79.5 L/16 MAPwd 38.7 75.9 78.6 80.6 62.0 79.9 81.5 83.2 Linear probe 54.7 77.1 79.8 81.1 66.5 79.6 81.5 82.4 MAPwd + FroFA 39.3 78.0 80.0 81.0 63.7 80.4 82.0 83.6 Table 9. Average top-1 accuracy for JFT-3B and ImageNet-21k modelson our ILSVRC-2012 test set trained on few-shotted ILSVRC- 2012 training sets. We report results for the weight-decayed MAP, i.e. MAPwd, and L2-regularized linear probe baseline, as well as our best FroFA-based approach, i.e., weight-decayed MAP combined with brightness c 2FroFA (MAPwd + FroFA). Depending on the setting, we sweep across a base, cf . Sec. 4.4, a weight decay or L2 decay, cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Our approach, i.e., MAPwd + FroFA, is on par or significantly better than MAPwd and/or linear probe on most 5- to 25-shot settings. 16ImageNet-21k JFT-3B Model Method 1-shot 5-shot 10-shot 25-shot 1-shot 5-shot 10-shot 25-shot Ti/16 MAP 20.4 53.2 59.5 64.7 17.9 45.5 53.5 60.1 Linear probe 36.8 53.7 58.0 61.1 33.0 48.0 52.2 55.4 MAP + FroFA 22.1 54.9 60.1 65.0 20.3 47.2 53.6 60.1 B/16 MAP 31.3 70.3 75.1 78.1 48.9 73.4 76.5 79.4 Linear probe 52.2 72.9 76.0 77.9 59.6 74.5 76.9 78.3 MAP + FroFA 30.6 73.4 76.3 78.3 52.4 75.2 77.8 79.9 L/16 MAP 38.8 74.9 78.5 80.7 57.9 78.8 80.9 83.2 Linear probe 54.7 77.1 79.8 81.1 66.5 79.6 81.5 82.4 MAP + FroFA 39.3 78.0 80.0 81.2 63.9 80.3 82.0 83.6 Table 10. Average top-1 accuracy for JFT-3B and ImageNet-21k modelson our ILSVRC-2012 test set trained on few-shotted ILSVRC- 2012 training sets. We report results for the MAP and L2-regularized linear probe baseline, as well as our best FroFA-based approach,i.e., MAP combined with brightness c2FroFA (MAP + FroFA). Depending on the setting, we sweep across a base,cf . Sec. 4.4, an L2 decay,cf . Sec. 4.5, and a brightness level sweep, cf . Sec. A3.2, to first find the best setting on our ILSVRC-2012 validation set for each model. The best results per shot are boldfaced (multiple ones if close, i.e., ±0.2). Our approach, i.e., MAP + FroFA, is on par or significantly better than MAP and linear probe on most 5- to 25-shot settings. 17",
      "references": [
        "Learning Multiple Layers of Features from Tiny Images",
        "DeepMind Lab",
        "Better Plain ViT Baselines for ImageNet-1k",
        "AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition",
        "PaLI: A Jointly-Scaled Multilingual Language- Image Model",
        "Remote Sensing Image Scene Classification: Benchmark and State of the Art",
        "Xception: Deep Learning With Depthwise Separable Convolutions",
        "Describing Textures in the Wild",
        "AutoAugment: Learning Augmentation Strategies From Data",
        "RandAugment: Practical Automated Data Augmentation with a Reduced Search Space",
        "CoAtNet: Marrying Convolution and Attention for All Data Sizes",
        "Scenic: A JAX Library for Computer Vision Research and Beyond",
        "Scaling Vision Transformers to 22 Billion Parameters",
        "ImageNet: A Large-Scale Hierarchical Image Database",
        "Dataset Augmentation in Feature Space",
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "CLIP-Adapter: Better Vision-Language Models with Feature Adapters",
        "Improving Neural Language Models with a Continuous Cache",
        "Parameter-Efficient Model Adaptation for Vision Transformers",
        "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty",
        "Distilling Knowledge in a Neural Network",
        "Parameter-Efficient Transfer Learning for NLP",
        "LoRA: Low-Rank Adaptation of Large Language Models",
        "Visual Prompt Tuning",
        "Adam: A Method for Stochastic Optimization",
        "Big Transfer (BiT): General Visual Representation Learning",
        "Three Towers: Flexible Contrastive Learning with Pretrained Image Models",
        "A Closer Look At Feature Space Data Augmentation For Few-Shot Intent Classification",
        "The Omniglot Challenge: a 3-year Progress Report",
        "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks",
        "Vision Transformer for Small-size Datasets",
        "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "Data Augmentation via Latent Space Interpolation for Image Classification",
        "Efficient Training of Visual Transformers With Small Datasets",
        "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
        "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows",
        "Learning Multimodal Data Augmentation in Feature Space",
        "Decoupled Weight Decay Regularization",
        "TrivialAugment: Tuning-Free Yet State-of-the-Art Data Augmentation",
        "Reading Digits in Natural Images with Unsupervised Feature Learning",
        "Dinov2: Learning Robust Visual Features Without Supervision",
        "A Simple Cache Model for Image Recognition",
        "Learning Transferable Visual Models From Natural Language Supervision",
        "ImageNet Large Scale Visual Recognition Challenge",
        "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost",
        "How to Train Your ViT? Data, Augmentation, and Regularization in Vision Transformers",
        "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era",
        "Rethinking the Inception Architecture for Computer Vision",
        "Training Data-Efficient Image Transformers & Distillation Through Attention",
        "Manifold Mixup: Better Representations by Interpolating Hidden States",
        "Matching Networks for One Shot Learning",
        "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions",
        "SUN Database: Large-Scale Scene Recognition from Abbey to Zoo",
        "SUN Database: Exploring a Large Collection of Scene Categories",
        "A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark",
        "Scaling Vision Transformers",
        "Sigmoid Loss for Language Image Pre-Training",
        "Mixup: Beyond Empirical Risk Minimization",
        "Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification"
      ],
      "meta_data": {
        "arxiv_id": "2403.10519v2",
        "authors": [
          "Andreas Bär",
          "Neil Houlsby",
          "Mostafa Dehghani",
          "Manoj Kumar"
        ],
        "published_date": "2024-03-15T17:59:40Z",
        "github_url": "https://github.com/google-research/big_vision"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates frozen feature augmentation (FroFA): applying data augmentations in the frozen feature space of pretrained vision transformers to improve few-shot image classification. It shows that simple point-wise (stylistic) FroFAs, especially per-channel variants, consistently improve accuracy across multiple architectures (Ti/16, B/16, L/16), pretraining datasets (JFT-3B, ImageNet-21k, WebLI with SigLIP), and eight downstream transfer datasets (e.g., ILSVRC-2012, CIFAR10/100, DMLab, DTD, Resisc45, SUN397, SVHN). The study identifies brightness, posterize, and brightness-based channel FroFA as top performers, introduces per-channel FroFA variants (cFroFA and c2FroFA), demonstrates that sequential FroFAs can yield larger gains, and provides practical guidelines for FroFA use in few-shot transfer with frozen features.",
        "methodology": "The approach caches intermediate features from a frozen vision transformer up to the last transformer block (L-th block) and trains a lightweight MAP (multi-head attention pooling) head on these frozen features. FroFA is implemented as a composition of feature-space augmentations and feature-range mappings: af = tf←x ∘ ax ∘ tf→x, where tf→x maps features to [0,1], ax applies a per-element or per-channel augmentation, and tf←x maps back to the original feature range. Three FroFA variants are studied: (1) default FroFA (single scalar per input, all channels), (2) channel FroFA (per-channel augmentation values), (3) channel2 FroFA (per-channel augmentation with per-channel fx min/max). They test 18 augmentations ( geometric, crop/drop, stylistic, etc.) and perform sweeps over hyperparameters on a validation set. They compare to two baselines: MAP (with optional weight decay MAPwd) and L2-regularized linear probe. The experiments cover three pretraining datasets, three ViT architectures, and eight downstream tasks, with a focus on 1/5/10/25-shot settings.",
        "experimental_setup": "Pretraining datasets: JFT-3B (≈3B images, 29k labels), ImageNet-21k, WebLI with SigLIP. Architectures: ViT variants Ti/16, B/16, L/16. Feature caching: features from the L-th transformer block are cached and a MAP head is trained on the cached features. Downstream evaluation: eight datasets for few-shot transfer (ILSVRC-2012, CIFAR10, CIFAR100, DMLab, DTD, RESISC45, SUN397, SVHN). Data splits include 1/5/10/25-shot sets sampled from the training data, with a minival validation set for hyperparameter tuning and a separate official test set for reporting results. Data augmentations: 18 augmentations (geometric, crop/drop, stylistic, and others) mapped to feature space via per-feature scaling and per-channel mappings; experiments include sequential FroFA combinations and extensions with RandAugment/TrivialAugment. Training details include small MAP heads, SGD/Adam optimizers, weight decay settings, and TPUv2-scale running times.",
        "limitations": "Limitations include: geometric feature-space augmentations (e.g., rotate, shear, translate) generally degrade performance on ImageNet; gains are larger on small transfer datasets and modest on ImageNet, indicating dependence on dataset size and baseline strength. Channel-wise FroFA variants improve stability but brightness cFroFA/c2FroFA can be sensitive to augmentation level, particularly cFroFA. The approach requires caching features and training a separate lightweight head, which may not scale to very large or different modalities; results are validated on specific ViT backbones and pretraining datasets, so transferability to other architectures or modalities may vary. Some improvements over linear probes are dataset/shot dependent, and sequential FroFA gains, while promising, need broader exploration. Hyperparameter sweeps add computational overhead.",
        "future_research_directions": "Future work could explore more advanced FroFA protocols, automated learning of feature-space augmentations rather than fixed sweeps, and integration with adapters or prompt-tuning in vision models. Potential directions include applying FroFA to other modalities (text, multimodal models), extending to segmentation/detection tasks, evaluating on larger or newer foundation models, studying per-layer FroFA choices, dynamic or curriculum-based augmentation schedules in feature space, and combining FroFA with other parameter-efficient transfer methods (LoRA, adapters) for robust, data-efficient downstream learning.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Consistency Regularization for Generative Adversarial Networks",
      "full_text": "Published as a conference paper at ICLR 2020 CONSISTENCY REGULARIZATION FOR GENERATIVE ADVERSARIAL NETWORKS Han Zhang, Zizhao Zhang, Augustus Odena, Honglak Lee Google Research {zhanghan,zizhaoz,augustusodena,honglak}@google.com ABSTRACT Generative Adversarial Networks (GANs) are known to be difﬁcult to train, de- spite considerable research effort. Several regularization techniques for stabilizing training have been proposed, but they introduce non-trivial computational over- heads and interact poorly with existing techniques like spectral normalization. In this work, we propose a simple, effective training stabilizer based on the notion of consistency regularization—a popular technique in the semi-supervised learning literature. In particular, we augment data passing into the GAN discriminator and penalize the sensitivity of the discriminator to these augmentations. We conduct a series of experiments to demonstrate that consistency regularization works effec- tively with spectral normalization and various GAN architectures, loss functions and optimizer settings. Our method achieves the best FID scores for unconditional image generation compared to other regularization methods on CIFAR-10 and CelebA. Moreover, Our consistency regularized GAN (CR-GAN) improves state- of-the-art FID scores for conditional generation from 14.73 to 11.48 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012. 1 I NTRODUCTION Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have recently demonstrated impressive results on image-synthesis benchmarks (Radford et al., 2016; Zhang et al., 2017; Miyato & Koyama, 2018; Zhang et al., 2018; Brock et al., 2018; Karras et al., 2019). In the original setting, GANs are composed of two neural networks trained with competing goals: the generator is trained to synthesize realistic samples to fool the discriminator and thediscriminator is trained to distinguish real samples from fake ones produced by the generator. One major problem with GANs is the instability of the training procedure and the general sensitivity of the results to various hyperparameters (Salimans et al., 2016). Because GAN training implicitly requires ﬁnding the Nash equilibrium of a non-convex game in a continuous and high dimensional parameter space, it is substantially more complicated than standard neural network training. In fact, formally characterizing the convergence properties of the GAN training procedure is mostly an open problem (Odena, 2019). Previous work (Arjovsky & Bottou, 2017; Miyato et al., 2018a; Odena et al., 2017; Chen et al., 2019; Wei et al., 2018) has shown that interventions focused on the discriminator can mitigate stability issues. Most successful interventions fall into two categories, normalization and regularization. Spectral normalization is the most effective normalization method, in which weight matrices in the discriminator are divided by an approximation of their largest singular value. For regularization, Gulrajani et al. (2017) penalize the gradient norm of straight lines between real data and generated data. Roth et al. (2017) propose to directly regularize the squared gradient norm for both the training data and the generated data. DRAGAN (Kodali et al., 2017) introduces another form of gradient penalty where the gradients at Gaussian perturbations of training data are penalized. One may anticipate simultaneous regularization and normalization could improve sample quality. However, most of these gradient based regularization methods either provide marginal gains or fail to introduce any improvement when normalization is used (Kurach et al., 2019), which is also observed in our experiments. These regularization methods and spectral normalization are motivated by controlling Lipschitz constant of the discriminator. We suspect this might be the reason that applying both does not lead to overlaid gain. 1 arXiv:1910.12027v2  [cs.LG]  18 Feb 2020Published as a conference paper at ICLR 2020 Image space Manifold space Semantic feature space Before consistency Afterconsistency Figure 1: An illustration of consistency regularization for GANs. Before consistency regularization, the zoomed-in dog and the zoomed-in cat (bottom left) can be closer than they are to their original images in feature space induced by the GAN discriminator. This is illustrated in the upper right (the semantic feature space), where the purple dot is closer to the blue dot than to the red dot, and so forth. After we enforce consistency regularization based on the implicit assumption that image augmentation preserves the semantics we care about, the purple dot pulled closer to the red dot. In this paper, we examine a technique called consistency regularization (Bachman et al., 2014; Saj- jadi et al., 2016; Laine & Aila, 2016; Zhai et al., 2019; Xie et al., 2019; Hu et al., 2017) in contrast to gradient-based regularizers. Consistency regularization is widely used in semi-supervised learning to ensure that the classiﬁer output remains unaffected for an unlabeled example even it is augmented in semantic-preserving ways. In light of this intuition, we hypothesize a well-trained discrimina- tor should also be regularized to have the consistency property, which enforces the discriminator to be unchanged by arbitrary semantic-preserving perturbations and to focus more on semantic and structural changes between real and fake data. Therefore, we propose a simple regularizer to the dis- criminator of GAN: we augment images with semantic-preserving augmentations before they are fed into the GAN discriminator and penalize the sensitivity of the discriminator to those augmentations. This technique is simple to use and surprisingly effective. It is as well less computationally expen- sive than prior techniques. More importantly, in our experiments, consistency regularization can always further improve the model performance when spectral normalization is used, whereas the performance gains of previous regularization methods diminish in such case. In extensive ablation studies, we show that it works across a large range of GAN variants and datasets. We also show that simply applying this technique on top of existing GAN models leads to new state-of-the-art results as measured by Frechet Inception Distance (Heusel et al., 2017). In summary, our contributions are summarized as follows: • We propose consistency regularization for GAN discriminators to yield a simple, effective regularizer with lower computational cost than gradient-based regularization methods. • We conduct extensive experiments with different GAN variants to demonstrate that our technique interacts effectively with spectral normalization. Our consistency regularized GAN (CR-GAN) achieves the best FID scores for unconditional image generation on both CIFAR-10 and CelebA. • We show that simply applying the proposed technique can further boost the performance of state-of-the-art GAN models. We improve FID scores for conditional image generation from 14.73 to 11.48 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012. 2 M ETHOD 2.1 GAN S A GAN consists of a generator network and a discriminator network. The generator G takes a latent variable z ∼p(z) sampled from a prior distribution and maps it to the observation space X. The discriminator Dtakes an observation x ∈X and produces a decision output over possible observation sources (either from Gor from the empirical data distribution). In the standard GAN training procedure the generator Gand the discriminator Dare trained by minimizing the following 2Published as a conference paper at ICLR 2020 objectives in an alternating fashion: LD = −Ex∼pdata [log D(x)] −Ez∼p(z) [1 −log D(G(z))] , LG = −Ez∼p(z) [log D(G(z))] , (1) where p(z) is usually a standard normal distribution. This formulation is originally proposed by Goodfellow et al. (2014) as non-saturating (NS) GAN. A signiﬁcant amount of research has been done on modifying this formulation in order to improve the training process. A notable example is the hinge-loss version of the adversarial loss (Lim & Ye, 2017; Tran et al., 2017): LD = −Ex∼pdata [min(0,−1 +D(x))] −Ez∼p(z) [min(0,−1 −D(G(z)))] , LG = −Ez∼p(z) [D(G(z))] . (2) Another commonly adopted GAN formulation is the Wassertein GAN (WGAN) (Arjovsky et al., 2017), in which the authors propose clipping the weights of the discriminator in an attempt to enforce that the GAN training procedure implicitly optimizes a bound on the Wassertein distance between the target distribution and the distribution given by the generator. The loss function of WGAN can be written as LD = −Ex∼pdata [D(x)] +Ez∼p(z) [D(G(z))] , LG = −Ez∼p(z) [D(G(z))] . (3) Subsequent work has reﬁned this technique in several ways (Gulrajani et al., 2017; Miyato et al., 2018a; Zhang et al., 2019), and the current widely-used practice is to enforce spectral normaliza- tion (Miyato et al., 2018a) on both the generator and the discriminator. 2.2 C ONSISTENCY REGULARIZATION Consistency regularization has emerged as a gold-standard technique (Sajjadi et al., 2016; Laine & Aila, 2016; Zhai et al., 2019; Xie et al., 2019; Oliver et al., 2018; Berthelot et al., 2019) for semi- supervised learning on image data. The basic idea is simple: an input image is perturbed in some semantics-preserving ways and the sensitivity of the classiﬁer to that perturbation is penalized. The perturbation can take many forms: it can be image ﬂipping, or cropping, or adversarial attacks. The regularization form is either the mean-squared-error (Sajjadi et al., 2016; Laine & Aila, 2016) between the model’s output for a perturbed and non-perturbed input or the KL divergence (Xie et al., 2019; Miyato et al., 2018b) between the distribution over classes implied by the output logits. 2.3 C ONSISTENCY REGULARIZATION FOR GAN S The goal of the discriminator in GANs is to distinguish real data from fake ones produced by the generator. The decision should be invariant to any valid domain-speciﬁc data augmentations. For example, in the image domain, the image being real or not should not change if we ﬂip the image horizontally or translate the image by a few pixels. However, the discriminator in GANs does not guarantee this property explicitly. To resolve this, we propose a consistency regularization on the GAN discriminator during train- ing. In practice, we randomly augment training images as they are passed to the discriminator and penalize the sensitivity of the discriminator to those augmentations. We use Dj(x) to denote the output vector before activation of the jth layer of the discriminator given input x. T(x) denotes a stochastic data augmentation function. This function can be linear or nonlinear, but aims to preserve the semantics of the input. Our proposed regularization is given by min D Lcr = min D n∑ j=m λj Dj(x) −Dj(T(x)) 2 , (4) where j indexes the layers, m is the starting layer and n is the ending layer that consistency is enforced. λj is weight coefﬁcient for jth layer and ∥·∥denotes L2 norm of a given vector. This consistency regularization encourages the discriminator to produce the same output for a data point under various data augmentations. 3Published as a conference paper at ICLR 2020 Algorithm 1Consistency Regularized GAN (CR-GAN). We use λ= 10by default. Input: generator and discriminator parameters θG,θD, consistency regularization coefﬁcient λ, Adam hyperparameters α,β1,β2, batch size M, number of discriminator iterations per gen- erator iteration ND 1: for number of training iterations do 2: for t= 1,...,N D do 3: for i= 1,...,M do 4: Sample z∼p(z), x∼pdata(x) 5: Augment xto get T(x) 6: L(i) cr ← D(x) −D(T(x)) 2 7: L(i) D ←D(G(z)) −D(x) 8: end for 9: θD ←Adam( 1 M ∑M i=1(L(i) D + λL(i) cr ),α,β 1,β2) 10: end for 11: Sample a batch of latent variables {z(i)}M i=1 ∼p(z) 12: θG ←Adam( 1 M ∑M i=1(−D(G(z))),α,β 1,β2) 13: end for In our experiments, we ﬁnd that consistency regularization on the last layer of the discriminator before the activation function is sufﬁcient. Lcr can be rewritten as Lcr = D(x) −D(T(x)) 2 , (5) where from now on we will drop the layer index for brevity. This cost is added to the discriminator loss (weighted by a hyper-parameter λ) when updating the discriminator parameters. The generator update remains unchanged. Thus, the overall consistency regularized GAN (CR-GAN) objective is written as Lcr D = LD + λLcr, L cr G = LG. (6) Our design of Lcr is general-purpose and thereby can work with any valid adversarial lossesLG and LD for GANs (See Section 2.1 for examples). Algorithm 1 illustrates the details of CR-GAN with Wassertein loss as an example. In contrast to previous regularizers, our method does not increase much overhead. The only extra computational cost comes from feeding an additional (third) image through the discriminator forward and backward when updating the discriminator parameters. 3 E XPERIMENTS This section validates our proposed CR-GAN method. First we conduct a large scale study to com- pare consistency regularization to existing GAN regularization techniques (Kodali et al., 2017; Gul- rajani et al., 2017; Roth et al., 2017) for several GAN architectures, loss functions and other hyper- parameter settings. We then apply consistency regularization to a state-of-the-art GAN model (Brock et al., 2018) and demonstrate performance improvement. Finally, we conduct ablation studies to in- vestigate the importance of various design choices and hyper-parameters. All our experiments are based on the open-source code from Compare GAN (Kurach et al., 2019), which is available at https://github.com/google/compare_gan. 3.1 D ATASETS AND EVALUATION METRICS We validate our proposed method on three datasets: CIFAR-10 (Krizhevsky, 2009), CELEBA-HQ- 128 (Karras et al., 2018), and ImageNet-2012 (Russakovsky et al., 2015). We follow the procedure in Kurach et al. (2019) to prepare datasets. CIFAR-10 consists of 60K of 32 ×32 images in 10 classes; 50K for training and 10K for testing. CELEBA-HQ-128 (CelebA) contains 30K images of faces at a resolution of 128 ×128. We use 3K images for testing and the rest of images for training. ImageNet-2012 contains roughly 1.2 million images with 1000 distinct categories and we down-sample the images to 128 ×128 in our experiments. We adopt the Fréchet Inception distance (FID) (Heusel et al., 2017) as primitive metric for quantita- tive evaluation, as FID has proved be more consistent with human evaluation. In our experiments the 4Published as a conference paper at ICLR 2020 (a) (b) (c) (d) (e) (f) Figure 2: Comparison of our method with existing regularization techniques under different GAN losses. Techniques include no regularization (W/O), Gradient Penalty (GP) (Gulrajani et al., 2017), DRAGAN (DR) (Kodali et al., 2017) and JS-Regularizer (JSR) (Roth et al., 2017). Results (a-c) are for CIFAR-10 and results (d-f) are for CelebA. FID is calculated on the test dataset. In particular, we use 10K generated images vs. 10K test images on CIFAR-10, 3K vs. 3K on CelebA and 50K vs. 50K on ImageNet. We also provide the Inception Score (Salimans et al., 2016) for different methods in the Appendix F for supplementary results. By default, the augmentation used in consistency regularization is a combination of randomly shifting the image by a few pixels and randomly ﬂipping the image horizontally. The shift size is 4 pixels for CIFAR-10 and CelebA and 16 for ImageNet. 3.2 C OMPARISON WITH OTHER GAN REGULARIZATION METHODS In this section, we compare our methods with three GAN regularization techniques, Gradient Penalty (GP) (Gulrajani et al., 2017), DRAGAN Regularizer (DR) (Kodali et al., 2017) and JS-Regularizer (JSR) (Roth et al., 2017) on CIFAR-10 and CelebA. Following the procedures from (Kurach et al., 2019; Lucic et al., 2018), we evaluate these methods across different optimizer parameters, loss functions, regularization coefﬁcient and neural architec- tures. For optimization, we use the Adam optimizer with batch size of 64 for all our experiments. We stop training after 200k generator update steps for CIFAR-10 and 100k steps for CelebA. By default, spectral normalization (SN) (Miyato et al., 2018a) is used in the discriminator, as this is the most effective normalization method for GANs (Kurach et al., 2019) and is becoming the standard for ‘modern’ GANs (Zhang et al., 2019; Brock et al., 2018). Results without spectral normalization can be seen in the Appendix B. 3.2.1 I MPACT OF LOSS FUNCTION In this section, we discuss how each regularization method performs when the loss function is changed. Speciﬁcally, we evaluate regularization methods using three loss functions: the non- saturating loss (NS) (Goodfellow et al., 2014), the Wasserstein loss (W AS) (Arjovsky et al., 2017), and the hinge loss (Hinge) (Lim & Ye, 2017; Tran et al., 2017). For each loss function, we evaluate over 7 hyper-parameter settings of the Adam optimizer (more details in Section A of the appendix). For each conﬁguration, we run each model 3 times with different random seeds. For the regulariza- tion coefﬁcient, we use the best value reported in the corresponding paper. Speciﬁcally λis set to be 10 for both GP, DR and our method and 0.1 for JSR. In this experiment, we use the SNDCGAN network architecture (Miyato et al., 2018a) for simplicity. In the end, similar as Kurach et al. (2019), we aggregate all runs and report the FID distribution of the top 15% of trained models. The results are shown in Figure 2. The consistency regularization improves the baseline across all different loss functions and both datasets. Other techniques have more mixed results: For example, 5Published as a conference paper at ICLR 2020 Setting W/O GP DR JSR Ours (CR-GAN) CIFAR-10 (SNDCGAN) 24.73 25.83 25.08 25.17 18.72 CIFAR-10 (ResNet) 19.00 19.74 18.94 19.59 14.56 CelebA (SNDCGAN) 25.95 22.57 21.91 22.17 16.97 Table 1: Best FID scores for unconditional image generation on CIFAR-10 and CelebA. CIFAR-10 CelebA FID FID Figure 3: Comparison of FID scores with different values of the regularization coefﬁcient λ on CIFAR-10 and CelebA. The dotted line is a model without regularization. GP and DR can marginally improve the performance for settings (d) and (e) but lead to worse results for settings (a) and (b) (which is consistent with ﬁndings from Kurach et al. (2019)). In all cases, our consistency-regularized GAN models have the lowest (best) FID. This ﬁnding is especially encouraging, considering that the consistency regularization has lower computational cost (and is simpler to implement) than the other techniques. In our experiments, the consistency regularization is around 1.7 times faster than gradient based regularization techniques, including DR, GP and JSR, which need to compute the gradient of the gradient norm ∥∇x(D)∥. Please see Table C1 in the appendix for the actual training speed. 3.2.2 I MPACT OF THE REGULARIZATION COEFFICIENT Here we study the sensitivity of GAN regularization techniques to the regularization coefﬁcient λ. We train SNDCGANs with non-saturating losses and ﬁx the other hyper-parameters. λis chosen among {0.1, 1, 10, 100}. The results are shown in Figure 3. From this ﬁgure, we can see consistency regularization is more robust to changes in λthan other GAN regularization techniques (it also has the best FID for both datasets). The results indicate that consistency regularization can be used as a plug-and-play technique to improve GAN performance in different settings without much hyper- parameter tuning. 3.2.3 I MPACT OF NEURAL ARCHITECTURES To validate whether the above ﬁndings hold across different neural architectures, we conduct exper- iments on CIFAR-10 using a ResNet (He et al., 2016; Gulrajani et al., 2017) architecture instead of an SNDCGAN. All other experimental settings are same as in Section 3.2.1. The FID values are presented in Figure 4. By comparing results in Figure 4 and Figure 2, we can see that results on SNDCGAN and results on ResNet are comparable, though consistency regularization favors even better in this case: In sub-plot (c) of Figure 4, we can see that consistency regularization is the only regularization method that can generate satisfactory samples with a reasonable FID score (The FID scores for other methods are above 100). Please see Figure D3 for the actual generated samples in this setting. As in Section 3.2.1, consistency regularization has the best FID for each setting. In Table 1, we show FID scores for the best-case settings from this section. Consistency regulariza- tion improves on the baseline by a large margin and achieves the best results across different network architectures and datasets. In particular, it achieves an FID 14.56 on CIFAR-10 16.97 on CelebA. In fact, our FID score of 14.56 on CIFAR-10 for unconditional image generation is even lower than the 14.73 reported in Brock et al. (2018) for class-conditional image-synthesis with a much larger network architecture and much bigger batch size. 6Published as a conference paper at ICLR 2020 (a) (b) (c) Figure 4: Comparison of FID scores with ResNet structure on different loss settings on CIFAR-10. 3.3 C OMPARISON WITH STATE -OF-THE -ART GAN MODELS In this section, we add consistency regularization to the state-of-the-art BigGAN model (Brock et al., 2018) and perform class conditional image-synthesis on CIFAR-10 and ImageNet. Our model has exactly the same architecture and is trained under the same settings as BigGAN ⋆, the open-source implementation of BigGAN from Kurach et al. (2019). The only difference is that our model uses consistency regularization. In Table 2, we report the original FID scores without noise truncation. Consistency regularization improves the FID score of BigGAN⋆ on CIFAR-10 from 20.42 to 11.48. In addition, the FID on ImageNet is improved from 7.75 to 6.66. Generated samples for CIFAR-10 and ImageNet with consistency regularized models and baseline models are shown in Figures E1, E2 and E3 in the appendix. Dataset SNGAN SAGAN BigGAN BigGAN ⋆ CR-BigGAN⋆ CIFAR-10 17.5 / 14.73 20.42 11.48 ImageNet 27.62 18.65 8.73 7.75 6.66 Table 2: Comparison of our technique with state-of-the-art GAN models including SNGAN (Miy- ato & Koyama, 2018), SAGAN (Zhang et al., 2019) and BigGAN (Brock et al., 2018) for class conditional image generation on CIFAR-10 and ImageNet in terms of FID. BigGAN ⋆ is the Big- GAN implementation of Kurach et al. (2019). CR-BigGAN ⋆ has the exactly same architecture as BigGAN⋆ and is trained with the same settings. The only difference is CR-BigGAN ⋆ adds consis- tency regularization. 4 A BLATION STUDIES AND DISCUSSION 4.1 H OW MUCH DOES AUGMENTATION MATTER BY ITSELF ? Our consistency regularization technique actually has two parts: we perform data augmentation on inputs from the training data, and then consistency is enforced between the augmented data and the original data. We are interested in whether the performance gains shown in Section 3 are merely due to data augmentation, since data augmentation reduces the over-ﬁtting of the discriminator to the input data. Therefore, we have designed an experiment to answer this question. First, we train three GANs: (1) a GAN trained with consistency regularization, as in Algorithm 1, (2) a baseline GAN trained without augmentation or consistency regularization, and (3) a GAN trained with only data augmentation and no consistency regularization. We then plot (Figure 5) both their FID and the test accuracy of their discriminator on a held-out test set. The FID tells us how ‘good’ the resulting GAN is, and the discriminator test accuracy tells us how much the GAN discriminator over-ﬁts. Interestingly, we ﬁnd that these two measures are not well correlated in this case. The model trained with only data augmentation over-ﬁts substantially less than the baseline GAN, but has almost the same FID. The model trained with consistency regularization has the same amount of over-ﬁtting as the model trained with just data augmentation, but a much lower FID. This suggests an interesting hypothesis, which is that the mechanism by which the consistency regu- larization improves GANs is not simply discriminator generalization (in terms of classifying images into real vs fake). We believe that the main reason for the impressive gain from the consistency regularization is due to learning more semantically meaningful representation for the discriminator. More speciﬁcally, data augmentation will simply treat all real images and their transformed images 7Published as a conference paper at ICLR 2020 0 1000 2000 3000 4000 Epochs 0.4 0.5 0.6 0.7 0.8 0.9Traing accuracy GAN GAN w/ Aug. GAN w/ Cons. Reg. 0 1000 2000 3000 4000 Epochs 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Test accuracy GAN GAN w/ Aug. GAN w/ Cons. Reg. 0 1000 2000 3000 4000 Epochs 20 25 30 35 40 45 50FID GAN GAN w/ Aug. GAN w/ Cons. Reg. Figure 5: A study of how much data augmentation matters by itself. Three GANs were trained on CIFAR-10: one baseline GAN, one GAN with data augmentation only, and one GAN with consis- tency regularization. (Left) Training accuracy of the GAN discriminator. (Middle) Test accuracy of the GAN discriminator on the held out test set. The accuracy is low for the baseline GAN, which in- dicates it suffered from over-ﬁtting. The accuracy for the other two is basically indistinguishable for each other. This suggests that augmentation by itself is enough to reduce discriminator over-ﬁtting, and that consistency regularization by itself does little to address over-ﬁtting. (Right) FID scores of the three settings. The score for the GAN with only augmentation is not any better than the score for the baseline, even though its discriminator is not over-ﬁtting. The score for the GAN with consis- tency regularization is better than both of the others, suggesting that the consistency regularization acts on the score through some mechanism other than by reducing discriminator over-ﬁtting. Metric Gaussian Noise Random shift & ﬂip Cutout Cutout w/ random shift & ﬂip FID 21.91±0.32 16.04±0.17 17.10±0.29 19.46±0.26 Table 3: FID scores on CIFAR-10 for different types of image augmentation. Gaussian noise is the worst, and random shift and ﬂip is the best, consistent with general consensus on the best way to perform image optimization on CIFAR-10 (Zagoruyko & Komodakis, 2016). with the same label as real without considering semantics, whereas our consistency regularization further enforces learning implicit manifold structure in the discriminator that pulls semantically similar images (i.e., original real image and the transformed image) to be closer in the discriminator representation space. 4.2 H OW DOES THE TYPE OF AUGMENTATION AFFECT RESULTS ? To analyze how different types of data augmentation affect our results, we conduct an ablation study on the CIFAR-10 dataset comparing the results of using four different types of image augmentation: (1) adding Gaussian noise to the image in pixel-space, (2) randomly shifting the image by a few pixels and randomly ﬂipping it horizontally, (3) applying cutout (DeVries & Taylor, 2017) trans- formations to the image, and (4) cutout and random shifting and ﬂipping. As shown in Table 3, random ﬂipping and shifting without cutout gives the best results (FID 16.04) among all four meth- ods. Adding Gaussian noise in pixel-space gives the worst results. This result empirically suggests that adding Gaussian noise is not a good semantic preserving transformation in the image manifold. It’s also noteworthy that the most extensive augmentation (random ﬂipping and shifting with cutout) did not perform the best. One possible reason is that the generator sometimes also generates samples with augmented artifacts (e.g., cutout). If such artifacts do not exist in the real dataset, it might lead to worse FID performance. 5 C ONCLUSION In this paper, we propose a simple, effective, and computationally cheap method – consistency reg- ularization – to improve the performance of GANs. Consistency regularization is compatible with spectral normalization and results in improvements in all of the many contexts in which we evaluated it. Moreover, we have demonstrated consistency regularization is more effective than other regular- ization methods under different loss functions, neural architectures and optimizer hyper-parameter settings. We have also shown simply applying consistency regularization on top of state-of-the-art GAN models can further greatly boost the performance. Finally, we have conducted a thorough study on the design choices and hyper-parameters of consistency regularization. 8Published as a conference paper at ICLR 2020 ACKNOWLEDGMENTS We thank Colin Raffel for feedback on drafts of this article. We also thank Marvin Ritter, Michael Tschannen and Mario Lucic for answering our questions of using compare GAN codebase for large scale GAN evaluation. REFERENCES Martín Arjovsky and Léon Bottou. Towards principled methods for training generative adversarial networks. In ICLR, 2017. Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein GAN. arXiv:1701.07875, 2017. Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. In NeurIPS, 2014. David Berthelot, Nicholas Carlini, Ian J. Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A holistic approach to semi-supervised learning. arXiv:1905.02249, 2019. Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-supervised gans via auxiliary rotation loss. In CVPR, 2019. Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. Ishaan Gulrajani, Faruk Ahmed, Martín Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Im- proved training of wasserstein GANs. In NeurIPS, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In CVPR, 2016. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2017. Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning discrete representations via information maximizing self-augmented training. In ICML, 2017. Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018. Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2019. Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans. arXiv preprint arXiv:1705.07215, 2017. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. A large-scale study on regularization and normalization in gans. In ICML, 2019. Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016. Jae Hyun Lim and Jong Chul Ye. Geometric GAN. arXiv:1705.02894, 2017. 9Published as a conference paper at ICLR 2020 Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal? A large-scale study. In NeurIPS, 2018. Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In ICLR, 2018. Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICLR, 2018a. Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori Koyama. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 2018b. Augustus Odena. Open questions about generative adversarial networks. Distill, 2019. doi: 10. 23915/distill.00018. https://distill.pub/2019/gan-open-problems. Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxil- iary classiﬁer gans. In ICML, 2017. Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. In NeurIPS, pp. 3235–3246, 2018. Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016. Kevin Roth, Aurélien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative adversarial networks through regularization. In NeurIPS, 2017. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. IJCV, 115(3):211–252, 2015. Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transforma- tions and perturbations for deep semi-supervised learning. In NeurIPS, 2016. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NeurIPS, 2016. Dustin Tran, Rajesh Ranganath, and David M. Blei. Deep and hierarchical implicit models. arXiv:1702.08896, 2017. Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang. Improving the improved training of wasserstein gans: A consistency term and its dual effect. In ICLR, 2018. Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation for consistency training. arXiv preprint arXiv:1904.12848, 2019. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016. Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S 4l: Self-supervised semi- supervised learning. arXiv preprint arXiv:1905.03670, 2019. Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks. In ICCV, 2017. Han Zhang, Ian J. Goodfellow, Dimitris N. Metaxas, and Augustus Odena. Self-attention generative adversarial networks. In ICML, 2019. Zizhao Zhang, Yuanpu Xie, and Lin Yang. Photographic text-to-image synthesis with a hierarchically-nested adversarial network. In CVPR, 2018. 10Published as a conference paper at ICLR 2020 APPENDIX A H YPERPARAMETER SETTINGS OF OPTIMIZER Setting lr β 1 β2 Ndis A 0.0001 0.5 0.9 5 B 0.0001 0.5 0.999 1 C 0.0002 0.5 0.999 1 D 0.0002 0.5 0.999 5 E 0.001 0.5 0.9 5 F 0.001 0.5 0.999 5 G 0.001 0.9 0.999 5 Table A1: Hyper-parameters of the optimizer used in our experiments. Here, similar as the experiments in Miyato et al. (2018a); Kurach et al. (2019), we evaluate all reg- ularization methods across 7 different hyperparameters settings for (1) learning rate lr(2) ﬁrst and second order momentum parameters of Adam β1, β2 (3) number of the updates of the discriminator per generator update, Ndis. The details of all the settings are shown in Table A1. Among all these 7 settings, A-D are the \"good\" hyperparameters used in previous publications (Radford et al., 2016; Gulrajani et al., 2017; Kurach et al., 2019); E, F, G are the \"aggressive\" hyperparameter settings in- troduced by Miyato et al. (2018a) to test model performance under noticeably large learning rate or disruptively high momentum. In practice, we ﬁnd setting C generally works the best for SNDCGAN and setting D is the optimal setting for ResNet. These two settings are also the default settings in the Compare GAN codebase for the corresponding network architectures. CIFAR-10 CelebA Figure A1: Comparison of FID scores with different optimizer settings. Figure A1 displays the FID score of all methods with 7 settings A-G. We can observe that con- sistency regularization is fairly robust even for some of the aggressive hyperparameter settings. In general, the proposed consistency regularization can generate better samples with different optimizer settings compared with other regularization methods. 11Published as a conference paper at ICLR 2020 B C OMPARISON OF DIFFERENT REGULARIZATION METHODS WHEN SPECTRAL NORMALIZATION IS NOT USED CIFAR-10 SNDCGAN CIFAR-10 ResNet CelebA SNDCGAN (a) (b) (c) (d) (e) (f) (g) (h) (i) Figure B1: Comparison of FID scores when SN is not used. Here, we compare different regularization methods when spectral normalization (SN) is not used. As shown in Figure B1, our consistency regularization always improves the baseline model (W/O). It also achieves the best FID scores in most of the cases, which demonstrates that consistency reg- ularization does not depend on spectral normalization. By comparing with the results in Figure 2 and Figure 4, we ﬁnd adding spectral normalization will further boost the results. More importantly, the consistency regularization is only method that improve on top of spectral normalization without exception. The other regularization methods do not have this property. C T RAINING SPEED Here we show the actual training speed of discriminator updates for SNDCGAN on CIFAR-10 with NVIDIA Tesla V100. Consistency regularization is around 1.7 times faster than gradient based regularization techniques. Method W/O GP DR JSR Ours (CR-GAN) Speed (step/s) 66.3 29.7 29.8 29.2 51.7 Table C1: Training speed of discriminator updates for SNDCGAN on CIFAR-10. 12Published as a conference paper at ICLR 2020 D G ENERATED SAMPLES FOR UNCONDITIONAL IMAGE GENERATION Figure D1: Comparison of generated samples of CelebA. Ours (FID:14.56)  DR (FID:18.94) JSR (FID: 19.59) W/O (FID:19.00) Real Images GP(FID: 19.74) Figure D2: Comparison of generated samples for unconditional image generation on CIFAR-10 with a ResNet architecture. 13Published as a conference paper at ICLR 2020 W/O GP DR JSR Ours (CR-GAN) Figure D3: Comparison of unconditional generated samples on CIFAR-10 with a ResNet architec- ture, Wasserstein loss and spectral normalization. This is a hard hyperparameter setting where the baseline and previous regularization methods fail to generate reasonable samples. Consistency Reg- ularization is the only regularization method that can generate satisfactory samples in this setting. FID scores are shown in sub-plot (c) of Figure 4. E G ENERATED SAMPLES FOR CONDITIONAL IMAGE GENERATION BigGAN* (FID: 20.42)CR-BigGAN* (FID: 11.67) Figure E1: Comparison of generated samples for conditional image generation on CIFAR-10. Each row shows the generated samples of one class. 14Published as a conference paper at ICLR 2020 Figure E2: Comparison of conditionally generated samples of BigGAN* and CR-BigGAN* on ImageNet. (Left) Generated samples of CR-BigGAN*. (Right) Generated samples of BigGAN*. 15Published as a conference paper at ICLR 2020 Figure E3: More results for conditionally generated samples of BigGAN* and CR-BigGAN* on ImageNet. (Left) Generated samples of CR-BigGAN*. (Right) Generated samples of BigGAN*. 16Published as a conference paper at ICLR 2020 F C OMPARISON WITH INCEPTION SCORE Inception Score (IS) is another GAN evaluation metric introduced by Salimans et al. (2016). Here, we compare the Inception Score of the unconditional generated samples on CIFAR-10. As shown in Table F1, Figure F1 and Figure F2, consistency regularization achieves the best IS result with both SNDCGAN and ResNet architectures. Setting W/O GP DR JSR Ours (CR-GAN) CIFAR-10 (SNDCGAN) 7.54 7.54 7.54 7.52 7.93 CIFAR-10 (ResNet) 8.20 8.04 8.09 8.03 8.40 Table F1: Best Inception Score for unconditional image generation on CIFAR-10. Figure F1: Comparison of IS with a SNDCGAN architecture on different loss settings. Models are trained on CIFAR-10. Figure F2: Comparison of IS with a ResNet architecture on different loss settings. Models are trained on CIFAR-10. 17Published as a conference paper at ICLR 2020 G E FFECT OF THE NUMBER OF LAYERS REGULARIZED IN DISCRIMINATOR Here, we examine the effect of the number of layers regularized in discriminator. In this experiment, we use SNDCGAN architecture with NS loss on the CIFAR-10 dataset. There are 8 intermediate layers in the discriminator. To start, we add consistency only to the last layer (0 intermediate lay- ers). Then we gradually enforce consistency for more intermediate layers. We use two weighting variations to combine the consistency loss across different layers. In the ﬁrst setting, the weight of each layer is the inverse of feature dimension dj in that layer, which corresponds to λj = 1/dj in Equation 4. In the second setting, we give equal weight to each layer, which corresponds toλj = 1. The results for both settings are shown in Figure G1. In both settings, we observe that consistency regularization on the ﬁnal layer achieves reasonably good results. Adding the consistency to ﬁrst few layers in the discriminator harms the performance. For simplicity, we only add consistency regularization in the ﬁnal layer of the discriminator in the rest of our experiments. (a) (b) Figure G1: Comparison of consistency regularization on different number of intermediate layers: (a) ﬁrst weight setting, where the weight for each layer is the inverse of its feature dimension (b) second weight setting, where each layer has equal weight. H C ONSISTENCY REGULARIZATION ON THE GENERATED SAMPLES In this section, we investigate the effect of adding consistency regularization for the generated sam- ples. We compare four settings, no consistency regularization (W/O), regularization only on the real samples (CR-Real), consistency regularization only on the fake samples produced by the genera- tor (CR-Fake) and regularization on both real and fake samples (CR-All). CR-Real is presented in Algorithm 1. CR-Fake has similar computational cost as CR-Real and CR-All doubles the compu- tational cost, since both the augmented real and fakes samples need to be fed into the discriminator to calculate the consistency loss. As shown in Figure H1, CR-Real, CR-Fake and CR-All are always better than the baseline without consistency regularization. In addition, CR-Real is consistently better than CR-Fake. It is interesting to note that CR-All is not always better than CR-real given the extra computational costs and stronger regularization. For example, CR-All improves FID from 20.21 of CR-Real to 15.51 for SNDCGAN, but it also gives slightly worse results for ResNet (14.93 vs 15.07) and for CR-BigGAN* (11.48 vs 12.51). We observe that enforcing additional consistency on the generated samples gives more performance gain when the model capacity is small and that gain decreases when model capacity increases. For computational efﬁciency and simplicity of the training algorithm, we use consistency regularization on real samples for the rest of our experiments. 18Published as a conference paper at ICLR 2020 W/O CR-Real CR-Fake CR-All (a) 0 5 10 15 20 25 30FID W/O CR-Real CR-Fake CR-All (b) 0 5 10 15 20FID W/O CR-Real CR-Fake CR-All (c) 0 5 10 15 20 25 30FID Figure H1: Comparison of FID scores with no consistency regularization (W/O), regularization only on the real samples (CR-Real), consistency regularization only on the fake samples produced by the generator (CR-Fake) and regularization on both real and fake samples (CR-All) for (a) unconditional image generation on CIFAR-10 with SNDCGAN, (b) unconditional image generation on CIFAR-10 with ResNet, (c) conditional image generation on CIFAR-10 with CR-BigGAN*. 19",
      "references": [
        "Towards principled methods for training generative adversarial networks.",
        "Wasserstein GAN.",
        "Learning with pseudo-ensembles.",
        "Mixmatch: A holistic approach to semi-supervised learning.",
        "Large scale gan training for high fidelity natural image synthesis.",
        "Self-supervised gans via auxiliary rotation loss.",
        "Improved regularization of convolutional neural networks with cutout.",
        "Generative adversarial nets.",
        "Improved training of wasserstein GANs.",
        "Deep residual learning for image recognition.",
        "GANs trained by a two time-scale update rule converge to a local nash equilibrium.",
        "Learning discrete representations via information maximizing self-augmented training.",
        "Progressive growing of GANs for improved quality, stability, and variation.",
        "A style-based generator architecture for generative adversarial networks.",
        "On convergence and stability of gans.",
        "Learning multiple layers of features from tiny images.",
        "A large-scale study on regularization and normalization in gans.",
        "Temporal ensembling for semi-supervised learning.",
        "Geometric GAN.",
        "Are gans created equal? A large-scale study.",
        "cGANs with projection discriminator.",
        "Spectral normalization for generative adversarial networks.",
        "Virtual adversarial training: a regularization method for supervised and semi-supervised learning.",
        "Open questions about generative adversarial networks.",
        "Conditional image synthesis with auxiliary classifier gans.",
        "Realistic evaluation of deep semi-supervised learning algorithms.",
        "Unsupervised representation learning with deep convolutional generative adversarial networks.",
        "Stabilizing training of generative adversarial networks through regularization.",
        "ImageNet large scale visual recognition challenge.",
        "Regularization with stochastic transformations and perturbations for deep semi-supervised learning.",
        "Improved techniques for training gans.",
        "Deep and hierarchical implicit models.",
        "Improving the improved training of wasserstein gans: A consistency term and its dual effect.",
        "Unsupervised data augmentation for consistency training.",
        "Wide residual networks.",
        "S4L: Self-supervised semi-supervised learning.",
        "StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks.",
        "Self-attention generative adversarial networks.",
        "Photographic text-to-image synthesis with a hierarchically-nested adversarial network."
      ],
      "meta_data": {
        "arxiv_id": "1910.12027v2",
        "authors": [
          "Han Zhang",
          "Zizhao Zhang",
          "Augustus Odena",
          "Honglak Lee"
        ],
        "published_date": "2019-10-26T09:06:03Z",
        "github_url": "https://github.com/google/compare_gan"
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes consistency regularization for GAN discriminators by augmenting inputs with semantic-preserving transformations and penalizing the discriminator’s sensitivity to these augmentations. Demonstrates that this simple regularizer is compatible with spectral normalization and with multiple GAN loss formulations, yielding state-of-the-art FID on CIFAR-10 and CelebA, and substantially improves conditional generation on ImageNet-2012 and CIFAR-10.",
        "methodology": "Introduces a consistency regularization term Lcr = ||D(x) - D(T(x))||^2 added to the discriminator loss, where T is a stochastic, semantics-preserving augmentation (e.g., random shift, horizontal flip, possibly cutout). Regularization targets the discriminator layers (typically the last layer before activation) with weight λ. The CR-GAN framework is compatible with various GAN losses (non-saturating, hinge, Wasserstein with spectral normalization) and requires only a small computational overhead (one extra forward/backward pass for the augmented input per discriminator update).",
        "experimental_setup": "Datasets: CIFAR-10, CelebA-HQ-128, ImageNet-2012 downsampled to 128x128. Evaluation with Frechet Inception Distance (FID) and Inception Score (IS). Baselines include no regularization, Gradient Penalty (GP), DRAGAN (DR), and JS-Regularizer (JSR). Architectures tested: SNDCGAN, a ResNet-based GAN, and BigGAN* with and without CR. Training details follow Compare GAN settings; spectral normalization is used in most experiments. Evaluations cover unconditional generation on CIFAR-10/CelebA and conditional generation on CIFAR-10/ImageNet, with ablations across loss functions, hyperparameters, and network architectures.",
        "limitations": "Limitations include dependence on the choice of semantics-preserving augmentations and dataset/architecture; some augmentations (e.g., Gaussian noise) can hurt performance; benefits can vary with model capacity (very large models may see smaller gains); added regularization adds some computational cost and is not inherently beneficial in non-image domains without an appropriate augmentation strategy; empirical results are primarily on image datasets.",
        "future_research_directions": "Explore a broader class of semantic augmentations and domain-adaptive perturbations; study theoretical underpinnings of why CR-GAN improves semantic representation in the discriminator; extend consistency regularization to other modalities (video, 3D, audio) and to diffusion-based or other generative models; analyze layer-wise regularization effects and automation for lambda scheduling; investigate interactions with alternative normalization schemes and more diverse GAN architectures; refine augmentation choices to avoid introducing artifacts in generated samples.",
        "experimental_code": "penalty_loss = penalty_lib.get_penalty_loss(\n        x=images, x_fake=generated, y=y, is_training=is_training,\n        discriminator=self.discriminator)\n    self.d_loss += self._lambda * penalty_loss\n\n@gin.configurable(whitelist=[])\ndef no_penalty():\n  return tf.constant(0.0)\n\n@gin.configurable(whitelist=[])\ndef dragan_penalty(discriminator, x, y, is_training):\n  \"\"\"Returns the DRAGAN gradient penalty.\n\n  Args:\n    discriminator: Instance of `AbstractDiscriminator`.\n    x: Samples from the true distribution, shape [bs, h, w, channels].\n    y: Encoded class embedding for the samples. None for unsupervised models.\n    is_training: boolean, are we in train or eval model.\n\n  Returns:\n    A tensor with the computed penalty.\n  \"\"\"\n  with tf.name_scope(\"dragan_penalty\"):\n    _, var = tf.nn.moments(x, axes=list(range(len(x.get_shape()))))\n    var = tf.maximum(var, 0.0)\n    std = tf.sqrt(var)\n    x_noisy = x + std * (ops.random_uniform(x.shape) - 0.5)\n    x_noisy = tf.clip_by_value(x_noisy, 0.0, 1.0)\n    logits = discriminator(x_noisy, y=y, is_training=is_training, reuse=True)[1]\n    gradients = tf.gradients(logits, [x_noisy])[0]\n    slopes = tf.sqrt(0.0001 + tf.reduce_sum(\n        tf.square(gradients), reduction_indices=[1, 2, 3]))\n    gradient_penalty = tf.reduce_mean(tf.square(slopes - 1.0))\n    return gradient_penalty\n\n@gin.configurable(whitelist=[])\ndef wgangp_penalty(discriminator, x, x_fake, y, is_training):\n  \"\"\"Returns the WGAN gradient penalty.\n\n  Args:\n    discriminator: Instance of `AbstractDiscriminator`.\n    x: samples from the true distribution, shape [bs, h, w, channels].\n    x_fake: samples from the fake distribution, shape [bs, h, w, channels].\n    y: Encoded class embedding for the samples. None for unsupervised models.\n    is_training: boolean, are we in train or eval model.\n\n  Returns:\n    A tensor with the computed penalty.\n  \"\"\"\n  with tf.name_scope(\"wgangp_penalty\"):\n    alpha = ops.random_uniform(shape=[x.shape[0].value, 1, 1, 1], name=\"alpha\")\n    interpolates = x + alpha * (x_fake - x)\n    logits = discriminator(\n        interpolates, y=y, is_training=is_training, reuse=True)[1]\n    gradients = tf.gradients(logits, [interpolates])[0]\n    slopes = tf.sqrt(0.0001 + tf.reduce_sum(\n        tf.square(gradients), reduction_indices=[1, 2, 3]))\n    gradient_penalty = tf.reduce_mean(tf.square(slopes - 1.0))\n    return gradient_penalty\n\n@gin.configurable(whitelist=[])\ndef l2_penalty(discriminator):\n  \"\"\"Returns the L2 penalty for each matrix/vector excluding biases.\n\n  Assumes a specific tensor naming followed throughout the compare_gan library.\n  We penalize all fully connected, conv2d, and deconv2d layers.\n\n  Args:\n    discriminator: Instance of `AbstractDiscriminator`.\n\n  Returns:\n     A tensor with the computed penalty.\n  \"\"\"\n  with tf.name_scope(\"l2_penalty\"):\n    d_weights = [v for v in discriminator.trainable_variables\n                 if v.name.endswith(\"/kernel:0\")]\n    return tf.reduce_mean(\n        [tf.nn.l2_loss(i) for i in d_weights], name=\"l2_penalty\")",
        "experimental_info": "- Method: Consistency Regularization for GAN discriminators (CR-GAN). Adds a consistency term Lcr = ||D(x) - D(T(x))||^2 where T is a stochastic, semantics-preserving augmentation of x. The regularizer is applied to the discriminator layers (typically the last layer before activation). A weight lambda scales the regularization term in the discriminator loss.\n- In codebase, regularization is implemented as a generic penalty added to the discriminator loss: penalty_loss = penalty_lib.get_penalty_loss(...); self.d_loss += self._lambda * penalty_loss. The penalty can be any of the penalty functions defined in penalty_lib (no_penalty, dragan_penalty, wgangp_penalty, l2_penalty) and is applied via the same interface. This setup allows CR-GAN-like behavior if one defines a penalty that computes ||D(x) - D(T(x))||^2 using the augmented input.\n- Experimental settings (as described in 'Experimental Setup') include:\n  - Datasets: CIFAR-10, CelebA-HQ-128, ImageNet-2012 downsampled to 128x128.\n  - Metrics: Frechet Inception Distance (FID) and Inception Score (IS).\n  - Baselines for comparison: no regularization (baseline), Gradient Penalty (GP), DRAGAN (DR), and JS-Regularizer (JSR).\n  - Architectures: SNDCGAN, a ResNet-based GAN, and BigGAN* with and without CR (i.e., with consistency reg).\n  - Training details: Following Compare GAN settings; spectral normalization is used in most experiments; evaluations include unconditional generation on CIFAR-10/CelebA and conditional generation on CIFAR-10/ImageNet; ablations across loss functions, hyperparameters, and network architectures.\n  - Reg weight lambda: CR-GAN uses a small additional hyperparameter lambda to scale Lcr in the discriminator loss.\n  - Deployment notes: The regularization introduces a small computational overhead (one extra forward/backward pass for the augmented input per discriminator update) and can depend on the choice of semantically-preserving augmentations (e.g., random shifts, flips, etc.)."
      }
    },
    {
      "title": "Consistency Regularization for Variational Auto-Encoders",
      "full_text": "Consistency Regularization for Variational Auto-Encoders Samarth Sinha Vector Institute University of Toronto Adji B. Dieng Google Brain Princeton University Abstract Variationalauto-encoders( vaes)areapowerfulapproachtounsupervisedlearning. They enable scalable approximate posterior inference in latent-variable models usingvariationalinference( vi). Avaepositsavariationalfamilyparameterizedby adeepneuralnetwork—calledan encoder—thattakesdataasinput. Thisencoderis shared across all the observations, which amortizes the cost of inference. However the encoder of avae has the undesirable property that it maps a given observation and a semantics-preserving transformation of it to diﬀerent latent representations. This“inconsistency\"oftheencoderlowersthequalityofthelearnedrepresentations, especially for downstream tasks, and also negatively aﬀects generalization. In this paper,weproposearegularizationmethodtoenforceconsistencyin vaes. Theidea is to minimize the Kullback-Leibler (kl) divergence between the variational distri- bution when conditioning on the observation and the variational distribution when conditioning on a random semantic-preserving transformation of this observation. This regularization is applicable to anyvae. In our experiments we apply it to four diﬀerent vae variants on several benchmark datasets and found it always improves the quality of the learned representations but also leads to better generalization. In particular, when applied to the nouveau variational auto-encoder (nvae), our regularizationmethodyieldsstate-of-the-artperformanceon mnist,cifar-10,and celeba. Wealsoappliedourmethodto3Ddataandfounditlearnsrepresentations of superior quality as measured by accuracy on a downstream classiﬁcation task. Finally, we show our method can even outperform the triplet loss, an advanced and popular contrastive learning-based method for representation learning.1 1 Introduction Variationalauto-encoders( vaes)havesigniﬁcantlyimpactedresearchonunsupervisedlearning. They have been used in several areas, including density estimation (Kingma & Welling, 2013; Rezende et al., 2014), image generation (Gregor et al., 2015), text generation (Bowman et al., 2015; Fang et al., 2019), music generation (Roberts et al., 2018), topic modeling (Miao et al., 2016; Dieng et al., 2019), and recommendation systems (Liang et al., 2018).Vaes have also been used for diﬀerent representation learning problems such as semi-supervised learning (Kingma et al., 2014), anomaly detection (An & Cho, 2015; Zimmerer et al., 2018), language modeling Bowman et al. (2015), active learning (Sinha et al., 2019), continual learning (Achille et al., 2018), and motion prediction of agents (Walker et al., 2016). This widespread application ofvae representations makes it critical that we focus on improving them. vaesextend deterministic auto-encoders to probabilistic generative modeling. The encoder of avae parameterizes an approximate posterior distribution over latent variables of a generative model. The encoder is shared between all observations, which amortizes the cost of posterior inference. Once ﬁtted, the encoder of avae can be used to obtain low-dimensional representations of data, (e.g. for 1Code for this work can be found athttps://github.Com/sinhAsAm/CRVAE 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2105.14859v2  [cs.LG]  6 Jun 2022(a)  (b)  (c) Figure 1:Illustration of theinconsistencyproblem invaesand howcr-vaesaddress this problem. Thered dotscorrespondtotherepresentationsoffewimagesfrom mnist. Theblue dotscorrespondto therepresentationsofthetransformedimages. Thetransformationsusedherearerotations,translations, and scaling; they are semantics-preserving. The arrows connect the representations of any two pairs of an image and its transformation. The shorter the arrow, the better.(a): Thevae maps the two sets of images to diﬀerent areas in the latent space.(b): Even when trained with the original dataset augmentedwiththetransformedimages, the vae stillmapsthetwosetsofimagestodiﬀerentpartsin the latent space.(c): Thecr-vaemaps an image and its transformation to nearby areas in the latent space. downstream tasks.) The quality of these representations is therefore very important to a successful application ofvaes. Researchers have looked at ways to improve the quality of the latent representations ofvaes, often tackling the so-calledlatent variable collapseproblem—in which the approximate posterior distribu- tion induced by the encoder collapses to the prior over the latent variables (Bowman et al., 2015; Kim et al., 2018; Dieng et al., 2018; He et al., 2019; Fu et al., 2019). In this paper, we focus on a diﬀerent problem pertaining to the latent representations ofvaes for image data. Indeed, the encoder of a ﬁttedvae tends to map an image and a semantics-preserving transformation of that image to diﬀerent parts in the latent space. This “inconsistency\" of the encoder aﬀects the quality of the learned representations and generalization. We propose a method to enforce consistencyin vaes. Theideaissimpleandconsistsinmaximizingthelikelihoodoftheimageswhile minimizing the Kullback-Leibler (kl) divergence between the approximate posterior distribution induced by the encoder when conditioning on the image, on one hand, and its transformation, on the other hand. This regularization technique can be applied to anyvae variant to improve the quality of the learned representations and boost generalization performance. We call avae with this form of regularization, a consistency-regularized variational auto-encoder (cr-vae). Figure 1 illustrates the inconsistency problem ofvaesand howcr-vaesaddress this problem on mnist. The red dots are representations of a few images and the blue dots are the representations of their transformations. We applied semantics-preserving transformations: rotation, translation, and scaling. Thevae maps each image and its transformation to diﬀerent parts in the latent space as evidencedbythelongarrowsconnectingeachpair(a). Evenwhenweincludethetransformedimages to the data and ﬁt thevae the inconsistency problem still occurs (b). Thecr-vaedoes not suﬀer from the inconsistency problem; it maps each image and its transformation to nearby areas in the latent space, as evidenced by the short arrows connecting each pair (c). In our experiments (see Section 4), we apply the proposed technique to fourvae variants, the original vae (Kingma & Welling, 2013), the importance-weighted auto-encoder (iwae) (Burda et al., 2015), theβ-vae (Higgins et al., 2017), and the nouveau variational auto-encoder (nvae) (Vahdat & Kautz, 2020). We found, on four diﬀerent benchmark datasets, thatcr-vaesalways yield better representations and generalize better than their basevaes. In particular, consistency-regularized nouveau variational auto-encoders (cr-nvaes) yield state-of-the-art performance onmnist and cifar-10. We also appliedcr-vaesto 3D data where these conclusions still hold. 2 Method We consider a latent-variable modelpθ(x,z) =pθ(x|z) ·p(z), wherex denotes an observation and z is its associated latent variable. The marginalp(z) is a prior over the latent variable andpθ(x|z) 2is an exponential family distribution whose natural parameter is a function ofz parameterized byθ, e.g. through a neural network. Our goal is to learn the parametersθand a posterior distribution over the latent variables. The approach ofvaesis to maximize the evidence lower bound (elbo), a lower bound on the log marginal likelihood of the data, Lvae = elbo = Eqφ(z|x) [ log (pθ(x,z) qφ(z|x) )] (1) whereqφ(z|x) is an approximate posterior distribution over the latent variables. The idea of avae is to let the parameters of the distributionqφ(z|x) be given by the output of a neural network, with parametersφ, that takesx as input. The parametersθandφare then jointly optimized by maximizing a Monte Carlo approximation of theelbo using the reparameterization trick (Kingma & Welling, 2013). Consider a semantics-preserving transformationt(˜x|x) of datax (e.g. rotation or translation for images.) A good representation learning algorithm should provide similar latent representations forx and ˜x. This is not the case for thevae that maximizes Equation 1 and its variants. Once ﬁt to data, theencoderofa vae isunabletoyieldsimilarlatentrepresentationsforadata x anditstranformation ˜x (see Figure 1). This is because there is nothing in Equation 1 that forces this desideratum. We now propose a regularization method that ensuresconsistency of the encoder of avae. We call avae with such a regularization acr-vae. The regularization proposed is applicable to many variants of thevae such as theiwae (Burda et al., 2015), theβ-vae (Higgins et al., 2017), and the nvae (Vahdat & Kautz, 2020). In what follows, we use the standardvae, the one that maximizes Equation 1, as the basevae to regularize to illustrate the method. Consider an imagex. Denote byt(˜x|x) the random process by which we generate˜x, a semantics- preserving transformation ofx. We draw˜x from t(˜x|x) as follows: ˜x ∼t(˜x|x) ⇐⇒ϵ∼p(ϵ) and ˜x = g(x,ϵ). (2) Hereg(x,ϵ) is a semantics-preserving transformation of the imagex, e.g. translation with random lengthϵdrawn fromp(ϵ) =U[−δ,δ] for some thresholdδ. Acr-vaethen maximizes Lcr-vae(x) =Lvae(x) +Et(˜x|x) [Lvae(˜x)] −λ·R(x,φ) (3) where the regularization termR(x,φ) is R(x,φ) =Et(˜x|x) [kl (qφ(z|˜x)||qφ(z|x))] . (4) Maximizing the objective in Equation 3 maximizes the likelihood of the data and their augmentations while enforcing consistency throughR(x,φ). MinimizingR(x,φ), which only aﬀects the encoder (with parametersφ), forces each observation and the corresponding augmentations to lie close to each other in the latent space. The hyperparameterλ≥0 controls the strength of this constraint. The objective in Equation 3 is intractable but we can easily approximate it using Monte Carlo with the reparameterization trick. In particular, we approximate the regularization term with one sample fromt(˜x|x) and make the dependence to this sample explicit using the notationR(x,˜x,φ). Algorithm 1 illustrates this in greater detail. Although we show the application of consistency regularization using thevae that maximizes theelbo,Lvae(·) in Equation 3 can be replaced with anyvae objective. 3 Related Work Applying consistency regularization tovaes, as we do in this paper, has not been previously explored. Consistency regularization is a widely used technique for semi-supervised learning (Bachman et al., 2014; Sajjadi et al., 2016; Laine & Aila, 2016; Miyato et al., 2018; Xie et al., 2019). The core idea behind consistency regularization for semi-supervised learning is to force classiﬁers to learn representations that are insensitive to semantics-preserving changes to images, so as to improve classiﬁcation of unlabeled images. Examples of semantics-preserving changes used in the literature include rotation, zoom, translation, crop, or adversarial attacks. Consistency is often enforced by minimizing theL2 distance between a classiﬁer’s logit output for an image and the logit output for its semantics-preserving transformation (Sajjadi et al., 2016; Laine & Aila, 2016), or by minimizing 3Algorithm 1:Consistency Regularization for Variational Autoencoders input : Datax, consistency regularization strengthλ, latent space dimensionality K Initialize parametersθ,φ for iteration t= 1,2,... do Draw minibatch of observations{xn}B n=1 for n= 1,...,B do Transform the data:ϵn ∼p(ϵn) and ˜xn = T(xn,ϵn) Get variational mean and variance for the data: µn = W⊤NN(xn; φ) +a and σn = softplus(Q⊤NN(xn; φ) +b) Get Ssamples from the variational distribution when conditioning onxn: η(s) ∼N(0,I) and z(s) n = µn + η(s) ·σn fors= 1,...,S Get variational mean and variance for the transformed data: ˜µn = W⊤NN(˜xn; φ) +a and ˜σn = softplus(Q⊤NN(˜xn; φ) +b) Get Ssamples from the variational distribution when conditioning on˜xn: η(s) ∼N(0,I) and ˜z(s) n = ˜µn + η(s) ·˜σn fors= 1,...,S end Compute Lvae(x): Lvae(x) ≈ 1 B ∑B n=1 1 S ∑S s=1 [ log pθ(xn,z(s) n ) −log qφ(z(s) n |xn) ] Compute Lvae(˜x): Lvae(˜x) ≈ 1 B ∑B n=1 1 S ∑S s=1 [ log pθ(˜xn,˜z(s) n ) −log qφ(˜z(s) n |˜xn) ] Compute KL consistency regularizer: R(x,˜x,φ) =1 2 ∑K k=1 ( ˜σ2 nk+(˜µnk−µnk)2 σ2 nk −1 + 2·log σnk ˜σnk ) Compute ﬁnal loss: Lcr-vae(x) =Lvae(x) +Lvae(˜x) −λ·R(x,˜x,φ) Backpropagate throughL(x,θ,φ ) =−Lcr-vae(x) and take a gradient step forθand φ end the kl divergence between the classiﬁer’s label distribution induced by the image and that of its tranformation (Miyato et al., 2018; Xie et al., 2019). More recently, consistency regularization has been applied to generative adversarial networks (gans)(Goodfellowetal.,2014). IndeedWeietal.(2018)andZhangetal.(2020)showthatapplying consistencyregularizationonthediscriminatorofa gan—alsoaclassiﬁer—cansubstantiallyimprove its performance. Theideawedevelopinthispaperdiﬀersfromtheworksaboveintwoways. First,itappliesconsistency regularization tovaesfor image data. Second, it leverages consistency regularization, not in the label or logit space, as done in the works mentioned above, but in the latent space. Although diﬀerent, consistency regularization forvaesrelates to works that study ways to constrain the sensitivity of encoders to various perturbations. For example, denoising auto-encoders (daes) and their variants (Vincent et al., 2008, 2010) corrupt an imagex intox′, typically using Gaussian noise, and then minimize the distance between the reconstruction ofx′and theun-corrupted imagex. The motivation is to learn representations that are insensitive to the added noise. Our work diﬀers in that we do not constrain the decoder to recover the original image from the corrupted image but, rather, to constrain the encoder to recover the latent representation of the original image from the corrupted image via akl divergence minimization constraint. Contractive auto-encoders (caes) (Rifai et al., 2011) share a similar goal withcr-vaes. Acae is an auto-encoder whose encoder is constrained by minimizing the norm of the Jacobian of the output of the encoder with respect to the input image. This norm constraint on the Jacobian forces the representations learned by the encoder to be insensitive to changes in the input. Our work diﬀers in several main ways. First,cr-vaesare not deterministic auto-encoders, contrary tocaes. We can easily sample from acr-vae, as for anyvae, which is not the case for acae. Second, acae does 4Table 1:cr-vaeslearn better representations than their basevaeson all three benchmark datasets. Although ﬁtting the basevae with augmentations does improve the representations, adding the consistency regularization further improves the quality of these learned representations. The value of βfor theβ-vae is inside the parentheses. mnist omniglot celeba Method MI AU MI AU MI AU vae 124.5 ±1.1 36 ±0.8 105 .4 ±1.2 50 ±0.0 33 .8 ±0.2 32 ±0.9 vae + Aug 125.9 ±0.2 42 ±0.5 105 .9 ±0.7 50 ±0.0 34 .1 ±0.8 33 ±0.9 cr-vae 126.3 ±0.9 47 ±0.5 107.8 ±1.1 50 ±0.0 34.9 ±0.5 33 ±1.2 iwae 127.1 ±0.7 39 ±0.5 110 .3 ±1.1 50 ±0.0 36 .9 ±0.5 36 ±1.6 iwae+Aug 129.0 ±0.9 45 ±0.8 112 .9 ±0.7 50 ±0.0 37 .0 ±0.2 36 ±1.2 cr-iwae 129.7 ±1.0 50 ±0.0 115.3 ±0.8 50 ±0.0 38.4 ±0.5 36 ±1.9 β-vae (0.5) 284.3 ±1.1 50 ±0.0 143 .4 ±1.0 50 ±0.0 75 .8 ±0.5 49 ±0.5 β-vae (0.5) + Aug 289.3 ±1.0 50 ±0.0 159 .6 ±1.3 50 ±0.0 75 .7 ±0.3 49 ±0.0 β-cr-vae(0.5) 291.9 ±0.7 50 ±0.0 169.5 ±0.5 50 ±0.0 77.1 ±0.1 50 ±0.0 β-vae (10) 6.3 ±0.6 8 ±1.7 1 .4 ±0.2 4 ±0.9 3 .6 ±0.3 7 ±0.8 β-vae (10) + Aug 6.5 ±0.5 9 ±1.1 1 .6 ±0.2 4 ±0.5 3 .7 ±0.1 7 ±0.0 β-cr-vae(10) 6.9 ±0.6 10 ±0.5 1.6 ±0.1 4 ±0.5 3.7 ±0.4 9 ±0.9 notapplytransformationstotheinputimage,whichlimitsthesensitivitiesitcanlearntolimittothose exhibited in the training set. Finally,caesuse the Jacobian to impose a consistency constraint, which are not as easy to compute as thekl divergence we use on the variational distribution induced by the encoder. 4 Empirical Study In this section we show that acr-vaeimproves the learned representations of its basevae and positively aﬀects generalization performance We also show that the proposed regularization method is amenable to diﬀerentvae variants by applying it not only to the originalvae but also to theiwae, the β-vae, and thenvae. We showcase the importance of the KL regularization term by conducting an ablation study. We found that only regularizing with data augmentation improves performance but that accounting for thekl term (λ> 0) further improves the quality of the learned representations and generalization. We will conduct three sets of experiments. In the ﬁrst experiment, we will apply the regularization method proposed in this paper to standardvaessuch as the originalvae, theiwae, and theβ-vae. We usemnist, omniglot, andceleba as datasets for this experiment. Forceleba, we choose the 32x32 resolution for this experiment. Our results show that adding consistency regularization always improves upon the basevae, both in terms of the quality of the learned representations and generalization. We conduct an ablation study and also report performance of the diﬀerentvae variants above when they are ﬁtted with the original data and their augmentations. The results from this ablation highlight the importance of settingλ> 0. Inthesecondsetofexperimentsweapplyourmethodtoalarge-scale vae, thelatestnvae (Vahdat& Kautz, 2020). We usemnist,cifar-10, andceleba as datasets for this experiment. We increased the resolution for theceleba dataset for this experiment to64x64. We reach the same conclusions as for the ﬁrst sets of experiments;cr-vaesimprove the learned representations and generalization of their basevaes. In this particular setting, thecr-nvaeachieves state-of-the-art generalization performanceonboth mnist andcifar-10. Thisstate-of-the-artperformancecouldn’tbereachsimply by training thenvae with augmentations, as our results show. Finally,inathirdsetofexperiments,weapplyourregularizationtechniquetoa3Dpoint-clouddataset called ShapeNet (Chang et al., 2015). We adapt a high-performing auto-encoding method called FoldingNet (Yang et al.,2018) to itsvae counterpart and apply the methodwe described in thispaper to thatvae variant on the ShapeNet dataset. We found that adding consistency regularization yields better learned representations. We next describe in great detail the set up for each of these experiments and the results showcasing the usefulness of the regularization method we propose in this paper. 5Table 2:cr-vaeslearn representations that yield higher accuracy on downstream classiﬁcation than their basevaes. These results correspond to the accuracy from a linear classiﬁer that was ﬁtted on the training. We fed this classiﬁer with the representations learned by each method. On bothmnist and cifar-10, cr-vaesyield higher accuracy. Method mnist cifar -10 vae 98.5 32.6 vae+Aug 98.9 40.1 cr-vae 99.4 44.7 iwae 98.6 35.8 iwae+Aug 99.9 37.1 cr-iwae 99.9 44.8 β-vae (0.5) 97.6 27.0 β-vae (0.5)+Aug 98.7 27.6 β-cr-vae(0.5) 98.9 30.0 β-vae (10) 99.4 36.5 β-vae (10)+Aug 99.6 42.1 β-cr-vae(10) 99.6 46.1 Table 3: cr-vaes generalize better than their basevaes on almost all cases; they achieve lower negative log-likelihoods. Although training the basevaeswith the augmented data improves general- ization, adding the consistency regularization term further improves generalization performance. Method mnist omniglot celeba vae 83.7 ±0.3 128 .2 ±0.8 66 .1 ±0.2 vae + Aug 82.8 ±0.4 125 .7 ±0.2 66 .0 ±0.2 cr-vae 81.2 ±0.2 124.1 ±0.1 65.9 ±0.2 iwae 81.7 ±0.3 127 .5 ±0.5 65 .3 ±0.1 iwae+Aug 80.4 ±0.2 125 .0 ±0.6 65 .3 ±0.1 cr-iwae 79.7 ±0.3 123.6 ±0.5 65.0 ±0.2 β-vae (0.5) 92.6 ±0.3 137 .1 ±0.2 68 .7 ±0.2 β-vae (0.5) + Aug 90.0 ±0.5 134 .6 ±0.5 68 .8 ±0.2 β-cr-vae(0.5) 85.7 ±0.6 132.5 ±0.3 68.2 ±0.1 β-vae (10) 126.1 ±1.8 157 .5 ±1.1 92 .7 ±0.5 β-vae (10) + Aug 127.1 ±1.0 157.3 ±0.5 92 .7 ±0.3 β-cr-vae(10) 126.2 ±0.5 157 .6 ±0.6 92.6 ±0.1 4.1 Application to standardvaes on benchmark datasets We apply consistency regularization, as described in this paper, to the originalvae, theiwae, and the β-vae. We now describe the set up and results for this experiment. Datasets. We study three benchmark datasets that we brieﬂy describe below. We ﬁrst consider mnist. mnist is a handwritten digit recognition dataset with60,000 images in the training set and 10,000 images in the test set (LeCun, 1998). We form a validation set of10,000 images randomly sampled from the training set. We also consideromniglot, a handwritten alphabet recognition dataset (Lake et al., 2011). This dataset is composed of19,280 images. We use16,280 randomly sampled images for training and 1,000 for validation and the remaining2,000 samples for testing. Finallyweconsider celeba. Itisadatasetoffaces,consistingof 162,770 imagesfortraining, 19,867 images for validation, and19,962 images for testing (Liu et al., 2018). We set the resolution to32x32 for this experiment. Transformations t(˜x|x). We consider three transformations variants for image datat(˜x|x). The ﬁrstrandomlytranslatesanimage [−2,2] pixelsinanydirection. Thesecondtransformationrandomly 6Table 4:The regularization termλaﬀects both generalization performance and the quality of the learned representations. Many values ofλperform better than the basevae. However a large enough value ofλ, e.g.λ= 1, can lead to worse performance than the basevae because for large values ofλ the regularization term takes over the data-term in the objective function. λ MI AU NLL vae −− 124.5 36 83 .7 cr-vae 0.001 125 .0 38 83 .5 cr-vae 0.01 125 .9 41 82 .4 cr-vae 0.1 126.3 47 81.2 cr-vae 1 124 .3 47 83 .9 Table 5:The choice of augmentation aﬀects both generalization performance and the quality of the learned representations. Jointly using all augmentations works best. Augmentation MI AU NLL Rotations only 125.8 45 82 .1 Translations only 126.1 45 81 .9 Scaling only 125.1 42 82 .7 All 126.3 47 81.2 rotates an image uniformly in[−15,15] degrees clockwise. Finally the third transformation randomly scales an image by a factor uniformly sampled from[0.9,1.1]. Evaluation metrics.Theregularizationmethodweproposeinthispaperismainlyaimedatimproving the learned representations ofvaes. To assess these representations we use three metrics: mutual information, number of active latent units, and accuracy on a downstream classiﬁcation task. We also evaluate the eﬀect of the proposed method on generalization to unseen data. For that we also report negative log-likelihood. We deﬁne each of these metrics next. Mutual information (MI).The ﬁrst quality metric is the mutual informationI(z; x) between the observations and the latents under the joint distribution induced by the encoder, I(z; x) =Epd(x) [KL(qφ(z|x)||p(z)) −kl(qφ(z)||p(z))] (5) where pd(x) is the empirical data distribution andqφ(z) is theaggregated posterior, the marginal overz induced by the joint distribution deﬁned bypd(x) and qφ(z|x). The mutual information is intractable but we can approximate it with Monte Carlo. Higher mutual information corresponds to more interpretable latent variables. Number of active latent units (AU).The second quality metrics we consider is the number of active latent units (AU). It is deﬁned in Burda et al. (2015) and measures the “activity\" of a dimension of the latent variablesz. A latent dimension is “active\" if Covx(Eu∼qφ(u|x)) >δ (6) where δ is a threshold deﬁned by the user. For our experiments we setδ = 0.01. The higher the number of latent active units, the better the learned representations. Accuracy on downstream classiﬁcation.This metric is calculated by ﬁtting a givenvae, taking the learned representations for each data in the test set and computing the accuracy from the prediction of the labels of the images in that same test set by a classiﬁer ﬁtted on the training set. This metric is only applicable to labelled datasets. Negative log-likelihood.We use negative held-out log-likelihood to assess generalization. Consider an unseen datax∗, its negative held-out log-likelihood under the ﬁtted model is log pθ(x∗) =−log ( Eqφ(z|x∗) [pθ(x∗,z) qφ(z|x∗) ]) . (7) 7Table 6: The cr-vaeoutperforms a popular and advanced contrastive learning technique called triplet losson both generalization performance and quality of learned representations. Method MI AU NLL vae 124.5 36 83 .7 vae + augmentations 125.9 42 82 .8 vae + triplet loss 124.9 39 83 .1 cr-vae 126.3 47 81.2 This is intractable and we approximate it using Monte Carlo, log pθ(x∗) ≈−log 1 S S∑ s=1 pθ(x∗,z(s)) qφ(z(s)|x∗) (8) where z(1),..., z(S) ∼qφ(z|x∗). Settings. The vaesare built on the same architecture as Tolstikhin et al. (2017). The networks are trainedwiththeAdamoptimizerwithalearningrateof 10−4 (Kingma&Ba,2014)andtrainedfor 100 epochs with a batch size of64. We set the dimensionality of the latent variables to50, therefore the maximumnumberofactivelatentunitsinthelatentspaceis 50. Wefoundλ= 0.1 tobebestaccording to cross-validation using held-out log-likelihood and exploring the range[1e−4,1.0] datasets. In an ablation study we exploreλ = 0. For theβ-vae we setλ = 0.1 ·β and study bothβ = 0.1 and β = 10, two regimes under which theβ-vae performs qualitatively very diﬀerently (Higgins et al., 2017). All experiments were done ona GPU cluster consisting ofNvidia P100 andRTX. The training took approximately 1 day for most experiments. Results. Table 1 shows that on all the three benchmark datasets all the diﬀerentvae variants we studied, consistency regularization as developed in this paper always improves the quality of the learned representations as measured by mutual information and the number of active latent units. These results are conﬁrmed by the numbers shown in Table 2 wherecr-vaesalways lead to better accuracy on downstream classiﬁcation. We proposed consistency regularization as a way to improve the quality of the learned representations. Incidentally, Table 3 also shows that it can improve generalization as measured by negative log- likelihood. Ablation Study. We now look at the impact of each factor that goes into the regularization method we introduced in this paper usingmnist. We test the impact of the regularization termλand the impact of the choice of augmentation on all metrics. Table 4 and Table 5 show the results. Table 4 shows that even small consistency regularization (a smallλvalue) results in improvement over the basevae but that a large enoughλvalue can hurt performance. Table 5 shows that rotations and translations are more important than scaling, but the combination of all three augmentations works best forcr-vaes. Comparison to Contrastive Learning.We look at howcr-vaes compare against a popular and advanced contrastive-learning-based technique, thetriplet loss(Schroﬀ et al., 2015) usingmnist. Table 6 shows that thecr-vaeoutperforms the triplet loss on both generalization performance and quality of learned representations. Table 6 also conﬁrms existing literature showing simply applying augmentations can outperform complex contrastive learning-based methods such as the triplet loss (Kostrikov et al., 2020; Sinha & Garg, 2021). 4.2 Application to the large-scalenvae on benchmark datasets Along with standard VAE variants, we also experiment with a large scale state-of-the-artvae, the nvae(Vahdat & Kautz, 2020). Similar to before, we simply add consistency regularization using the image-based augmentations techniques to the NVAE model and experiment on benchmark datasets: mnist (LeCun, 1998),cifar-10 (Krizhevsky et al., 2009) andceleba (Liu et al., 2018). The results for large scale generative modeling are tabulated in Table 8 and Table 7, where we see that usingcr-nvaewe are able to learn representations that yield better accuracy on downstream 8Table 7:The cr-nvaeslearns better representations than the basenvae as measured by accuracy on a downstream classiﬁcation on bothmnist and cifar-10. We get to this same conclusion when lookingatthenumberofactiveunitsasanindicatorforthequalityofthelearnedlatentrepresentations; cr-nvaerecovers226 units whereasnvae recovers211 units. Method mnist cifar -10 nvae 99.9 57.9 nvae+Aug 99.9 66.4 cr-nvae 99.9 71.4 Table 8: Large-scale experiments withnvaes with and without consistency-regularization on 3 benchmark datasets: dynamically binarizedmnist, cifar-10 andceleba. We report generalization using negative log-likelihood onmnist and bits per dim oncifar-10 andceleba. On all datasets consistency regularization improves generalization performance. In particularcr-nvae achieves state-of-the-art performance onmnist and cifar-10. mnist (28 ×28) cifar-10 (32 ×32) celeba (64 ×64) nvae 78.19 2.91 2.03 nvae+Aug 77.53 2.70 1.96 cr-nvae 76.93 2.51 1.86 Figure 2: Interpolation between two samples of a lamp, airplane and table using a trained CR- FoldingNet trained on the ShapeNet dataset. The CR-FoldingNet is able to learn an interpretable latent space. classiﬁcation and set new state-of-the-art values on each of the datasets, improving upon the baseline log-likelihood values. This shows the ability of consistency regularization to work at scale on challenging generative modeling tasks. 4.3 Application to the FoldingNet on 3D point-cloud data Along with working with image data, we additionally experiment with 3D point cloud data using a FoldingNet Yang et al. (2018) and the ShapeNet dataset Chang et al. (2015) which consists of 55 distinct object classes. FoldingNet learns a deep AutoEncoder to learn unsupervised representations from the point cloud data. To add consistency regularization, we ﬁrst substitute the AutoEncoder to a 9Table 9:The FoldingNet yields higher accuracy when paired with consistency regularization on the ShapeNet dataset. The results shown here correspond to a FoldingNet that was trained with augmented data, the same used to apply consistency regularization. As can be seen from these results, enforcing consistency through KL as we do in this paper leads to representations that perform well on a downstream classiﬁcation. Here the classiﬁer used is a linear SVM. We also report mean reconstruction error through Chamfer distance where the same conclusion holds. Method Accuracy Reconstruction Loss Folding Net (Aug) 82.5% 0.0355 CR-Folding Net 84.6% 0.0327 vae by adding the KL term from the ELBO to the baseline FoldingNet. We then add the additional consistency regularization KL term to the latent space of FoldingNet. For the ShapeNet point cloud data, we perform data augmentation using a similar scheme to what we did for the previous experiments, we randomly translate, rotate and add jitter to the(x,y,z ) coordinates of the point cloud data. We follow the same scheme detailed in FoldingNet (Yang et al., 2018). We train both the FoldingNet turned in avae and the CR-FoldingNet with these augmentations. To train CR-FoldingNet, we additionally apply the consistency regularization term as proposed in Equation 3. The results on the validation set for reconstruction (as measured by Chamfer distance) and accuracy are shown in Table 9. Wealsovisualizethepointcloudsreconstructionsandinterpolationsbetween3diﬀerentobjectclasses using a CR-FoldingNet in Figure 2. We perform 4 interpolation steps for each of the objects, to highlight the interpretable learned latent space. Additionally, we perform the same interpolation on the baseline FoldingNet model. We show these interpolations in the appendix. 5 Conclusion We proposed a simple regularization technique to constrain encoders ofvaesto learn similar latent representations for an image and a semantics-preserving transformation of the image. The idea consists in maximizing the likelihood of the pair of images while minimizing thekl divergence between the variational distribution induced by the encoder when conditioning on the image on one hand, and its transformation, on the other hand. We applied this technique to severalvae variants on severaldatasets,includinga3Ddataset. Wefounditalwaysleadstobetterlearnedrepresentationsand also better generalization to unseen data. In particular, when applied to thenvae, the regularization technique we developed in this paper yields state-of-the-art results onmnist and cifar-10. Broader Impact Inthispaper,weproposeasimplemethodthatperformsaKL-basedconsistencyregularizationscheme using data augmentation forvaes. The broader impact of the study includes practical applications such as graphics and computer vision applications. The method we propose improves the learned representations ofvaes, and as an artifact, also improves their generalization to unseen data. In this regard, any implications ofvaesalso apply to this work. For example, the generative model ﬁt by avae may be used to generate artiﬁcial data such as images, text, and 3D objects. Biases may arise as a result of poor data selection. Furthermore, text generated from generative systems may amplify harmful speech contained in the data. However, the method we propose can also improve the performance ofvaeswhen used in certain practical domains as we discussed in the introduction of the paper. 6 Acknowledgements We thank Kevin Murphy, Ben Poole, and Augustus Odena for their comments on this work. 10References Achille, A., Eccles, T., Matthey, L., Burgess, C. P., Watters, N., Lerchner, A., and Higgins, I. Life-long disentangled representation learning with cross-domain latent homologies.arXiv preprint arXiv:1808.06508, 2018. An, J. and Cho, S. Variational autoencoder based anomaly detection using reconstruction probability.Special Lecture on IE, 2(1), 2015. Bachman, P., Alsharif, O., and Precup, D. Learning with pseudo-ensembles. InAdvances in neural information processing systems, pp. 3365–3373, 2014. Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R., and Bengio, S. Generating sentences from a continuous space.arXiv preprint arXiv:1511.06349, 2015. Burda, Y., Grosse, R., and Salakhutdinov, R. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015. Chang, A. X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., et al. Shapenet: An information-rich 3d model repository.arXiv preprint arXiv:1512.03012, 2015. Dieng, A. B., Kim, Y., Rush, A. M., and Blei, D. M. Avoiding latent variable collapse with generative skip models. arXiv preprint arXiv:1807.04863, 2018. Dieng,A.B.,Ruiz,F.J.,andBlei,D.M. Topicmodelinginembeddingspaces. arXivpreprintarXiv:1907.04907 , 2019. Fang, L., Li, C., Gao, J., Dong, W., and Chen, C. Implicit deep latent variable models for text generation.arXiv preprint arXiv:1908.11527, 2019. Fu, H., Li, C., Liu, X., Gao, J., Celikyilmaz, A., and Carin, L. Cyclical annealing schedule: A simple approach to mitigating kl vanishing.arXiv preprint arXiv:1903.10145, 2019. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. InAdvances in neural information processing systems, pp. 2672–2680, 2014. Gregor, K., Danihelka, I., Graves, A., Rezende, D. J., and Wierstra, D. Draw: A recurrent neural network for image generation.arXiv preprint arXiv:1502.04623, 2015. Hadjeres,G.,Nielsen,F.,andPachet,F. Glsr-vae: Geodesiclatentspaceregularizationforvariationalautoencoder architectures. In2017 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 1–7. IEEE, 2017. He, J., Spokoyny, D., Neubig, G., and Berg-Kirkpatrick, T. Lagging inference networks and posterior collapse in variational autoencoders.arXiv preprint arXiv:1901.05534, 2019. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., andLerchner, A. beta-vae: Learning basic visual concepts with a constrained variational framework.Iclr, 2(5):6, 2017. Jun, H., Child, R., Chen, M., Schulman, J., Ramesh, A., Radford, A., andSutskever, I. Distributionaugmentation for generative modeling. InInternational Conference on Machine Learning, pp. 5006–5019. PMLR, 2020. Kim, Y., Wiseman, S., Miller, A. C., Sontag, D., and Rush, A. M. Semi-amortized variational autoencoders. arXiv preprint arXiv:1802.02550, 2018. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization.arXiv preprint arXiv:1412.6980, 2014. Kingma, D. P. and Welling, M. Auto-encoding variational bayes.arXiv preprint arXiv:1312.6114, 2013. Kingma, D. P., Rezende, D. J., Mohamed, S., and Welling, M. Semi-supervised learning with deep generative models. arXiv preprint arXiv:1406.5298, 2014. Kostrikov, I., Yarats, D., and Fergus, R. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels.arXiv preprint arXiv:2004.13649, 2020. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Laine, S. and Aila, T. Temporal ensembling for semi-supervised learning.arXiv preprint arXiv:1610.02242, 2016. Lake, B., Salakhutdinov, R., Gross, J., and Tenenbaum, J. One shot learning of simple visual concepts. In Proceedings of the annual meeting of the cognitive science society, volume 33, 2011. 11LeCun, Y. The mnist database of handwritten digits.http://yann. lecun. com/exdb/mnist/, 1998. Liang, D., Krishnan, R. G., Hoﬀman, M. D., and Jebara, T. Variational autoencoders for collaborative ﬁltering. InProceedings of the 2018 World Wide Web Conference, pp. 689–698, 2018. Liu, Z., Luo, P., Wang, X., and Tang, X. Large-scale celebfaces attributes (celeba) dataset.Retrieved August, 15: 2018, 2018. Miao, Y., Yu, L., and Blunsom, P. Neural variational inference for text processing. InInternational conference on machine learning, pp. 1727–1736, 2016. Miyato, T., Maeda, S.-i., Ishii, S., and Koyama, M. Virtual adversarial training: a regularization method for supervised and semi-supervised learning.IEEE transactions on pattern analysis and machine intelligence, 2018. Osada, G., Ahsan, B., Bora, R. P., and Nishide, T. Regularization with latent space virtual adversarial training. InEuropean Conference on Computer Vision, pp. 565–581. Springer, 2020. Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models.arXiv preprint arXiv:1401.4082, 2014. Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. Contractive auto-encoders: Explicit invariance during feature extraction. 2011. Roberts, A., Engel, J., Raﬀel, C., Hawthorne, C., and Eck, D. A hierarchical latent vector model for learning long-term structure in music.arXiv preprint arXiv:1803.05428, 2018. Sajjadi, M., Javanmardi, M., and Tasdizen, T. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. InNeurIPS, 2016. Schroﬀ, F., Kalenichenko, D., and Philbin, J. Facenet: A uniﬁed embedding for face recognition and clustering. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 815–823, 2015. Sinha, S. and Garg, A. S4rl: Surprisingly simple self-supervision for oﬄine reinforcement learning.arXiv preprint arXiv:2103.06326, 2021. Sinha, S., Ebrahimi, S., and Darrell, T. Variational adversarial active learning. InProceedings of the IEEE International Conference on Computer Vision, pp. 5972–5981, 2019. Tolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf, B. Wasserstein auto-encoders. arXiv preprint arXiv:1711.01558, 2017. Vahdat, A. and Kautz, J. Nvae: A deep hierarchical variational autoencoder.arXiv preprint arXiv:2007.03898, 2020. Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096–1103, 2008. Vincent,P.,Larochelle,H.,Lajoie,I.,Bengio,Y.,andManzagol,P.-A. Stackeddenoisingautoencoders: Learning usefulrepresentationsinadeepnetworkwithalocaldenoisingcriterion. Journalofmachinelearningresearch , 11(Dec):3371–3408, 2010. Walker, J., Doersch, C., Gupta, A., and Hebert, M. An uncertain future: Forecasting from static images using variational autoencoders. InEuropean Conference on Computer Vision, pp. 835–851. Springer, 2016. Wei, X., Gong, B., Liu, Z., Lu, W., and Wang, L. Improving the improved training of wasserstein gans: A consistency term and its dual eﬀect.arXiv preprint arXiv:1803.01541, 2018. Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q. V. Unsupervised data augmentation for consistency training. arXiv preprint arXiv:1904.12848, 2019. Yang, Y., Feng, C., Shen, Y., and Tian, D. Foldingnet: Point cloud auto-encoder via deep grid deformation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 206–215, 2018. Zhang, H., Zhang, Z., Odena, A., and Lee, H. Consistency regularization for generative adversarial networks. 2020. Zimmerer, D., Kohl, S. A., Petersen, J., Isensee, F., and Maier-Hein, K. H. Context-encoding variational autoencoder for unsupervised anomaly detection.arXiv preprint arXiv:1812.05941, 2018. 12",
      "references": [
        "Life-long disentangled representation learning with cross-domain latent homologies",
        "Variational autoencoder based anomaly detection using reconstruction probability",
        "Learning with pseudo-ensembles",
        "Generating sentences from a continuous space",
        "Importance weighted autoencoders",
        "ShapeNet: An information-rich 3D model repository",
        "Avoiding latent variable collapse with generative skip models",
        "Topic modeling in embeddings spaces",
        "Implicit deep latent variable models for text generation",
        "Cyclical annealing schedule: A simple approach to mitigating kl vanishing",
        "Generative adversarial nets",
        "Draw: A recurrent neural network for image generation",
        "GLSR-VAE: Geodesic latent space regularization for variational autoencoder architectures",
        "Lagging inference networks and posterior collapse in variational autoencoders",
        "beta-vae: Learning basic visual concepts with a constrained variational framework",
        "Distribution augmentation for generative modeling",
        "Semi-amortized variational autoencoders",
        "Adam: A method for stochastic optimization",
        "Auto-encoding variational bayes",
        "Semi-supervised learning with deep generative models",
        "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels",
        "Learning multiple layers of features from tiny images",
        "Temporal ensembling for semi-supervised learning",
        "One shot learning of simple visual concepts",
        "The mnist database of handwritten digits",
        "Variational autoencoders for collaborative filtering",
        "Large-scale celebfaces attributes (celebA) dataset",
        "Neural variational inference for text processing",
        "Virtual adversarial training: a regularization method for supervised and semi-supervised learning",
        "Regularization with latent space virtual adversarial training",
        "Stochastic backpropagation and approximate inference in deep generative models",
        "Contractive auto-encoders: Explicit invariance during feature extraction",
        "A hierarchical latent vector model for learning long-term structure in music",
        "Regularization with stochastic transformations and perturbations for deep semi-supervised learning",
        "FaceNet: A unified embedding for face recognition and clustering",
        "S4RL: Surprisingly simple self-supervision for offline reinforcement learning",
        "Variational adversarial active learning",
        "Wasserstein auto-encoders",
        "NVAE: A deep hierarchical variational autoencoder",
        "Extracting and composing robust features with denoising autoencoders",
        "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
        "An uncertain future: Forecasting from static images using variational autoencoders",
        "Improving the improved training of Wasserstein GANs: A consistency term and its dual effect",
        "Unsupervised data augmentation for consistency training",
        "Foldingnet: Point cloud auto-encoder via deep grid deformation",
        "Consistency regularization for generative adversarial networks",
        "Context-encoding variational autoencoder for unsupervised anomaly detection"
      ],
      "meta_data": {
        "arxiv_id": "2105.14859v2",
        "authors": [
          "Samarth Sinha",
          "Adji B. Dieng"
        ],
        "published_date": "2021-05-31T10:26:32Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces consistency-regularized variational auto-encoders (cr-VAEs) to enforce semantic invariance of latent representations under semantics-preserving input transformations. Proposes a KL-based consistency term that aligns the encoder's approximate posteriors for an input x and its transformed version t(x). Demonstrates that cr-VAEs improve latent representation quality and generalization across multiple VAE variants (VAE, IWAE, beta-VAE, NVAE) on several benchmarks, achieving state-of-the-art results on MNIST and CIFAR-10 with NVAE, extends to 3D data with FoldingNet (CR-FoldingNet), and can surpass triplet-loss based methods in representation learning.",
        "methodology": "The core idea is to maximize the data likelihood while minimizing the KL divergence between the encoder posteriors conditioned on x and on a semantics-preserving transformation t(x). Specifically, cr-vae optimizes L_cr-vae(x) = L_vae(x) + E_{t(x)|x}[L_vae(t(x))] − λ * E_{t(x)|x}[ KL(q_φ(z|t(x)) || q_φ(z|x)) ]. Transformations t(x) are random semantic-preserving perturbations (e.g., rotations, translations, scaling). The method uses Monte Carlo estimates with the reparameterization trick, and is applicable to various VAE variants (VAE, IWAE, β-VAE, NVAE) and to 3D FoldingNet. Training uses Adam with standard ELBO objectives augmented by the consistency term, and evaluation includes mutual information, number of active latent units, downstream classification accuracy, and negative log-likelihood. Hyperparameters include λ (regularization strength) and augmentation choices; ablations explore λ and augmentation effects.",
        "experimental_setup": "Datasets and settings include: image data (MNIST, Omniglot, CelebA at 32x32 for standard experiments; 64x64 for NVAE experiments) and a 3D point-cloud dataset (ShapeNet with FoldingNet). Semantics-preserving transformations include random rotations (−15 to 15 degrees), random translations (−2 to 2 pixels), and random scaling (0.9 to 1.1). Evaluation metrics: mutual information I(z; x), number of active latent units (AU), downstream classification accuracy using a linear classifier on learned representations, and negative held-out log-likelihood (NLL). Baselines include vanilla VAE, IWAE, β-VAE, and NVAE, with and without augmentations. Experiments show cr-vae improves representations and generalization across all variants; ablation studies analyze λ and augmentations; comparisons with triplet loss show cr-vae can outperform leading contrastive methods. Training details: architectures aligned with prior works (e.g., Tolstikhin et al. 2017), Adam optimizer with learning rate 1e-4, 100 epochs, batch size 64, latent dimension 50, λ around 0.1, and exploration of λ in [1e−4, 1.0]. For NVAE, results reported on MNIST, CIFAR-10, CelebA, and ShapeNet adaptations with corresponding metrics.",
        "limitations": "The method relies on the choice of semantics-preserving augmentations and the regularization strength λ; too large λ can hurt performance, indicating a trade-off between data fit and representation consistency. Additional computational overhead arises from evaluating L_vae on augmented data and the KL consistency term, which may increase training time. The empirical validation focuses on image data and 3D point clouds; generalization to other modalities or very large-scale settings beyond NVAE is not exhaustively explored. Assumes augmentations are semantics-preserving and do not alter label information for supervised tasks. Possible impact on reconstruction fidelity under heavy regularization.",
        "future_research_directions": "Explore adaptive or learned augmentation policies and scheduling of λ to balance data fit and representation consistency. Extend consistency regularization to other generative models (e.g., flows, diffusion models) and to other modalities (text, audio). Investigate theoretical guarantees and connections to disentanglement and latent geometry, and analyze how cr-regularization affects interpretability and controllability of latent factors. Combine with other representation-learning approaches (e.g., contrastive learning) or self-supervised objectives, and scale to larger datasets and architectures with different inductive biases. Apply to broader unsupervised tasks (anomaly detection, continual learning) and study robustness to distribution shifts.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Towards Better Robust Generalization with Shift Consistency Regularization",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "G-Mixup: Graph Data Augmentation for Graph Classification",
      "full_text": "G-Mixup: Graph Data Augmentation for Graph Classiﬁcation Xiaotian Han 1 Zhimeng Jiang 1 Ninghao Liu 2 Xia Hu 3 Abstract This work develops mixup for graph data. Mixup has shown superiority in improving the gener- alization and robustness of neural networks by interpolating features and labels between two ran- dom samples. Traditionally, Mixup can work on regular, grid-like, and Euclidean data such as im- age or tabular data. However, it is challenging to directly adopt Mixup to augment graph data be- cause different graphs typically: 1) have different numbers of nodes; 2) are not readily aligned; and 3) have unique typologies in non-Euclidean space. To this end, we propose G-Mixup to augment graphs for graph classiﬁcation by interpolating the generator (i.e., graphon) of different classes of graphs. Speciﬁcally, we ﬁrst use graphs within the same class to estimate a graphon. Then, in- stead of directly manipulating graphs, we interpo- late graphons of different classes in the Euclidean space to get mixed graphons, where the synthetic graphs are generated through sampling based on the mixed graphons. Extensive experiments show that G-Mixup substantially improves the general- ization and robustness of GNNs. 1. Introduction Recently deep learning has been widely adopted to graph analysis. Graph Neural Networks (GNNs) (Wu et al., 2020; Zhou et al., 2020a; Zhang et al., 2020; Xu et al., 2018) have shown promising performance on graph classiﬁcation. Meanwhile, data augmentation (e.g., DropEdge (Rong et al., 2020), Subgraph (You et al., 2020; Wang et al., 2020a) ) has also been adopted to graph analysis by generating syn- thetic graphs to create more training data for improving the generalization of graph classiﬁcation models. However, existing graph data augmentation strategies typically aim to augment graphs at a within-graph level by either modifying *Equal contribution 1Department of Computer Sci- ence&Engineering, Texas A&M University 2Department of Computer Science, University of Georgia 3Department of Computer Science, Rice University. Correspondence to: Xiaotian Han <han@tamu.edu>. Preprint edges or nodes in an individual graph, which does not en- able information exchange between different instances. The between-graph augmentation methods (i.e., data augmenta- tion between graphs) are still under-explored. In parallel with the development of graph neural networks, Mixup (Zhang et al., 2017) and its variants (e.g., Manifold Mixup (Verma et al., 2019a)), as data augmentation methods, have been theoretically and empirically shown to improve the generalization and robustness of deep neural networks in image recognition (Zhang et al., 2017; Verma et al., 2019a; Zhang et al., 2021) and natural language processing (Guo et al., 2019a; Guo, 2020). The basic idea of Mixup is to linearly interpolate continuous values of random sample pairs to generate more synthetic training data. The formal mathematical expression of Mixup is xnew = λxi + (1 − λ)xj,ynew = λyi +(1 −λ)yj,where (xi,yi) and (xj,yj) are two samples drawn at random from training data and the target y are one-hot labels. With graph neural networks and mixup in mind, the following question naturally arises: Can we mix up graph data to improve the generalization and robustness of GNNs? It remains an open and challenging problem to mix up graph data due to the characteristics of graphs and the requirements of applying Mixup. Typically, Mixup requires that original data instances are regular and well-aligned in Euclidean space, such as image data and table data. However, graph data is distinctly different from them due to the following reasons: (i) graph data is irregular , since the number of nodes in different graphs are typically different from each other; (ii) graph data is not well-aligned , where nodes in graphs are not naturally ordered and it is hard to match up nodes between different graphs; (iii) graph topology between classes are divergent, where the topologies of a pair of graphs from different classes are usually different while the topologies of those from the same class are usually similar. Thus, it is nontrivial to directly adopt the Mixup strategy to graph data. To tackle the aforementioned problems, we propose G- Mixup, a class-level graph data augmentation method, to mix up graph data based on graphons. The graphs within one class are produced under the same generator (i.e., graphon). We mix up the graphons of different classes and then gener- ate synthetic graphs. Informally, a graphon can be thought arXiv:2202.07179v2  [cs.LG]  16 Feb 2022G-Mixup: Graph Data Augmentation for Graph Classiﬁcation of as a probability matrix (e.g., the matrix WG and WH in Figure 1), where W(i,j) represents the probability of edge between node iand j. The real-world graphs can be regraded as generated from graphons. Since the graphons of different graphs is regular, well-aligned, and is deﬁned in Euclidean space, it is easy and natural to mix up the graphons and then generate the synthetic graphs therefrom. On this basis, we can achieve graphs mixup by mixing their generators. We also provide theoretical analysis of graphons mixup, which guarantees that the generated graphs will pre- serve the key characteristics of both original classes. Our proposed method is illustrated in Figure 1 with an example. Given two graph training sets G= {G1,G2,··· ,Gm}and H= {H1,H2,··· ,Hm}with different labels and distinct topologies (i.e., Ghas two communities while Hhas eight communities), we estimate graphons WGand WHrespec- tively from Gand H. We then mix up the two graphons and obtain a mixed graphon WI. After that, we sample syn- thetic graphs from WIas additional training graphs. The generated synthetic graphs have two major communities and each of them have four sub-communities, which is a mixture of the two graph sets. It thus shows that G-Mixup is capable of mixing up graphs. Our main contributions are highlighted as follows: Firstly, we propose G-Mixup to augment the training graphs for graph classiﬁcation. Since directly mixing up graphs is in- tractable, G-Mixup mixes the graphons of different classes of graphs to generate synthetic graphs. Secondly, we theo- retically prove that the synthetic graph will be the mixture of the original graphs, where the key topology (i.e., discrim- inative motif) of source graphs will be mixed up. Thirdly, we demonstrate the effectiveness of the proposed G-Mixup on various graph neural networks and datasets. Extensive experimental results show that G-Mixup substantially im- proves the performance of graph neural networks in terms of enhancing their generalization and robustness. 2. Preliminaries In this section, we ﬁrst go over the notations used in this paper, and then introduce graph related concepts including graph homomorphism and graphons, which will be used for theoretical analysis in this work. Finally, we brieﬂy review the graph neural networks for graph classiﬁcation. 2.1. Notations Given a graph G, we use V(G) and E(G) to denote its nodes and edges, respectively. The number of nodes is v(G) = |V(G)|, and the number of edges is e(G) = |E(G)|. We use m,l to denote the number of graphs and N,K to denote the number of nodes. We use G,H,I /G,H,Ito denote graphs/graph set. yG ∈RC de- notes the label of graph set G, where Cis number of classes of graphs. A graph could contain some frequent subgraphs which are called motifs. The motifs in graph Gis denoted as FG. The set of motifs in graph set Gis denoted as FG. WG denotes the graphon of graph set G. W denotes the step function. G(K,W) denotes the random graph with K nodes based on graphon W. 2.2. Graph Homomorphism and Graphons Graph Homomorphism. A graph homomorphism is an adjacency-preserving mapping between two graphs, i.e., mapping adjacent vertices in one graph to adjacent vertices in the other. Formally, a graph homomorphism φ: F →G is a map from V(F) to V(G), where if {u,v}∈ E(F), then {φ(u),φ(v)} ∈E(G). For two graphs H and G, there could be multiple graph homomorphisms between them. Let hom(H,G) denotes the total number of graph homomorphisms from graph H to graph G. For exam- ple, hom(  ,G) = |V(G)|if graph H is  , hom(  ,G) = 2|E(G)|if graph H is  , and hom(  ,G) is six times the number of triangles in G. There are in total |V(G)||V (H)| mappings from H to G, but only some of them are ho- momorphisms. Thus, we deﬁne homomorphism density to measure the relative frequency that the graph H ap- pears in graph Gas t(H,G) = hom(H,G) |V (G)||V (H)| . For example, t(  ,G) = |V(G)|/N1 = 1, t(  ,G) = 2|E(G)|/N2. Graphon. A graphon (Airoldi et al., 2013) is a continu- ous, bounded and symmetric function W : [0,1]2 →[0,1] which may be thought of as the weight matrix of a graph with inﬁnite number of nodes. Then, given two points ui, uj ∈ [0,1], W(i,j) represents the probability that nodes i and j be related with an edge. Various quan- tities of a graph can be calculated as a function of the graphon. For example, the degree of nodes in graphs can be easily extended to a degree distribution function in graphons, which is characterized by its graphon marginal dW (x) = ∫1 0 W(x,y)dy. Similarly, the concept of homo- morphism density can be naturally extended from graphs to graphons. Given an arbitrary graph motif F, its homomor- phism density with respect to graphon W is deﬁned by t(F,W ) = ∫ [0,1]V (F) ∏ i,j∈E(F) W(xi,xj) ∏ i∈V (F) dxi. For example, the edge density of graphon W is t(  ,W) =∫ [0,1]2 W(x,y) dxdy, and the triangle density of graphon W is t(  ,W) = ∫ [0,1]3 W(x,y)W(x,z)W(y,z) dxdydz. 2.3. Graph Classiﬁcation with Graph Neural Networks Given a set of graphs, graph classiﬁcation aims to assign a class label for each graph G. Recently, graph neural networks have become the state-of-the-art approach for graph classiﬁcation. Without loss of generalization, we present the formal expression of a graph convolution net- work (GCN) (Kipf & Welling, 2016). The forward propaga-G-Mixup: Graph Data Augmentation for Graph Classiﬁcation W I W H W G I = { I 1 ,I 2 , ··· ,I k } with label (0 ⊿ 5 , 0 ⊿ 5) H = { H 1 ,H 2 , ··· ,H k } with label (0 , 1) G = { G 1 ,G 2 , ··· ,G k } with label (1 , 0) … … … 1) graphon estimation 3) graph sampling W I =0 ⊿5 ⇤ W G +0 ⊿5 ⇤ W H 1) graphon estimation 2) graphon mixup Figure 1.An overview of G-Mixup. The task is binary graph classiﬁcation. We have two classes of graphs Gand Hwith different topologies (Ghas two communities while Hhas eight communities). Gand Hhave different graphons. We mix up the graphons WG and WHto obtain a mixed graphon WI, and then sample new graphs from the mixed graphon. Intuitively, the synthetic graphs have two major communities and each of which has four sub-communities, demonstrating that the generated graphs preserve the structure of original graphs from both classes. tion at k-th layer is described as the following: a(k) i = AGG(k) ({ h(k−1) j : j ∈N(i) }) , h(k) i = COMBINE(k) ( h(k−1) i ,a(k) i ) , (1) where h(k) i ∈Rn×dk is the intermediate representation of node i at the k-th layer, N(i) denotes the neighbors of node i. AGG(·) is an aggregation function to collect em- bedding representations from neighbors, and COMBINE(·) combines neighbors’ representation and its representation at (k−1)-th layer. For graph classiﬁcation, a graph-level representation is obtained by summarizing all node-level representations in the graph by a readout function: hG = READOUT ({ h(k) i : i∈E(G) }) , ˆy = softmax(hG), (2) where READOUT(·) is the readout function, which can be a simple function such as average or sophisticated pooling function (Gao & Ji, 2019; Ying et al., 2018), hG is the representation of graph G, and ˆy ∈RC is the predicted probability that Gbelongs to each of the Cclasses. 3. Methodology In this section, we formally introduce the proposedG-Mixup and elaborate its implementation details. 3.1. G-Mixup Different from the interpolation of image data in Euclidean space, adopting Mixup to graph data is nontrivial since graphs are irregular, unaligned and non-Euclidean data. In this work, we show that the challenges could be tackled with graphon theory. Intuitively, a graphon can be regarded as a graph generator. Graphs of the same class can be seen as being generated from the same graphon. With this in mind, we propose G-Mixup, a class-level data augmentation method via graphon interpolation. Speciﬁcally, G-Mixup interpolates different graph generators to obtain a new mixed one. Then, synthetic graphs are sampled based on the mixed graphon for data augmentation. The graphs sampled from this generator partially possess properties of the original graphs. Formally, G-Mixup is formulated as: Graphon Estimation: G→ WG,H→ WH (3) Graphon Mixup: WI= λWG+ (1 −λ)WH (4) Graph Generation: {I1,I2,··· ,Im} i.i.d ∼G(K,WI) (5) Label Mixup: yI= λyG+ (1 −λ)yH (6) where WG,WHare graphons of the graph set Gand H. The mixed graphon is denoted by WI, and λ ∈[0,1] is the trade-off hyperparameter to control the contributions from different source sets. The set of synthetic graphs generated from WI is I = {I1,I2,··· ,Im}. The yG ∈ RC and yH ∈RC are vectors containing ground-truth labels for graph G and H, respectively, where C is the number of classes. The label vector of synthetic graphs in graph set I is denoted as yI∈RC. As illustrated in Figure 1 and the above equations, the pro- posed G-Mixup includes three key steps: i) estimate a graphon for each class of graphs, ii) mix up the graphons of different graph classes, and iii) generate synthetic graphs based on the mixed graphons. Speciﬁcally, suppose we have two graph sets G= {G1,G2,··· ,Gm}with label yG, and H= {H1,H2,··· ,Hm}with label yH. Graphons WGand WHare estimated from graph sets Gand H, respec- tively. Then, we mix them up by linearly interpolating the two graphons and their labels, and obtain WIand yI. Fi- nally, a set of synthetic graphs Iis sampled based on WI, which will be used as additional training graph data.G-Mixup: Graph Data Augmentation for Graph Classiﬁcation 3.2. Implementation In this section, we introduce the implementation details of graphon estimation and synthetic graphs generation. We provide the pseudo-code of G-Mixup in Appendix E. Graphon Estimation and Mixup. Estimating graphons from observed graphs is a prerequisite for G-Mixup. How- ever, it is intractable because a graphon is an unknown func- tion without a closed-form expression for real-world graphs. Therefore, we use the step function (Lov´asz, 2012; Xu et al., 2021) to approximate graphons1. In general, the step func- tion can be seen as a matrix W = [ wkk′] ∈[0,1]K×K, where Wij is the probability that an edge exists between node i and node j. In practice, we use the matrix-form step function as graphon to mix up and generate synthetic graphs. The step function estimation methods are well- studied, which ﬁrst align the nodes in a set of graphs based on node measurements (e.g., degree) and then estimate the step function from all the aligned adjacency matrices. The typical step function estimation methods includes sorting- and-smoothing (SAS) method (Chan & Airoldi, 2014), stochastic block approximation (SBA) (Airoldi et al., 2013), “largest gap” (LG) (Channarond et al., 2012), matrix com- pletion (MC) (Keshavan et al., 2010), universal singular value thresholding (USVT) (Chatterjee et al., 2015). 2 For- mally, a step function WP : [0,1]2 ↦→[0,1] is deﬁned as WP (x,y) = ∑K k,k′=1 wkk′1 Pk×Pk′ (x,y), where P = (P1,.., PK) denotes the partition of [0,1] into K adjacent intervals of length 1/K, wkk′ ∈[0,1], and indicator func- tion 1 Pk×Pk′ (x,y) equals to 1 if (x,y) ∈Pk ×Pk′ and otherwise it is 0. For binary classiﬁcation, we have G= {G1,G2,··· ,Gm}and H= {H1,H2,··· ,Hm}with dif- ferent labels, we estimate their step functionsWG∈RK×K and WH∈RK×K, where we let Kbe the average number of nodes in all graphs. For multi-class classiﬁcation, we ﬁrst estimate the step function for each class of graphs and then randomly select two to perform mix-up. The resultant step function is WI = λWG+ (1 −λ)WH∈RK×K, which serves as the generator of synthetic graphs. Synthetic Graphs Generation. A graphon W provides a distribution to generate arbitrarily sized graphs. Speciﬁ- cally, a K-node random graph G(K,WI) can be generated following the process: u1,...,u K iid ∼Unif[0,1],G(K,W)ij iid ∼Bern(W(ui,uj)), ∀i,j ∈[K]. Since we use the step function W to approximate the 1Because weak regularity lemma of graphon (Frieze & Kannan, 1999) indicates that an arbitrary graphon can be approximated well by step function. Detailed discussion is in Appendix A.4. 2The details about these step function estimation methods are presented in Appendix B. graphon W, we set W(ui,uj) = W[⌊1/ui⌋,⌊1/uj⌋], and ⌊·⌋is the ﬂoor function. The ﬁrst step samples Knodes in- dependently from a uniform distribution Unif[0,1] on [0,1]. The second step generates an adjacency matrix A = [aij] ∈ {0,1}K×K, whose element values follow the Bernoulli distributions Bern(·) determined by the step function. A graph is thus obtained as Gwhere V(G) = {1,...,K }and E(G) = {(i,j) |aij = 1}. A set of synthetic graphs can be generated by conducting the above process multiple times. The generation of node features of synthetic graphs includes two steps: 1) build the graphon node features based on the original node features, 2) generate node features of synthetic graphs based on the graphon node features. Speciﬁcally, at the graphon estimation phase, we align original node fea- tures while aligning the adjacency matrices, so we have a set of aligned original node features for each graphon, then we conduct pooling (average pooling in our experiments) on the aligned original node features to obtain the graphon node features. The node features of generated graphs are the same as graphon features. Table 1.Computational com- plexity of graphon estimation. Method Complexity MC O(N3) USVT O(N3) LG O(mN2) SBA O(mKN log N) SAS O(mN log N + K2 log K2) Computational Complex- ity Analysis. We hereby discuss computational com- plexity of G-Mixup. The ma- jor computation costs come from graphon estimation and synthetic graph gener- ation. For graphon estima- tion, suppose we have mgraphs and each of them has N nodes, and estimate step function with K partitions to ap- proximate a graphon, the complexity of used graphon esti- mation methods (Xu et al., 2021) is in Table 1. For graph generation, suppose we need to generate lgraphs with K nodes, the computational complexity is O(lK) for node generation and O(lK2) for edge generation, so the overall complexity of graph generation is O(lK2). 4. Theoretical Justiﬁcation In the following, we theoretically prove that: the synthetic graphs generated by G-Mixup will be a mixture of origi- nal graphs. We ﬁrst deﬁne the discriminative motif, and then we justify the graphon mixup operation (Equation (4)) and graph generation operation (Equation (5)) by analysing the homomorphism density of discriminative motifs of the original graphs and the synthetic graphs. Deﬁnition 4.1 (Discriminative Motif). A discriminative mo- tif FG of graph Gis the subgraph, with the minimal number of nodes and edges, that can decide the class the graph G. Furthermore, FGis the set of discriminative motifs for graphs in the set G. Intuitively, the discriminative motif is the key topology of a graph. We assume that (i) every graph Ghas a discrimina-G-Mixup: Graph Data Augmentation for Graph Classiﬁcation tive motif FG , and (ii) a set of graphs Ghas a ﬁnite set of discriminative motifs FG. The goal of graph classiﬁcation is to ﬁlter out structural noise in graphs (Fox & Rajamanickam, 2019) and recognize the key typologies (discriminative mo- tifs) to predict the class label. For example, benzene (a chemical compound) is distinguished by the motif  (ben- zene ring). In the following, we analyze G-Mixup from the perspective of discriminative motifs. 4.1. Will discriminative motifs FGand FHexist in λWG+ (1 −λ)WH? We answer this question by exploring the difference in ho- momorphism density of discriminative motifs between the original and mixed graphon, as the following theorems, Theorem 4.2. Given two sets of graphs Gand H, the corre- sponding graphons areWGand WH, and the corresponding discriminative motif set FGand FH. For every discrimina- tive motif FG∈FGand FH∈FH, the difference between the homomorphism density of FG/FHin the mixed graphon WI= λWG+(1 −λ)WHand that of the graphonWH/WG is upper bounded by |t(FG,WI) −t(FG,WG)|≤ (1 −λ)e(FG)||WH−WG||□, |t(FH,WI) −t(FH,WH)|≤ λe(FH)||WH−WG||□ where e(F) is the number of nodes in graphF, and ||WH− WG||□ denotes the cut norm 3. Proof Sketch. The proof follows the derivation of Counting Lemma for Graphons (Lemma 10.23 in Lov ´asz (2012)), which associates the homomorphism density with the cut norm ||WH−WG||□ of graphons. Speciﬁcally, we take the two graphons in this Lemma to deduce the bound of the difference of homomorphism densities of WIand WG/WH. Detailed proof are in Appendix A.2. ■ Theorem 4.2 suggests that the difference in the homomor- phism densities of the mixed graphon and original graphons is upper bounded. Note that difference depends on the hy- perparameter λ, the edge number e(FG)/e(FH) and the cut norm ||WH−WG||□. Since the e(FG)/e(FH) and the cut norm ||WH−WG||□ are decided by the dataset (can be seen as a constant), the difference in homomorphism densities will be decided by λ. On this basis, the label of the mixed graphon is set to λyG+ (1 −λ)yH. Therefore, G-Mixup can preserve the different discriminative motifs of the two different graphons into one mixed graphon. 4.2. Will the generated graphs from graphon WI preserve the mixture of discriminative motifs? Ideally, the generated graphs should inherit the homomor- phism density of discriminative motifs from the graphon. 3Cut norm is used to measure the similarity between graphs, Details about cut norm are in Appendix A.1 To verify this, we propose the following theorem. Theorem 4.3. Let WIbe the mixed graphon, n≥1, 0 < ε <1, and let FIbe the mixed discriminative motif, then the WI-random graph G = G(n,WI) satisﬁes P (|t(FI,G) −t(FI,WI)|>ε) ≤2exp ( − ε2n 8v(FI)2 ) . Theorem 4.3 states that for any speciﬁed nonzero margin ε, with a sufﬁcient number of graphs sampled from the mixed graphon, the homomorphism density of discrimina- tive motif in synthetic graphs will approximately equal to that in graphon t(FI,G) ≈t(FI,WI) with high probabil- ity. In other words, the synthetic graphs will preserve the discriminative motif of the mixed graphon with a very high probability if the sample number nis large enough. The de- tailed proof is in Appendix A.3. Therefore, G-Mixup can preserve the discriminative motifs of the two different graphs into one mixed graph. 4.3. Discussion We discuss the differences and relations between G-Mixup and other augmentation strategies. Relation to Edge Perturbation Methods. The commonly used edge perturbation methods are spacial cases of G- Mixup. Edge perturbation methods randomly perturb the edges to improve the GNNs, inlcuding DropEdge (Rong et al., 2020), and Graphon-based edge perturbation (Hu et al., 2021). DropEdge removes graph edges indepen- dently with a speciﬁed probability, aiming to prevent over- smoothing and over-ﬁtting issues in GNNs. Graphon-based edge perturbation (Rong et al., 2020) improves the Drope- dge by dropping edge based on an estimated probability. One limitation of such methods is that the edge permuta- tion is based on one individual graph, so the graphs will not mix up. DropEdge and Graphon-based edge perturba- tion (Hu et al., 2021) are special cases of G-Mixup while setting different hyperparameter λ. i) G-Mixup will de- generate into Graphon-based edge perturbation, if λ= 0 in Equation (4), where the mathematical expression is WI = WH,{I1,I2,··· ,Im} i.i.d ∼G(k,WI),yI = yH. ii) G-Mixup will degenerate into DropEdge, if λ= 0 and using the element-wise product of graphons W and adjacency ma- trix A in Equation (4) as edge probability. The expression is WI = A ⊙WH,{I1,I2,··· ,Im} i.i.d ∼G(k,WI),yI = yH, where ⊙is element-wise multiplication. Relation to Manifold Mixup . As a model-agnostic aug- mentation method, G-Mixup has broader applications, e.g., creating graphs for graph contrastive learning, than Man- ifold Mixup. Manifold Mixup (Wang et al., 2021) is pro- posed to mix up graphs in the embedding space, which interpolates hidden representations of graphs. Interpolating hidden representation could limit its applications because: 1) algorithms must have hidden representation of graphs, andG-Mixup: Graph Data Augmentation for Graph Classiﬁcation IMDB-BINARY Class 0 Class 1 REDDIT-BINARY Class 0 Class 1 IMDB-MULTI Class 0 Class 1 Class 2 Figure 2.Estimated graphons on IMDB-BINARY , REDDIT-BINARY , and IMDB-MULTI. Obviously, graphons of different graph classes are quiet different. This observation validates the divergence of graphons between different classes of graphs, which is the basis of the G-Mixup. The graphons are estimated by LG. More estimated graphons via various methods are in Appendix G.1. (c) graphs generated from 1 ⇤ W0 +0 ⇤ W1  (d) graphs generated from 0 ⇤ W0 +1 ⇤ W1 (e) graphs generated from 0 ⊿5 ⇤ W 0 +0 ⊿5 ⇤ W 1 W0 (a) graphs of class 0 and the graphon W0  (b) graphs of class 1 and the graphon W1 W1 Original GraphsGenerated Graphs Figure 3.The visualization of generated synthetic graphs on REDDIT-BINARY dataset. The ﬁrst row is the original graphs while the second row is the generated graphs from G-Mixup. The graphs in (a) and (b) are the original graphs of class 0 and class 1. The distinct difference between the two classes is that graphs of class 0 have one high-degree node while graphs of class 1 have two (marked with in (a) and (b)). (c)/(d) shows graphs generated with the mixed graphon (1 ∗W0 + 0∗W1) / (0 ∗W0 + 1∗W1), which have one/two high-degree node/nodes (marked with  in (c) and (d)) because the mixed graphon only containsW0/W1. The synthetic graphs generated from (0.5 ∗W0 + 0.5 ∗W1) is the mixture of graphs of class 0 and 1, which appears as a high-degree node and a dense subgraph (marked with  and  in (e), respectively). The results show that synthetic graphs are the mixture of the original graphs. 2) models must be modiﬁed to adapt it. In contrast,G-Mixup generates synthetic graphs without modifying models. 5. Experiments We evaluate the performance of G-Mixup in this section. First, we visualize graphons and graph generation results in Sections 5.1 and 5.2 to investigate what G-Mixup actually do on real-world datasets. Then, we evaluate the effec- tiveness of G-Mixup in graph classiﬁcation with various datasets and GNN backbones in Section 5.3, as well as how it improves the robustness of GNNs against label corruption and adversarial attacks in Section 5.4. The experiment set- ting and more experiments are in Appendices F and G. The observations are highlighted with # boldface. 5.1. Do different classes of real-world graphs have different graphons? We visualize the estimated graphons in Figure 2. It shows that, 1 the graphons of different class of graphs in one dataset are distinctly different. The graphons of IMDB- BINAERY in Figure 2 shows that the graphon of class 1 has larger dense area, which indicates that the graphs in this class have a more large communities than the graphs of class 0. The graphons of REDDIT-BINARY in Figure 2 shows that graphs of class 0 have one high-degree nodes while the graphs of class 1 have two. This observation validates that real-world graphs of different classes have distinctly different graphons, which lays a solid foundation for generating the mixture of graphs by mixing up graphons. 5.2. What is G-Mixup doing? A case study To investigate the outcome ofG-Mixup in real-world scenar- ios, we visualize the generated synthetic graphs in REDDIT- BINARY dataset in Figure 3. We observed that 2 The synthetic graphs are indeed the mixture of the origi- nal graphs. Original graphs and the generated synthetic graphs are visualized in Figure 3(a)(b) and Figure 3(c)(d)(e), respectively. Figure 3 demonstrates that mixed graphon 0.5 ∗WG+ 0.5 ∗WHis able to generate graphs with a high- degree node and a dense subgraph, which can be regarded as the mixture of graphs with one high-degree node and two high-degree nodes. It validates that G-Mixup prefer to preserve the discriminative motifs from the original graphs. 5.3. Can G-Mixup improve the performance and generalization of GNNs? To validate the effectiveness ofG-Mixup, we compare the performance of GNNs with various backbones on differ-G-Mixup: Graph Data Augmentation for Graph Classiﬁcation IMDB-BINARY IMDB-MULTI REDDIT-BINARY REDDIT-MULTI-5K Epoch Epoch Epoch Epoch Cross-entropy Loss Figure 4.The training/validation/test curves on IMDB-BINARY , IMDB-MULTI, REDDIT-BINARY and REDDIT-MULTI-5K with GCN as backbone. The curves are depicted on ten runs. Table 2.Performance comparisons of G-Mixup with different GNNs on different datasets. The metric is the classiﬁcation accu- racy. Experimental settings are in Appendix F. Dataset IMDB-B IMDB-M REDD-B REDD-M5 REDD-M12 #graphs 1000 1500 2000 4999 11929 #classes 2 3 2 5 11 #avg.nodes 19.77 13.00 429.63 508.52 391.41 #avg.edges 96.53 65.94 497.75 594.87 456.89 GCN vanilla 72.18±1.55 48.79±2.72 78.82±1.33 45.07±1.70 46.90±0.73 w/ Dropedge 72.50±0.31 49.08±1.89 81.25±8.15 51.35±1.54 47.08±0.55 w/ DropNode 72.00±4.09 48.58±2.85 79.25±0.35 49.35±1.80 47.93±0.64 w/ Subgraph 68.50±4.76 49.58±2.61 74.33±2.88 48.70±1.63 47.49±0.93 w/ M-Mixup 72.83±1.75 49.50±1.97 75.75±4.53 49.82±0.85 46.92±1.05 w/ G-Mixup 72.87±3.85 51.30±2.14 89.81±0.74 51.51±1.70 48.06±0.53 GIN vanilla 71.55±3.53 48.83±2.75 92.59±0.86 55.19±1.02 50.23±0.83 w/ Dropedge 72.20±1.82 48.83±3.02 92.00±1.13 55.10±0.44 49.77±0.76 w/ DropNode 72.16±0.28 48.33±0.98 90.25±0.98 53.26±4.99 49.95±1.70 w/ Subgraph 68.50±0.86 47.25±3.78 90.33±0.87 54.60±3.15 49.67±0.90 w/ M-Mixup 70.83±1.04 49.88±1.34 90.75±1.78 54.95±0.86 49.81±0.80 w/ G-Mixup 71.94±3.00 50.46±1.49 92.90±0.87 55.49±0.53 50.50±0.41 ent datasets, and summarize results in Tables 2 and 3 as well as the training curves in Figure 4. We make the fol- lowing observations: 3 G-Mixup can improve the per- formance of graph neural networks on various datasets. From Table 2, G-Mixup gain 12 best performances among 15 reported accuracies, which substantially improve the performance of GNNs. Overall, G-Mixup performs 2.84% better than the vanilla model. Note that G-Mixup and base- line models adopt the same architecture of GNNs (e.g., layers, activation functions) and the same training hyper- parameters (e.g., optimizer, learning rate). From Table 3, G-Mixup gains 7 best performances among 8 cases, which substantially improve the performance of DiffPool and Min- cutPool. Meanwhile, 4 G-Mixup can improve the gener- alization of graph neural networks. From the loss curve on test data (green line) in Figure 4, the loss of test data of G-Mixup (dashed green lines) are consistently lower than the vanilla model (solid green lines). Considering both the better performance and the better test loss curves, G- Mixup is able to substantially improve the generalization of GNNs. Also, 5 G-Mixup could stabilize the model training. As shown in Table 2, G-Mixup achieves 11 lower standard deviation among total 15 reported numbers than the vanilla model. Additionally, the train/validation/test curves of G-Mixup (dashed line) in Figure 4 are more stable than vanilla model (solid line), indicating that G-Mixup sta- Table 3.Performance comparisons of G-Mixup with different Pool- ing methods. The metric is classiﬁcation accuracy. Method IMDB-B IMDB-M REDD-B REDD-M5k TopKPool vanilla 72.37 ±5.01 50.57±1.62 90.30±1.47 45.07±1.70 w/ Dropedge 71.75 ±2.18 48.75±2.94 88.96±1.90 47.43±1.82 w/ DropNode 69.16 ±1.04 48.50±2.50 81.33±4.48 46.15±2.28 w/ Subgraph 67.83 ±4.01 50.83±2.38 86.08±2.12 45.75±2.47 w/ M-Mixup 71.83 ±3.03 51.22±1.17 87.58±3.16 45.60±2.35 w/ G-Mixup 72.80±3.33 51.30±2.14 90.40±0.89 46.48±1.70 DiffPool vanilla 71.68 ±3.40 47.75±2.34 78.40±4.38 31.61±5.95 w/ Dropedge 69.16 ±2.51 49.44±2.50 76.00±5.50 34.46±6.80 w/ DropNode 70.25 ±3.01 46.83±1.34 76.68±2.57 33.10±5.53 w/ Subgraph 69.50 ±2.16 46.00±4.43 76.06±2.81 31.65±4.43 w/ M-Mixup 66.50 ±4.04 45.16±4.63 78.37±2.29 34.46±6.80 w/ G-Mixup 73.25±3.89 50.70±2.79 78.87±2.27 38.42±6.51 MincutPool vanilla 73.25 ±3.27 49.04±3.57 84.95±3.25 49.32±2.67 w/ Dropedge 69.16 ±2.51 49.66±1.73 81.37±1.59 47.20±1.10 w/ DropNode 73.50 ±3.89 49.91±2.83 85.68±2.04 46.82±4.60 w/ Subgraph 70.25 ±1.84 48.18±1.10 84.91±2.50 49.22±2.49 w/ M-Mixup 70.62 ±2.09 49.96±1.86 85.12±2.29 47.20±1.10 w/ G-Mixup 73.93±2.84 50.29±2.30 85.87±1.37 50.12±2.47 Table 4.Robustness to label corruption with different ratios. Models Methods 10% 20% 30% 40% IMDB-B vanilla 72.30 ±3.67 69.43±4.80 63.65±8.87 55.21±8.75 w/ Dropedge 72.00 ±2.44 69.52±3.25 64.12±3.44 48.50±0.00 w/ M-Mixup 71.87 ±3.56 69.03±4.85 65.62±9.89 48.50±0.00 w/ G-Mixup 72.56±3.08 69.87±5.41 65.50±8.90 52.56±6.97 REDD-B vanilla 73.90±1.43 75.68±2.75 68.12±0.81 46.50±0.00 w/ Dropedge 73.75 ±1.28 72.06±1.42 46.50±0.00 46.50±0.00 w/ M-Mixup 71.96 ±1.97 76.00±2.24 54.43±1.09 46.50±0.00 w/ G-Mixup 71.94 ±3.00 76.34±1.49 74.21±1.85 53.50±0.00 bilize the training of graph neural networks. Experiments on OGB (Hu et al., 2020) and more pooling method (GMT) are in Appendices G.2 and G.3. 5.4. Can G-Mixup improve the robustness of GNNs? We investigate the two kinds of robustness of G-Mixup, including Label Corruption Robustness and Topology Cor- ruption Robustness, and report the results in Table 4 and Table 5, respectively. More experimental settings are pre- sented in Appendix F.4. 6 G-Mixup improves the robust- ness of graph neural networks. Table 4 shows G-Mixup gains better performance in general, indicating it is more robust to noisy labels than the vanilla baseline. Table 5 shows that G-Mixup is more robust when graph topology is corrupted since the accuracy is consistently better than baselines. This can be an advantage ofG-Mixup when graph label or topology are noisy.G-Mixup: Graph Data Augmentation for Graph Classiﬁcation Table 5.Robustness to topology corruption with different ratios. Models Methods 10% 20% 30% 40% Removing vanilla 77.96 ±3.71 67.59±5.73 64.96±8.87 65.71±8.31 edges w/ Dropedge 74.40 ±2.26 65.12±3.51 65.93±2.32 57.87±4.14 w/ M-Mixup 75.62 ±1.59 65.81±3.84 59.81±9.45 57.31±3.15 w/ G-Mixup 81.46±3.08 71.12±7.47 67.46±8.90 66.25±7.78 Adding vanilla 76.12 ±5.73 74.37±6.48 72.31±2.69 72.00±2.92 edges w/ Dropedge 70.53 ±1.47 70.18±1.29 71.18±1.53 70.90±1.53 w/ M-Mixup 73.41 ±2.40 71.87±1.28 71.50±2.03 71.21±2.00 w/ G-Mixup 84.31±3.21 82.21±4.31 77.00±2.25 75.56±3.05 IMDB-BINARY REDDIT-BINARY Avg. #Nodes of Original Graphs Avg. #Nodes of Original Graphs Figure 5.The impact of the node numbers of generated synthetic graphs. The red vertical line indicates the average number of all the original training graphs. The blue line represents that classiﬁcation accuracy with different number of nodes of generated graphs. 5.5. Further Analysis 5.5.1. T HE NODES NUMBER OF GENERATED GRAPHS We investigate the impact of the nodes number in generated synthetic graphs by G-Mixup and present the results in Fig- ure 5. Speciﬁcally, G-Mixup generates synthetic graphs with different numbers (hyperparameters K) of nodes and use them to train graph neural networks. We observed form Fig- ure 5 that 7 using the average node number of all the original graphs is a better choice for hyperparameter Kin G-Mixup, which is in line with the intuition. 5.5.2. I MPACT ON DEEPER MODELS We investigate the performance of G-Mixup when GCN goes deeper. We experiment with different numbers (2 −9) of layers and report the results in Figure 6. 8 G-Mixup im- proves the performance of graph neural networks with varying layers. In Figure 6, the left ﬁgure shows G-Mixup gains better performance while the depth of GCNs is 2 −6. The performance with deeper GCNs (7 −9) are compara- ble to baselines, however, the accuracy is much lower than shallow ones. The right ﬁgure shows G-Mixup gains bet- ter performance by a signiﬁcant margin while the depth of GCNs is 2 −9. This validates the effectiveness of G-Mixup when graph neural network goes deeper. 6. Related Works Graph Data Augmentation. Graph neural networks (GNNs) achieve the state-of-the-art performance on graph classiﬁcation tasks (Kipf & Welling, 2016; Veliˇckovi´c et al., 2017; Hamilton et al., 2017; Xu et al., 2018; Zhang et al., IMDB-BINARY REDDIT-BINARY Figure 6.The performance of G-Mixup using GCNs with different layers on IMDB-BINARY and REDDIT-BINARY . 2018). In parallel, graph data augmentation methods im- prove the performance of GNNs. There are three cate- gories of graph data augmentation, including node pertur- bation (You et al., 2020; Huang et al., 2018), edge pertur- bation (Rong et al., 2020; You et al., 2020), and subgraph sampling (You et al., 2020; Wang et al., 2020a). However, the major limitation of the existing graph augmentation methods is that they are based on one single graph while G-Mixup leverages multiple input graphs. Besides, there are a line of works focusing on graph data augmentation methods for node classiﬁcation (Zhao et al., 2021; Wang et al., 2020b; Tang et al., 2021; Park et al., 2021; Verma et al., 2019b). The more discussion are in Appendix D. Graphon Estimation. Graphons and convergent graph se- quences have been broadly studied in mathematics (Lov´asz, 2012; Lov´asz & Szegedy, 2006; Borgs et al., 2008) and have been applied to network science (Avella-Medina et al., 2018; Vizuete et al., 2021) and graph neural networks (Ruiz et al., 2020a;b). There are tow lines of works to estimate step functions, one is based on stochastic block models, such as stochastic block approximation (SBA) (Airoldi et al., 2013), “largest gap” (LG) (Channarond et al., 2012) and sorting- and-smoothing (SAS) (Chan & Airoldi, 2014); another one is based on low-rank matrix decomposition, such as matrix completion (MC) (Keshavan et al., 2010), universal singular value thresholding (USVT) (Chatterjee et al., 2015). More discussion about graphon estimation are in Appendix B. 7. Conclusion This work develops a novel graph augmentation method called G-Mixup. Unlike image data, graph data is irregular, unaligned and in non-Euclidean space, making it hard to be mixed up. However, the graphs within one class have the same generator (i.e., graphon), which is regular, well- aligned and in Euclidean space. Thus we turn to mix up the graphons of different classes to generate synthetic graphs. G-Mixup is mix up and interpolate the topology of different classes of graphs. Comprehensive experiments show that GNNs trained with G-Mixup achieve better performance and generalization, and improve the model robustness to noisy labels and corrupted topology.G-Mixup: Graph Data Augmentation for Graph Classiﬁcation References Airoldi, E. M., Costa, T. B., and Chan, S. H. Stochastic blockmodel approximation of a graphon: Theory and consistent estimation. In Proceedings of the 26th Inter- national Conference on Neural Information Processing Systems-Volume 1, pp. 692–700, 2013. Avella-Medina, M., Parise, F., Schaub, M. T., and Segarra, S. Centrality measures for graphons: Accounting for uncertainty in networks. IEEE Transactions on Network Science and Engineering, 7(1):520–537, 2018. Baek, J., Kang, M., and Hwang, S. J. Accurate learning of graph representations with graph multiset pooling. In International Conference on Learning Representations, 2020. Bianchi, F. M., Grattarola, D., and Alippi, C. Spectral clustering with graph neural networks for graph pooling. In International Conference on Machine Learning , pp. 874–883. PMLR, 2020. Borgs, C., Chayes, J. T., Lov´asz, L., S´os, V . T., and Veszter- gombi, K. Convergent sequences of dense graphs i: Sub- graph frequencies, metric properties and testing. Ad- vances in Mathematics, 219(6):1801–1851, 2008. Chan, S. and Airoldi, E. A consistent histogram estimator for exchangeable graph models. In International Confer- ence on Machine Learning, pp. 208–216, 2014. Channarond, A., Daudin, J.-J., and Robin, S. Classiﬁcation and estimation in the stochastic blockmodel based on the empirical degrees. Electronic Journal of Statistics , 6: 2574–2601, 2012. Chatterjee, S. et al. Matrix estimation by universal singular value thresholding. The Annals of Statistics, 43(1):177– 214, 2015. Fox, J. and Rajamanickam, S. How robust are graph neural networks to structural noise? arXiv preprint arXiv:1912.10206, 2019. Frieze, A. and Kannan, R. Quick approximation to matrices and applications. Combinatorica, 19(2):175–220, 1999. Gao, H. and Ji, S. Graph u-nets. arXiv preprint arXiv:1905.05178, 2019. Guo, H. Nonlinear mixup: Out-of-manifold data augmen- tation for text classiﬁcation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2020. Guo, H., Mao, Y ., and Zhang, R. Augmenting data with mixup for sentence classiﬁcation: An empirical study. arXiv preprint arXiv:1905.08941, 2019a. Guo, H., Mao, Y ., and Zhang, R. Mixup as locally lin- ear out-of-manifold regularization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pp. 3714– 3722, 2019b. Hamilton, W., Ying, Z., and Leskovec, J. Inductive repre- sentation learning on large graphs. In Advances in neural information processing systems, pp. 1024–1034, 2017. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. In Advances in Neural Information Processing Systems, volume 33, pp. 22118–22133, 2020. Hu, Z., Fang, Y ., and Lin, L. Training graph neural networks by graphon estimation, 2021. Huang, W., Zhang, T., Rong, Y ., and Huang, J. Adaptive sampling towards fast graph representation learning. Ad- vances in Neural Information Processing Systems , 31: 4558–4567, 2018. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448– 456. PMLR, 2015. Keshavan, R. H., Montanari, A., and Oh, S. Matrix comple- tion from a few entries.IEEE transactions on information theory, 56(6):2980–2998, 2010. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In ICLR (Poster), 2015. Kipf, T. N. and Welling, M. Semi-supervised classiﬁca- tion with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. Lov´asz, L. Large networks and graph limits , volume 60. American Mathematical Soc., 2012. Lov´asz, L. and Szegedy, B. Limits of dense graph sequences. Journal of Combinatorial Theory, Series B, 96(6):933– 957, 2006. Nair, V . and Hinton, G. E. Rectiﬁed linear units improve restricted boltzmann machines. In Icml, 2010. Park, H., Lee, S., Kim, S., Park, J., Jeong, J., Kim, K.- M., Ha, J.-W., and Kim, H. J. Metropolis-hastings data augmentation for graph neural networks. Advances in Neural Information Processing Systems, 34, 2021. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: To- wards deep graph convolutional networks on node classi- ﬁcation. In ICLR 2020 : Eighth International Conference on Learning Representations, 2020.G-Mixup: Graph Data Augmentation for Graph Classiﬁcation Ruiz, L., Chamon, L., and Ribeiro, A. Graphon neural networks and the transferability of graph neural networks. Advances in Neural Information Processing Systems, 33, 2020a. Ruiz, L., Wang, Z., and Ribeiro, A. Graph and graphon neu- ral network stability. arXiv preprint arXiv:2010.12529, 2020b. Suresh, S., Li, P., Hao, C., and Neville, J. Adversarial graph augmentation to improve graph contrastive learning. arXiv preprint arXiv:2106.05819, 2021. Tang, Z., Qiao, Z., Hong, X., Wang, Y ., Dharejo, F. A., Zhou, Y ., and Du, Y . Data augmentation for graph convolutional network on semi-supervised classiﬁcation. arXiv preprint arXiv:2106.08848, 2021. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y . Graph attention networks.arXiv preprint arXiv:1710.10903, 2017. Verma, V ., Lamb, A., Beckham, C., Najaﬁ, A., Mitliagkas, I., Lopez-Paz, D., and Bengio, Y . Manifold mixup: Better representations by interpolating hidden states. In Interna- tional Conference on Machine Learning, pp. 6438–6447. PMLR, 2019a. Verma, V ., Qu, M., Kawaguchi, K., Lamb, A., Bengio, Y ., Kannala, J., and Tang, J. Graphmix: Improved training of gnns for semi-supervised learning. arXiv preprint arXiv:1909.11715, 2019b. Vizuete, R., Garin, F., and Frasca, P. The laplacian spectrum of large graphs sampled from graphons. IEEE Transac- tions on Network Science and Engineering, 2021. Wang, Y ., Wang, W., Liang, Y ., Cai, Y ., and Hooi, B. Graphcrop: Subgraph cropping for graph classiﬁcation. arXiv preprint arXiv:2009.10564, 2020a. Wang, Y ., Wang, W., Liang, Y ., Cai, Y ., Liu, J., and Hooi, B. Nodeaug: Semi-supervised node classiﬁcation with data augmentation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 207–217, 2020b. Wang, Y ., Wang, W., Liang, Y ., Cai, Y ., and Hooi, B. Mixup for node and graph classiﬁcation. In Proceedings of the Web Conference 2021, pp. 3663–3674, 2021. Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Philip, S. Y . A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning sys- tems, 2020. Xu, H., Luo, D., Carin, L., and Zha, H. Learning graphons via structured gromov-wasserstein barycenters. In Pro- ceedings of the AAAI Conference on Artiﬁcial Intelli- gence, pp. 10505–10513, 2021. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2018. Ying, R., You, J., Morris, C., Ren, X., Hamilton, W. L., and Leskovec, J. Hierarchical graph representation learn- ing with differentiable pooling. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 4805–4815, 2018. You, Y ., Chen, T., Sui, Y ., Chen, T., Wang, Z., and Shen, Y . Graph contrastive learning with augmentations.Advances in Neural Information Processing Systems, 33, 2020. You, Y ., Chen, T., Wang, Z., and Shen, Y . Bringing your own view: Graph contrastive learning without prefabricated data augmentations. arXiv preprint arXiv:2201.01702, 2022. Zhang, H., Cisse, M., Dauphin, Y . N., and Lopez-Paz, D. mixup: Beyond empirical risk minimization. In Interna- tional Conference on Learning Representations, 2017. Zhang, L., Deng, Z., Kawaguchi, K., Ghorbani, A., and Zou, J. How does mixup help with robustness and gen- eralization? In International Conference on Learning Representations, 2021. Zhang, M., Cui, Z., Neumann, M., and Chen, Y . An end-to- end deep learning architecture for graph classiﬁcation. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. Zhang, Z., Cui, P., and Zhu, W. Deep learning on graphs: A survey. IEEE Transactions on Knowledge and Data Engineering, 2020. Zhao, T., Liu, Y ., Neves, L., Woodford, O., Jiang, M., and Shah, N. Data augmentation for graph neural networks. In Proceedings of the AAAI Conference on Artiﬁcial In- telligence, volume 35, pp. 11015–11023, 2021. Zhao, Y . Graph theory and additive combinatorics, 2019. URL https://yufeizhao.com/gtac/. Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and Sun, M. Graph neural networks: A review of methods and applications. AI Open, 1:57–81, 2020a. Zhou, J., Shen, J., and Xuan, Q. Data augmentation for graph classiﬁcation. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pp. 2341–2344, 2020b.G-Mixup: Graph Data Augmentation for Graph Classiﬁcation A. Proof of Theorem In the appendix, we ﬁrst present the preliminaries in Appendix A.1. And then we present complete proof for Theorems 4.2 and 4.3 in Appendices A.2 and A.3, respectively. A.1. Preliminaries Cut norm (Lov´asz, 2012; Zhao, 2019) is used to measure structural similarity of two graphons. The deﬁnition of cut norm is as follow: Deﬁnition A.1. The cut norm of grapon W is deﬁned as ∥W∥□ = sup S,T⊂[0,1] ⏐⏐⏐ ∫ S×T W(x,y)dxdy ⏐⏐⏐, (7) where the supremum is taken over all measurable subsets S and T. The following lemma follows the derivation of counting lemma for graphons, are known in the paper (Lov´asz, 2012). It will be used to prove the Theorem 4.2. Lemma A.2. Let F be a simple graph and let W,W′∈W. Then |t(F,W ) −t(F,W ′)|≤ e(F)||W −W′||□ (8) Proof of Lemma A.2: The proof follows Zhao (2019). For an arbitrary simple graph F, by the triangle inequality we have |t(F,W ) −t(F,W ′)| = ⏐⏐⏐⏐⏐ ∫ ( ∏ uivi∈E W(ui,vi) − ∏ uivi∈E W′(ui,vi) )∏ v∈V dv ⏐⏐⏐⏐⏐ ≤ |E|∑ i=1 ⏐⏐⏐⏐⏐⏐ ∫   i−1∏ j=1 W′(uj,vj) (W(ui,vi) −W′(ui,vi)) |E|∏ k=i+1 W(uk,vk)  ∏ v∈V dv ⏐⏐⏐⏐⏐⏐ (9) Here, each absolute value term in the sum is bounded by the cut norm ∥W −W′∥□ if we ﬁx all other irrelavant variables (everything except ui and vi for the i-th term), altogether implying that |t(F,W ) −t(F,W ′)|≤ e(F)||W −W′||□ (10) ■ Lemma A.3 (Corollary 10.4 in (Lov´asz & Szegedy, 2006)). Let W be a graphon, n≥1, 0 <ε< 1, and let F be a simple graph, then the W-random graph G = G(n,W) satisﬁes P (|t(F,G) −t(F,W )|>ε) ≤2exp ( − ε2n 8v(F)2 ) (11) A.2. Proof of Theorem 1 We have the mixed graphonWI= λWG+ (1 −λ)WH. Let W = WI, W′= WG, and F = FGin Lemma A.2, we have, |t(FG,WI) −t(FG,WG)|≤ e(FG)||WI−WG||□ |t(FG,λWG+ (1 −λ)WH) −t(F,WG)|≤ e(FG)||λWG+ (1 −λ)WH−WG||□ ≤e(FG)||(1 −λ)(WH−WG)||□ (12) Recall that the cut norm ∥W∥□ = supS,T⊆[0,1] ⏐⏐⏐ ∫ S×T W ⏐⏐⏐.G-Mixup: Graph Data Augmentation for Graph Classiﬁcation obviously, suppose α∈R, we have ∥αW∥□ = sup S,T⊆[0,1] ⏐⏐⏐⏐ ∫ S×T αW ⏐⏐⏐⏐= sup S,T⊆[0,1] ⏐⏐⏐⏐α ∫ S×T W ⏐⏐⏐⏐= α∥W∥□ (13) Based on Equation (12) and Equation (13), we have |t(FG,λWG+ (1 −λ)WH) −t(FG,WG)|≤ e(FG)||(1 −λ)(WH−WG)||□ ≤(1 −λ)e(FG)||WH−WG||□ (14) Similarly, let W = WI, W′= WHand F = FHin Lemma A.2, We can also easily obtain |t(FH,λWG+ (1 −λ)WH) −t(FH,WH)|≤ λe(FH)||WH−WG||□ (15) Equation (14) and Equation (15) produce the upper bound in Equation (7). ■ A.3. Proof of Theorem 2 Let F and W be the discriminative motif FIand the mixed graphon WIin Lemma A.3, we will have P (|t(FI,G) −t(FI,WI)|>ε) ≤2exp ( − ε2n 8v(FI)2 ) (16) which produces the result in Equation (7). ■ A.4. Graphons Estimation by Step Function The proof follows Xu et al. (2021). A graphon can always be approximated by a step function in the cut norm (Frieze & Kannan, 1999). Let P= (P1,.., PK) be a partition of Ω into Kmeasurable sets. We deﬁne a step function WP: Ω2 ↦→[0,1] as WP(x,y) = ∑K k,k′=1 wkk′1Pk×Pk′ (x,y), (17) where each wkk′ ∈[0,1] and the indicator function 1Pk×Pk′ (x,y) is 1 if (x,y) ∈Pk ×Pk′, otherwise it is 0. The weak regularity lemma (Lov´asz, 2012) shown below guarantees that every graphon can be approximated well in the cut norm by step functions. Theorem A.4 (Weak Regularity Lemma (Lemma 9.9 in (Lov ´asz, 2012)) ). For every graphonW and K ≥1, there always exists a step function W with |P|= Ksteps such that ∥W −W∥□ ≤ 2√log K∥W∥L2 . (18) B. Graphons Estimation Methods The adopted graphon estimated methods (e.g., LG, USVT, SBA) are well-studied methods. Typically they have rigorous mathematical proof to upper bound the graphon estimation error. For example, Theorem 2.10 in (Chatterjee et al., 2015) shows the graphon estimation error of USVT is strictly upper bounded. And we also copy the results of graphon estimation methods on synthetic graphon from (Xu et al., 2021) in Table 6. The results show the graphon estimation methods in our work can precisely estimate graphon. The details of them are listed as the following: • SBA (Airoldi et al., 2013) The Stochastic Block Approximation learns stochastic block models to approximate graphons. This method can consistently estimate the graphon with extremely small error and the estimation error vanishes provably as the node number of the graph goes inﬁnity.G-Mixup: Graph Data Augmentation for Graph Classiﬁcation Table 6.The MSE error of graphon estimation methods on synthetic graphs(Xu et al., 2021). The graphon estimation is based on 10 graphs, the error is Mean Square Error, and the resolution of graphon is 1000 ×1000. W(x,y) SBA LG MC USVT SAS xy 65.6±6.5 29.8±5.7 11.3±0.8 31.7±2.5 125.0±1.3 e−(x0.7+y0.7) 58.7±7.8 22.9±3.1 71.7±0.5 12.2±1.5 77.7±0.8 x2+y2+√x+√y 4 63.4±7.6 24.1±2.5 73.2±0.7 33.8±1.1 99.3±1.2 1 2 (x+ y) 66.2±8.3 24.0±2.5 71.9±0.6 40.2±0.8 108.3±1.0 1 1+exp(−10(x2+y2)) 5.0±9.5 23.1±3.2 64.6±0.5 37.3±0.6 73.3±0.7 • LG (Channarond et al., 2012) The “largest gap” algorithm improve the SBA method, which can be used for both large-scale and small graphs. • SAS (Chan & Airoldi, 2014) The smoothing-and-sorting (SAS) is a improved variant of SBA, which ﬁrst sorts the graphs based on the node degree, then smooths the sorted graph using total variation minimization. • MC and USVT (Keshavan et al., 2010; Chatterjee et al., 2015) Matrix Completion and Universal Singular Value Thresholding are matrix decomposition based methods, which learn low-rank matrices to approximate graphons. C. Discussion about Manifold Intrusion in G-Mixup In this appendix, we discuss that manifold intrusion in G-Mixup and argue that G-Mixup does not suffer from manifold intrusion issue. The manifold intrusion may be harmful for mixup method. Manifold intrusion in mixup is a form of under-ﬁtting resulting from conﬂicts between the labels of the synthetic examples and the labels of original training data (Guo et al., 2019b). The manifold intrusion in graph learning represents that the generated graphs have identical topology but different labels. In our method, the adjacency matrix A ∈RK×K of generated graphs are generated from the matrix-from graphon W ∈RK×K, thus we have Aij iid ∼Bern(Wij),∀i,j ∈[K]. In the graph generation phase, G-Mixup may cause manifold intrusion in two cases: 1) two generated two graphs are identical, 2) a generated graph is identical to an original graph. We hereby show that graph manifold intrusion issue will not happen with a very high probability in G-Mixup as follows: • Two generated two graphs are identical. The probability of generating two identical graphs from the same graphon W is ΠK i=1ΠK j=1(W2 ij + (1 −Wij)2), which is extremely small since 0 <W2 ij + (1 −Wij)2 <1 and K is large enough in the real-world graphs. The probability that two generated two graphs are identical are extremely small. • A generated graph is identical to an original graph. The probability of generating a new graph that is identical to an original graph (the adjacency matrix is ˜A) is ΠK i=1ΠK j=1(W ˜Aij ij (1 −Wij)1−˜Aij ), which is extremely small since 0 < W ˜Aij ij (1 −Wij)1−˜Aij < 1 and K is large enough in the real-world graphs. The probability that a generated graph is identical to an original graph are identical are extremely small too. D. More Discussion about Related Works In this appendix, we discuss two categories of related works. The ﬁrst one is graph data augmentation for node classiﬁcation, and the second is model-dependent graph data augmentation for graph classiﬁcation. Both of them are different to our proposed G-Mixup. Graph Data Augmentation for Node Classiﬁcation. There is another line of works targeting graph data augmentation for node classiﬁcation (Zhao et al., 2021; Wang et al., 2020b; Tang et al., 2021; Park et al., 2021; Verma et al., 2019b). Zhou et al. (2020b) leverage information inherent in the graph to predict edge probability to augment a new graph for node classiﬁcation task. Verma et al. (2019b) proposed GraphMix to augment the vanilla GNN with a Fully-Connected NetworkG-Mixup: Graph Data Augmentation for Graph Classiﬁcation Algorithm 1 Graphon Estimation Input: graph set G, graphon estimator g ⊿ each graph Ghas adjacency matrix A and node features matrix X Init: sorted adjacency matrix set ¯A= {} for each graph Gin Gdo Calculate the degree of each nodes in G Calculate sorted adjacency matrix ¯A by sorting A based on the degree Calculate sorted node features matrix ¯X by sorting X based on the degree Add the sorted adjacency matrix ¯A to ¯A end for Estimate step function WGwith ¯Ausing g. ⊿we use LG as gin experiments Obtain graphon node feature ¯XGby average pooling X ⊿we can use other pooling method (e.g., maxpooling) Return: WG, ¯XG Algorithm 2 G-Mixup Input: train graph set S, graphon estimator g, mixup ratio λ, augmented ratio α ⊿ 0 <λ,α< 1 Init: Synthetic graph set I= {}. Obtain two graph sets Gand Hwith different labels yGand yH Estimate (WG, ¯XG) and (WH, ¯XH) from Gand Husing Algorithm 1 Mix up step function WI= λWG+ (1 −λ)WH Mix up graphon node features ¯XI= λ¯XG+ (1 −λ) ¯XH Sample α·|S| synthetic graphs based on WIand ¯XIand add them to I ⊿|I|= α·|S| after augmentation Return: synthetic graph set I (FCN) and the FCN loss is computed using Manifold Mixup. Verma et al. (2019b) proposed to generate augmented graphs from an explicit target distribution for semi-supervised learning, which has ﬂexible control of the strength and diversity of augmentation. Many graph augmentation methods are proposed to solve node classiﬁcaiton task. However, the node classiﬁcation task is a different task in graph learning from graph classiﬁcation task. The node classiﬁcation task usually has one input graph, thus the graph augmentation methods for node classiﬁcation is limited to one graph while the graph augmentation for graph classiﬁcation can manipulate multiple graphs. Thus graph data augmentation for node-level task is not applicable to our scenario. Model-Dependent Graph Data Augmentation for Graph Classiﬁcation. There are some model-dependent graph aug- mentation methods (Suresh et al., 2021; You et al., 2022; Zhou et al., 2020b) for graph classiﬁcation task. Suresh et al. (2021) proposed to enable GNNs to avoid capturing redundant information during the training by optimizing adversarial graph augmentation strategies used in graph contrastive learning during the training phase. You et al. (2022) proposed to learn a continuous prior parameterized by a neural network from data during contrastive training, which is used to augment graph. The difference between our proposal and these methods is that G-Mixup an general model-agnostic graph data augmentation methods for graph classiﬁcation. E. Implementation Details In this appendix, we present the pseudo code for G-Mixup. We ﬁrst present the pseudo code for graphon estimation in Algorithm 1, which depicts how to generate the graphon and the node features. Since our proposed method is a model- agnostic method, which can be conducted before the model training. Then we present the pseudo code G-Mixup. The graphon estimation is based on the one class of graphs, thus we can estimate on graphon using all the graphs in the same class or a random batch of graphs in the same class. On this basis, we have two version of concrete implementations: 1) estimating graphon on graphon using all the graphs in the same class (Algorithm 2), 2) estimating graphon on graphon using a random batch of graphs in the same class (Algorithm 3). The ﬁrst implementation provide more accurate estimated graphons while the second encourages more diversity of the synthetic graphs. Note that all these two versions can be done as a pre-processing before model training.G-Mixup: Graph Data Augmentation for Graph Classiﬁcation Algorithm 3 G-Mixup (batch) Input: train graph set Swith Bbatches, graphon estimator g, mixup ratio λ, augmented ratio α ⊿ 0 <λ,α< 1 Init: Synthetic graph set I= {} for batch in Sdo Obtain two graph sets Gand Hwith different labels yGand yH Estimate (WG, ¯XG) and (WH, ¯XH) from Gand Husing Algorithm 1 Mix up step function WI= λWG+ (1 −λ)WH Mix up graphon node features ¯XI= λ¯XG+ (1 −λ) ¯XH Sample α·|S|/Bsynthetic graphs based on WIand ¯XIand add them to I ⊿|I|= α·|S| after for loop ends end for Return: synthetic graph set I F. Experiments Details F.1. Experimental Setting To ensure a fair comparison, we use the same hyperparater for modeling training and the same architecture for vanilla model and other baselines. For model training, we use the Adam optimizer(Kingma & Ba, 2015). The initial learning rate is 0.01 and will drop the learning rate by half every 100 epochs. The batch size is set to 128. We split the dataset into train/val/test data by 7 : 1 : 2. Note that best test epoch is selected on a validation set, and we report the test accuracy on ten runs. For hyperparemeter in G-Mixup, we generate 20% more graph for training graph. The graphons are estimated based on the training graphs. We use different λ ∈[0.1,0.2] to mix up the graphon and generate synthetic with different strength of mixing up. F.2. Architectures of Graph Neural Networks We adopted two categories of graph neural networks as our baselines, The ﬁrst category is Graph Convolutional Network (GCN) and Graph Isomorphism Network (GIN). The second category is graph polling methods, including TopK Pooling (TopKPool), Differentiable Pooling (DiffPool), MinCut Pooling (MincutPool) and Graph Multiset Pooling (GMT). The details of the GNNs are listed as follows: • GCN4 (Kipf & Welling, 2016). Four GNN layers and global mean pooling are applied. All the hidden units is set to 64. The activation is ReLU (Nair & Hinton, 2010). • GIN5 (Xu et al., 2018). We apply ﬁve GNN layers and all MLPs have two layers. Batch normalization (Ioffe & Szegedy, 2015) is applied on every hidden layer. All hidden units are set to 64. The activation is ReLU (Nair & Hinton, 2010). • TopKPool6 (Gao & Ji, 2019). Three GNN layers and three TopK pooling are applied. A there-layer percetron are adopted to predict the labels. All the hidden units is set to 64. The activation is ReLU (Nair & Hinton, 2010). • DiffPool7 (Ying et al., 2018) is a differentiable graph pooling methods that can be adapted to various GNN architectures, which maps nodes to clusters based on their learned embeddings. • MincutPool8 (Bianchi et al., 2020) is a differentiable pooling baselines. It learns a clustering function that can be quickly evaluated on out-of-sample graphs. • GMT9 (Baek et al., 2020) is a multi-head attention based global pooling layer to generate graph representation, which captures the interaction between nodes according to their structure. 4https://github.com/pyg-team/pytorch_geometric/blob/1.7.2/examples/gcn2_ppi.py 5https://github.com/pyg-team/pytorch_geometric/blob/1.7.2/examples/mutag_gin.py 6https://github.com/pyg-team/pytorch_geometric/blob/1.7.2/examples/proteins_topk_pool.py 7https://github.com/pyg-team/pytorch_geometric/blob/1.7.2/examples/proteins_diff_pool.py 8https://github.com/pyg-team/pytorch_geometric/blob/1.7.2/examples/proteins_mincut_pool. py 9https://github.com/JinheonBaek/GMTG-Mixup: Graph Data Augmentation for Graph Classiﬁcation LG MC SAS SBA USVT IMDB-BREDDIT-BIMDB-M Figure 7.The estimated graphon on various dataset with different graphon estimation methods. F.3. Baseline Methods We adopted three mainstream graph data augmentation methods as our baselines, including DropEdge, DropNode, Subgraph and Manifold-Mixup. The details of the baselines are listed as follows, • DropEdge10 (Rong et al., 2020). DropEdge randomly removes a certain ratios of edges from the input graph at each training epoch, which can prevent over-ﬁtting and alleviate over-smoothing. • DropNode11 (You et al., 2020). DropNode randomly remove certain portion of nodes as well as their connections, which under a underlying assumption that missing part of nodes will note affect the semantic meaning of original graph. • Subgraph12 (You et al., 2020; Wang et al., 2020a). Subgraph method samples a subgraph from the original graph using random walk The generated graph will keep part of the the semantic meaning of original graphs. • M-Manifold13 (Wang et al., 2021) Manifold-Mixup conducts Mixup operation for graph classiﬁcation in the embedding space, which interpolates graph-level embedding after the READOUT function. F.4. Experimental Setting of Robustness The graph neural network adopted in this experiment is GCN, the architecture of which is as above. For label corruption, we randomly corrupt the graph labels with different corruption ratio 10%,20%,30%,40%. For topology corruption, we we randomly remove/add edges with different corruption ratio 10%,20%,30%,40%. The dataset for topology corruption is REDDIT-BINARY . G. Additional Experiments In this appendix, we conduct additional experiments to further investigate the proposed method. G.1. Visualization of Graphons on More Real-world Dataset G-Mixup explores ﬁve graphon estimation methods, including sorting-and-smoothing (SAS) method (Chan & Airoldi, 2014), stochastic block approximation (SBA) (Airoldi et al., 2013), “largest gap” (LG) (Channarond et al., 2012), matrix completion (MC) (Keshavan et al., 2010) and the universal singular value thresholding (USVT) (Chatterjee et al., 2015). We present the estimated graphon by LGin Figure 2. Here we present more visualization of graphons on IMDB-BINARY , REDDIT-BINARY and IMDB-MULTI dataset. An obvious observation is that graphons of different classes of graphs are different. This observation further validates the divergence of graphon between different classes of graphs. 10https://github.com/DropEdge/DropEdge 11https://github.com/Shen-Lab/GraphCL 12https://github.com/Shen-Lab/GraphCL 13https://github.com/vanoracai/MixupForGraphG-Mixup: Graph Data Augmentation for Graph Classiﬁcation Table 7.Performance comparisons of G-Mixup with GMT on different dataset. The metric is the classiﬁcation accuracy and its standard deviation. The best performance is in boldface. Backbone Method D&D MUTAG PROTEINS IMDB-B IMDB-M GMT vanilla 78.29 ±5.77 82.77±6.30 74.59±5.29 73.60±3.87 50.73±3.03 w/ Dropedge 78.37 ±4.17 82.22±8.88 74.32±5.42 73.40±3.85 50.73±3.09 w/ M-Mixup 77.69 ±3.81 82.22±10.48 74.41±3.97 73.70±3.79 49.93±3.49 w/ G-Mixup 79.57±3.69 84.44±8.88 75.13±5.06 74.70±3.76 51.33±3.52 Table 8.Performance comparisons of G-Mixup on molecular property prediction task. The metric is AUROC 16 and its standard deviation. The best performance is in boldface. Backbones Mehtods ogbg-molhiv ogbg-molbbbp ogbg-molbace GCN vanilla 76.24 ±0.98 68.05±1.52 80.36±1.56 w/ Dropedge 75.93 ±0.76 68.02±0.95 80.22±1.59 w/ ManifoldMixup 76.24 ±1.40 68.36±2.05 80.46±2.05 w/ G-Mixup 76.29±0.80 69.51±1.20 80.73±2.06 GCN-virtual vanilla 75.62 ±1.65 65.13±1.11 74.49±3.04 w/ Dropedge 74.64 ±1.32 66.46±1.61 69.75±3.47 w/ ManifoldMixup 74.04 ±2.06 65.51±1.74 73.10±4.97 w/ G-Mixup 76.56±0.80 70.05±1.78 73.55±4.79 GIN vanilla 77.08 ±1.96 68.42±2.31 75.91±1.01 w/ Dropedge 75.77 ±1.75 66.16±2.96 70.50±6.24 w/ ManifoldMixup 75.73 ±1.25 68.15±2.04 77.44±4.13 w/ G-Mixup 77.14±0.45 70.17±1.03 77.79±3.34 GIN-virtual vanilla 77.52±1.56 67.10±2.10 74.19±4.99 w/ Dropedge 76.83 ±1.11 68.87±1.17 72.20±3.37 w/ ManifoldMixup 76.51 ±2.22 68.04±2.87 74.17±1.38 w/ G-Mixup 77.09 ±1.07 69.18±0.87 73.53±3.98 G.2. Experiment on More Graph Neural Networks Pooling Method (GMT) To further validate the effectiveness ofG-Mixup on more graph neural networks, we experiment with GMT (Baek et al., 2020), a modern pooling method. To reproduce GMT results, we the released code and the recommended hyperparameters for their used datasets (D&D, MUTAG, PROTEINS, IMDB-B, IMDB-M) in their paper. The results are presented in Table 7. 9 G-Mixup can signiﬁcantly improve the performance of GMT. Table 7 shows that G-Mixup outperform all the baselines on all datasets. Overall, G-Mixup outperform vanilla, Dropedge, ManifoldMixup by 1.44%, 1.28%, 2.01%, respectively. This indicates the superiority of G-Mixup for graph classiﬁcation task. G.3. Experiment on Molecular Property Prediction We experiment on molecular property prediction task (Hu et al., 2020), including ogbg-molhiv, ogbg-molbace, ogbg- molbbbp. In these dataset, each graph represents a molecule, where nodes are atoms, and edges are chemical bonds. We adopte ofﬁcial reference graph neural network backbones (gcn, gcn-vitual, gin, gin-vitual) 14 as our backbones, and we generate the edge attributes randomly for synthetic graphs. The results are presented in Table 8. 10 G-Mixup can improve the performance of GNNs on molecular property prediction task with the experimental setting for a fair comparison. Table 8 shows that G-Mixup gains 9 best performances among 12 reported AUCs. 14https://github.com/snap-stanford/ogb/tree/master/examples/graphproppred/mol 16The metric is Area Under Receiver Operating Characteristic.G-Mixup: Graph Data Augmentation for Graph Classiﬁcation Table 9.The sensitivity of G-Mixup to Mixup Ratio λ on ogbg-molbbbp dataset. The p-value is 0.00515,0.0994,0.0109,0.0471 , indicating the 3 improvements are statistically signiﬁcant (p< 0.05). λ 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 Vanilla GCN 68.23 ±0.75 68.23±1.81 68.45±0.84 67.54±3.09 69.51±1.20 67.79±0.82 67.60±1.31 69.48±2.62 67.86±1.02 68.78±2.61 68.05±1.52 GCN-virtual 68.57 ±2.61 68.81±1.57 67.20±1.30 68.64±2.09 70.05±1.78 68.77±2.31 69.11±1.12 68.82±0.98 69.07±1.48 68.37±0.95 65.13±1.11 GIN 68.20 ±1.04 69.37±1.38 69.28±1.24 68.89±2.70 70.17±1.03 66.95±0.92 69.86±1.05 70.01±1.14 68.65±1.03 69.73±1.32 68.42±2.31 GIN-virtual 70.58 ±1.55 69.44±1.88 70.02±1.68 69.77±0.88 69.18±0.87 68.17±1.67 68.62±1.15 69.16±1.87 70.15±1.32 68.66±0.68 67.10±2.10 Table 10.The sensitivity of G-Mixup to Mixup Ratio λ on ogbg-molbace dataset. The p-value is 0.0227,0.0375,0.0401,0.0427, indicating the 4 improvements are statistically signiﬁcant (p< 0.05). λ 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 Vanilla GCN 77.41 ±2.24 77.33±2.10 80.73±2.06 78.42±2.25 77.98±2.03 79.25±1.64 75.80±4.31 78.40±1.88 79.54±1.25 77.90±2.67 80.36±1.56 GCN-virtual 75.64 ±4.03 76.80±1.74 73.55±4.79 76.46±1.05 73.97±4.11 76.55±2.28 75.91±2.73 77.99±2.59 78.34±1.10 72.84±5.52 74.49±3.04 GIN 76.44 ±2.19 75.55±4.05 77.79±3.34 75.20±2.91 74.79±2.64 76.27±4.61 73.02±3.68 76.29±3.55 75.77±2.30 74.12±4.12 75.91±1.01 GIN-virtual 74.51 ±4.91 74.07±2.76 73.53±3.98 78.85±1.98 77.15±2.44 76.85±3.42 79.69±1.37 75.13±5.46 77.04±1.37 78.63±2.04 74.19±4.99 G.4. Sensitivity Analysis to Mixup Ratio λ To further investigate the performance of G-Mixup, we provide experimental results of G-Mixup to analyse the sensitivity to hyperparameter mixup ratioλ. Speciﬁcally, we use the different mixing ratioλin WI= λWG+(1−λ)WH,λyG+(1−λ)yH on molecular property prediction task (i.e., ogbg-molbbbp, ogbg-molbace). The p-value 17 is computed with the best performance compared to the Vanilla GCN (last column in Table 9 and Table 9). We can observed thatG-Mixup signiﬁcantly improves graph neural networks’ performance while we tune the hyperparameter ofG-Mixup. 17A p-value less than 0.05 (typically ≤0.05) is statistically signiﬁcant.",
      "references": [
        "Stochastic blockmodel approximation of a graphon: Theory and consistent estimation.",
        "Centrality measures for graphons: Accounting for uncertainty in networks.",
        "Accurate learning of graph representations with graph multiset pooling.",
        "Spectral clustering with graph neural networks for graph pooling.",
        "Convergent sequences of dense graphs i: Subgraph frequencies, metric properties and testing.",
        "A consistent histogram estimator for exchangeable graph models.",
        "Classification and estimation in the stochastic blockmodel based on the empirical degrees.",
        "Matrix estimation by universal singular value thresholding.",
        "How robust are graph neural networks to structural noise?",
        "Quick approximation to matrices and applications.",
        "Graph u-nets.",
        "Nonlinear mixup: Out-of-manifold data augmentation for text classification.",
        "Augmenting data with mixup for sentence classification: An empirical study.",
        "Mixup as locally linear out-of-manifold regularization.",
        "Inductive representation learning on large graphs.",
        "Open graph benchmark: Datasets for machine learning on graphs.",
        "Training graph neural networks by graphon estimation.",
        "Adaptive sampling towards fast graph representation learning.",
        "Batch normalization: Accelerating deep network training by reducing internal covariate shift.",
        "Matrix completion from a few entries.",
        "Adam: A method for stochastic optimization.",
        "Semi-supervised classification with graph convolutional networks.",
        "Large networks and graph limits.",
        "Limits of dense graph sequences.",
        "Rectified linear units improve restricted boltzmann machines.",
        "Metropolis-hastings data augmentation for graph neural networks.",
        "Dropedge: Towards deep graph convolutional networks on node classification.",
        "Graphon neural networks and the transferability of graph neural networks.",
        "Graph and graphon neural network stability.",
        "Adversarial graph augmentation to improve graph contrastive learning.",
        "Data augmentation for graph convolutional network on semi-supervised classification.",
        "Graph attention networks.",
        "Manifold mixup: Better representations by interpolating hidden states.",
        "Graphmix: Improved training of gnns for semi-supervised learning.",
        "The laplacian spectrum of large graphs sampled from graphons.",
        "Graphcrop: Subgraph cropping for graph classification.",
        "Nodeaug: Semi-supervised node classification with data augmentation.",
        "Mixup for node and graph classification.",
        "A comprehensive survey on graph neural networks.",
        "Learning graphons via structured gromov-wasserstein barycenters.",
        "How powerful are graph neural networks?",
        "Hierarchical graph representation learning with differentiable pooling.",
        "Graph contrastive learning with augmentations.",
        "Bringing your own view: Graph contrastive learning without prefabricated data augmentations.",
        "mixup: Beyond empirical risk minimization.",
        "How does mixup help with robustness and generalization?",
        "An end-to- end deep learning architecture for graph classification.",
        "Deep learning on graphs: A survey.",
        "Data augmentation for graph neural networks.",
        "Graph theory and additive combinatorics,",
        "Graph neural networks: A review of methods and applications.",
        "Data augmentation for graph classification."
      ],
      "meta_data": {
        "arxiv_id": "2202.07179v2",
        "authors": [
          "Xiaotian Han",
          "Zhimeng Jiang",
          "Ninghao Liu",
          "Xia Hu"
        ],
        "published_date": "2022-02-15T04:09:44Z",
        "github_url": "https://github.com/pyg-team/pytorch_geometric"
      },
      "llm_extracted_info": {
        "main_contributions": "G-Mixup tackles the challenge of applying Mixup to graphs by performing class-level data augmentation at the generator level rather than by interpolating irregular graphs directly. It estimates class-specific graphons, interpolates them to form a mixed graphon, samples synthetic graphs from the mixed generator, and uses label mixup. The approach comes with theoretical guarantees that the synthetic graphs preserve the discriminative motifs of the original classes via homomorphism-density analysis and concentration bounds, and empirical evidence showing improved generalization and robustness of GNNs across diverse datasets and architectures.",
        "methodology": "The method leverages graphon theory to enable inter-class mixup for graphs. For each class, a step-function graphon W is estimated from the class's graphs (using established graphon estimation methods such as SAS, SBA, LG, MC, USVT). A mixed graphon WI = λWG + (1−λ)WH is formed and used to generate synthetic graphs by sampling G(K,WI) with iid uniform node labels and Bernoulli edges determined by the graphon. Node features of synthetic graphs are derived from graphon features; the synthetic graphs are assigned label vectors yI = λyG + (1−λ)yH. The approach is model-agnostic and can be plugged before training any GNN; theoretical results rely on the cut norm and homomorphism densities to show motif preservation, supported by concentration bounds for finite samples.",
        "experimental_setup": "Datasets included IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, and REDDIT-MULTI-5K/12K, with multiple graph classification benchmarks. Baselines include DropEdge, DropNode, Subgraph, and Manifold-Mixup, and pooling variants like TopKPool, DiffPool, MinCutPool, and GMT. GNN backbones span GCN and GIN, with broader experiments on GMT and molecular-property datasets (OGB: ogbg-molhiv, ogbg-molbbbp, ogbg-molbace). Training uses Adam (lr=0.01 with decay), batch size 128, and train/val/test split 7:1:2; evaluation over ten runs. Graphon estimation uses step-function approximations with K partitions; λ is tuned (e.g., 0.1–0.2) and synthetic augmentation is about 20% of the training data. Additional analyses include robustness tests to label and topology corruption, ablations with deeper GNNs, and visualization of graphons and generated graphs to illustrate motif preservation.",
        "limitations": "Limitations include dependence on accurate graphon estimation and step-function approximation, which require choosing an appropriate K and estimation method. The approach incurs computational overhead for graphon estimation and sampling. It assumes graphs within a class share a single generator (graphon), which may be violated in highly multi-modal distributions. Sensitivity to hyperparameters λ, augmentation ratio α, and graph size K, and potential scalability issues on very large graphs or highly heterogeneous datasets.",
        "future_research_directions": "Future work could extend G-Mixup to multi-class mixup and explore more graphon estimation methods (including adaptive or per-class K). Investigations could integrate G-Mixup with graph contrastive learning or other self-supervised signals, apply to node-level tasks, dynamic or heterogeneous graphs, and real-world large-scale datasets. Additional theoretical work could provide deeper guarantees on motif preservation, robustness analyses, and optimal strategies for choosing λ and K across domains.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning",
      "full_text": "Published as a conference paper at ICLR 2021 FEDMIX: A PPROXIMATION OF MIXUP UNDER MEAN AUGMENTED FEDERATED LEARNING Tehrim Yoon & Sumin Shin & Sung Ju Hwang & Eunho Yang Korea Advanced Institute of Science and Technology (KAIST) Daejeon, South Korea {tryoon93,sym807,sjhwang82,eunhoy}@kaist.ac.kr ABSTRACT Federated learning (FL) allows edge devices to collectively learn a model with- out directly sharing data within each device, thus preserving privacy and elim- inating the need to store data globally. While there are promising results under the assumption of independent and identically distributed (iid) local data, cur- rent state-of-the-art algorithms suffer from performance degradation as the het- erogeneity of local data across clients increases. To resolve this issue, we propose a simple framework, Mean Augmented Federated Learning (MAFL), where clients send and receive averaged local data, subject to the privacy requirements of target applications. Under our framework, we propose a new augmentation algorithm, named FedMix, which is inspired by a phenomenal yet simple data augmenta- tion method, Mixup, but does not require local raw data to be directly shared among devices. Our method shows greatly improved performance in the standard benchmark datasets of FL, under highly non-iid federated settings, compared to conventional algorithms. 1 I NTRODUCTION As we enter the era of edge computing, more data is being collected directly from edge devices such as mobile phones, vehicles, facilities, and so on. By decoupling the ability to learn from the delicate process of merging sensitive personal data, Federated learning (FL) proposes a paradigm that allows a global neural network to learn to be trained collaboratively from individual clients without directly accessing the local data of other clients, thus preserving the privacy of each client (Kone ˇcn´y et al., 2016; McMahan et al., 2017). Federated learning lets clients do most of the computation using its local data, with the global server only aggregating and updating the model parameters based on those sent by clients. One of the standard and most widely used algorithm for federated learning is FedAvg (McMahan et al., 2017), which simply averages model parameters trained by each client in an element-wise manner, weighted proportionately by the size of data used by clients. FedProx (Li et al., 2020b) is a variant of FedAvg that adds a proximal term to the objective function of clients, improving statistical stability of the training process. While several other methods have been proposed until recently (Mohri et al., 2019; Yurochkin et al., 2019; Wang et al., 2020), they all build on the idea that updated model parameters from clients are averaged in certain manners. Although conceptually it provides an ideal learning environment for edge devices, the federated learning still has some practical challenges that prevent the widespread application of it (Li et al., 2020a; Kairouz et al., 2019). Among such challenges, the one that we are interested in this paper is the heterogeneity of the data, as data is distributed non-iid across clients in many real-world settings; in other words, each local client data is not fairly drawn from identical underlying distribution. Since each client will learn from different data distributions, it becomes harder for the model to be trained efﬁciently, as reported in (McMahan et al., 2017). While theoretical evidence on the convergence of FedAvg with non-iid case has recently been shown in (Li et al., 2020c), efﬁcient algorithms suitable for this setting have not yet been developed or systematically examined despite some efforts (Zhao et al., 2018; Hsieh et al., 2020). 1 arXiv:2107.00233v1  [cs.LG]  1 Jul 2021Published as a conference paper at ICLR 2021 (a) Global Mixup  (b) Local Mixup  (c) NaiveMix  (d) FedMix Figure 1: Brief comparisons of Mixup strategies in FL and MAFL. (a) Global Mixup: Raw data is exchanged and directly used for Mixup between local and received data, which violates privacy. (b) Local Mixup: Mixup is only applied within client’s local data. (c) NaiveMix: Under MAFL, Mixup is performed between local data and received averaged data. (d) FedMix: Under MAFL, our novel algorithm approximates Global Mixup using input derivatives and averaged data. In addition to non-iid problem, another important issue is that updating model parameters indi- vidually trained by each client is very costly and becomes even heavier as the model complexity increases. Some existing works Smith et al. (2016); Sattler et al. (2019) target this issue to decrease the amount of communications while maintaining the performance of FedAvg. A more practical approach to reduce communication cost is to selectively update individual models at each round, rather than having all clients participate in parameter updates. This partial participation of clients per round hardly affects test performance in ideal iid settings but it can exacerbate the heterogeneity of weight updates across clients and as a result, the issue of non-iid (McMahan et al., 2017). In order to mitigate the heterogeneity across clients while protecting privacy, we provide a novel yet simple framework, mean augmented federated learning (MAFL), in which each client exchanges the updated model parameters as well as its mashed (or averaged) data. MAFL framework allows the trade-off between the amount of meaningful information exchanged and the privacy across clients, depending on several factors such as the number of data instances used in computing the average. We ﬁrst introduce a naive approach in our framework that simply applies Mixup (Zhang et al., 2018) between local data and averaged external data from other clients to reduce a myopic bias. Here, we go further in our framework and ask the following seemingly impossible question: can only averaged data in our framework that has lost most of the discriminative information, bring the similar effect as a global Mixup in which clients directly access others’ private data without considering privacy issues? Toward this, we introduce our second and more important approach in our framework, termed Federated Mixup (FedMix), that simply approximates the loss function of global Mixup via Taylor expansion (it turns out that such approximation only involves the averaged data from other clients!). Figure 1 brieﬂy describes the concept of our methods. We validate our method on standard benchmark datasets for federated learning, and show its effec- tiveness against the standard federated learning methods especially for non-iid settings. In particular, we claim that FedMix shows better performance and smaller drop in accuracy with more hetero- geneity or fewer clients update per communication round, further increasing difﬁculty of federated learning. Our contribution is threefold: • We propose a simple framework for federated learning that averages and exchanges each local data. Even naive approach in this framework performing Mixup with other clients’ mashed data shows performance improvement over existing baselines on several settings. • We further develop a novel approximation for insecure global Mixup accessing other clients’ local data, and ﬁnd out that Taylor expansion of global Mixup only involves the averaged data from other clients. Based on this observation, we propose FedMix in our framework approximating global Mixup without accessing others’ raw data. • We validate FedMix on several FL benchmark datasets especially focusing on non-iid data settings where our method signiﬁcantly outperforms existing baselines while still preserv- ing privacy with minimal increases in communication cost. 2Published as a conference paper at ICLR 2021 2 R ELATED WORK Federated learning Federated learning was ﬁrst proposed in Kone ˇcn´y et al. (2016) where the prevalent asynchronous SGD (Dean et al., 2012) is used to update a global model in a distributed fashion. A pioneering work in this ﬁeld proposed the currently most widely used algorithm, Fe- dAvg (McMahan et al., 2017), which is also the ﬁrst synchronous algorithm dedicated to federated setting. Shortly after, Li et al. (2020b) proposed a variant of FedAvg, named FedProx, where the authors claimed to overcome statistical heterogeneity and increase stability in federated learning. Recent studies attempt to expand federated learning with the aim of providing learning in more di- verse and practical environments such as multi-task learning (Smith et al., 2017), generative models (Augenstein et al., 2020), continual learning (Yoon et al., 2020), semi-supervised learning (Jeong et al., 2020), and data with noisy labels (Tuor et al., 2020). Our paper focuses on general federated settings, but it could be considered in such various situations. However, these algorithms may obtain suboptimal performance when clients participating in FL have non-iid (Zhao et al., 2018; Hsieh et al., 2020) distributions. While the convergence of FedAvg on such settings was initially shown by experiments in McMahan et al. (2017) and later proved in Li et al. (2020c), it does not guarantee performance as good as it would have been for iid setting. Existing algorithms that pointed out this issue have major limitations, such as privacy violation by partial global sharing of local data (Zhao et al., 2018) or no indication of improvement over baseline algorithms such as FedAvg (Hsieh et al., 2020). Our method aims to improve performance particularly on these non-iid situations, without compromising privacy. Mixup Mixup (Zhang et al., 2018) is a popular data augmentation technique that generates addi- tional data by linear interpolation between actual data instances. Mixup has been usually applied to image classiﬁcation tasks and shown to improve test accuracy on various datasets such as CIFAR10 and ImageNet-2012 (Russakovsky et al., 2015), and, on popular architectures such as ResNet (He et al., 2016) and ResNeXt (Xie et al., 2017), for various model complexity. It is also reported in Zhang et al. (2018) that Mixup helps with stability, adversarial robustness (Zhang et al., 2018), calibration, and predictive certainty (Thulasidasan et al., 2019). Mixup is expanding from various angles due to its simplicity and popularity. First, beyond image classiﬁcation tasks, its effectiveness has been proven in various domains such as image segmentation (Eaton-Rosen et al., 2020), speech recognition (Warden, 2018), and natural language processing (Guo et al., 2019). Also, several ex- tensions such as Manifold Mixup (Verma et al., 2018), which performs Mixup in latent space, or CutMix (Yun et al., 2019), which replaces speciﬁc regions with others patches, have been proposed. In most of the previous studies on federated learning, Mixup was partially (or locally) used as a gen- eral data augmentation technique. Some recent studies (Oh et al., 2020; Shin et al., 2020) proposed to send blended data to server using Mixup, but they require sending locally- and linearly-mixed (mostly from two instances) data to server at every round, therefore being susceptible to privacy issues with huge communication costs. Our work properly modiﬁes Mixup under the restrictions of federated learning and mitigates the major challenges of federated learning such as non-iid clients. 3 M EAN AUGMENTED FEDERATED LEARNING (MAFL) AND FEDMIX We now provide our framework exchanging averaged data for federated learning and main method approximating insecure global Mixup under our framework, after brieﬂy introducing the setup. 3.1 S ETUP AND BACKGROUND Federated learning and FedAvg Federated Averaging (FedAvg) (McMahan et al., 2017) has been the most popular algorithmic framework for federated learning. For every communication round t = 0 ,...,T −1, a client k ∈1,...,N selected for local training sends back its model wk t (or only difference to reduce communication cost) to a global server. For every round, K number of clients are selected to locally update and send model parameters. The server simply averages parameters received, so that the global model wt after t rounds of communications be- comes wt = ∑n k=1 pkwk t where pk is the importance of client k based on the relative number of data in k among all selected clients at t. The updated global model is sent back to clients for the next round, which undergoes the following E local updates via stochastic gradient descent (SGD): 3Published as a conference paper at ICLR 2021 wk t+1,i+1 ←wk t+1,i −ηt+1∇ℓ(f(xk i; wt+1,i),yk i) for i= 0,1,...,E −1, batch size B, and local learning rate η. Here, ℓis the loss function for learning and f(x; wt) is the model output for input x given model weight wt. Mixup Mixup (Zhang et al., 2018) is a simple data augmentation technique using a linear inter- polation between two input-label pairs (xi,yi) and (xj,yj) to augment ˜x = λxi + (1 −λ)xj and ˜y = λyi + (1 −λ)yj. The variable λ ∈[0,1] is a hyperparameter that is chosen from the beta distribution for each training step. 3.2 MAFL: M EAN AUGMENTED FEDERATED LEARNING The most obvious and powerful way for local models to receive information about data from other clients is to simply receive raw individual data. However, under typical federated setting, each client does not have direct access to individual external data due to privacy constraints, leading to overall performance degradation. We propose a federated learning framework that relaxes the limitation of accessing others’ raw data and allows a more granular level of privacy depending on applications. In our new framework, termed mean augmented federated learning (MAFL), clients not only exchange model parameters but also its mashed (or averaged) data. In MAFL, only the part that exchanges averaged data of each client has been added to the standard FL paradigm (Algorithm 1). Here, the number of data instances used in computing the average, Mk, controls the key features of MAFL such as privacy and communication costs. LowerMk value results in more relevant information passed over, but only in cost of less secure privacy and larger communication cost. In one extreme of Mk = 1, raw data is thoroughly exchanged and privacy is not protected at all, which is clearly inappropriate for FL. But, in the other extreme, all data of each client is averaged to ensure a considerable degree of privacy. In addition, it also has an advantage on communication cost; each client sends a set of nk/Mk averaged data where nk is local data size of client k. The remaining question is whether it is possible to improve performance even when exchanging information that is averaged from all local data and loses discriminative characteristics. The most naive way we can consider in our MAFL framework is to directly use the mashed data from other clients, just like regular local data. However, since mashed data has a lot less usable Algorithm 1: Mean Augmented Federated Learning (MAFL) Input: Dk = {Xk,Yk} for k= 1,...,N Mk: number of data instances used for computing average ¯x,¯y Initialize w0 for global server for t= 0,...,T −1 do for client kwith updated local data do Split local data into Mk sized batches Compute ¯x,¯y for each batch Send all ¯x,¯y to server end St ←Kclients selected at random Send wt to clients k∈St if updated then Aggregate all ¯x,¯y to Xg,Yg Send Xg,Yg to clients k∈St end for k∈St do wk t+1 ←LocalUpdate(k,wt; Xg,Yg) end wt+1 ← 1 K ∑ k∈St pkwk t+1 end Algorithm 2: FedMix LocalUpdate(k,wt; Xg,Yg) under MAFL (Algorithm 1): w ←wt for e= 0,...,E −1 do Split Dk into batches of size B for batch(X,Y ) do Select an entry xg,yg from Xg,Yg ℓ1 = (1 −λ)ℓ ( f((1 −λ)X; w),Y ) ℓ2 = λℓ ( f((1 −λ)X; w),yg ) ℓ3 = λ∂ℓ1 ∂x ·xg (derivative calculated at x = (1 −λ)xi and y= yi for each of xi,yi in X,Y ) ℓ= ℓ1 + ℓ2 + ℓ3 w ←w −ηt+1∇ℓ end end return w 4Published as a conference paper at ICLR 2021 information than local data, we can think of a method of mixing it with local data: ℓNaiveMix = (1 −λ)ℓ ( f ( (1 −λ)xi + λ¯xj ) ,yi ) + λℓ ( f ( (1 −λ)xi + λ¯xj ) ,¯yj ) (1) where (xi,yi) is an entry from local data and (¯xj, ¯yj) corresponds to means of (inputs,labels) from other client j. Note that Eq. (1) can be understood as the generalization of the loss of directly using the mashed data mentioned above in the sense that such loss can be achieved if λin Eq. (1) is set deterministically to 0 and 1. In the experimental section, we conﬁrm the effectiveness of MAFL using ℓNaiveMix. However, in the next subsection, we will show how to achieve better performance by approximating the global Mixup in a more systematical way in our MAFL framework. 3.3 F EDMIX: APPROXIMATING GLOBAL MIXUP VIA INPUT DERIVATIVE We now provide our main approach in the MAFL framework that aims to approximate the effect of global Mixup only using averaged data from other clients. Consider some client iwith its local data (xi,yi). It is not allowed in federated learning, but let us assume that clientihas access to client j’s local data (xj,yj). Then, client iwould leverage (xj,yj) to improve the performance of its local model especially in non-iid settings by augmenting additional data via Mixup: ˜x = (1 −λ)xi + λxj and ˜y= (1 −λ)yi + λyj. (2) If Mixup rate λ is 1, (xj,yj) from client j is again directly used like a regular local data, and it would be much more efﬁcient than indirect update of local models through the server. The essence of our method is to approximate the loss function ℓ ( f(˜x),˜y ) for the augmented data from Eq. (2), with Taylor expansion for the ﬁrst argument x. Speciﬁcally, we derive the following proposition: Proposition 1 Consider the loss function of the global Mixup modulo the privacy issues, ℓGlobalMixup ( f(˜x),˜y ) = ℓ ( f ( (1 −λ)xi + λxj ) ,(1 −λ)yi + λyj ) (3) for cross-entropy lossℓ1. Suppose that Eq. (3) is approximated by applying Taylor series around the place where λ≪1. Then, if we ignore the second order term (i.e., O(λ2)), we obtain the following approximated loss: (1 −λ)ℓ ( f ( (1 −λ)xi ) ,yi ) + λℓ ( f ( (1 −λ)xi ) ,yj ) + λ∂ℓ ∂x ·xj (4) where the derivative ∂ℓ ∂x is evaluated at x = (1 −λ)xi and y= yi. While Eq. (4) still involvesxj and yj, invading the privacy of clientj, the core value of Proposition 1 gets clearer when mixing up multiple data instances from other clients. Note that the vanilla Mixup is not mixing one speciﬁc instance with other data, but performing augmentations among several random selected data. In a non-iid FL environment, we can also expect that the effect will be greater as we create Mixup data by accessing as much private data as possible from other clients. From this point of view, let us assume that client ihas received a set of M private instances, J, from client j. Then, the global Mixup loss in Eq. (3) is 1 |J| ∑ j∈J ℓ ( f ( (1 −λ)xi + λxj ) ,(1 −λ)yi + λyj ) , and the approximated FedMix loss in Proposition 1 becomes ℓFedMix = 1 |J| ∑ j∈J (1 −λ)ℓ ( f ( (1 −λ)xi ) ,yi ) + λℓ ( f ( (1 −λ)xi ) ,yj ) + λ∂ℓ ∂x ·xj = (1 −λ)ℓ ( f ( (1 −λ)xi ) ,yi ) + λℓ ( f ( (1 −λ)xi ) ,¯yj ) + λ∂ℓ ∂x ·¯xj (5) where we utilize the linearity of Equation 4 in terms ofxj and yj, and ¯xj and ¯yj correspond to mean of M inputs and labels in J, respectively. The algorithmic details are provided in the appendix due to the space constraint (see Algorithm 2 in Appendix A). 1Throughout the paper, we implicitly assume the classiﬁcation tasks. For regression tasks, we can consider the squared loss function, and the proposition still holds. 5Published as a conference paper at ICLR 2021 3.4 P RIVACY ISSUES AND ADDITIONAL COSTS OF MAFL Privacy issues of MAFL MAFL requires exchanging averaged data by construction. Even though MAFL exchanges only the limited information allowed by the application, it may causes new types of privacy issues. The potential privacy risk of FL or MAFL is beyond the main scope of our study, but in this section, we brieﬂy discuss some basic privacy issues of MAFL and potential solutions. • There is possibility that local data distribution can be inferred relatively easily from aver- aged data. This issue simply arises as Mk is not large enough, so that individual data could be inferred from the averaged data easily. On the other hand, if nk is not big enough, each entry in Xg,Yg could reveal too much about the whole local distribution of the client it came from. • It could be easy to infer ownership of each entry in Xg,Yg, if it contains client-id speciﬁc information. If clients could identify what other client each entry came from, information about local data of that client could be inferred. • Additional concerns involve identiﬁcation of data by detecting change in exchanged av- eraged data, in case of continual learning, which involves local data change across time. This issue is exacerbated as there is update of averaged data for every minute change on local data, which makes the client receiving Xg,Yg easier to infer the changed portion. One simple suggestion to alleviate this issue would be to only updateXg,Yg when there is enough change in local data across enough number of clients, so that such changes are not easily exploitable. • As a way to strengthen privacy protection under MAFL (and possibly to help with issues mentioned above), we in the server can average within entries of Xg,Yg. If this additional average is done across every random mentries at the server, it would effectively provide averaged data across all local data of mclients, but would result in an m-fold decrease in the number of averaged data. This variant is considered in Appendix J. • In case where the global server is not credential, the averaged data itself should ensure privacy as it is sent to the server. A most obvious concern comes from when Mk is not large enough, so that each entry of Mk reveals more of information of each individual input. Simply using a sufﬁciently large value of Mk can alleviate this issue, although this might result in worse performance. • However, for clients whose nk is quite small, there is a limit for Mk to be large enough. One way to alleviate this issue is to introduce a cut-off threshold for allowing clients to send averaged data to server. We report the results in Appendix H. Communication cost Since MAFL requires sending averaged input data between server and clients, additional communication costs are incurred. However, it turns out that this additional cost is very small compared to communication cost required for exchanging model parameters. This is mainly due to the fact that input dimension is typically much smaller than number of model param- eters. Speciﬁcally, for input dimension di, exchange of averaged data among N clients incurs 2Ndi cost (factor of 2 for server receiving and sending the values). Meanwhile, the cost for exchange of model parameters is 2Npm where pm is number of model parameters. Under typical circumstances, averaged data is only exchanged at the beginning of the ﬁrst communication round, while model parameters have to be exchanged every round. Thus the ratio between the two costs afterT commu- nication rounds is di/(Tpm). Since di ≪pm in general, we consider extra communication burden to be negligible (even in the worst case where we update averaged data every round, the ratio is still di/(pm). FedMix also requires calculation of input derivative term in its loss function, so potentially extra memory is required. We further provide additional computation costs of MAFL in Appendix G. 4 E XPERIMENTS We test our result on various benchmark datasets with NaiveMix (direct mixup between local data and averaged data) and FedMix, then compare the results with FedAvg (McMahan et al., 2017) and FedProx (Li et al., 2020b), as well as other baseline Mixup scenarios. We create a highly non-iid environment to show our methods excel in such situations. 6Published as a conference paper at ICLR 2021 (a) FEMNIST  (b) CIFAR10  (c) CIFAR100 Figure 2: Learning curves for various algorithms on benchmark datasets. Learning curves correspond to results in Table 1. (For simplicity, we only show key algorithms to compare.) Table 1: Test accuracy after (target rounds) and number of rounds to reach (target test accu- racy) on various datasets. Algorithms in conjunction with FedProx are compared separately (bot- tom). MAFL-based algorithms are marked in bold. Algorithm FEMNIST CIFAR10 CIFAR100test acc. (200) rounds (80%) test acc. (500) rounds (70%) test acc. (500) rounds (40%)Global Mixup 88.2 8 88.2 85 61.4 54FedAvg 85.3 26 73.8 283 50.4 101LocalMix 82.8 28 73.0 267 54.8 91NaiveMix85.9 23 77.4 198 53.8 85FedMix 86.5 18 81.2 162 56.7 34FedProx 84.6 29 77.3 266 51.2 79FedProx + LocalMix 84.1 39 74.1 314 54.0 90FedProx +NaiveMix85.7 37 76.7 230 53.1 74FedProx +FedMix 86.032 78.9 223 54.5 63 4.1 E XPERIMENTAL SETUP Dataset We implement the typical federated setting where clients have their own local data and one centralized server that receives and sends information from/to the clients. We utilize a large number of clients and only utilize partial set of clients chosen each round to locally update. We used three popular image classiﬁcation benchmark datasets: FEMNIST (Caldas et al., 2019), CIFAR10, and CIFAR100, as well as a popular natural language processing benchmark dataset, Shakespeare. See Appendix B for more details about dataset, models, and hyperparameters used. We introduce data size heterogeneity for FEMNIST dataset: each client has different size of local data, each from a unique writer. Meanwhile, we introduce label distribution heterogeneity for CIFAR datasets, with clients having data with only a limited number of classes. Algorithms We study the performance of FedMix and NaiveMix and compare with FedAvg and FedProx. We also compare our method against FedAvg with Mixup within local data (labeled Lo- calMix; see Figure 1(b)), to show whether Mixup within local data is sufﬁcient to allow the model to perform well on external data. To show the effectiveness of FedMix, we also compare our method to the case where we perform direct Mixup with external data (and thus violating privacy, labeled Global Mixup; see Figure 1(a)). 4.2 P ERFORMANCE OF FEDMIX AND NAIVE MIX ON NON-IID F EDERATED SETTINGS We compare the learning curves of each method under the same federated settings, in terms of num- ber of communication rounds conducted. Comparing MAFL-based algorithms, NaiveMix shows slight performance increases than FedAvg, FedProx, and Localmix, while FedMix outperforms and shows faster convergence than all of FedAvg, FedProx, Localmix and NaiveMix for all datasets tested as in Figure 2. While NaiveMix and FedMix is already superior to FedProx, they are parallel to FedProx modiﬁ- cation and can be applied in conjunction with FedProx. We compare performances across FedProx 7Published as a conference paper at ICLR 2021 Table 2: Test accuracy after 50 rounds on Shakespeare dataset. Algorithm Global Mixup FedAvg FedProx LocalMix NaiveMix FedMix Test Acc. (%) 54.4 54.7 54.4 53.7 56.9 56.9 Table 3: Test accuracy on CIFAR10, under varying number of clients (N). Number of samples per client is kept constant. # of Clients( N ) 20 40 60 Global Mixup 86.3 89.2 88.2 FedAvg 65.8 73.4 73.8 LocalMix 46.9 71.4 73.0 NaiveMix 62.2 75.1 77.4 FedMix 68.5 76.4 81.2 Table 4: Test accuracy on CIFAR10, under varying number of local data per client. Number of clients (N) is kept constant. Number of data is indicated in percentage of the case where all 50,000 data are used. Local data (%) 20 50 100 Global Mixup 71.4 86.1 88.2 FedAvg 61.8 74.7 73.8 LocalMix 43.7 60.3 73.0 NaiveMix 51.5 69.6 77.4 FedMix 65.2 77.8 81.2 variants of various Mixup algorithms in Table 1. FedMix outperforms vanilla FedProx for various datasets, although they do fall short of default version of FedMix used for the main experiment. To conﬁrm whether received information is properly incorporated, we compare FedMix with possi- ble Mixup scenarios under MAFL. We show the results in Appendix D. While Mixup is usually performed for image classiﬁcation tasks, it could be applied for language models. For language datasets, since Mixup cannot be performed on input, we perform Mixup on embeddings (for a detailed explanation of Mixup between hidden states, see Appendix E). When tested on Shakespeare dataset, FedMix and NaiveMix both show better performance than baseline algorithms (Table 2). Note that for this task, LocalMix has the lowest performance, and global Mixup does not result in the superior performance above federated algorithms as expected. We think Mixup does not provide performance boost for this speciﬁc task, but claim that MAFL algorithms still result in better performance compared to FedAvg. We also claim that FedMix is superior compared to other methods under various settings, in terms of varying number of clients (N) and varying number of local data per clients. We observe superior performance of FedMix compared to other algorithms for all settings (see Tables 3 and 4). We also vary the number of local epochs ( E) between global updates, and still observe that FedMix outperforms other methods (see Appendix F). FedMix compared to global Mixup with ﬁxed mixup ratio Since FedMix approximates loss function of global Mixup for ﬁxed value of λ≪1, we can evaluate the efﬁciency of approximation by comparing between FedMix and a global Mixup scenario with ﬁxed λ value. Table 5 shows varying performance between global Mixup and FedMix under various values of λ. As λincreases, Mixup data reﬂects more of the features of external data, resulting in better performance in case of global Mixup. However, this also results in our approximation being much less accurate, and we indeed observe performance of FedMix decreasing instead. The result shows that the hyperparameter λshould be chosen to balance between better Mixup and better approximation. However, it seems that high λresults in signiﬁcant decrease in both methods, probably due to external data (which is out-of-distribution for local distribution) being overrepresented during local update. Table 5: Test accuracy on CIFAR10, under varying mixup ratioλ. λ 0.05 0.1 0.2 0.5 Global Mixup 79.4 80.4 81.1 63.6 FedMix 81.2 80.5 77.7 67.1 8Published as a conference paper at ICLR 2021 Mk 5 10 20 50 All FEMNISTNaiveMix 85.786.3 86.2 86.1 85.9FedMix 86.0 85.7 86.4 86.2 86.5 CIFAR10NaiveMix 79.6 77.9 79.1 77.1 77.4FedMix 81.4 79.9 80.4 79.5 81.2 Figure 3: Performance of MAFL-based algorithms for various Mk values (left), and samples of averaged images from EMNIST/CIFAR10 for variousMk values (right). Table 6: Test accuracy after 500 rounds on CIFAR10, under varying number of classes per client. ———class/client——— Algorithm 2 3 5 10 (iid) Global Mixup 88.2 90.7 90.9 91.4 FedAvg 73.8 84.2 86.8 89.3 Localmix 73 83.3 86.4 89.1 NaiveMix 77.4 84.5 87.7 89.4 FedMix 81.2 85.1 87.9 89.1 Table 7: Test accuracy after 500 rounds on CI- FAR10, under varying number of clients trained per communication round. ————— K/N ————— Algorithm 0.1 0.15 0.25 0.5 1.0 Global Mixup 89.3 89.7 88.2 91.2 90.7 FedAvg 63.3 73.2 73.8 76.3 83.1 Localmix 64.7 64.5 73 77.9 79.8 NaiveMix 73.6 74.7 77.4 81.4 83.5 FedMix 74.7 76.9 80.5 82.1 84.3 Effect of Mk to compute mean In our algorithm, we chose to calculateXg,Yg with all local data for each client. To observe a potential effect of Mk, we varied Mk used to compute the averaged data that is sent from other clients. Inevitably, reducingMk will result in Xg,Yg having much more rows, imposing additional computation burden and less preservation of privacy. In general, for both FEMNIST and CIFAR10, there is only small performance decline as privacy is enhanced, as can be seen in Figure 3. We show that using all local data to calculate each mean is sufﬁcient to both preserve privacy and still have good performance. Mixup between hidden states Manifold Mixup (Verma et al., 2018) was proposed to show im- provements over input Mixup (Zhang et al., 2018) in various image classiﬁcation tasks such as CIFAR10, CIFAR100, and SVHN. We discuss the possibilities and implications of applying Mixup between hidden states in Appendix E. In summary, we show that variants of using hidden states do not show meaningful advances over FedMix using input Mixup, suggesting that in general, it is relatively inefﬁcient since it imposes additional communication burden. Effect of non-iid-ness and client participation We claim that our method is efﬁcient when faced with non-iid federated settings. For example, our setting of CIFAR10 having only data from 2 classes per client is very non-iid, as in average a pair of clients share only roughly 20% of data distribution. We test settings for CIFAR10 where clients have data from greater number of classes, and while there is little difference for iid (10 class/client) setting, we observe that FedMix outperform other methods and suffer less from increased heterogeneity from highly non-iid settings (Table 6). In addition, we also observe less decline and better performance for MAFL-based algorithms, FedMix in particular, as we train less number of clients per round, reducing communication burden in cost of performance (Table 7). 5 C ONCLUSION We proposed MAFL, a novel framework, that exchangesaveraged local data, to gain relevant infor- mation while still ensuring privacy. Under the new framework, we ﬁrst suggested NaiveMix, which is a naive implementation of Mixup between local and received data. More interestingly, we pro- posed FedMix, which provides approximation of global Mixup only using averaged data. MAFL, and FedMix in particular, showed improved performance over existing algorithms in various bench- marks, particularly in non-iid environments where each client has data distributed heterogeneously. While our method is very effective and still preserving privacy, future work needs to be done to deal with various non-iid environments, desirably with better privacy and beyond image classiﬁcation tasks. 9Published as a conference paper at ICLR 2021 ACKNOWLEDGMENTS This work was supported by the National Research Foundation of Korea (NRF) grants (No.2018R1A5A1059921, No.2019R1C1C1009192) and Institute of Information & Communica- tions Technology Planning & Evaluation (IITP) grants (No.2017-0-01779, XAI, No.2019-0-01371, Development of brain-inspired AI with human-like intelligence, and No.2019-0-00075, Artiﬁcial Intelligence Graduate School Program(KAIST)) funded by the Korea government (MSIT). REFERENCES Sean Augenstein, H. Brendan McMahan, Daniel Ramage, Swaroop Ramaswamy, Peter Kairouz, Mingqing Chen, Rajiv Mathews, and Blaise Aguera y Arcas. Generative models for effective ml on private, decentralized datasets. International Conference on Learning Representations (ICLR), 2020. Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Koneˇcn´y, H. Brendan McMa- han, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. Inter- national Conference on Machine Learning (ICML) Workshop on Federated Learning for Data Privacy and Conﬁdentiality, 2019. Olivier Chapelle, Jason Weston, L ´eon Bottou, and Vladimir Vapnik. Vicinal risk minimization. Conference on Neural Information Processing Systems (NIPS), 2000. Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr ´e van Schaik. Emnist: an extension of mnist to handwritten letters. International Joint Conference on Neural Networks (IJCNN), 2017. Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V . Le, and Andrew Y . Ng. Large scale distributed deep networks. Conference on Neural Information Processing Systems (NIPS), 2012. Z. Eaton-Rosen, Felix J. S. Bragman, S ´ebastien Ourselin, and M. Cardoso. Improving data aug- mentation for medical image segmentation. Conference on Medical Imaging with Deep Learning (MIDL), 2020. Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit conﬁ- dence information and basic countermeasures.Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, 2015. Hongyu Guo, Yongyi Mao, and Richong Zhang. Augmenting data with mixup for sentence classiﬁ- cation: An empirical study. CoRR, abs/1905.08941, 2019. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. Conference on Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip B. Gibbons. The non-iid data quagmire of decentralized machine learning. International Conference on Machine Learning (ICML), 2020. Wonyong Jeong, Jaehong Yoon, Eunho Yang, and Sung Ju Hwang. Federated semi-supervised learning with inter-client consistency. International Workshop on Federated Learning for User Privacy and Data Conﬁdentiality in Conjunction with ICML 2020 (FL-ICML’20), 2020. Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur ´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri `a Gasc ´on, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Kone ˇcn´y, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancr `ede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer ¨Ozg¨ur, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebas- tian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram`er, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. ArXiv, abs/1912.04977, 2019. 10Published as a conference paper at ICLR 2021 Jakub Koneˇcn´y, H. Brendan McMahan, Felix X. Yu, Peter Richt´arik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efﬁciency. Conference on Neural Information Processing Systems (NIPS) Workshop on Private Multi-Party Machine Learning, 2016. Yann Lecun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998. Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. IEEE Signal Processing Magazine, 37:50–60, 2020a. Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Sys- tems (MLSys), 2020b. Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. International Conference on Learning Representations (ICLR), 2020c. H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag ¨uera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. International Con- ference on Artiﬁcial Intelligence and Statistics (AISTATS), 2017. H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. International Conference on Learning Representations (ICLR), 2018. Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. Interna- tional Conference on Machine Learning (ICML), 2019. Seungeun Oh, Jihong Park, Eunjeong Jeong, Hyesung Kim, Mehdi Bennis, and Seong-Lyun Kim. Mix2ﬂd: Downlink federated learning after uplink federated distillation with two-way mixup. IEEE Communication Letters, 2020. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision (IJCV) , 115 (3):211–252, 2015. Felix Sattler, Simon Wiedemann, Klaus-Robert M ¨uller, and Wojciech Samek. Robust and communication-efﬁcient federated learning from non-iid data. IEEE Transactions on Neural Net- works and Learning Systems, 2019. MyungJae Shin, Chihoon Hwang, Joongheon Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. Xor mixup: Privacy-preserving data augmentation for one-shot federated learning. Interna- tional Workshop on Federated Learning for User Privacy and Data Conﬁdentiality in Conjunction with ICML 2020 (FL-ICML’20), 2020. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations (ICLR), 2015. Virginia Smith, Simone Forte, Chenxin Ma, Martin Takac, Michael I. Jordan, and Martin Jaggi. Cocoa: A general framework for communication-efﬁcient distributed optimization. Journal of Machine Learning Research (JMLR), 2016. Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar. Federated multi-task learn- ing. Conference on Neural Information Processing Systems (NIPS), 2017. Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. On mixup training: Improved calibration and predictive uncertainty for deep neural networks. Conference on Neural Information Processing Systems (NIPS), 2019. Tiffany Tuor, Shiqiang Wang, Bong Jun Ko, Changchang Liu, and Kin K. Leung. Overcoming noisy and irrelevant data in federated learning. International Conference on Pattern Recognition (ICPR), 2020. 11Published as a conference paper at ICLR 2021 Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najaﬁ, Ioannis Mitliagkas, Aaron Courville, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. International Conference on Machine Learning (ICML), 2018. Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. International Conference on Learning Representa- tions (ICLR), 2020. Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. ArXiv, abs/1804.03209, 2018. Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated residual trans- formations for deep neural networks. Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung Ju Hwang. Federated con- tinual learning with adaptive parameter communication. ArXiv, abs/2003.03196, 2020. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classiﬁers with localizable features. International Conference on Computer Vision (ICCV), 2019. Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks.International Conference on Machine Learning (ICML), 2019. Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empiri- cal risk minimization. International Conference on Learning Representations (ICLR), 2018. Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018. 12Published as a conference paper at ICLR 2021 A A LGORITHMS We present a brief depiction of FedAvg in Algorithm 3. Algorithm 3: FedAvg Input: N,T,K,E,B,p k,Dk = {Xk,Yk},k = 1,...,N,η t,t = 0,...,T −1 Initialize w0 for global server for t= 0,...,T −1 do St ←Kclients selected at random Send wt to clients k∈St for k∈St do wk t+1 ←LocalUpdate(k,wt) end wt+1 ← 1 K ∑ k∈St pkwk t+1 end LocalUpdate(k,wt): w ←wt for e= 0,...,E −1 do Split Dk into batches of size B for batch(X,Y ) do w ←w −ηt+1∇ℓ(f(X; w),Y ) end end return w B E XPERIMENTAL DETAILS FEMNIST FEMNIST is EMNIST (Cohen et al., 2017), a handwritten MNIST dataset, organized into federated setting, as in Caldas et al. (2019). EMNIST is very similar to MNIST, but has several differences. It includes all 26 capital and small letters of alphabet as classes along with numbers, making it 62 classes in total to classify. Also, each image contains information of the writer of the letter. In a realistic non-iid setting, each client has local data consists of only one writer, which is about 200 to 300 samples per client in average, with differing number of samples. We useN = 100 clients and trained only K = 10 clients per communication round. We used LeNet-5 (Lecun et al., 1998) architecture for client training. LeNet is consisted of 2 conv layers followed by 2x2 maxpool layer then 3 fc layers. We used 5x5 conv layers with 6 and 16 channels. Following fc layers have exactly the same hidden dimension of original LeNet-5 model. CIFAR10 and CIFAR100 CIFAR10 and CIFAR100 are very popular and simple image classiﬁ- cation datasets for federated setting. Both contain 50,000 training data and 10,000 test data. We split the data into each client, N = 60 in case of CIFAR10 andN = 100 in case of CIFAR100. To create an artiﬁcial non-iid environment, we allocate data such that each client only has data from 2 (20 for CIFAR100) randomly chosen classes. We train only K = 15 clients per round for CIFAR10 and K = 10 for CIFAR100. No validation data was split and we used all training data for local training. We used modiﬁed version of VGG architecture (Simonyan & Zisserman, 2015). Modiﬁed VGGnet is consisted of 6 convolutional layers with 3 max pooling layers. 3x3 conv layers are stacked and 2x2 maxpool layer is stacked after every 2 conv layers. Conv layers have channel sizes of 32, 64, 128, 128, 256, 256. Then 3 fc layers are stacked with hidden dimension 512. We use Dropout layer three times with probability 0.1 after the second, third maxpool layers and before the last fc layer. We re- move all batch normalization layers since it is reported that they hurt federated learning performance (Hsieh et al., 2020). Shakespeare We use dataset from The Complete Works of William Shakespeare, which is a popu- lar dataset for next-character prediction task. We partition the dataset so that each client has conver- sations of one speaking role, as in Caldas et al. (2019), which naturally results in a heterogeneous setting, as in FEMNIST. We use N = K = 60 , each with different number of data (minimum is 200). Since input-level Mixup cannot be performed for discrete character labels, we performed Mixup on the embedding layer. Additional concerns for this variation is considered at Appendix E. We used 2-layer LSTM, both with hidden dimension of 256. The recurret network is followed by an embedding layer. The output of LSTM is passed to a fully-connected layer, with softmax output of one node per character. There are 84 characters used in the dataset. Local clients are trained by SGD optimizer with learning rate 0.01 and learning decay rate per round 0.999. We set local batch size as 10 for training. Speciﬁc hyperparameter setting for each dataset is 13Published as a conference paper at ICLR 2021 Table 8: Hyperparameter settings for each dataset. dataset FEMNIST CIFAR10 CIFAR100 Shakespeare local epochs (E) 10 2 10 2 local batch size 10 10 10 10 class per clients - 2 20 - fraction of Clients (K/N) 0.1 0.25 0.1 1 total dataset classes 62 10 100 84 λ(for NaiveMix) 0.2 0.1 0.1 0.1 λ(for FedMix) 0.2 0.05 0.1 0.1 µ(for FedProx) 0.1 0.1 0.01 0.001 explained in following Table 8. Throughout the experiment,Mk is ﬁxed to local client’s dataset size. Changes in these parameters are indicated, if made, are stated for all experiments. Note that we use a ﬁxed small value of λfor MAFL-based algorithms to show superior performance. C P ROOF OF PROPOSITION 1 We demonstrate mathematical proof of Proposition 1 for FedMix. Starting from Eq. (3), since the loss function is linear in yfor cross-entropy loss ℓ2, we have ℓ ( f(˜x),˜y ) = (1 −λ)ℓ ( f ( (1 −λ)xi + λxj ) ,yi ) + λℓ ( f ( (1 −λ)xi + λxj ) ,yj ) . (6) Unlike the original paper, if we assume λ ≪1, we can treat this loss as objective loss function for vicinal risk minimization (VRM). Under this assumption, each term in Eq. (6) can be approximated by independent Taylor expansion on the ﬁrst and second argument of ℓ, so that we have (1 −λ)ℓ ( f ( (1 −λ)xi ) ,yi ) + (1 −λ) ×∂ℓ ∂x ⏐⏐⏐⏐ (1−λ)xi,yi ·(λxj) +λℓ ( f ( (1 −λ)xi ) ,yj ) + λ×∂ℓ ∂x ⏐⏐⏐⏐ (1−λ)xi,yj ·(λxj). (7) Since λ≪1, we can ignore the last term in the second row of Eq. (7), which isO(λ2). We simplify this equation and switch the second term in the ﬁrst row and the ﬁrst term in the second row, to ﬁnally obtain ℓ ( f(˜x),˜y ) ≈(1 −λ)ℓ ( f ( (1 −λ)xi ) ,yi ) + λℓ ( f ( (1 −λ)xi ) ,yj ) + λ∂ℓ ∂x ·xj. (8) The derivative ∂ℓ ∂x is calculated at x = (1 −λ)xi and y = yi. The coefﬁcient of the last term is changed to λfrom λ(1 −λ) since we are ignoring O(λ2) terms. D C OMPARISON OF FEDMIX WITH BASELINE MIXUP SCENARIOS Since our MAFL-based algorithms could be considered as VRM, it could be considered as a data augmentation (Chapelle et al., 2000). Thus, it is important to conﬁrm that increased performance from MAFL not only comes from data augmentation but also from relevant information received from other clients. To check whether this is true, we compare FedMix with algorithms where we use either randomly generated noises as averaged data for Mixup (labeled Mixup w/ random noise) and where we use only locally averaged data for Mixup (labeled Mixup w/ local means). In Table 9, we observe that if averaged data for MAFL is substituted for randomly generated noise or locally generated images, it does not show the level of performance FedMix is able to show. Thus, we claim that FedMix properly incorporates relevant information received. 2We implicitly assume the classiﬁcation tasks. For regression tasks, we can consider the squared loss func- tion and use the equivalent loss that is linear in y 14Published as a conference paper at ICLR 2021 Figure 4: Results for variants of Mixup algorithms for Mixup between hidden states. (a) Learning curves for various algorithms with hidden representation Mixup after k = 2 layers. (b) Learning curves for FedMix when Mixup is applied after different numbers of layers. (a)  (b) E V ARIANT OF FEDMIX AND NAIVE MIX WITH MIXUP BETWEEN HIDDEN STATES While input Mixup methods promise signiﬁcant enhancements, one can expect similar performance from hidden state Mixup, originally proposed by Verma et al. (2018). The authors of this work suggest that Manifold Mixup demonstrates similar, if not greater, advantage in terms of performance and adversarial robustness. We can think of variants of FedMix and NaiveMix that implement hidden state Mixup, and test if this variant outperforms vanilla methods based on input Mixup. Although the original paper proposed randomizing the layerkjust before hidden states that undergo Mixup for each batch, we propose setting this layer k constant. This is to reduce communication cost signiﬁcantly, since selecting randomized layer for Mixup will require other clients having to send multiple hidden states (which usually have large dimensions), further imposing communication burden. Another change is that while original Manifold Mixup (Verma et al., 2018) suggests backpropagat- ing the entire computational graph through the whole network, including the encoder (part of model projecting input to designated hidden representation) and the decoder (rest of the model projecting hidden representation to output). Such thing is impossible to do in typical federated setting, since the computational graph to calculate hidden representation of local data and the graph to calculate hidden representation of other clients’ data is separated, and they cannot be updated simultaneously through local update (doing so requires communicating encoder weights across clients every local update, which is highly inefﬁcient in terms of communication). Thus, during Mixup between hid- den representations, only the decoder weights can be updated, since only updating encoder of the selected local client will desynchronize encoder weight values for calculating hidden states of local data from those for calculating hidden states of other clients’ data every local update, so that the model does not learn properly. Table 9: Test accuracy after (target rounds) and number of rounds to reach (target test accuracy) on various datasets. We compare FedMix with baseline Mixup algorithms. Algorithm FEMNIST CIFAR10 CIFAR100test acc. (200) rounds (80%) test acc. (500) rounds (70%) test acc. (500) rounds (40%)NaiveMix85.9 23 77.4 198 53.8 85Mixup w/ random noise 86.1 23 77.9 201 51.2 105Mixup w/ local means 85.5 21 73.5 233 51.0 87FedMix 86.5 18 81.2 162 56.7 34 15Published as a conference paper at ICLR 2021 Table 10: Test accuracy after 500 rounds on CI- FAR10, under varying local epochs (E). # of Local Epochs ( E) 1 2 5 10 FedAvg 74.4 73.8 80.7 78.9 LocalMix 63.7 73.0 74.7 80.0 NaiveMix 72.0 77.4 81.0 82.6 FedMix 75.8 81.2 83.0 82.5 To compensate for this downside, we propose performing vanilla SGD updates without Mixup after Manifold Mixup SGD (which updates weights only in decoder). The vanilla updates will have both local encoder and decoder weights to be updated, thus driving the model to have better hidden representations for Mixup. However, this difference not only imposes additional computation cost, but also does not guarantee that it will show better performance compared to input Mixup methods. While utilizing hidden representations from other clients sound like a safe idea, it does not ensure data privacy, primarily because the updating client has knowledge of the exact encoder weight val- ues used to calculate hidden states received, and based on our modiﬁcation, the received hidden states are treated as constants during Mixup. Model inversion attacks (Fredrikson et al., 2015) have been suggested to recover input images from hidden states or outputs, with access to weight val- ues. Thus, direct Mixup between hidden states does not guarantee data privacy. Variant of FedMix and NaiveMix can be applied during decoder training phase, so that privacy is ensured while we successfully approximate Mixup. The performance of proposed algorithms is shown in Figure 4 on CIFAR10 (same settings with main experiment for dataset, model, and training is used; see Appendix B). Comparison between methods in Figure 4(a) shows that while variants of FedMix and NaiveMix show improved performance over existing methods, they still do not outperform our method based on input Mixup (compare with dotted line). Meanwhile, comparison between using different layers for Mixup is shown in Figure 4(b). It is shown that k= 4 has fastest learning curve but converges similarly to case ofk= 2, both being slightly outperformed by case of input Mixup. Considering additional computation burden required to communicate hidden states (which often have larger dimensions than raw input) and necessity to communicate the hidden states every com- munication round (since hidden representations change with encoder weights), we propose that Fed- Mix using input Mixup is superior, and use this method for our main analyses. F E FFECT OF LOCAL EPOCHS Previous works (McMahan et al., 2017; Caldas et al., 2019) show that number of local epochs, E, affects federated learning performance. We tested the effect of E on CIFAR10. In general, we showed that test performance increases as Eincreases. In addition, we observed that under various values of E, FedMix shows the best performance compared to other algorithms (see Table 10), being a close second after NaiveMix forE = 10. MAFL-based algorithms outperform existing algorithms for all values of Etested. G A DDITIONAL COMPUTATION COST INCURRED BY MAFL For FedMix, additional computation and memory are required on edge devices during model training for each communication round, since ℓFedMix requires additional terms, including gradient by input, ∂ℓ ∂x, compared to vanilla FedAvg. We claim that FedMix does not result in an additional computation burden. Speciﬁcally, we trained FedMix on CIFAR10 with the same settings as the main experiment to 70% accuracy in 1.94 hours; FedAvg takes 1.95 hours. FedMix spends a comparable amount of time to reach a similar level of performance of FedAvg. While in the memory aspect, FedMix requires about twice more GPU memory allocation compared to FedAvg, this phenomenon is also observed on LocalMix and NaiveMix. The extra memory burden comes from Mixup by enlarging the input dimension twice. For instance, FedAvg requires 46.00MB to allocate, LocalMix requires 94.00MB and 98.00MB for FedMix. Calculating gradient of the input derivative gives only negligi- 16Published as a conference paper at ICLR 2021 ble 2-3MB additional memory usage, which is reasonable concerning the substantial performance increase from LocalMix to FedMix. H I NTRODUCTION OF CUT-OFF THRESHOLD IN MAFL To better ensure privacy, a cut-off threshold that prevents clients with fewer data to send averaged data could be introduced. We performed this in FEMNIST, since for such procedure to be effective, heterogeneous size of local client data is necessary. We test with N = 300 clients, and introduce different threshold levels to test its efﬁciency. In addition, we also test with multipleλvalues, to see whether threshold level affects optimal value of λfor FedMix. We present the results in Table 11. While threshold does not hugely affect performance, we observe that a moderately small threshold level of 100 results in the best performance. We suggest that as the threshold level is heightened, there is less overﬁtting to clients with small size local data, but it also results in a decrease in the number of averaged data received by each client. We indeed ﬁnd an appropriate value of threshold that maximizes performance. In case where there are a different number of data per client, the sensitivity of λ could also be different compared to when all clients have the same number of data. Results in Table 13 show that there is little change in performance by change in λ, especially compared to Table 5. In addition, an inspection of the performance of a global model on individual test data of clients does not reveal any noticeable pattern by the size of local data (see Table 13). Table 11: Test accuracy on FedMix with introduc- tion of cut-off threshold, tested on a number of threshold level. Optimal value of λis also shown. ———–threshold———– λ 1 100 150 200 Test Acc. (%) 83.0 83.3 83.1 82.9 λoptimal 0.2 0.05 0.1 0.2 Table 12: Test accuracy on FEMNIST, N = 300 under various Mixup ratio λ. λ 0.05 0.1 0.2 Test Acc(%) 82.8 82.8 83.0 Table 13: Mean and standard variation of local test accuracy of FedMix on FEM- NIST, tested on clients with varying local data size, under varying λ. n k < 100 100 ≤ n k ≥ 199 n k > 199 λ mean std mean std mean std 0.05 85.8 15.1 77.6 14.8 85.4 9.3 0.1 81.1 17.4 76.4 14.1 86.2 9.5 0.2 83.9 16.2 77.6 14.8 85.8 10.0 I MAFL IN CONJUNCTION WITH GAUSSIAN NOISE With results in Figure 3, we expressed concern with small values ofMk causing privacy issues with only a small performance boost, if at all. A common practice of introducing additional privacy is adding Gaussian noise. This is a popular method associated with differential privacy (McMahan et al., 2018), but adding noise alone does not guarantee differential privacy, since the noise level should be explicitly linked to differential privacy levels, ϵ and δ. Addition of artiﬁcial pixel-wise noise will enhance privacy but will result in a quality drop of averaged data. While privacy added by noise and privacy from averaging data cannot be directly compared, we can select a noise level which in conjunction with smallMk, visually provides data privacy similar to that of maximumMk. Results show that the introduction of Gaussian noise does result in a decline in performance (Table 14),although the decline is very small. Interestingly as noise gets larger as σ = 0.3, random noise 17Published as a conference paper at ICLR 2021 Table 14: Performance of FedMix with Gaussian noise. σ refers to standard deviation of Gaussian noise. σ 0 0.05 0.075 0.15 0.3 Mk = 5 81.4 80.1 81.1 78.7 81.5 Mk = 10 79.9 80.8 79.1 79.4 81.7 Mk = 20 80.4 80.5 79.7 80.7 81.0 Table 15: Test accuracy on CIFAR10 varying m, number of entries in Xg,Yg to be further averaged. m 1 4 10 Test acc (% ) 81.2 81.6 78.4 provides an effect as data augmentation and results in a performance increase compared to σ = 0. This experiment is in line with Appendix D. We conclude that introduction of noise in averaged data could provide us with a reasonable alternative to FedMix with large Mk. While our method does not align directly with differential privacy, we leave as future work how FedMix could be smoothly combined with DP-related methods and how its privacy could be quantiﬁed in terms of differential privacy. J A DDITIONAL EXPERIMENTS : VARIATIONS OF FEDMIX Averaging within Xg,Yg Further averaging between entries of Xg,Yg practically provides an extension of the range of viable Mk such that it exceeds nk, in the sense that each averaged data is from multiple clients’ data. Such a process would also result in fewer data included in Xg,Yg, so we tested effect of this procedure on model performance. Table 15 shows that for m-fold extra averaging, we even observe increase in performance, but it quickly declines asmgets too large. This method provides an improvement in privacy while even possibly resulting in better performance. Effect of same-class split for averaging We perform random split of local data for averaging, but an unbalanced split, such as only averaging data with the same class labels, could result in better performance. We compared between random split and same-class split while keeping Mk = 0.5nk be equal for both methods. Same-class split resulted in a signiﬁcant decline in performance, and we conclude that there is no advantage of such split over random split that we are using for our main results. Table 16: Test accuracy on CIFAR10, with class/client = 2, under different split meth- ods. Mk = 0.5nk for both splits. random split class split FedMix 81.2 78.8 Table 17: Test accuracy of NaiveMix on CI- FAR10, under varying Mixup ratio λ. λ 0.05 0.1 0.2 0.5 NaiveMix 79.5 79.9 80.6 29.8 NaiveMix with varying Mixup ratioλ We varied Mixup ratioλfor NaiveMix as well. Results in Table 17 shows that NaiveMix also has an intermediate optimal value ofλ. The drop in performance for λ= 0.5 is much more dramatic than for FedMix (see Table 5 for comparison with Global Mixup and FedMix). We think that NaiveMix loss also suffers as it gives more weight to the averaged data, especially for large Mk. Heterogeneity from skewed label distribution Recent papers (Yurochkin et al., 2019; Wang et al., 2020) suggested an alternative heterogeneous environment, which does not limit the num- ber of classes per client but skews label distribution in local data. We used a Dirichlet distribution of α = 0.2,0.5 as described by Yurochkin et al. (2019) and Wang et al. (2020). Results show that FedMix still outperforms all other algorithms. We think that such label skewing introduces less het- erogeneity compared to our practice of limiting the number of classes per client, but nevertheless, FedMix is still the most powerful method in terms of performance. 18Published as a conference paper at ICLR 2021 Table 18: Test accuracy on CIFAR10 under label-skewed heterogeneous envi- ronment. We used Dirichlet distribution for uneven label distribution. α FedAvg GlobalMix LocalMix NaiveMix FedMix 0.2 83.9 91.1 84.0 85.0 86.4 0.5 87.6 91.1 88.0 88.2 88.4 19",
      "references": [
        "Generative models for effective ml on private, decentralized datasets.",
        "Leaf: A benchmark for federated settings.",
        "Vicinal risk minimization.",
        "Emnist: an extension of mnist to handwritten letters.",
        "Large scale distributed deep networks.",
        "Improving data augmentation for medical image segmentation.",
        "Model inversion attacks that exploit confidence information and basic countermeasures.",
        "Augmenting data with mixup for sentence classification: An empirical study.",
        "Deep residual learning for image recognition.",
        "The non-iid data quagmire of decentralized machine learning.",
        "Federated semi-supervised learning with inter-client consistency.",
        "Advances and open problems in federated learning.",
        "Federated learning: Strategies for improving communication efficiency.",
        "Gradient-based learning applied to document recognition.",
        "Federated learning: Challenges, methods, and future directions.",
        "Federated optimization in heterogeneous networks.",
        "On the convergence of fedavg on non-iid data.",
        "Communication-efficient learning of deep networks from decentralized data.",
        "Learning differentially private recurrent language models.",
        "Agnostic federated learning.",
        "Mix2ﬂd: Downlink federated learning after uplink federated distillation with two-way mixup.",
        "Imagenet large scale visual recognition challenge.",
        "Robust and communication-efficient federated learning from non-iid data.",
        "Xor mixup: Privacy-preserving data augmentation for one-shot federated learning.",
        "Very deep convolutional networks for large-scale image recognition.",
        "Cocoa: A general framework for communication-efficient distributed optimization.",
        "Federated multi-task learning.",
        "On mixup training: Improved calibration and predictive uncertainty for deep neural networks.",
        "Overcoming noisy and irrelevant data in federated learning.",
        "Manifold mixup: Better representations by interpolating hidden states.",
        "Federated learning with matched averaging.",
        "Speech commands: A dataset for limited-vocabulary speech recognition.",
        "Aggregated residual transformations for deep neural networks.",
        "Federated continual learning with adaptive parameter communication.",
        "Cutmix: Regularization strategy to train strong classifiers with localizable features.",
        "Bayesian nonparametric federated learning of neural networks.",
        "mixup: Beyond empirical risk minimization.",
        "Federated learning with non-iid data."
      ],
      "meta_data": {
        "arxiv_id": "2107.00233v1",
        "authors": [
          "Tehrim Yoon",
          "Sumin Shin",
          "Sung Ju Hwang",
          "Eunho Yang"
        ],
        "published_date": "2021-07-01T06:14:51Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Federated learning suffers from performance degradation under non-iid data distributions and privacy constraints on sharing raw data. The paper introduces Mean Augmented Federated Learning (MAFL), where clients exchange averaged local data, and proposes FedMix, a novel algorithm that approximates the loss of a globally shared Mixup using only these averaged data. NaiveMix is a simpler baseline within MAFL. FedMix achieves substantial improvements over standard FL methods in highly non-iid settings across common benchmarks while incurring minimal additional communication and preserving privacy to a practical extent.",
        "methodology": "MAFL framework: clients share updated models and mashed (averaged) data; Mk controls the amount of data used for averaging, balancing privacy, utility, and communication. NaiveMix performs Mixup between local data and averaged external means. FedMix approximates the global Mixup loss by Taylor expanding the Mixup loss around small λ, resulting in an objective that uses local data (xi, yi), the averaged external means (¯xj, ¯yj), and the input gradient term ∂ℓ/∂x evaluated at (1−λ)xi. The core update in FedMix (LocalUpdate) adds a gradient term with respect to input: ℓ1 = loss with (1−λ)xi, yi; ℓ2 = loss with (1−λ)xi, ¯yj; ℓ3 = λ ∂ℓ/∂x · ¯xj. Privacy considerations, communication costs, and variants (e.g., averaging within Xg,Yg, hidden-state Mixup) are discussed. Empirical validation compares FedMix against FedAvg, FedProx, LocalMix, NaiveMix, and Global Mixup.",
        "experimental_setup": "Experiments on FEMNIST, CIFAR-10, CIFAR-100, and Shakespeare. Data is distributed across many clients with partial participation per round (e.g., K selected clients per round). Models: LeNet-5 for FEMNIST; a modified VGG-style network for CIFAR-10/100; 2-layer LSTM with embedding for Shakespeare. Hyperparameters (E, B, λ, Mk) and dataset-specific settings are detailed in the paper’s Appendix and Table 8. Baselines include FedAvg, FedProx, LocalMix, NaiveMix, and Global Mixup. Evaluation metrics are test accuracy at target round counts and rounds-to-target accuracy, with ablations on Mk, λ, and other MAFL variants.",
        "limitations": "The privacy guarantees are not formal differential privacy (DP); MAFL introduces potential information leakage through averaged data when Mk is small, and the threat depends on nk and Mk. Additional communication exists for averaged data, though it is small relative to model parameters. The Taylor expansion approximation assumes λ is small and may degrade with larger λ; experiments are limited to image classification and text-ready Shakespeare; theoretical convergence guarantees under MAFL are not fully derived. There are practical assumptions about data averaging and non-iid distributions that may not hold in all FL deployments.",
        "future_research_directions": "Integrate formal differential privacy mechanisms with MAFL-fed Mixup (DP-FedMix) and quantify privacy-utility trade-offs. Develop adaptive Mk and λ schemes to balance privacy, performance, and communication. Extend evaluations to more domains (language, speech, regression, time-series) and to more extreme non-iid distributions; analyze convergence properties of FedMix under heterogeneous data and partial participation; explore additional privacy-preserving averaging methods (e.g., server-side averaging, secure aggregation); investigate dynamic or learned mixup ratios and deep generative augmentation within MAFL.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Data driven semi-supervised learning",
      "full_text": "DATA DRIVEN SEMI -SUPERVISED LEARNING Maria-Florina Balcan School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 ninamf@cs.cmu.edu Dravyansh Sharma Department of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 dravyans@cs.cmu.edu October 1, 2021 ABSTRACT We consider a novel data driven approach for designing learning algorithms that can effectively learn with only a small number of labeled examples. This is crucial for modern machine learning applica- tions where labels are scarce or expensive to obtain. We focus on graph-based techniques, where the unlabeled examples are connected in a graph under the implicit assumption that similar nodes likely have similar labels. Over the past decades, several elegant graph-based semi-supervised learning algorithms for how to infer the labels of the unlabeled examples given the graph and a few labeled examples have been proposed. However, the problem of how to create the graph (which impacts the practical usefulness of these methods signiﬁcantly) has been relegated to domain-speciﬁc art and heuristics and no general principles have been proposed. In this work we present a novel data driven approach for learning the graph and provide strong formal guarantees in both the distributional and online learning formalizations. We show how to leverage problem instances coming from an underlying problem domain to learn the graph hyperparameters from commonly used parametric families of graphs that perform well on new instances coming from the same domain. We obtain low regret and efﬁcient algorithms in the online setting, and generalization guarantees in the distributional setting. We also show how to com- bine several very different similarity metrics and learn multiple hyperparameters, providing general techniques to apply to large classes of problems. We expect some of the tools and techniques we develop along the way to be of interest beyond semi-supervised learning, for data driven algorithms for combinatorial problems more generally. 1 Introduction In recent years machine learning techniques have found gainful application in diverse settings including textual, visual, or acoustic data. A major bottleneck of the currently used approaches is the heavy dependence on expensive labeled data. At the same time advances in cheap computing and storage have made it relatively easier to store and process large amounts of unlabeled data. Therefore, an important focus of the present research community is to develop general (domain-independent) methods to learn effectively from the unlabeled data, along with a small amount of labels. Achieving this goal would signiﬁcantly elevate the state-of-the-art machine intelligence, which currently lags behind the human capability of learning from a few labeled examples. Our work is a step in this direction, and provides algorithms and guarantees that enable fundamental techniques for learning from limited labeled data to provably adapt to problem domains. Graph-based approaches have been popular for learning from unlabeled data for the past two decades [38]. Labeled and unlabeled examples form the graph nodes and (possibly weighted) edges denote the feature similarity between examples. The implicit modeling assumption needed to make semi-supervised learning possible is that the likelihood of having a particular label increases with closeness to nodes of that label (Balcan and Blum [3]). The graph therefore captures how each example is related to other examples, and by optimizing a suitably regularized objective over it one obtains an efﬁcient discriminative, nonparametric method for learning the labels. There are several well-studied ways to deﬁne and regularize an objective on the graph [18, 38], and all yield comparable results which strongly depend on the graph used. A general formulation is described as follows, variations on which are brieﬂy discussed under related work. arXiv:2103.10547v4  [cs.LG]  29 Sep 2021Problem formulation: Given sets Land U of labeled and unlabeled examples respectively, and a similarity metric d over the data, the goal is to usedto extrapolate labels inLto U. A graph Gis constructed with L+U as the nodes and weighted edges W with w(u,v) = g(d(u,v)) for some g: R≥0 →R≥0. We seek labels f(·) for nodes uof Gwhich minimize a regularized loss function l(f) = α∑ v∈L ˆl(f(v),yv) + βH(f,W ) + γ∥f∥2, under some constraints on f. The objective H captures the smoothness (regularization) induced by the graph (see Table 1 for examples) and ˆl(f(v),yv) is the misclassiﬁcation loss (computed here on labeled examples). The graph Gtakes a central position in this formulation. However, the majority of the research effort on this problem has focused on how to design and optimize the regularized loss function l(f), the effectiveness of which crucially depends on G. Indeed the graph Gis expected to reﬂect a deep understanding of the problem structure and how the unlabeled data is expected to help. Despite the central role of Gin the semi-supervised learning process, only some heuristics are known for setting the graph hyperparameters [41]. There is no known principled study on how to do this and prior work largely treats this as a domain-speciﬁc art. Is it possible to acquire the required domain expertise, without involving human experts? In this work we provide an afﬁrmative answer by introducing data-driven algorithms for building the graphs, that is techniques which learn a provably good problem-speciﬁc graph from instances of a learning problem. More precisely, we are required to solve not only one instance of the problem, but multiple instances of the underlying algorithmic problem that come from the same domain [2, 5, 23]. This approach allows us to model the problem of identifying a good algorithm from data as an online or statistical learning problem. We formulate the problem of creating the learning graph as a data-speciﬁc decision problem, where we select the graph from well-known inﬁnite families of candidates which capture a range of ways to encode example similarity. We show learning a near-optimal graph over these families is possible in both online and distributional settings. In the process we generalize and extend results developed in the context of other data-driven learning problems, and obtain practical methods to build the graphs with strong guarantees. In particular, we show that the approach may be used for learning several parameters at once, and it is useful for learning a broader class of parameters than previously known. 1.1 Our results Semi-supervised learning: We consider common ways of creating the graph Gand formulating the regularized loss l(f) as families of algorithms to learn over, and by learning over these families determine the most suitable semi- supervised learning method for any given problem domain. The graph is created by setting the edge weights according to some (monotonic) function gof the similarity metric d, and is parameterized by ρ. We denote this graph by G(ρ) (omitting dfor conciseness). Note that each value of ρcorresponds to a semi-supervised learning algorithm, where labels for the unlabeled examples are predicted by optimizing over the graph G(ρ) (i.e., similar nodes according to G(ρ) get similar labels). We consider online and distributional settings and provide efﬁcient algorithms to obtain low regret and low error respectively for learning ρ. In the online setting, we receive problem instances Li,Ui sequentially and must predict labels fi for the ith instance before receiving the next by optimizing over some graphG(ρi). We also observe the lossl(f) for prediction according to G(ρ) for all ρ(full information setting ) or some interval containing ρi after our prediction ( semi-bandit setting). The performance is measured by the regret of our predictions using G(ρi) relative to the optimal graph G(ρ∗) in hindsight. Our key insight is to note that the loss is a piecewise constant function of the parameter ρwith dispersed discontinuities (Deﬁnition 4) under mild smoothness assumptions on the similarity function, that the metric is not exact and small perturbations to the similarities does not affect learning. Roughly speaking, dispersion means that the discontinuities are not concentrated in a small region, across instances. The full information setting however can be computationally inefﬁcient, since it involves computing the loss for each of potentially prohibitively many constant performance “pieces”. This is overcome by Algorithm 2 in the semi-bandit setting, where it is sufﬁcient to compute the loss for a small number of pieces contained in an efﬁciently computablefeedback set. Our implementation involves a novel min-cut and ﬂow recomputation algorithm on a graph with continuously changing edge-weights, and may be of independent interest to the broader theory community. In the distributional setting, the problem instances are assumed to be sampled according to some underlying distri- bution, and we would like to show PAC bounds for learning with low error with a high conﬁdence. We provide asymptotically tight upper and lower bounds on the pseudodimension of learning the best parameter from a parame- terized family of semi-supervised learning algorithms, each algorithm corresponding to a graph G(ρ). We consider both unweighted and weighted graph families. Our bounds imply efﬁcient algorithms with PAC guarantees for the unweighted setting, and hardness of efﬁcient learning of worst case instances of weighted graphs. For commonly used approaches to create a weighted graph from a similarity metric, we show efﬁcient learning is still possible under the mild smoothness assumptions used in the online setting above. The lower bounds are fairly technical and involve con- 2structing a family of graph instances while setting the distances between graph nodes in a precise correlated manner to ensure that the loss as a function of the parameter oscillates highly and at carefully determined points in the domain. Compared to known heuristics to build graphs which consider a ﬁxed problem instance, our approach may be viewed as building graphs by learning over subsets of the full dataset (which previous approaches have not considered to the best of our knowledge) and doing learning with these instances as examples for the hyperparameter learning problem. The setting however is more general and captures batches of partially labeled data arriving online or according to some distribution. Multiple metrics: In practice, we might have several natural, but very different types of metrics for our problem. The Euclidean distance metric d(u,v) over the representation (embedding) of the examples alone may not best capture the similarity measure between node pairs. When learning over multiple channels or modes simultaneously, for ex- ample in detecting people or activities from video footages, one needs to combine information from several natural similarity metrics [4]. We can view this as a graph with multiple hyperparametersG(ρ1,ρ2,... ), where the additional parameters indicate relative importance of the different metrics. We show how to select a provably good interpola- tion by generalizing results from [10] to multiple parameters. We use tools from algebraic geometry including the Tarski–Seidenberg theorem and properties of the cylindrical algebraic decomposition to accomplish this. Data-driven algorithm design: This work employs and extends powerful general techniques and results for selecting algorithms from a parameterized family in a data-driven way i.e. from several problem instances. Dispersion is a property of problem sequences (observed online, or drawn from a distribution) which has been shown to be necessary and sufﬁcient for provably learning optimal parameters for combinatorial algorithms [7, 11]. Algorithms for learning dispersed instances are known for both full information and semi-bandit online settings [7, 10]. We study data driven algorithm design for a completely new setting, learning the graph for graph based semi-supervised learning techniques, and undertake the technical work needed to understand the underlying structure of the induced loss functions in these new settings. In the process we extend general tools for deducing dispersion for general algorithm design problems. Firstly, for one dimensional loss functions, we show a novel structural result for proving dispersion when discontinuities (for loss as function of the algorithm parameter) occur along roots of exponential polynomials with random coefﬁcients with bounded joint distributions (previously shown only for algebraic polynomials [10]). This is crucial for showing learnability in the Gaussian graph kernels setting. Secondly, prior work [10] was only able to prove dispersion when the discontinuities occur along algebraic curves with random coefﬁcients in just two dimensions. By a novel algebraic and learning theoretic argument we are able to analyze higher dimensions, and show dispersion for an arbitrary constant number of parameters, making it much more generally applicable. Key challenges: We present the ﬁrst theoretically grounded work outlining how to create good graphs for learning from unlabeled data. Graph-based semi-supervised learning literature has largely been focused on learning approaches given a graph and very little progress made on the arguably more signiﬁcant problem of designing good graphs. The problem was noted by [41] and has remained largely open for two decades. We use a data-driven algorithm design perspective [7, 10] and take steps towards resolving this problem. We remark that our techniques are very general and they apply simultaneously for learning the graph when we do prediction by optimizing various quadratic objectives with hard or soft labels (Table 1). Online learning in our setting poses some interesting challenges. The loss function is a non-Lipschitz function of the parameter, so usual gradient-based approaches do not work. We use mild perturbation invariance assumptions to show dispersion of the non-Lipschitzness which is necessary to overcome the worst case lower bounds. Furthermore, most previously studied settings for dispersion involve polynomially many discontinuities, so efﬁcient algorithms are immediate, which may not be the case for our setting. Instead we crucially rely on semi-bandit algorithms to ensure that the parameters may be learned efﬁciently, which involve development of careful local search techniques in the parameter space. For weighted graphs and combinatorial optimizations, the challenge of computing changing mincuts with continuously varying graph weights requires a novel algorithm with combinatorial and continuous elements. In the distributional setting, we provide lower bounds for the pseudo-dimension of learning the algorithm, which require technical constructed instances. In particular we seek to show instances with highly oscillating loss functions at carefully determined points as the graph parameter is varied, which is especially difﬁcult for the weighted graph families. We note that even for single-parameter families the lower bound is superconstant (for exampleΩ(log n) and even Ω(n)). Another key challenge we overcome is to show how the results may be extended to tuning several graph hyperparame- ters at the same time, making our results much more powerful. Similar results are known for linkage-based clustering [8] but they crucially rely on the algorithm depending on relative distances and not the actual values, and therefore do not extend to our setting. The problem is signiﬁcantly more complex than the one-dimensional problem as non- Lipschitzness now occurs along high-dimensional surfaces instead of just points of discontinuity for which learnability may be deduced by arguing about the concentration of these points in intervals. For algebraic curves (learning two 3Algorithm (α,β,γ ) H(f,W ),∥·∥ Constraints on f A. Mincut (Blum and Chawla [16]) (∞,1,0) fT(D−W)f f ∈{0,1}n B. Harmonic functions (Zhu et al. [39]) (∞,1,0) fT(D−W)f f ∈[0,1]n C. Normalized cut (Shi and Malik [29]) (∞,1,0) fT(D−W)f fT1 = 0,fTf = n2, f ∈[0,1]n D. Label propagation (Zhou et al. [37]) (1,µ, 1) fTLf, ∥·∥2 f ∈[0,1]n Table 1: Optimization using a quadratic objective involved in some prominent algorithms for graph-based semi-supervised learning. Here Dij := I[i= j] ∑ kWik,L:= D−1/2(D−W)D−1/2 and the objective is l(f) = α∑ u∈L(f(u) −yu)2 + βH(f,W ) + γ∥f∥2. parameters) one may use a bound on the number of local extrema and curve intersections in a ﬁxed direction [10], but we need a careful projection argument and tools from algebraic geometry to generalize to higher dimensions. Extension to active learning: We also consider an active learning setting where we learn the graph for the graph-based active learning procedure for a ﬁxed constant label budget. We consider data-driven construction of the graph family, where the labels used for semi-supervised learning are obtained by the greedy active learning algorithm from [40]. The procedure selects the next node which minimizes the expected estimated risk after querying the node. We show how to learn a graph for which the budgeted active learning procedure, followed by semi-supervised label predictions, results in provably near-minimum loss. 1.2 Related work Semi-supervised learning is a paradigm for learning from both labeled and unlabeled data (Zhu and Goldberg [38]). It resembles human learning behavior more closely than fully supervised and fully unsupervised models (Gibson et al. [22], Zhu et al. [42]). Learning from unlabeled data may be possible due to implicit compatibility between the target concept and the underlying data distribution (Balcan and Blum [3]). A semi-supervised learning method makes a particular compatibility assumption and provides a procedure to determine a predictor which ﬁts the labeled data well and has high compatibility. Graph-based methods: A prominent and effective approach widely used in practice for semi-supervised learning is to optimize a graph-based objective. Here the compatibility assumption is that the labels are smooth over the graph, and as such the performance is highly sensitive to the graph structure and the edge weights. Since labels partition the graph, we seek a (possibly soft) graph cut as the predictor. Several methods have been proposed to obtain predictors given a graph including st-mincuts (Blum and Chawla [16]), soft mincuts that optimize a quadratic energy objective (Zhu et al. [39]), label propagation (Xiaojin and Zoubin [34]), and many more (Blum et al. [17], Belkin et al. [14]). Table 1 summarizes the optimization involved in some prominent algorithms.α= ∞corresponds to forcing labels of labeled examples L. However, it is not clear how to create the graph itself on which the extensive literature stands, although some heuristics are known (Zhu et al. [41]). Zemel and Carreira-Perpi ˜n´an [35] discuss how to create a robust graph by considering an ensemble of minimum spanning trees for several data perturbations and randomly retaining edges which appear often. The algorithm however uses a parametertfor expected graph density and it is unclear how to set it for any given problem instance, and no theoretical guarantees are provided. Sindhwani et al. [30] construct warped kernels more aligned with the data geometry, but the performance may vary strongly with warping and it is not clear how to optimize over it. To the best of our knowledge, we provide the ﬁrst techniques that yield provably near-optimal graphs. Prior art largely compares semi-supervised graph learning in terms of assumption generality and experimental evidence. Data-driven design and dispersion : Gupta and Roughgarden [23] deﬁne a formal learning framework for selecting algorithms from a family of heuristics or a range of hyperparameters. The framework is further developed by Balcan et al. [5] and its usefulness as a fundamental algorithm design perspective has been noted [2, 15]. It has been suc- cessfully applied to several combinatorial problems like integer programming and clustering [6, 8, 12] and for giving powerful guarantees like adversarial robustness, adaptive learning and differential privacy [7, 9, 11, 33]. Balcan [2] provides a simple introduction to and a comprehensive survey on this rapidly expanding research direction. 4Balcan et al. [7] introduce dispersion, a useful property of the problem instances with respect to an algorithm family, which, if satisﬁed, intuitively allows it to be efﬁciently learned in full information online as well as distributional settings. Full information may be expensive to compute and work with, and a semi-bandit algorithm is introduced by Balcan et al. [10]. The same work also presents a general technique for analyzing dispersion which is used to show dispersion when the non-Lipschitzness occurs along roots of polynomials with random coefﬁcients, for up to two parameters. We apply dispersion in a new problem setting, and show how to learn algorithms for semi-supervised learning of labels by carefully studying the properties of these problems. We also extend the technique, and prove that dispersion holds for a broader setting involving non-polynomial discontinuities and employ tools from algebraic geometry to extend the theory to an arbitrary constant number of parameters. 2 Notation and deﬁnitions We are given some labeled pointsLand unlabeled points U. One constructs a graph Gby placing (possibly weighted) edges w(u,v) between pairs of data pointsu,v which are ‘similar’, and labels for the unlabeled examples are obtained by optimizing some graph-based score. We have an oracleOwhich on querying provides us the labeled and unlabeled examples, and we need to pick Gfrom some family Gof graphs. We commit to using some algorithm A(G,L,U ) (abbreviated as AG,L,U) which provides labels for examples inU, and we should pick aGsuch that A(G,L,U ) results in small error in its predictions on U. To summarize more formally, Problem statement: Given data space X, label space Yand an oracle Owhich yields a number of labeled examples L ⊂X×Y and some unlabeled examples U ⊂X such that |L|+ |U|= n. We are further given a parameterized family of graph construction procedures over parameter spaceP, G: P→ (X×X→ R≥0), graph labeling algorithm A: (X×X→ R≥0)×2X×(Y∪{⊥}) →(X→Y ), a loss function l: Y×Y→ [0,1] and a target labeling τ : U →Y. We need to select ρ∈P such that corresponding graph G(ρ) minimizes ∑ U l(AG(ρ),L,U(u),τ(u)) w.r.t. ρ. We will consider online and distributional settings of the above problem. In the online setting we make no distributional assumptions about the data and simply seek to minimize the regret, i.e. the loss suffered in an arbitrary online sequence of oracle queries Orelative to that endured by the best parameter ρ∗in hindsight. In the distributional setting we will assume that the data and labels supplied by Ocome from an underlying distribution Dand we would like to minimize the expected loss suffered on test examples drawn from the distribution with high probability. We will present further details and notations for the respective settings in the subsequent sections. We will now describe graph families Gand algorithms AG,L,U considered in this work. We assume there is a feature based similarity function d: X×X→ R≥0, a metric which monotonically captures similarity between the examples. In section 4.2.2, we will consider creating graphs with several similarity functions, but for now assume we have a single d. Deﬁnition 1 summarizes commonly used parametric methods to build a graph using the similarity function. In this work, we will consider three parametric families of graph construction algorithms deﬁned below. I[·] is the indicator function taking values in {0,1}. Deﬁnition 1. Graph kernels. a) Threshold graph, G(r). Parameterized by a threshold r, we set w(u,v) = I[d(u,v) ≤r]. b) Polynomial kernel, G(˜α). w(u,v) = ( ˜d(u,v) + ˜α)d for ﬁxed degree d, parameterized by ˜α.1 c) Gaussian RBF or exponential kernel, G(σ). w(u,v) = e−d(u,v)2/σ2 , parameterized by σ. Remark 1. Another popular family of graphs used in practice is the k nearest neighbor graphs, where k ∈ {0,1,...,n −1}, n is the number of nodes in the graph, is the parameter. Even though k-NN graphs may result in different graphs the ones considered in the paper, learning how to build an optimal graph over the algorithm family G(k) is much simpler. Online learning of the parameter kin this setting can be recognized as an instance of learn- ing with experts advice for a ﬁnite hypothesis class (Section 3.1 of [28]), where an upper bound of O(√Tlog n) is known for the Weighted Majority algorithm. Online-to-batch conversion provides generalization guarantees in the distributional setting (Section 5 of [28]). We remark that our algorithm families need more sophisticated analysis due to continuous ranges of the algorithm parameters. The threshold graph adds (unweighted) edges to Gonly when the examples are closer than some r∈R≥0, i.e. a step function of the distance. Polynomial and exponential kernels add (weighted) edges to the graph, with weights varying polynomially and exponentially (respectively) with the similarity. Note that similarity function˜d(u,v) in the deﬁnition 1With some notational abuse here, we have das the integer degree of the polynomial, and ˜d(·,·) as the similarity function. 5for polynomial kernels increases monotonically with similarity of examples, as opposed to the other two 2. Usually the threshold graph setting (Deﬁnition 1a) will be easier to optimize over, but it is also a small parameter family often with relatively weaker performance in practice. In the following, we will refer to this setting by the unweighted graph setting, and the other two settings (Deﬁnitions 1b and 1c) by the weighted graph setting. Often we will discuss just the Gaussian RBF setting since it is more technically challenging and more commonly used in practice for building graphs. However, in some instances working through the polynomial kernel setting can provide useful insights. Once the graph is constructed using one of the above kernels, we can assign labels using a suitable algorithmAG,L,U. A popular and effective approach is by optimizing a quadratic objective 1 2 ∑ u,vw(u,v)(f(u) −f(v))2 = fT(D− W)f. Here f may either be discrete f(u) ∈ {0,1}which corresponds to ﬁnding a graph mincut separating the oppositely labeled vertices [16], or f may be continuous, i.e. f ∈[0,1], and we can round f to obtain the labels [39]. These correspond to algorithms A and B respectively from Table 1. It is noted that all algorithms have comparable performance provided the graph Gencodes the problem well [38]. We restrict our attention to these two algorithms for simplicity of presentation, although our algorithms and proofs may be extended to any quadratic objective based algorithm in Table 13. Finally we note deﬁnitions of some useful learning theoretic complexity measures. First recall the deﬁnitions of pseudodimension and Rademacher complexity, well-known measures for hypothesis-space complexity in statistical learning theory. Bounding these quantities implies immediate bounds on learning error using classic learning theoretic results. In Section 4.3 we will bound the pseudodimension and Rademacher complexity for the problems of learning unweighted and weighted graphs. Deﬁnition 2. Pseudo-dimension [27]. Let Hbe a set of real valued functions from input space X. We say that C = (x1,...,x m) ∈Xm is pseudo-shattered by Hif there exists a vectorr= (r1,...,r m) ∈Rm (called “witness”) such that for all b= (b1,...,b m) ∈{±1}m there exists hb ∈H such that sign(hb(xi) −ri) = bi. Pseudo-dimension of His the cardinality of the largest set pseudo-shattered by H. Deﬁnition 3. Rademacher complexity [13]. Let F = {fρ : X →[0,1],ρ ∈C⊂ Rd}be a parameterized family of functions, and sample S= {xi,...,x T}⊆X . The empirical Rademacher complexity of Fwith respect to Sis deﬁned as ˆR(F,S) = Eσ [ supf∈F 1 T ∑T i=1 σif(xi) ] , where σi ∼U({−1,1}) are Rademacher variables. We will also need the deﬁnition of dispersion which, informally speaking, captures how amenable a non-Lipschitz function is to online learning. As noted in [7, 11], dispersion is necessary and sufﬁcient for learning piecewise Lips- chitz functions. Deﬁnition 4. Dispersion [10]. The sequence of random loss functions l1,...,l T is β-dispersed for the Lipschitz constant Lif, for all T and for all ϵ≥T−β, we have that, in expectation, at most ˜O(ϵT) functions (the soft-O notation suppresses dependence on quantities beside ϵ,T and β, as well as logarithmic terms) are not L-Lipschitz for any pair of points at distance ϵin the domain C. That is, for all T and for all ϵ≥T−β, E   max ρ,ρ′∈C ∥ρ−ρ′∥2≤ϵ ⏐⏐{t∈[T] |lt(ρ) −lt(ρ′) >L ∥ρ−ρ′∥2} ⏐⏐  ≤ ˜O(ϵT). 3 New general dispersion-based tools for data-driven design We present new techniques and generalize known tools for analyzing data-driven algorithms [7, 10]. Our new tools apply to a very broad class of algorithm design problems, for which we derive sufﬁcient smoothness conditions to infer dispersion of a random sequence of problems, i.e. the algorithmic performance as a function of the algorithm parameters is dispersed. Balcan et al. [10] provide a general tool for verifying dispersion if non-Lipschitzness occurs along roots of (algebraic) polynomials in one and two dimensions. We improve the results in the following two ways. Our ﬁrst result is that dispersion for one-dimensional loss functions follows when the points of discontinuity occur at the roots of exponential polynomials if the coefﬁcients are random, lie within a ﬁnite range, and are drawn according to a bounded joint distribution. In addition to generalizing prior results, we present a new simpler proof. The full proof appears in Appendix A.2. 2Common choices are setting d(u,v) as the Euclidean norm and ˜d(u,v) as the dot product when u,v ∈Rn 3Speciﬁcally by extending arguments for algorithm B since the optimization is similar. In contrast, Algorithm A is combinatorial and the reasoning diverges somewhat. 6Theorem 5. Let φ(x) = ∑n i=1 aiebix be a random function, such that coefﬁcients ai are real and of magnitude at most R, and distributed with joint density at most κ. Then for any interval I of width at most ϵ, P(φhas a zero in I)≤ ˜O(ϵ) (dependence on bi,n,κ,R suppressed). Proof Sketch. For n = 1 there are no roots, so assume n >1. Suppose ρis a root of φ(x). Then a = (a1,...,a n) is orthogonal to ϱ(ρ) = ( eb1ρ,...,e bnρ) in Rn. For a ﬁxed ρ, the set Sρ of coefﬁcients a for which ρ is a root of φ(y) lie along an n−1 dimensional linear subspace of Rn. Now φ has a root in any interval I of length ϵ, exactly when the coefﬁcients lie on Sρ for some ρ ∈ I. The desired probability is therefore upper bounded by maxρVOL(∪Sy |y ∈[ρ−ϵ,ρ + ϵ])/VOL(Sy |y ∈R) which we will show to be ˜O(ϵ). The key idea is that if |ρ−ρ′|< ϵ, then ϱ(ρ) and ϱ(ρ′) are within a small angle θρ,ρ′ = ˜O(ϵ) for small ϵ(the probability bound is vacuous for large ϵ). But any point in Sρ is at most ˜O(θρ,ρ′) from a point in Sρ′, which implies the desired bound. We further go beyond single-parameter discontinuties, which occur as points along a line to general small dimensional parameter spaces Rp, where discontinuties can occur along algebraic hypersurfaces. We employ tools from algebraic geometry to establish a bound on shattering of algebraic hypersurfaces by axis-aligned paths (Theorem 6), which implies dispersion using a VC dimension based argument (Theorem 7). Our result is the ﬁrst of its kind, a general sufﬁcient condition for dispersion for any constant number pof parameters, and applies to a broad class of algorithm families. Full proofs may be found in Appendix A.3. Theorem 6. There is a constantkdepending only on dand psuch that axis-aligned line segments inRpcannot shatter any collection of kalgebraic hypersurfaces of degree at most d. Proof Sketch. Let Cdenote a collection of kalgebraic hypersurfaces of degree at most din Rp. We say that a subset of Cis hit by a line segment if the subset is exactly the set of curves in Cwhich intersect the segment, and hit by a line if some segment of the line hits the subset. We can upper bound the subsets of Cby line segments in a ﬁxed axial direction xin two steps. Along a ﬁxed line, Bezout’s theorem bounds the number of intersections and therefore subsets hit by different line segments. The lines along xcan further be shown to belong to equivalence classes corresponding to cells in the cylindrical algebraic decomposition of the projection of the hypersurfaces, orthogonal to x. Finally, we can extend this to axis-aligned segments by noting they may hit only ptimes as many subsets. Theorem 7. Let l1,...,l T : Rp → R be independent piecewise L-Lipschitz functions, each having discontinu- ities speciﬁed by a collection of at most K algebraic hypersurfaces of bounded degree. Let L denote the set of axis-aligned paths between pairs of points in Rp, and for each s ∈ L deﬁne D(T,s) = |{1 ≤ t ≤ T | lt has a discontinuity along s}|. Then we have E[sups∈LD(T,s)] ≤sups∈LE[D(T,s)] + O( √ Tlog(TK)). Proof Sketch. We relate the number of ways line segments can label vectors ofKalgebraic hypersurfaces of degree d to the VC-dimension of line segments (when labeling algebraic hypersurfaces), which from Theorem 6 is constant. To verify dispersion, we need a uniform-convergence bound on the number of Lipschitz failures between the worst pair of points ρ,ρ′at distance ≤ϵ, but the deﬁnition allows us to bound the worst rate of discontinuties along any path between ρ,ρ′ of our choice. We can bound the VC dimension of axis aligned segments against bounded-degree algebraic hypersurfaces, which will allow us to establish dispersion by considering piecewise axis-aligned paths between points ρand ρ′. 4 Data-driven semi-supervised learning We will warm up this section with a simple example demonstrating the need for and challenges posed by the problem of learning how to build a good graph from data. We will then consider online and distributional settings in sections 4.2 and 4.3 respectively. For online learning we show how to employ and extend dispersion based analysis to our setting, and obtain algorithms which learn good graphs with low regret and are efﬁcient to implement, under mild assumptions on data niceness. For the distributional setting, we analyze the pseudodimension and Rademacher complexity of our learning problems which imply generalization guarantees for learning the graph parameters. Transductive and inductive aspects: For semi-supervised learning, we may distinguish the transductive setting where predictions are evaluated only on provided unlabeled examples U, with the inductive setting where we also care about new unseen examples coming from the same distribution. Graph-based methods were originally introduced as transductive learning approaches [16, 34], but may be used in either setting. For induction we may recompute the graph, or use a ﬁxed subgraph (and assume that new points do not affect the transductive labels) for more efﬁcient prediction [20]. Our setting has an inductive aspect since we learn a graph (by learning graph parameter values) which we expect to use for unseen problem instances. 74.1 Any threshold may be optimal We consider the setting of learning thresholds for unweighted graphs (Deﬁnition 1a). We give a simple demonstration that in a single instance any threshold may be optimal for labelings consistent with graph smoothness assumptions, therefore providing motivation for the learning in our setting. The example below captures the intuition that any unlabeled point may get weakly connected to examples from one class for a small threshold but may get strongly connected to another class as the threshold is increased to a larger value. Therefore depending on the unknown true label either threshold may be optimal or suboptimal, and it makes sense to learn the correct value through repeated problem instances. Theorem 8. Let rmin denote the smallest value of threshold r for which every unlabeled node of G(r) is reachable from some labeled node, and rmax be the smallest value of threshold rfor which G(r) is the complete graph. There exists a data instance (L,U) such that for any rζ = ζrmin + (1−ζ)rmax for ζ ∈(0,1), there exists a set of labelings Uof the unlabeled points such that for some Uζ, ¯Uζ ∈U, rζ minimizes lA(G(r),L,Uζ) but not lA(G(r),L,¯Uζ). L1 L2 a rmin = r∗ 2 r∗ rmax Figure 1: G(r) connects ato nodes in L1 for rmin ≤r<r ∗. Proof. Note that for any r < rmin, there is no graph similarity information for at least one node, and therefore all labels cannot be predicted. Also, the graph is unchanged for all r ≥rmax. Therefore, r ∈[rmin,rmax] captures all graphs of interest on a given data instance. Intuitively the statement claims that any threshold r (modulo the scaling factors for the data embedding) may be optimal or suboptimal for some data labeling for a given constructed instance. Therefore it is useful to consider several problem instances and learn the optimal value ofrfor the data distribution. We will present an example where an unlabeled point is closest to some labeled point of one class but closer to more points of another class on average. So for small thresholds it may be labeled as the ﬁrst class and for larger thresholds as the second class. Let L= L1 ∪L2 with |L1|<|L2|and d(u,v) = 0 for u,v ∈Li,i ∈{1,2}, d(u,v) = 3r∗/2 for u∈Li,v ∈Lj,i ̸= j, where r∗is a positive real. Further let U = {a}such that d(a,ui) = ir∗/2 for each ui ∈Li. It is straightforward to verify that the triangle inequality is satisﬁed. Further note that rmin = r∗/2 and rmax = 3r∗/2. Our set of labelings Uwill include one that labels aaccording to each class. Now we have two cases 1. ζ ∈(0,1 2 ): rmin ≤r<r ∗, G(rζ) connects ato L1 but not L2 and we have that the loss is minimized exactly for the labeling where amatches L1. 2. ζ ∈[1 2 ,1): r∗≤r ≤rmax, G(rζ) connects ato both L1 and L2. But since |L1|<|L2|, we predict that the label of amatches that of L2. Finally we note that d(u,v) may not be exactly zero when u ̸= v for a metric. This is easily ﬁxed by making tiny perturbations to the labeled points, for any given rζ. The example presented above captures some essential challenges of our setting in the following sense. Firstly, we see that the loss function may be non-Lipschitz (as a function of the parameter r), which makes the optimization problem more challenging. More importantly, it highlights that graph similarity only approximately corresponds to label similarity, and how the accuracy of this correspondence is strongly inﬂuenced by the graph parameters. In this sense, it may not be possible to learn from a single instance, and considering a data-driven setting is crucial. 4.2 Dispersion and online learning We consider the problem of learning the graph online. In the online setting, we are presented with instances of the problem and want to learn the best value of the parameter ρwhile making predictions. We also assume we get all the 8Algorithm 1 Data-driven Graph-based SSL (λ) 1: Input: Graphs Gt with labeled and unlabeled nodes (Lt,Ut) and node similarities d(u,v)u,v∈Lt∪Ut. 2: Hyperparameter: step size parameter λ∈(0,1]. 3: Output: Graph parameter ρt for times t= 1,2,...,T . 4: Set w1(ρ) = 1 for all ρ∈R≥0. 5: for t= 1,2,...,T do 6: Wt := ∫ C wt(ρ)dρ. 7: Sample ρwith probability pt(ρ) = wt(ρ) Wt , output as ρt. 8: Compute average loss function lt(ρ) = 1 |Ut| ∑ u∈U l(AGt(ρ),Lt,Ut(u),τ(u)). 9: Set ut(ρ) = 1 −lt(ρ) (loss is converted to utility function, ut(ρ) ∈[0,1]). 10: For each ρ∈C, set wt+1(ρ) = eλut(ρ)wt(ρ). labels for past instances which may be used to determine the loss for anyρ(full information)4. A choice of ρuniquely determines the graph G(ρ) (for example in single parameter families in Deﬁnition 1) and we use some algorithm AG(ρ),L,U to make predictions (e.g. minimizing the quadratic penalty score above) and suffer loss lA(G(ρ),L,U) :=∑ u∈U l(AG(ρ),L,U(u),τ(u)) which we seek to minimize relative to the best ﬁxed choice of ρin hindsight. Formally, at time t∈[T] we predict pt ∈P (the parameter space) based on labeled and unlabeled examples (Li,Ui),i ∈[t] and past labels τ(u) for each u∈Uj,j <t and seek to minimize RT := T∑ t=1 lA(G(ρt),Lt,Ut) −min ρ∈P T∑ t=1 lA(G(ρ),Lt,Ut) A key difﬁculty in the online optimization for our settings is that the losses, as noted above, are discontinuous functions of the graph parameters ρ. We can efﬁciently solve this problem if we can show that the loss functions are dispersed, in fact 1 2 -dispersed functions may be learned with ˜O( √ T) regret ([7, 11]). Algorithm 1 adapts the general algorithm of [7] to data-driven graph-based learning and achieves low regret for dispersed functions. Recall that dispersion roughly says that the discontinuities in the loss function are not too concentrated. We will exploit an assumption that the embeddings are approximate, so small random perturbations to the distance metric will likely not affect learning. This mild distributional assumption allows us to show dispersion, and therefore learnability. For further background and additional proofs and details from this section, see Appendix A. 4.2.1 Dispersion of the loss functions We start with showing dispersion for the unweighted graph family, with threshold parameterr. Here dispersion follows from a simple assumption that the distance d(u,v) for any pair of nodes u,v follows a κ-bounded distribution5, and observing that discontinuities of the loss (as a function ofr) must lie on the set of distances d(u,v) in the samples (for any optimization algorithm). Lemma 9. Let ¯l(r) = lA(G(r),L,U) be the loss function for graph G(r) created using the threshold kernel w(u,v) = I[d(u,v) ≤r]. Then ¯l(r) is piecewise constant and any discontinuity occurs at r∗ = d(u,v) for some graph nodes u,v. Proof. This essentially follows from the observation that as ris increased, the graph gets a new edge only for some r∗= d(u,v). Therefore no matter what the optimization algorithm is used to predict labels to minimize the loss, the loss is ﬁxed given the graph, and has discontinuities potentially only when new edges are added. We can use it to show the following theorem. Theorem 10. Let l1,...,l T : R →R denote an independent6 sequence of losses as a function of parameter r, when the graph is created using a threshold kernel w(u,v) = I[d(u,v) ≤r] and labeled by applying any algorithm on the 4We can think of each problem instance to be of a small size, so we do not need too many labels if we can learn with a reasonable number of problem instances. We improve on the label requirement further in the semi-bandit setting. 5A density function f : R →R corresponds to a κ-bounded distribution if maxx∈R{f(x)}≤ κ. For example, N(µ,σ) is 1 2πσ-bounded for any µ. 6Note that the problems arriving online are adversarial. The adversary is smoothed [31] in the sense it has a distribution which it can choose as long as it has bounded density over the parameters, independent samples are drawn from adversary’s distribution. 9graph. If d(u,v) follows a κ-bounded distribution for any u,v, the sequence is 1 2 -dispersed, and there is an algorithm (Algorithm 1) for setting rwith regret upper bounded by ˜O( √ T). Proof. Assume a ﬁxed but arbitrary ordering of nodes in each Vt = Lt ∪Ut denoted by V(i) t ,i ∈ [n]. De- ﬁne di,j = {d(u,v) | u = V(i) t ,v = V(j) t ,t ∈ [T]}. Since di,j is κ-bounded, the probability that it falls in any interval of length ϵ is O(κϵ). Since different problem instances are independent and using the fact that the VC dimension of intervals is 2, with probability at least 1 −δ/D, every interval of width ϵ contains at most O ( κϵT + √ Tlog D/δ ) discontinuities from each di,j (using Lemma 9). Now a union bound over the failure modes for di,j for different i,j gives O ( n2κϵT + n2√ Tlog n/δ ) discontinuities with probability at least 1 −δ for any ϵ-interval. Setting δ= 1/ √ T, for each ϵ≥1/ √ T the maximum number of discontinuities in any ϵ-interval is at most (1 −δ)O ( κn2 √ Tlog n √ T ) + δT = ˜O(ϵT), in expectation, proving 1 2 -dispersion. We can show dispersion for weighted graph kernels as well, but under stronger assumptions than before. We assume that distances d(u,v) are jointly κ-bounded on a closed and bounded support. The plan is to use results for dispersion analysis from [10] (summarized in Appendix A.1), which implies that roots of a random polynomial are dispersed provided it has ﬁnite coefﬁcients distributed jointly with a κ-bounded distribution. We establish the following for learning polynomial kernels (full proof in Appendix A.1). Theorem 11. Let l1,...,l T : R →R denote an independent sequence of losses as a function of parameter ˜α, when the graph is created using a polynomial kernel w(u,v) = ( ˜d(u,v) + ˜α)d and labeled by optimizing the quadratic objective ∑ u,vw(u,v)(f(u)−f(v))2. If ˜d(u,v) follows a κ-bounded distribution with a closed and bounded support, the sequence is 1 2 -dispersed, and the regret of Algorithm 1 may be upper bounded by ˜O( √ T). Proof Sketch. w(u,v) is a polynomial in ˜αof degree dand the coefﬁcients are Kκ-bounded, where K is a constant that only depends on d and the support of ˜d(u,v). Consider the harmonic solution of the quadratic objective [39] which is given by fU = (DUU −WUU)−1WULfL. For any u∈U, f(u) = 1/2 is a polynomial equation in ˜αwith degree at most nd. Therefore the labeling, and consequently also the loss function, may only change when ˜αis a root of one of |U|polynomials of degree at most dn. The dispersion result is now a simple application of results from [10] (noted as Theorems 22 and 23 in Appendix A.1). The regret bound is implied by Theorem 2 of [10]. Remark 2 (Extension to exponential kernel). We can also extend the analysis to obtain similar results when using the exponential kernel w(u,v) = e−||u−v||2/σ2 . The results of [10] no longer directly apply as the points of discontinuity are no longer roots of polynomials, and we need to analyze points of discontinuities of exponential polynomials, i.e. φ(x) = ∑k i=1 aiebix (See Section 3 and Appendix A.2). The number of discontinuities may be exponentially high in this case. Indeed, solving the quadratic objective shows that discontinuities lie at zeros of exponential polynomials with k= O(|U|n) terms. Remark 3 (Extension to local and global classiﬁcation [37]) . Above results can be extended to the classiﬁca- tion algorithm used in [37]. The key observation is that the labels are given by a closed-form matrix, f∗ = (I −αD−1/2WD1/2)Y or f∗ = (D−αW)Y (for the two variants considered). For threshold graphs G(r), the regret bound in Theorem 10 applies to any classiﬁcation algorithm. Extension to polynomial kernelsG(˜α) is described below. For ﬁxed α(in the notation of [37], in expression for f∗above), the discontinuities in the loss as a function of the parameter ˜αlie along roots of polynomials in the parameter˜αand therefore the same proof as Theorem 11 applies (essentially we get polynomial equations with slightly different but still K-bounded coefﬁcients). On the other hand, if we consider αas another graph parameter, we can still learn the kernel parameter ˜αtogether with αby applying Theorem 22 and Theorem 7 (instead of Theorem 23) in the proof of Theorem 11. 4.2.2 Combining several similarity measures Often the distance metric used for measuring similarity between the data points is a heuristic, and we can have multiple reasonable metrics. Different metrics may have their own advantages and issues and often a weighted combination of metrics, say ∑ iρidi(·,·), works better than any individual metric. This has been observed in practice for semi- supervised learning [4]. The combination weights ρi are additional graph hyperparameters. A combination of metrics has been shown to boost performance theoretically and empirically for linkage-based clustering [8]. However the argument therein crucially relies on the algorithm depending on relative distances and not the actual values, and 10Algorithm 2 Efﬁcient Data-driven Graph-based SSL (λ) 1: Input: Graphs Gt with labeled and unlabeled nodes (Lt,Ut) and node similarities d(u,v)u,v∈Lt∪Ut. 2: Hyperparameter: step size parameter λ∈(0,1]. 3: Output: Graph parameter ρt for times t= 1,2,...,T . 4: Set w1(ρ) = 1 for all ρ∈C 5: for t= 1,2,...,T do 6: Wt := ∫ C wt(ρ)dρ. 7: Sample ρwith probability pt(ρ) = wt(ρ) Wt , output as ρt. 8: Compute the feedback set A(t)(ρ) containing ρt. For example, for the min-cut objective use Algorithm 3 to set A(t)(ρ) = DYNAMIC MINCUT(Gt,ρt,1/ √ T), similarly for the harmonic objective use Algorithm 4. 9: Compute average loss function lt(ρ) = 1 |Ut| ∑ u∈U l(AGt(ρ),Lt,Ut(u),τ(u)). 10: Set ˆlt(ρ) = I[ρ∈A(t)(ρ)]∫ A(t)(ρ) pt(ρ) lt(ρ). 11: For each ρ∈C, set wt+1(ρ) = eλˆlt(ρ)wt(ρ). therefore does not extend directly to our setting. We develop a ﬁrst general tool for analyzing dispersion for multi- dimensional parameters (Section 3), which implies the multi-parameter analogue of Theorem 11, stated as follows. See Appendix A.3 for proof details. Theorem 12. Let l1,...,l T : Rp →R denote an independent sequence of losses as a function of parameters ρi,i ∈ [p], when the graph is created using a polynomial kernelw(u,v) = (∑p−1 i=1 ρi˜d(u,v)+ ρp)d and labeled by optimizing the quadratic objective ∑ u,vw(u,v)(f(u) −f(v))2. If ˜d(u,v) follows a κ-bounded distribution with a closed and bounded support, the sequence is 1 2 -dispersed, and the regret may be upper bounded by ˜O( √ T). 4.2.3 Semi-bandit setting and efﬁcient algorithms Online learning with full information is usually inefﬁcient in practice since it involves computing and working with the entire domain of hyperparameters. For our setting in particular this is computationally infeasible for weighted graphs since the number of pieces (in loss as a piecewise constant function of the parameter) may be exponential in the worst case (see section 4.3.1). Fortunately we have a workaround provided by Balcan et al. [10] where dispersion implies learning in a semi-bandit setting as well. This setting differs from the full information online problem as follows. In each round as we select the parameter ρi, we only observe losses for a single interval containing ρi (as opposed to the entire domain). We call the set of these observable intervals the feedback set, and these provide a partition of the domain. The trade-off is slightly slower convergence, the regret bound for these approaches is weaker (it is O( √ K) in the size Kof the feedback set instead of O(√log K)) but still converges to optimum as ˜O(1/ √ T). For the case of learning the unweighted threshold graph, computing the feedback set containing a given ris easy as we only need the next and previous thresholds from among the O(n2) values of pairwise distances where loss may be discontinuous in r. We present algorithms for computing the semi-bandit feedback sets (constant performance interval containing any σ) for the weighted graph setting (we will use Deﬁnition 1c for concreteness, but the algorithms easily extend to Deﬁnition 1b). We propose a hybrid combinatorial-continuous algorithm for the min cut objective and use continuous optimization for the harmonic objective (recall objectives from Table 1). See Appendix A.4 for background on the ﬂow-cut terminology in Algorithm 3, and for a complete proof of its correctness. Theorem 13. For the mincut objective and exponential kernel (Deﬁnition 1c), Algorithm 3 outputs the interval containing σ in time O(n3K(n,ϵ)), where K(n,ϵ) is the complexity of solving an exponential equation φ(y) =∑n i=1 aiybi = 0 to within error ϵ (for example K(n,ϵ) = O(nlog log 1 ϵ) using Newton’s method with quadratic convergence). Proof Sketch. Let L1 and L2 denote the labeled points Lof different classes. To obtain the labels for U, we seek the smallest cut (V1,V \\V1) of Gseparating the nodes in L1 and L2. If Li ⊆V1, label exactly the nodes in V1 with label i. The loss function, l(σ) gives the fraction of labels this procedure gets right for the unlabeled set U. It is easy to see, if the min-cut is the same for two values ofσ, then so is the loss functionl(σ). So we seek the smallest amount of change in σ so that the mincut changes. Consider a ﬁxed value of σ = σ0 and the corresponding graph G(σ0). We can compute the max-ﬂow on G(σ), and simultaneously obtain a min-cut (V1,V \\V1) in time O(n3). Clearly, all the edges in∂V1 are saturated by the ﬂow. For each ei ∈∂V1, let fi denote the ﬂow that saturated ei. Note 11Algorithm 3 DYNAMIC MINCUT(G,σ0,ϵ) 1: Input: Graph Gwith unlabeled nodes, query parameter σ0, error tolerance ϵ. 2: Output: Piecewise constant interval containing σ0. 3: Use a max-ﬂow algorithm to compute max-ﬂow and min-cut Cfor G(σ), σh = σ0. 4: Compute the ﬂow decomposition of the max-ﬂow, F. 5: Let fe be a unique path ﬂow (i.e. along an st-path, Deﬁnition 31) through e∈C. 6: Say eis augmentable if ﬂow fe can be increased by amount we(σ) −we(σh) for some σ > σh. eacts as the bottleneck for increasing the ﬂow fe. 7: Initialize Sto C(a set of saturated edges). 8: while All edges e∈Sare augmentable, do 9: Increase ﬂow in all fe for e∈Sto keep esaturated. 10: Find ﬁrst saturating edge e1 /∈Sfor some fe′ (e′∈S) and σ′to within ϵ. 11: Reassociate ﬂow through e1,e′as fe1 . fe′ will now be along an alternate path in the residual capacities graph (Deﬁnition 32). 12: Add e1 to S. 13: Set σh = σ′. 14: Similarly ﬁnd the start of the interval σl by detecting saturation while reducing ﬂows. 15: return [σl,σh]. that the fi are distinct. Now as σis increased, we increment each fi by the additional capacity in the corresponding edge ei, until an edge in E\\∂V1 saturates (at a faster rate than the ﬂow through it). We now increment ﬂows while keeping this edge saturated. The procedure stops when we can no longer ﬁnd an alternate path for some ﬂow among the unsaturated edges, which implies the existence of a new min-cut. This gives us a new critical value of σ. Finally note that each time we perform step 10 of the algorithm, a new saturated edge stays saturated for all further σ until the new cut is found. So we can do this at most O(n2) times. In each loop we need to obtain the saturation condition for O(n) edges. For the harmonic objective, we can obtain similar efﬁciency (Algorithm 4). We seek points wherefu(σ) = 1 2 for some u∈U closest to given σ0. For each uwe can ﬁnd the local minima of ( fu(σ) −1 2 )2 or simply the root of fu(σ) −1 2 using gradient descent or Newton’s method. The gradient computation requires O(n3) time for matrix inversion, and we can obtain quadratic convergence rates for ﬁnding the root. Theorem 14. For the harmonic function objective (Table 1) and exponential kernel (Deﬁnition 1c), Algorithm 4 outputs the interval containingσwithin accuracy ϵin time O(n4K1(ϵ)), where K1(ϵ) is the complexity of convergence for Newton’s method (K1(ϵ) = O(log log 1 ϵ) under standard assumptions). Proof. The key observation is that any boundary pointσ′of a piece (where the loss function is constant) hasfu(σ′) = 1 2 for some u ∈U. This follows from continuity of fu(σ) (it is in fact differentiable). Algorithm 4 simply estimates the locations of these σ′closest to σ0 for each u(by using Newton’s method) to ﬁnd the root ofgu(σ) = (fu(σ)−1 2 )2. For each of O(n) nodes, Algorithm 4 computes the gradient and function value of gu(σ) in O(n3) time for different values of σuntil convergence, which gives the bound on time complexity. 4.3 Distributional setting In the distributional setting, we are presented with instances of the problem assumed to be drawn from an un- known distribution Dand want to learn the best value of the graph parameter ρ. We also assume we get all the labels for past instances ( full information). A choice of ρ uniquely determines the graph G(ρ) and we use some algorithm AG(ρ),L,U to make predictions (e.g. minimizing the quadratic penalty score above) and suffers loss lA(G(ρ),L,U) := ∑ U l(AG(ρ),L,U(u),τ(u)) which we seek to minimize relative to smallest possible loss by some graph in the hypothesis space, in expectation over the data distribution D. We will show a divergence in the weighted and unweighted graph learning problems. We analyze and provide asymp- totically tight bounds for the pseudodimension of the set of loss functions (composed with the graph creation algo- rithm family and the optimization algorithm for predicting labels) prarmeterized by the graph family parameter ρ, i.e. Hρ = {lA(G(ρ),L,U) |ρ∈P}. For learning the unweighted threshold graphs, the pseudodimension is O(log n) which 12Algorithm 4 HARMONIC FEEDBACK SET(G,σ0,ϵ) 1: Input: Graph Gwith unlabeled nodes, query parameter σ0, error tolerance ϵ. 2: Output: Piecewise constant interval containing σ0. 3: Let fU = (DUU −WUU)−1WULfL denote the harmonic objective minimizer, where Dij := I[i= j] ∑ kWik. 4: for all u∈U do 5: Let gu(σ) = (fu(σ) −1 2 )2. 6: σ1 = σ0 −gu(σ0) g′u(σ0) , where g′ u(σ) is given by ∂gu ∂σ = 2 ( fu(σ) −1 2 )∂fu ∂σ , ∂f ∂σ = (I−P−1 UU) (∂PUU ∂σ fU −∂PUL ∂σ fL ) , ∂Pij ∂σ = ∂w(i,j) ∂σ −Pij ∑ k∈L+U ∂w(i,k) ∂σ∑ k∈L+U w(i,k) , ∂w(i,j) ∂σ = 2w(i,j)d(i,j)2 σ3 , where P = D−1W. 7: n= 0 8: while |σn+1 −σn|≥ ϵdo 9: n←n+ 1 10: σn+1 = σn −gu(σn) g′u(σn) 11: σu = σn+1 12: σl = maxu{σu |σu <σ0}, σh = minu{σu |σu >σ0} 13: return [σl,σh]. implies existence of an efﬁcient algorithm with generalization guarantees in this setting. However, the pseudodimen- sion is shown to be Ω(n) for the weighted graph setting, and therefore the smoothness assumptions are necessary for learning over the algorithm family. Both these bounds are shown to be tight up to constant factors (Section 4.3.1). We establish uniform convergence guarantees in Section 4.3.2. For the unweighted graph setting, our pseudodimension bounds are sufﬁcient for uniform convergence. We resort to bounding the Rademacher complexity in the weighted graph setting which allows us to prove distribution dependent generalization guarantees, that hold under distributional niceness assumptions of Section 4.2.1 (unlike pseudodimension which gives generalization guarantees that are worst- case over the distribution). These guarantees are derived by extending our results in the dispersion setting, and follow under the same perturbation invariance assumptions. Additional proofs and details from this section may be found in Appendix B. The online learning results above only work for smoothed but adversarial instances, while the distributional learning sample complexity results based on pseudodimension work for any types (no smoothness needed) of independent and identically distributed instances. So these results are not superseded by the online learning results, the settings are strictly speaking incomparable, and the pseudodimension results in the distributional setting provide new upper and lower bounds for the problem. 4.3.1 Pseudodimension bounds We can efﬁciently learn the unweighted graph with polynomially many samples. We show this by providing a bound on the pseudodimension of the set of loss functions Hr = {lA(G(r),L,U) |0 ≤r <∞}, where G(r) is speciﬁed by Deﬁnition 1a. Our bounds hold for both the min-cut and quadratic objectives (Table 1). Theorem 15. The pseudo-dimension of Hr is O(log n), where nis the total number of (labeled and unlabeled) data points. Proof. There are at most (n 2 ) distinct distances between pairs of data points. As ris increased from 0 to inﬁnity, the graph changes only when r corresponds to one of these distances, and so at most (n 2 ) + 1 distinct graphs may be obtained. Thus given set Sof minstances (A(i),L(i)), we can partition the real line into O(mn2) intervals such that all values 13of rbehave identically for all instances within any ﬁxed interval. Since Aand therefore its loss is deterministic once Gis ﬁxed, the loss function is a piecewise constant with only O(n2) pieces. Each piece can have a witness above or below it as ris varied for the corresponding interval, and so the binary labeling of Sis ﬁxed in that interval. The pseudo-dimension msatisﬁes 2m ≤O(mn2) and is therefore O(log n). We can also show an asymptotically tight lower bound on the pseudodimension of Hr. We do this by presenting a collection of graph thresholds and well-designed labeling instances which are shattered by the thresholds. An intuitive simpliﬁed sketch for the proof is included below, for full details see Appendix B. Theorem 16. The pseudo-dimension of Hr is Ω(log n). Proof Sketch. We have three labeled nodes, a1 with label 0 and b1,b2 labeled 1, and n′ = O(n) unlabeled nodes U = {u1,...,u n′}. We can show that given a sequence {r1,...,r n′}of values of r, it is possible to construct an instance with suitable true labels of U such that the loss as a function of roscillates above and below some witness as rmoves along the sequence of intervals (ri,ri+1)i≥0. (a) G(ri)  (b) Loss oscillations in shattered instances Figure 2: Graphs G(r) as ris varied, for lower bound construction for pseudodimension of Hr. Labels are set in the instances to match bit ﬂips in the sequence of binary numbers as shown. At the initial threshold r0, all unlabeled points have a single incident edge, connecting to a1, so all predicted labels are 0. As the threshold is increased to ri, (the distances are set so that) ui gets connected to both nodes with label 1 and therefore its predicted label changes to 1. If the sequence of nodes ui is alternately labeled, the loss decreases and increases alternately as all the predicted labels turn to 1 as ris increased to rn′. This oscillation between a high and a low value can be achieved for any subsequence of distances r1,...,r n′, and a witness may be set as a loss value between the oscillation limits. By precisely choosing the subsequences so that the oscillations align with the bit ﬂips in the binary digit sequence, we can construct minstances which satisfy the 2m shattering constraints. For learning weighted graphs, we can show a Θ(n) bound on the pseudodimension of the set of loss functions. We show this for the loss functions for learning graphs with exponential kernels, Hσ = {lA(G(σ),L,U) |0 ≤σ <∞}, where G(σ) is speciﬁed by Deﬁnition 1c. For simplicity of presentation, the lower bound will be shown for the min-cut objective. Theorem 17. The pseudo-dimension of Hσ is O(n), where n is the total number of (labeled and unlabeled) data points. Proof. This bound trivially follows by noting that we have nvertices and therefore only 2n possible labelings in an instance. Thus, for mproblems, 2m ≤m2n gives m= O(n). Theorem 18. The pseudo-dimension of Hσ is Ω(n). Proof Sketch. We ﬁrst show that we can carefully set the distances between the examples so that we have at least2Ω(n) intervals of σsuch that G(σ) has distinct vertex sets as min-cuts in each interval. We start with a pair of labeled nodes of each class, and a pair of unlabeled nodes which may be assigned either label depending on σ. We then build the graph in N = (n−4)/2 stages, adding two new nodes at each step with meticulously chosen distances from existing nodes. Before adding the ith pair xi,yi of nodes, there will be 2i−1 intervals of σsuch that the intervals correspond to 14distinct min-cuts which result in all possible labelings of{x1,...,x i−1}. Moreover, yj will be labeled differently from xj in each of these intervals. The edges to the new nodes will ensure that the cuts that differ exactly in xi will divide each of these intervals giving us2i intervals where distinct mincuts give all labelings of{x1,...,x i}, and allowing an inductive proof. The challenge is that we only get to set O(i) edges in round ibut seek properties about 2i cuts, so we must do this carefully. The full construction and proof is presented in Appendix B. Finally, we use this construction to generate a set of Θ(n) instances that correspond to cost functions that oscillate in a manner that helps us pick 2Ω(n) values of σthat shatter the samples. 4.3.2 Uniform convergence Our results above implies a uniform convergence guarantee for the ofﬂine distributional setting, for both weighted and unweighted graph families. For the unweighted case, we can use the pseudodimension bounds from section 4.3.1, and for the weighted case we use dispersion guarantees from section 4.2. For either case it sufﬁces to bound the empirical Rademacher complexity. We will need the following theorem (slightly rephrased) from [7]. Theorem 19. Let F= {fρ : X→ [0,1],ρ ∈C⊂ Rd}be a parametereized family of functions, whereClies in a ball of radius R. For any set S= {xi,...,x T}⊆X , suppose the functions uxi(ρ) = fρ(xi) for i ∈[T] are piecewise L-Lipschitz and β-dispersed. Then ˆR(F,S) ≤O(min{ √ (d/T) logRT + LT−β, √ Pdim(F)/T}). Now, using classic results from learning theory, we can conclude that Empirical Risk Minimization has good general- ization under the distribution. Theorem 20. For both weighted and unweighted graph w(u,v) deﬁned above, with probability at least 1 −δ, the average loss on any sample x1,...,x T ∼ DT, the loss suffered w.r.t. to any parameter ρ ∈ Rd satisﬁes |1 T ∑T i=1 lρ(xi) −Ex∼Dlρ(x)|≤ O (√ dlog Tlog 1/δ T ) . 5 Extension to active learning So far we have considered an oracle which gives labeled and unlabeled points, where the labels of the unlabeled points possibly revealed later but it does not allow us to select a subset of the unlabeled points for which we obtain the labels. A natural question to ask is if we can further reduce the need for labels by active learning. The S 2 algorithm of [19] allows efﬁcient active learning given the graph, but we would like to learn the graph itself. We will present an algorithm which extends the approach of [40] to do data-driven active learning with a constant budgetLof labels in a general (non-realizable) setting. We have the same problem setup as before (Section 2), except we will now learn the set of labeled examples Lvia an active learning procedure A′. We set ℓ := |L|as the budget for the active learning which we will think of as a small constant. A′takes as input the graph G, the budget ℓand outputs a set of nodes Lwith |L|= ℓto query labels for. In our budgeted active learning setup we provide all the queries in a single batch, we consider sequential querying in the next section. Let A′(G,ℓ) denote the set Lof labeled examples obtained using A′. In addition, we assume we are given a set of initially labeled points L0 (not included in the budget) consisting of some example from each class of labels. This is needed for known graph-based active learning procedures to work [40], our results in the next section imply upper bounds on the size of L0 for a purely unsupervised initialization. As with the semi-supervised learning above, the effectiveness of the active learning also heavily depends on how well the graph captures the label similarity. We are interested in learning the graph parameter ρwhich minimizes the loss lA(G(ρ),L0∪A′(G(ρ),ℓ),U) where U is the set of nodes not selected for querying by A′. That is, we seek the graph that works best with the combined active semi-supervised procedure for predicting the labels (by learning over multiple problem instances). We will consider the harmonic objective minimization as the semi-supervised algorithmA. Several reasonable heuristics for A′may be considered, we will restrict our attention to Algorithm 5 which adapts the greedy algorithm of [40] to our budgeted setting. Our main contribution in this section is to prove dispersion for the sequence of loss functions, which implies learnability of the graph for the composite active semi-supervised procedure. The loss function is piecewise constant in the parameter ρ, with discontinuities corresponding to changes in algorithmic decisions in selecting the labeling set Land subsequent predictions for the unlabeled examples. The following theorem establishes this for polynomial kernels, extensions to other kernels may be made as before. Theorem 21. Let l1,...,l T : R →R denote an independent sequence of losses as a function of parameter ˜α, when the graph is created using a polynomial kernel w(u,v) = ( ˜d(u,v) + ˜α)d and labeled by Algorithm 5 followed by predicting the remaining labels by optimizing the quadratic objective ∑ u,vw(u,v)(f(u) −f(v))2. If ˜d(u,v) follows a κ-bounded distribution with a closed and bounded support, the sequence is 1 2 -dispersed. 15Algorithm 5 BUDGETED ACTIVE LEARNING (G,ℓ) 1: Input: Graph Gwith unlabeled nodes, initial labels L0, label budget ℓ. 2: Output: Nodes for label query L, |L|= ℓ. 3: Compute soft labeling f to minimize the harmonic objective fT(D−W)f using initial labels L0. 4: for each subset Sof unlabeled nodes, |S|= ℓdo 5: for each labeling LS ∈{0,1}ℓ of Sdo 6: Estimate probability of LS, pLS = Πs∈S[LS(s)fs + (1 −LS(s))(1 −fs)]. 7: Compute soft labels fLS, by minimizing the harmonic objective with labels LS for S(in addition to L0). 8: Estimate uncertainty as uLS = ∑ i 1 2 −|1 2 −fLS i |. 9: Return arg minSuS where uS := ∑ LS pLSuLS. (a) MNIST  (b) Omniglot  (c) CIFAR-10 Figure 3: Loss for different unweighted graphs as a function of the threshold r. (a) MNIST  (b) Omniglot  (c) CIFAR-10 Figure 4: Loss for different weighted graphs as a function of the parameter σ. Proof Sketch. The proof reuses ideas from the proof of Theorem 11 with additional insights to handle the dependence of the loss functions on the budgeted active learning procedure. Discontinuities in the loss function may only occur when the label set from the active procedure changes (which only happens if uS = uT for some candidate subsets S ̸= T in Algorithm 5) or when the semi-supervised prediction changes ( f(u) = 1/2 for the soft labeling using the full labeled set after active learning). We show that both kinds of discontinuities lie along roots of polynomials with bounded discontinuities and are therefore dispersed. Detailed argument appears in Appendix C We remark that while we have only considered learning the graph family with the active learning procedure of [40], but it would be interesting to extend our results to other similar active learning algorithms [26, 36]. 6 Experiments In this section we evaluate the performance of our learning procedures when ﬁnding application-speciﬁc semi- supervised learning algorithms (i.e. graph parameters). Our experiments demonstrate that the best parameter for different applications varies greatly, and that the techniques presented in this paper can lead to large gains. We will look at image classiﬁcation based on standard pixel embedding for different datasets. 16(a) MNIST  (b) Omniglot  (c) CIFAR-10 Figure 5: Comparing different subsets of the same problem. (a) MNIST  (b) Omniglot  (c) CIFAR-10 Figure 6: Average regret vs. T for online learning of parameter σ Setup: We consider the task of semi-supervised classﬁcation on image datasets. We restrict our attention to binary classiﬁcation and pick two classes for each data set. We then draw random subsets of the dataset (with class restriction) of size n = 100 and randomly select L(10 ≤L ≤20) examples for labeling. For any data subset S, we measure distance between any pairs of images using theL2 distance between their pixel intensities. We would like to determine data-speciﬁc parameters rand σwhich lead to good weighted and unweighted graphs for semi-supervised learning on the datasets. We will optimize the harmonic function objective (Table 1) and round the fractional labelsf to make our predictions. Data sets: We use three popular benchmark datasets — MNIST, Omniglot and CIFAR-10. The MNIST dataset [25] contains images of hand-written digits from 0 to 9 as 28 ×28 binary images, with 5000 training examples for each class. We consider examples with labels 0 or 1. We generate a random semi-supervised learning instance from this data by sampling 100 random examples and further sampling L= 10 random examples from the subset for labeling. Omniglot [24] has 105 ×105 binary images of handwritten characters across 30 alphabets with 19,280 examples. We consider the task of distinguishing alphabets 0 and 1, and set L = 20 in this setting. CIFAR-10 [32] has 32 ×32 color images (an integer value in [0,255] for each of three colors) for object recognition among 10 classes. Again we consider objects 0 and 1 and set L= 20. Results and discussion: For the MNIST dataset we get optimal parameters with near-perfect classiﬁcation even with small values of L, while the error of the optimal parameter is ∼0.2 −0.3 even with larger values of L, indicating differences in the inherent difﬁculties of the classiﬁcation tasks (like label noise and how well separated the classes are). We examine the full variation of performance of graph-based semi-supervised learning for all possible graphs G(r) (rmin <r<r max) and G(σ) for σ∈[0,10] (Figures 3, 4). The losses are piecewise constant and can have large discontinuities in some cases. The optimal parameter values vary with the dataset, but we observe at least 10% gap in performance between optimal and suboptimal values within the same dataset. Another interesting observation is the variation of optima across subsets, indicating transductively optimal parameters may not generalize well. We plot the variation of loss with graph parameter σ for several subsets of the same size N = 100 for MNIST and Omniglot datasets in Figure 5. In MNIST we have two optimal ranges in most subsets but only one shared optimum (around σ = 2) across different subsets. This indicates that local search based techniques that estimate the optimal parameter values on a given data instance may lead to very poor performance on unseen instances. The CIFAR-10 example further shows that the optimal algorithm may not be easy to empirically discern. 17We also implement our online algorithms and compute the average regret (i.e. excess error in predicting labels of unlabeled examples over the best parameter in hindsight) for ﬁnding the optimal graph parameter σ for the different datasets. To obtain smooth curves we plot the average over 50 iterations for learning from 50 problem instances each (T = 50 , Figure 6). We observe fast convergence to the optimal parameter regret for all the datasets considered. The starting part of these curves ( T = 0) indicates regret for randomly setting the graph parameters, averaged over iterations, which is strongly outperformed by our learning algorithms as they learn from problem instances. 7 Acknowledgments This material is based on work supported by the National Science Foundation under grants CCF-1535967, CCF- 1910321, IIS-1618714, IIS-1901403, and SES-1919453; the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003; an AWS Machine Learning Research Award; an Amazon Research Award; a Bloomberg Research Grant; a Microsoft Research Faculty Fellowship. The views expressed in this work do not necessarily reﬂect the position or the policy of the Government and no ofﬁcial endorsement should be inferred. References [1] Doug Altner and Ozlem Ergun. Rapidly solving an online sequence of maximum ﬂow problems. Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems (L. Michel, ed.),(Spain), pages 283–287, 2008. [2] Maria-Florina Balcan. Book chapter Data-Driven Algorithm Design. In Beyond Worst Case Analysis of Algo- rithms, T. Roughgarden (Ed). Cambridge University Press, 2020. [3] Maria-Florina Balcan and Avrim Blum. A discriminative model for semi-supervised learning. Journal of the ACM (JACM), 57(3):1–46, 2010. [4] Maria-Florina Balcan, Avrim Blum, Patrick Pakyan Choi, John Lafferty, Brian Pantano, Mugizi Robert Rweban- gira, and Xiaojin Zhu. Person identiﬁcation in webcam images: An application of semi-supervised learning. In ICML 2005 Workshop on Learning with Partially Classiﬁed Training Data, volume 2, page 6, 2005. [5] Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. Learning-theoretic foundations of algorithm conﬁguration for combinatorial partitioning problems. In Conference on Learning Theory , pages 213–274. PMLR, 2017. [6] Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch. In International conference on machine learning, pages 344–353. PMLR, 2018. [7] Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm design, online learning, and private optimization. In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS), pages 603–614. IEEE, 2018. [8] Maria-Florina Balcan, Travis Dick, and Manuel Lang. Learning to link. In International Conference on Learning Representations, 2019. [9] Maria-Florina Balcan, Avrim Blum, Dravyansh Sharma, and Hongyang Zhang. On the power of abstention and data-driven decision making for adversarial robustness. arXiv preprint arXiv:2010.06154, 2020. [10] Maria-Florina Balcan, Travis Dick, and Wesley Pegden. Semi-bandit optimization in the dispersed setting. In Conference on Uncertainty in Artiﬁcial Intelligence, pages 909–918. PMLR, 2020. [11] Maria-Florina Balcan, Travis Dick, and Dravyansh Sharma. Learning piecewise Lipschitz functions in changing environments. In International Conference on Artiﬁcial Intelligence and Statistics , pages 3567–3577. PMLR, 2020. [12] Maria-Florina F Balcan, Travis Dick, and Colin White. Data-driven clustering via parameterized lloyd’s families. Advances in Neural Information Processing Systems, 31:10641–10651, 2018. [13] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002. [14] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of machine learning research, 7(Nov):2399–2434, 2006. 18[15] Avrim Blum. Technical perspective: Algorithm selection as a learning problem. Communications of the ACM, 63(6):86–86, 2020. [16] Avrim Blum and Shuchi Chawla. Learning from labeled and unlabeled data using graph mincuts. In ICML, 2001. [17] Avrim Blum, John Lafferty, Mugizi Robert Rwebangira, and Rajashekar Reddy. Semi-supervised learning using randomized mincuts. In Proceedings of the twenty-ﬁrst international conference on Machine learning, page 13, 2004. [18] Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning . The MIT Press, 1st edition, 2010. ISBN 0262514125. [19] Gautam Dasarathy, Robert Nowak, and Xiaojin Zhu. S2: An efﬁcient graph based active learning algorithm with application to nonparametric classiﬁcation. In Conference on Learning Theory, pages 503–522, 2015. [20] Olivier Delalleau, Yoshua Bengio, and Nicolas Le Roux. Efﬁcient non-parametric function induction in semi- supervised learning. In AISTATS, volume 27, page 100, 2005. [21] Matthew England and James H Davenport. The complexity of cylindrical algebraic decomposition with respect to polynomial degree. In International Workshop on Computer Algebra in Scientiﬁc Computing, pages 172–192. Springer, 2016. [22] Bryan R Gibson, Timothy T Rogers, and Xiaojin Zhu. Human semi-supervised learning. Topics in cognitive science, 5(1):132–172, 2013. [23] Rishi Gupta and Tim Roughgarden. A PAC approach to application-speciﬁc algorithm selection. SIAM Journal on Computing, 46(3):992–1017, 2017. [24] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332–1338, 2015. [25] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [26] Jun Long, Jianping Yin, Wentao Zhao, and En Zhu. Graph-based active learning based on label propagation. In International Conference on Modeling Decisions for Artiﬁcial Intelligence, pages 179–190. Springer, 2008. [27] David Pollard. Convergence of stochastic processes. Springer Science & Business Media, 2012. [28] Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and trends in Machine Learning, 4(2):107–194, 2011. [29] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on pattern analysis and machine intelligence, 22(8):888–905, 2000. [30] Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin. Beyond the point cloud: from transductive to semi- supervised learning. In Proceedings of the 22nd international conference on Machine learning, pages 824–831, 2005. [31] Daniel A Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time. Journal of the ACM (JACM), 51(3):385–463, 2004. [32] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9, 2015. [33] Ellen Vitercik, Maria-Florina Balcan, and Tuomas Sandholm. Estimating approximate incentive compatibility. In ACM Conference on Economics and Computation, 2019. [34] Zhu Xiaojin and Ghahramani Zoubin. Learning from labeled and unlabeled data with label propagation. Tech. Rep., Technical Report CMU-CALD-02–107, Carnegie Mellon University, 2002. [35] Richard Zemel and Miguel Carreira-Perpi ˜n´an. Proximity graphs for clustering and manifold learning. Advances in neural information processing systems, 17:225–232, 2004. 19[36] Wentao Zhao, Jun Long, En Zhu, and Yun Liu. A scalable algorithm for graph-based active learning. Frontiers in Algorithmics, page 311, 2008. [37] Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Sch ¨olkopf. Learning with local and global consistency. Advances in neural information processing systems, 16(16):321–328, 2004. [38] Xiaojin Zhu and Andrew B Goldberg. Introduction to semi-supervised learning. Synthesis lectures on artiﬁcial intelligence and machine learning, 3(1):1–130, 2009. [39] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using Gaussian ﬁelds and harmonic functions. In Proceedings of the 20th International conference on Machine learning (ICML-03), pages 912–919, 2003. [40] Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani. Combining active learning and semi-supervised learn- ing using gaussian ﬁelds and harmonic functions. In ICML 2003 workshop on the continuum from labeled to unlabeled data in machine learning and data mining, page V ol. 3, 2003. [41] Xiaojin Zhu, John Lafferty, and Ronald Rosenfeld. Semi-supervised learning with graphs. PhD thesis, Carnegie Mellon University, Language Technologies Institute, 2005. [42] Xiaojin Zhu, Timothy Rogers, Ruichen Qian, and Chuck Kalish. Humans perform semi-supervised classiﬁcation too. In AAAI, volume 2007, pages 864–870, 2007. 20Appendix A Dispersion and Online learning In this appendix we include details of proofs and algorithms from section 4.2. A.1 A general tool for analyzing dispersion If the weights of the graph are given by a polynomial kernel w(u,v) = ( ˜d(u,v) + ˜α)d, we can apply the general tool developed by Balcan et al. [10] to learn ˜α, which we summarize below. 1. Bound the probability density of the random set of discontinuities of the loss functions. 2. Use a VC-dimension based uniform convergence argument to transform this into a bound on the dispersion of the loss functions. Formally, we have the following theorems from [10], which show how to use this technique when the discontinuities are roots of a random polynomial. Theorem 22 ([10]). Consider a random degree dpolynomial φwith leading coefﬁcient 1 and subsequent coefﬁcients which are real of absolute value at mostR, whose joint density is at mostκ. There is an absolute constantKdepending only on dand Rsuch that every interval I of length ≤ϵsatisﬁes Pr(φhas a root in I) ≤κϵ/K. Theorem 23 ([10]). Let l1,...,l T : R →R be independent piecewise L-Lipschitz functions, each having at most K discontinuities. Let D(T,ϵ,ρ ) = |{1 ≤t ≤T |lt is not L-Lipschitz on [ρ−ϵ,ρ + ϵ]}|be the number of functions that are not L-Lipschitz on the ball [ρ−ϵ,ρ + ϵ]. Then we have E[maxρ∈R D(T,ϵ,ρ )] ≤maxρ∈R E[D(T,ϵ,ρ )] + O( √ Tlog(TK)). We will now use Theorems 22 and 23 to establish dispersion in our setting. We ﬁrst need a simple lemma about κ-bounded distributions. We remark that similar properties have been proved in [7, 10], in other problem contexts. Speciﬁcally, [7] show the lemma for a ratio of random variables, Z = X/Y, and [10] establish it for the sum Z = X+ Y but for independent variables X,Y . Lemma 24. Suppose Xand Y are real-valued random variables taking values in[m,m + M] and [m′,m′+ M′] for some m,m′,M,M ′∈R+ and suppose that their joint distribution is κ-bounded. Then, (i) Z = X+ Y is drawn from a K1κ-bounded distribution, where K1 ≤min{M,M ′}. (ii) Z = XY is drawn from a K2κ-bounded distribution, where K2 ≤min{M/m,M ′/m′}. Proof. Let fX,Y(x,y) denote the joint density of X,Y . (i) The case where X,Y are independent has been studied (Lemma 25 in [10]), the following is slightly more involved. The cumulative density function for Zis given by FZ(z) = Pr(Z ≤z) = Pr(X+ Y ≤z) = Pr(X ≤z−Y) = ∫ m′+M′ m′ ∫ z−y m fX,Y(x,y)dxdy. The density function for Zcan be obtained using Leibniz’s rule as fZ(z) = d dzFZ(z) = d dz ∫ m′+M′ m′ ∫ z−y m fX,Y(x,y)dxdy = ∫ m′+M′ m′ (d dz ∫ z−y m fX,Y(x,y)dx ) dy = ∫ m′+M′ m′ fX,Y(z−y,y)dy ≤M′κ. A symmetric argument shows that fZ(z) ≤Mκ, together with above this completes the proof. 21(ii) The cumulative density function for Zis given by FZ(z) = Pr(Z ≤z) = Pr(XY ≤z) = Pr(X ≤z/Y) = ∫ m′+M′ m′ ∫ z/y m fX,Y(x,y)dxdy. The density function for Zcan be obtained using Leibniz’s rule as fZ(z) = d dzFZ(z) = d dz ∫ m′+M′ m′ ∫ z/y m fX,Y(x,y)dxdy = ∫ m′+M′ m′ ( d dz ∫ z/y m fX,Y(x,y)dx ) dy = ∫ m′+M′ m′ 1 yfX,Y(z/y,y)dy ≤ ∫ m′+M′ m′ 1 m′fX,Y(z/y,y)dy ≤M′ m′κ. Similarly we can show that fZ(z) ≤Mκ/m, together with above this completes the proof. Theorem 11. Let l1,...,l T : R →R denote an independent sequence of losses as a function of parameter ˜α, when the graph is created using a polynomial kernel w(u,v) = ( ˜d(u,v) + ˜α)d and labeled by optimizing the quadratic objective ∑ u,vw(u,v)(f(u)−f(v))2. If ˜d(u,v) follows a κ-bounded distribution with a closed and bounded support, the sequence is 1 2 -dispersed, and the regret of Algorithm 1 may be upper bounded by ˜O( √ T). Proof. w(u,v) is a polynomial in ˜αof degree dwith coefﬁcient of ˜αi given by ci = Dd,i˜d(u,v)Ed,i for i∈[d]. Since the support of ˜d(u,v) is closed and bounded, we have m≤˜d(u,v) ≤M with probability 1 for some M >1,m> 0 (since ˜d(u,v) is a metric, ˜d(u,v) >0 for u̸= v). To apply Theorem 22, we note that we have an upper bound on the coefﬁcients, R <(dM)d. Moreover, if f(x) denotes the probability density of d(u,v) and F(x) its cumulative density, Pr(ci ≤xi) = Pr ( Dd,i˜d(u,v)Ed,i ≤xi ) = Pr ( ˜d(u,v) ≤ ( xi Dd,i )1/Ed,i ) = F (( xi Dd,i )1/Ed,i ) . Thus, Pr(ci ≤xi for each i∈[d]) = F ( min i ( xi Dd,i )1/Ed,i ) . The joint density of the coefﬁcients is therefore Kκ-bounded where K only depends on d,m. ( K ≤ maxiDd,i −1/Ed,im−1+1/Ed,i). Consider the harmonic solution of the quadratic objective [39] which is given byfU = (DUU −WUU)−1WULfL. For any u∈U, f(u) = 1/2 is a polynomial equation in ˜αwith degree at most nd. The coefﬁcients of these polynomials are formed by multiplying sets of weights w(u,v) of size up to n and adding the products, and are also bounded density on a bounded support (using above observation in conjunction with Lemma 24). The dispersion result now follows by an application of Theorems 22 and 23. The regret bound is implied by results from [7, 11]. A.2 Dispersion for roots of exponential polynomials In this section we will extend the applicability of the dispersion analysis technique from Appendix A.1 to exponential polynomials, i.e. functions of the form φ(x) = ∑n i=1 aiebix. We will now extend the analysis to obtain similar results when using the exponential kernel w(u,v) = e−||u−v||2/σ2 . The results of Balcan et al. [10] no longer directly apply as the points of discontinuity are no longer roots of polynomials. To this end, we extend and generalize arguments from [10] below. We need to generalize Theorem 22 to exponential polynomials below. 22Theorem 25. Let φ(x) = ∑n i=1 aiebix be a random function, such that coefﬁcients ai are real and of magnitude at most R, and distributed with joint density at most κ. Then for any interval I of width at most ϵ, P(φhas a zero in I)≤ ˜O(ϵ) (dependence on bi,n,κ,R suppressed). Proof. For n = 1 there are no roots, so assume n > 1. Suppose ρ is a root of φ(x). Then a = ( a1,...,a n) is orthogonal to ϱ(ρ) = ( eb1ρ,...,e bnρ) in Rn. For a ﬁxed ρ, the set Sρ of coefﬁcients a for which ρ is a root of φ(y) lie along an n−1 dimensional linear subspace of Rn. Now φ has a root in any interval I of length ϵ, exactly when the coefﬁcients lie on Sρ for some ρ ∈ I. The desired probability is therefore upper bounded by maxρVOL(∪Sy |y ∈[ρ−ϵ,ρ + ϵ])/VOL(Sy |y ∈R) which we will show to be ˜O(ϵ). The key idea is that if |ρ−ρ′|< ϵ, then ϱ(ρ) and ϱ(ρ′) are within a small angle θρ,ρ′ = ˜O(ϵ) for small ϵ(the probability bound is vacuous for large ϵ). But any point in Sρ is at most ˜O(θρ,ρ′) from a point in Sρ′, which implies the desired bound (similar arguments to Theorem 22). We will now ﬂesh out the above sketch. Indeed, sin θρ,ρ′ = √ 1 −(⟨ϱ(ρ),ϱ(ρ′)⟩)2 ∥ϱ(ρ)∥∥ϱ(ρ′)∥ = √ 1 − (∑ iebiρebiρ′ )2 ∑ ie2biρ∑ ie2biρ′ = √∑ i̸=je2(biρ+bjρ′) −e(bi+bj)(ρ+ρ′) ∑ ie2biρ∑ ie2biρ′ . Now, for ρ′= ρ+ ε, |ε|<ϵ, sin θρ,ρ′ = √∑ i̸=je2(biρ+bjρ+bjε) −e(bi+bj)(2ρ+ε) ∑ ie2biρ∑ ie2biρ′ = √∑ i̸=je2ρ(bi+bj)(e2bjε −e(bi+bj)ε)∑ ie2biρ∑ ie2biρ′ . Using the Taylor’s series approximation fore2bjεand e(bi+bj)ε, we note that the largest terms that survive are quadratic in ε. sin θρ,ρ′, and therefore also θρ,ρ′, is ˜O(ϵ). Next it is easy to show that any point in Sρ is at most ˜O(θρ,ρ′) from a point in Sρ′. For n = 2, Sρ and Sρ′ are along lines orthogonal to ρand ρ′and are thus themselves at an angle θρ,ρ′. Since we further assume that the coefﬁcients are bounded by R, any point on Sρ is within O(Rθρ,ρ′) = ˜O(θρ,ρ′) of the nearest point in Sρ′. For n >2, consider the 3-space spanned by ρ, ρ′and an arbitrary ς ∈Sρ. Sρ and Sρ′ are along 2-planes in this space with normal vectors ρ,ρ′respectively. Again it is straightforward to see that the nearest point in the projection of Sρ′ to ς is ˜O(θρ,ρ′). The remaining proof is identical to that of Theorem 22 (see Theorem 18 of [10]), and is omitted for brevity. We will also need the following lemma for the second step noted above, i.e. obtain a result similar to Theorem 23 for exponential polynomials. Lemma 26. The equation ∑n i=1 aiebix = 0 where ai,bi ∈R has at most n−1 distinct solutions x∈R. Proof. We will use induction on n. It is easy to verify that there is no solution for n = 1. We assume the statement holds for all 1 ≤n≤N. Consider the equation φN+1(x) = ∑N+1 i=1 aiebix = 0. WLOG a1 ̸= 0 and we can write φN+1(x) = N+1∑ i=1 aiebix = a1eb1x ( 1 + N+1∑ i=2 ai a1 e(bi−b1)x ) = a1eb1x(1 + g(x)) . By our induction hypothesis, g′(0) = 0 has at most N −1 solutions, and so (1 + g(x))′has at most N −1 roots. By Rolle’s theorem,(1 + g(x)) has at most N roots, and therefore φN+1(x) = 0 has at most N solutions. Lemma 26 implies that Theorem 23 may be applied. The number of discontinuities may be exponentially high in this case. Indeed solving the quadratic objective can result in an exponential equation of the form in Lemma 26 with O(|U|n) terms. 23A.3 Learning several metrics simultaneously We start by getting a couple useful deﬁnitions out of the way. Deﬁnition 27 (Homogeneous algebraic hypersurface). An algebraic hypersurface is an algebraic variety (a system of polynomial equations) that may be deﬁned by a single implicit equation of the form p(x1,...,x n) = 0 , where pis a multivariate polynomial. The degree dof the algebraic hypersurface is the total degree of the polynomial p. We say that the algebraic hypersurface is homogeneous if pis a homogeneous polynomial, i.e. p(λx1,...,λx m) = λdp(x1,...,x n). In the following we will refer to homogeneous algebraic hypersurfaces as simply algebraic hypersurfaces. We will also need the standard deﬁnition of set shattering, which we restate in our context as follows. Deﬁnition 28 (Hitting and Shattering). Let Cdenote a set of curves in Rp. We say that a subset of Cis hit by a curve sif the subset is exactly the set of curves in Cwhich intersect the curve s. A collection of curves Sshatters the set Cif for each subset Cof C, there is some element sof Ssuch that shits C. To extend our learning results to learning graphs built from several metrics, we will now state and prove a couple theorems involving algebraic hypersurfaces. Our results generalize signiﬁcantly the techniques from [10] by bringing in novel connections with algebraic geometry. Theorem 6. There is a constantkdepending only on dand psuch that axis-aligned line segments inRpcannot shatter any collection of kalgebraic hypersurfaces of degree at most d. Proof. Let Cdenote a collection of k algebraic hypersurfaces of degree at most din Rp. We say that a subset of C is hit by a line segment if the subset is exactly the set of curves in Cwhich intersect the segment, and hit by a line if some segment of the line hits the subset. We seek to upper bound the number of subsets of Cwhich may be hit by axis-aligned line segments. We will ﬁrst consider shattering by line segments in a ﬁxed axial direction x. We can easily extend this to axis-aligned segments by noting they may hit only ptimes as many subsets. Let Lc be a line in the xdirection. The subsets of Cwhich may be hit by (segments along) Lc is determined by the pattern of intersections of Lc with hypersurfaces in C. By Bezout’s theorem, there are at most kd+ 1 distinct regions of Lc due to the intersections. Therefore at most (kd+1 2 ) distinct subsets may be hit. Deﬁne the equivalence relation Lc1 ∼Lc2 if the same hypersurfaces in Cintersect Lc1 and Lc2 , and in the same order (including with multiplicities). To determine these equivalence classes, we will project the hypersurfaces in Con to a hyperplane orthogonal to the x-direction. By the Tarski-Seidenberg-Łojasiewicz Theorem, we get a semi-algebraic collection Cx, i.e. a set of polynomial equations and constraints in the projection space. Each cell ofCx corresponds to an equivalence class. Using well-known upper bounds forcylindrical algebraic decomposition (see for example [21]), we get that the number of equivalence classes is at most O ( (2d)2p−1k2p−122p−1 ) . Putting it all together, the number of subsets hit by any axis aligned segment is at most O ( p (kd+ 1 2 ) (2d)2p−1k2p−122p−1 ) . We are done as this is less than 2k for ﬁxed dand pand large enough k, and therefore all subsets may not be hit. Theorem 7. Let l1,...,l T : Rp → R be independent piecewise L-Lipschitz functions, each having discontinu- ities speciﬁed by a collection of at most K algebraic hypersurfaces of bounded degree. Let L denote the set of axis-aligned paths between pairs of points in Rp, and for each s ∈ L deﬁne D(T,s) = |{1 ≤ t ≤ T | lt has a discontinuity along s}|. Then we have E[sups∈LD(T,s)] ≤sups∈LE[D(T,s)] + O( √ Tlog(TK)). Proof. The proof is similar to that of Theorem 23 (see [10]). The main difference is that instead of relating the number of ways intervals can label vectors of discontinuity points to the VC-dimension of intervals, we instead relate the number of ways line segments can label vectors ofKalgebraic hypersurfaces of degree dto the VC-dimension of line segments (when labeling algebraic hypersurfaces), which from Theorem 6 is constant. To verify dispersion, we need a uniform-convergence bound on the number of Lipschitz failures between the worst pair of pointsα,α′at distance ≤ϵ, but the deﬁnition allows us to bound the worst rate of discontinuties along any path between α,α′of our choice. We can bound the VC dimension of axis aligned segments against bounded-degree algebraic hypersurfaces, which will allow us to establish dispersion by considering piecewise axis-aligned paths between points αand α′. 24Let Cdenote the set of all algebraic hypersurfaces of degree d. For simplicity, we assume that every function has its discontinuities speciﬁed by a collection of exactly K algebraic hypersurfaces. For each function lt, let γ(t) ∈CK denote the ordered tuple of algebraic hypersurfaces in Cwhose entries are the discontinuity locations of lt. That is, lt has discontinuities along (γ(t) 1 ,...,γ (t) K ), but is otherwise L-Lispchitz. For any axis aligned path s, deﬁne the function fs : CK →{0,1}by fs(γ) = {1 if for some i∈[K] γi intersects s 0 otherwise, where γ = (γ1,...,γ K) ∈CK. The sum ∑T t=1 fs(γ(t)) counts the number of vectors (γ(t) 1 ,...,γ (t) K ) that intersect s or, equivalently, the number of functions l1,...,l T that are not L-Lipschitz on s. We will apply VC-dimension uniform convergence arguments to the class F= {fs : CK →{0,1}| sis an axis-aligned path}. In particular, we will show that for an independent set of vectors (γ(t) 1 ,...,γ (t) K ), with high probability we have that 1 T ∑T t=1 fs(γ(t)) is close to E[ 1 T ∑T t=1 fs(γ(t))] for all paths s. This uniform convergence argument will lead to the desired bounds. Indeed, Theorem 6 implies that VC dimension of Fis O(log K). Now standard VC-dimension uniform convergence arguments for the class Fimply that with probability at least 1 −δ, for all fs ∈F ⏐⏐⏐⏐⏐ 1 T T∑ t=1 fs(γ(t)) −E [ 1 T T∑ t=1 fs(γ(t)) ]⏐⏐⏐⏐⏐≤O (√ log(K/δ) T ) , or ⏐⏐⏐⏐⏐ T∑ t=1 fs(γ(t)) −E [ T∑ t=1 fs(γ(t)) ]⏐⏐⏐⏐⏐≤O (√ Tlog(K/δ) ) . Now since D(T,s) = ∑T t=1 fs(γ(t)), we have for all s and δ, with probability at least 1 −δ, sups∈LD(T,s) ≤ sups∈LE[D(T,s)] + O( √ Tlog(K/δ)). Taking expectation and setting δ = 1/ √ T completes the proof as it allows us to bound the expected discontinuities by O( √ T) when the above high probability event fails. Theorem 7 above generalizes the second step of the dispersion tool from single parameter families to several hyperpa- rameters, and uses Theorem 6 as a key ingredient. To complete the ﬁrst step of in the multi-parameter setting, we can use a simple generalization of Theorem 22 by showing that few zeros are likely to occur on a piecewise axis-aligned path on whose pieces the zero sets of the multivariate polynomial is the zero set of a single-variable polynomial. Putting together we get Theorem 12. Theorem 12. Let l1,...,l T : Rp →R denote an independent sequence of losses as a function of parameters ρi,i ∈ [p], when the graph is created using a polynomial kernelw(u,v) = (∑p−1 i=1 ρi˜d(u,v)+ ρp)d and labeled by optimizing the quadratic objective ∑ u,vw(u,v)(f(u) −f(v))2. If ˜d(u,v) follows a κ-bounded distribution with a closed and bounded support, the sequence is 1 2 -dispersed, and the regret may be upper bounded by ˜O( √ T). Proof. Notice that w(u,v) is a homogeneous polynomial in ρ = (ρi,i ∈[p]). Further, the solutions of the quadratic objective subject to f(u) = 1/2 for some uare also homogeneous polynomial equations, of degree nd. Now to show dispersion, consider an axis-aligned path between any two parameter vectors ρ,ρ′ such that ∥ρ−ρ′∥< ϵ(notice that the deﬁnition of dispersion allows us to use any path between ρ,ρ′for counting discontinuities). To compute the expected number of non-Lipchitzness in along this path, notice that for any ﬁxed segment of this path, all but one variable are constant and the discontinuities are the zeros of single variable polynomial with bounded-density random coefﬁcients, and that Theorem 22 applies. Summing along these paths we get at most ˜O(pϵ) discontinuities in expectation for any ∥ρ−ρ′∥<ϵ. Theorem 7 now completes the proof of dispersion in this case. A.4 Semi-bandit efﬁcient algorithms In this appendix we present details of the efﬁcient algorithms for computing the semi-bandit feedback sets in Algorithm 2. For unweighted graphs, we only have a polynomial numberO(n2) of feedback sets and the feedback set for a given ρt is readily computed by looking up a sorted list of distances d(u,v)u,v∈Li∪Ui. For the weighted graph setting, we need non-trivial algorithms as discussed in Section 4.2.3. 25A.4.1 Min-cut objective First some notation for this section. We will use G= (V,E) to denote an undirected graph with V as the set of nodes and E ⊆V ×V the weighted edges with capacity d: E →R≥0. We are given special nodes s,t ∈V called source and target vertices. Recall the following deﬁnitions. Deﬁnition 29. (s,t)-ﬂows An (s,t)-ﬂow (or just ﬂow if the source and target are clear from context) is a function f : V ×V → R≥0 that satisﬁes the conservation constraint at every vertex v except possibly s and t given by∑ (u,v)∈Ef(u,v) = ∑ (v,u)∈Ef(v,u). The value of ﬂow (also refered by just ﬂow when clear from context) is the total ﬂow out of s, ∑ u∈V f(s,u) −∑ u∈V f(u,s). Deﬁnition 30. (s,t)-cut An (s,t)-cut (or just a cut if the source and target are clear from context) is a partition of V into S,T such that s∈S,t ∈T. We will denote the set {(u,v) ∈E |u∈S,v ∈T}of edges in the cut by ∂S or ∂T. The capacity of the cut is the total capacity of edges in the cut. For convenience we also deﬁne Deﬁnition 31. Path ﬂow. An (s,t)-ﬂow is a path ﬂow along a path p = (s = v0,v1,...,v n = t) if f(u,w) > 0 iff (u,w) = (vi,vi+1) for some i∈[n−1]. Deﬁnition 32. Residual capacity graph. Given a set of path ﬂows F, the residual capacity graph (or simply the residual graph) is the graph G′= (V,E) with capacities given by c′(e) = c(e) −∑ f∈F f(e). We will list without proof some well-known facts about maximum ﬂows and minimum cuts in a graph which will be useful in our arguments. Fact. 1. Let f be any feasible (s,t)-ﬂow, and let (S,T) be any (s,t)-cut. The value of f is at most the capacity of (S,T). Moreover, the value of f equals the capacity of (S,T) if and only if f saturates every edge in the cut. 2. Max-ﬂow min-cut theorem. The value of maximum (value of) (s,t)-ﬂow equals the capacity of the minimum (s,t)- cut. It may be computed in O(VE) time. 3. Flow Decomposition Theorem. Every feasible (s,t)-ﬂow f can be written as a weighted sum of directed (s,t)- paths and directed cycles. Moreover, a directed edge (u,v) appears in at least one of these paths or cycles if and only if f(u,v) >0, and the total number of paths and cycles is at most the number of edges in the network. It may be computed in O(VE) time. We now have the machinery to prove the correctness and analyze the time complexity of our Algorithm 3. Theorem 14. For the harmonic function objective (Table 1) and exponential kernel (Deﬁnition 1c), Algorithm 4 outputs the interval containingσwithin accuracy ϵin time O(n4K1(ϵ)), where K1(ϵ) is the complexity of convergence for Newton’s method (K1(ϵ) = O(log log 1 ϵ) under standard assumptions). Proof. First, we brieﬂy recall the set up of the mincut objective. LetL1 and L2 denote the labeled pointsLof different classes. To obtain the labels for U, we seek the smallest cut (V1,V \\V1) of Gseparating the nodes in L1 and L2. To frame as s,t-cut we can augment the data graph with nodes s,t, and add inﬁnite capacity edges to nodes in L1 and L2 respectively. If Li ⊆V1, label exactly the nodes in V1 with label i. The loss function, l(σ) gives the fraction of labels this procedure gets right for the unlabeled set U. If the min-cut is the same for two values of σ, then so is prediction on each point and thus the loss function l(σ). So we seek the smallest amount of change in σso that the mincut changes. Our semi-bandit feedback set is given by the intervals for which the min-cut is ﬁxed. Consider a ﬁxed value of σ= σ0 and the corresponding graph G(σ0). We can compute the max-ﬂow on G(σ0), and simultaneously obtain a min-cut (V1,V \\V1) in time O(VE) = O(n3). All the edges in ∂V1 are saturated by the ﬂow. Obtain the ﬂow decomposition of the max-ﬂow (again O(VE) = O(n3)). For each ei ∈∂V1, let fi be a path ﬂow through ei from the ﬂow decomposition (cycle ﬂows cannot saturate, or even pass through, ei since it is on the min-cut). Note that the fi are distinct due to the max-ﬂow min-cut theorem. Now as σis increased, we increment each fi by the additional capacity in the corresponding edge ei, until an edge e′in E\\∂V1 saturates (at a faster rate than the ﬂow through it). This can be detected by expressing fi as a function of σfor each fi and computing the zero of an exponential polynomial capturing the change in residual capacity of any edge e /∈∂V1. Let fj be one of the path ﬂows through e′. We reassign this ﬂow to e′(it will now increase with e′as its bottleneck) and ﬁnd an alternate path avoiding this edge through non-saturated edges and ej (if one exists) along which we send the new fj. We now increment all the path ﬂows as before keeping their bottleneck edges saturated. The procedure stops when we can no longer ﬁnd an alternate path for some ej. But this means we must have a new cut with the saturated edges, and therefore a new min-cut. This gives us a new critical value ofσ, and the desired upper end for the 26feedback interval. Obtaining the lower end is possible by a symmetric procedure that decreases the path ﬂows while keeping edges saturated. We remark that our procedure differs from the well-known algorithms for obtaining min-cuts in a static graph. The greedy procedures for static graphs need directed edges (u,v) and (v,u) in the residual graph, and ﬁnd paths through unsaturated edges through this graph to increase the ﬂow, and cannot work with monotonically increasing path ﬂows. We however start with a max ﬂow and maintain the invariant that our ﬂow equals some cut size throughout. Finally note that each time we perform step 9 of the algorithm, a new saturated edge stays saturated for all further σ until the new cut is found. So we can do this at most O(n2) times. In each loop we need to obtain the saturation condition for O(n) edges corresponding to one new path ﬂow. We remind the reader that a remarkable property of ﬁnding the min-cuts dynamically in our setting is an interesting “hybrid” combinatorial and continuous set-up, which may be of independent interest. A similar dynamic, but purely combinatorial, setting for recomputing ﬂows efﬁciently online over a discrete graph sequence has been studied in [1]. B Distributional setting In this appendix we include details of proofs and algorithms from section 4.3. Recall that we deﬁne the set of loss functions Hr = {lA(G(r),L,U) |0 ≤r <∞}, where G(r) is the family of threshold graphs speciﬁed by Deﬁnition 1a, and Hσ = {lA(G(σ),L,U) |0 ≤σ <∞}, where G(σ) is the family of exponential kernel graphs speciﬁed by Deﬁnition 1c. We show lower bounds on the pseudodimension of these function classes below. Theorem 16. The pseudo-dimension of Hr is Ω(log n). We ﬁrst prove the following useful statement which helps us construct general examples with desirable properties. In particular, the following lemma guarantees that given a sequence of values ofrof size O(n), it is possible to construct an instance S of partially labeled points such that the cost of the output of algorithm A(G(r),L) on V as a function of r oscillates above and below some threshold as r moves along the sequence of intervals (ri,ri+1). Given this powerful guarantee, we can then pick appropriate sequences of rand generate a sample set of Ω(log n) instances that correspond to cost functions that oscillate in a manner that helps us pick Ω(n) values of rthat shatters the samples. Lemma 33. Given integer n >5 and a sequence of n′r’s such that1 < r1 < r2 <··· < rn′ <2 and n′≤n−5, there exists a real valued witness w >0 and a labeling instance S of partially labeled npoints, such that for 0 ≤ i ≤n′/2 −1, lA(G(r),L) < wfor r ∈(r2i,r2i+1), and lA(G(r),L) > wfor r ∈(r2i+1,r2i+2) (where r0 and rn′+1 correspond to immediate left and right neighborhoods respectively of r1 and rn′). Proof. We ﬁrst present a sketch of the construction. We will use binary labels aand b. We further have three points labeled a(namely a1,a2,a3) and two points labeled b(say b1,b2). At some intial r = r0, all the like-labeled points are connected in G(r0) and all the unlabeled points (namely u1,...,u n′) are connected to a1 as shown in Figure 7a. The algorithm A(G(r),L) labels everything aand gets exactly half the labels right. As r is increased to ri, ui gets connected to b1 and b2 (Figure 7b). If the sequenceuiis alternately labeled, the loss increases and decreases alternately as all the predicted labels turn to bas ris increased to rn′. Further increasing rmay connect all the unlabeled points with true label ato a2 and a3 (Figure 7c), although this is not crucial to our argument. The rest of the proof gives concrete values of rand veriﬁes that the construction is indeed feasible. We will ensure all the pairwise distances are between 1 and 2, so that triangle inequality is always satisﬁed. It may also be readily veriﬁed that O(log n) dimensions sufﬁce for our construction to exist. We start by deﬁning some useful constants. We pick r−,r+,rmax ∈(1,2) such that r−<r1 <··· <rn′ <r+ <rmax, r−= 1 + r1 2 , r+ = 1 + rn′ 2 , rmax = 1 + r+ 2 . We will now specify the distances of the labeled points. The points with the same label are close together and away from the oppositely labeled points. d(ai,aj) = r−, 1 ≤i<j ≤3, d(b1,b2) = r−, d(ai,bj) = rmax, 1 ≤i≤3,1 ≤j ≤2. 27a1a2 a3 un′ u2 u1 b1 b2 (a) G(r−) a1 a2 a3 un′ u1 b1 b2 ui (b) G(ri) a1 a2 a3 un′ u1 b1 b2 (c) G(r+) Figure 7: Graphs G(r) as ris varied, for lower bound construction for pseudodimension of Hr. Further, the unlabeled points are located as follows d(a1,uk) = r−, 1 ≤k≤n′, d(bi,uk) = rk, 1 ≤k≤n′,1 ≤i≤2, d(ai,uk) = r+, 1 ≤k≤n′,2 ≤i≤3, d(ui,uj) = rmax, 1 ≤i<j ≤n′. That is, all unknown points are closest to a1, followed by bi’s, remainingai’s and otherui’s in order. Further let the true labels of the unlabeled nodes be alternating with the index, i.e. uk is aif and only if kis even. We will now compute the loss for the soft labeling algorithm A(G(r),L) of [39] as rvaries from r−to r+, starting with r = r0 = r−. We note that our construction also works for other algorithms as well, for example the min-cut based approach of [16], but omit the details. For the graph G(r−), A(G,L) labels each unknown node as asince each unknown point is a leaf node connected to a1. Indeed if f(a1) = 1, the quadratic objective attains the minimum of 0 for exactlyf(uk) = 1 for each 1 ≤k≤n′. This results in half the labels in the dataset being incorrectly labeled since we stipulate that half the unknown labels are of each category. This results in loss lA(G(r−),L) =: lhigh say. Now as r is increased to r1, the edges (bi,u1), i = 1 ,2 are added with bi labeled as f(bi) = 0 . This results in a fractional label of 1 3 for f(u1) while f(uk) = 1 for k ̸= 1 . Indeed the terms involving f(u1) in the objective are (1 −f(u1))2 + 2f(u1)2, which is minimized at 1 3 . Since u1 has true label b, this results in a slightly smaller loss of lA(G(r1),L) =: llow. This happens when Auses rounding, or in expectation if Auses randomized prediction with probability f(u). At the next critical point r2, u2 gets connected to bi’s and gets incorrectly classiﬁed asb. This increases the loss again to lhigh. The loss function thus alternates asris varied through the speciﬁed values, betweenlhigh and llow. We therefore set the witness wto something in between. w= llow + lhigh 2 . Proof of Theorem 16. We will now use Lemma 33 to prove our lower bound. Arbitarily choose n′= n−5 (assumed to be a power of 2 just for convenient presentation) real numbers r[000...01] < r[000...10] < ··· < r[111...11] in (1,2). The indices are increasing binary numbers of length m= log n′. We create labeling instances using Lemma 33 which can be shattered by these rvalues. Instance Si = (Gi,Li) corresponds to ﬂuctuation of i-th bit bi in our rb sequence, where b = (b1,...,b m) ∈{0,1}m, i.e., we apply the lemma by using a subset of the rb values which correspond to the bit ﬂips in the i-th binary digit. For example, S1 just needs a single bit ﬂip (at r[100...00]). The lemma gives us both the instances and corresponding witnesses wi. This construction ensures sign(lA(Gi(rb),Li)−wi) = bi, i.e. the set of instances is shattered. Thus the pseudodimension is at least log(n−5) = Ω(log n). Theorem 18. The pseudo-dimension of Hσ is Ω(n). 28A B y1 x1 a1 a2 b1 b2 1.5 + ϵ 1.5 + ϵ 1.5 1.5 + 12 Nϵ 1.5 + 12Nϵ 1.5 + 12 Nϵ 1.5 1.5 + ϵ1.5 + ϵ Figure 8: The base case of our inductive construction. Proof. The plan for the proof is to ﬁrst construct a graph where the edge weights are carefully selected, so that we have 2N oscillations in the loss function with σ for N = Ω(n). Then we use this construction to create Θ(n) instances, each having a subset of the oscillations so that each interval leads to a unique labeling of the instances, for a total of 2N labelings, which would imply pseudodimension is Ω(n). We will present our discussion in terms of the min-cut objective, for simplicity of presentation. Graph construction: First a quick rough overview. We start with a pair of labeled nodes of each class, and a pair of unlabeled nodes which may be assigned either label depending on σ. We then build the graph in N = ( n−4)/2 stages, adding two new nodes at each step with carefully chosen distances from existing nodes. Before adding the ith pair xi,yi of nodes, there will be2i−1 intervals of σsuch that the intervals correspond to distinct min-cuts which result in all possible labelings of {x1,...,x i−1}. Moreover, yj will be labeled differently from xj in each of these intervals. The edges to the new nodes will ensure that the cuts that differ exactly inxiwill divide each of these intervals giving us 2i intervals where distinct mincuts give all labelings of {x1,...,x i}, and allowing an inductive proof. The challenge is that we only get to set O(i) edges but seek properties about 2i cuts, so we must do this carefully. Let ς = e−1/σ2 . Notice ς ∈(0,1), and bijectively corresponds to σ ∈(0,∞) (due to monotonicity) and therefore it sufﬁces to specify intervals of ς corresponding to different labelings. Further we can specify distances d(u,v) between pairs of nodes u,v by specifying the squared distance d(u,v)2. For the remainder of this proof we will refer to δ(u,v) = d(u,v)2 by distance and set values in [1.5,1.6]. Consequently, d(u,v) ∈(1.22,1.27) and therefore the triangle inequality is always satisﬁed. Notice that with this notation, the graph weights will be w(u,v) = ςδ(u,v). We now provide details of the construction. We have four labeled nodes as follows. a1,a2 are labeled 0 and are collectively denoted by A= {a1,a2}, similarly b1,b2 are labeled 1 and B = {b1,b2}. Note that edges between these nodes are on all or no cut separatingA,B, we set the distances to 1.6 and call this graphG0. We further add unlabeled nodes in pairs (xj,yj) in rounds 1 ≤j ≤N. In round i, we construct graph Gi by adding nodes (xi,yi) to Gi−1. The distances are set to ensure that for GN there are 2N unique values of ς corresponding to distinct min-cuts, each giving a unique labeling for {x1,...,x n}(and the complementary labeling for {y1,...,y n}). Moreover subsets of these points also obtain the unique labeling for {x1,...,x i}for each Gi. We set the distances in round 1 such that there are intervals I0 = (ς0,ς′ 0) ⊂(0,1) and I1 = (ς1,ς′ 1) ⊂(0,1) such that ς′ 0 <ς1 and (x1,y1) are labeled (l,1−l) in interval Il. In general, there will be 2i−1 intervals at the end of roundi−1, any interval I(i−1) will be split into disjoint intervals I(i) 0 ,I(i) 1 ⊂I(i−1) where labelings of {x1,...,x i−1}match that of I(i−1) and (xi,yi) are labeled (l,1 −l) in I(i) l . Now we set up the edges to achieve these properties. In round 1, we set the distances as follows. δ(x1,a1) = δ(y1,b2) = 1.5, δ(x1,a2) = δ(y1,b1) = δ(x1,y1) = 1.5 + 12Nϵ, δ(x1,b1) = δ(x1,b2) = δ(y1,a1) = δ(y1,a2) = 1.5 + ϵ. where ϵis a small positive quantity such that the largest distance1.5 + 12Nϵ< 1.6. It is straightforward to verify that for I0 = (0,1 2 1/ϵ ) we have that (x1,y1) are labeled (0,1) by determining the values of ς for which the corresponding cut is the min-cut (Figure 8). Indeed, we seek ς such that wC01 = w(x1,b1) + w(x1,b2) + w(x1,y1) + w(y1,a1) + 29Ab Bb Cb A B xi yi Figure 9: The inductive step in our lower bound construction for pseudodimension ofHσ. The min-cut Cb is extended to two new min-cuts (depicted by dashed lines) for which labels ofxi,yi are ﬂipped, at controlled parameter intervals. w(y1,a2) satisﬁes wC01 ≤wC00 = w(x1,b1) + w(x1,b2) + w(y1,b1) + w(y1,b2), wC01 ≤wC11 = w(x1,a1) + w(x1,a2) + w(y1,a1) + w(y1,a2), wC01 ≤wC10 = w(x1,a1) + w(x1,a2) + w(x1,y1) + w(y1,b1) + w(y1,b2), which simultaneously hold for ς <1 2 1/ϵ . Moreover, we can similarly conclude that (x1,y1) are labeled (1,0) for the interval I1 = (ς1,ς′ 1) where ς1 < ς′ 1 are given by the two positive roots of the equation 1 −2ςϵ + 2ς12Nϵ = 0. We now consider the inductive step, to set the distances and obtain an inductive proof of the claim above. In round i, the distances are as speciﬁed. δ(xi,a1) = δ(yi,b2) = 1.5, δ(xi,a2) = δ(yi,b1) = δ(xi,yi) = 1.5 + 12Nϵ, δ(xi,b1) = δ(xi,b2) = δ(yi,a1) = δ(yi,a2) = 1.5 + ϵ, δ(xi,yj) = δ(yi,xj) = 1.5 + 6(2j−1)ϵ (1 ≤j ≤i−1), δ(xi,xj) = δ(yi,yj) = 1.5 + 12jϵ (1 ≤j ≤i−1). We denote the (inductively hypothesized) 2i−1 ς-intervals at the end of round i −1 by I(i−1) b , where b = {b(1),...,b (i−1)}∈{ 0,1}i−1 indicates the labels of xj,j ∈[i−1] in I(i−1) b . Min-cuts from round i−1 extend to min-cuts of round idepending on how the edges incident on (xi,yi) are set (Figure 9). It sufﬁces to consider only those min-cuts where xj and yj have opposite labels for each j. Consider an arbitrary such min-cut Cb = (Ab,Bb) of Gi−1 which corresponds to the interval I(i−1) b , that is Ab = {xj |b(j) = 0}∪{yj |b(j) = 1}and Bb contains the remaining unlabeled nodes of Gi−1. It extends to C[b 0] and C[b 1] for ς ∈I(i−1) b satisfying, respectively, Eb,0(ς) := 1 −2ςϵ + F(Cb; ς) >0, Eb,1(ς) := 1 −2ςϵ + 2ς12Nϵ + F(Cb; ς) <0, where F(Cb; ς) = ∑ z∈Ab ςδ(xi,z) −∑ z∈Bb ςδ(xi,z) = ∑ z∈Bb ςδ(yi,z) −∑ z∈Ab ςδ(yi,z). If we show that the solu- tions of the above inequations have disjoint non-empty intersections with ς ∈I(i−1) b , our induction step is complete. We will use an indirect approach for this. 30ςφ,1ςφ,0 ς0,1ς0,0 ς1,1ς1,0 ς[0,0],1ς[0,0],0 ς[0,1],1ς[0,1],0 ς[1,0],1ς[1,0],0 ς[1,1],1ς[1,1],0 Figure 10: Relative positions of critical values of the parameter ς = e−1/σ2 . For 1 ≤i ≤N, given b = {b(1),...,b (i−1)}∈{ 0,1}i−1, let Eb,0 and Eb,1 denote the expressions (exponential polynomials in ς) in round iwhich determine labels of (xi,yi), in the case where for all 1 ≤j <i, xj is labeled b(j) (and let Eφ,0,Eφ,1 denote the expressions for round 1). Let ςb,i ∈(0,1) denote the smallest solution to Eb,i = 0. Then we need to show the ςb,i’s are well-deﬁned and follow a speciﬁc ordering. This ordering is completely speciﬁed by two conditions: (i) ς[b 0],1 <ς[b],0 <ς[b],1 <ς[b 1],0, and (ii) ς[b 0 c],1 <ς[b 1 d],0 for all b,c,d ∈∪i<N{0,1}i and |c|= |d|. First we make a quick observation that all ςb,i’s are well-deﬁned and less than (3/4)1/ϵ. To do this, it will sufﬁce to note that Eb,i(0) = 1 and Eb,i(3 4 1/ϵ ) < 0 for all b,i, since the functions are continuous in (0,3 4 1/ϵ ). This holds because Eb,0 ( 3 4 1/ϵ ) <Eb,1 ( 3 4 1/ϵ ) = 1 −3 2 + (3 4 )12N + F ( Cb; 3 4 1/ϵ ) ≤−1 2 + (3 4 )12N + |b|∑ j=1 (3 4 )6j( 1 − (3 4 )6j) <−1 2 + N∑ j=1 (3 4 )6j <0 Let’s now consider condition (i). We begin by showing ς[b],0 < ς[b],1 for any b. The exponential polynomials Eb,0 and Eb,1 both evaluate to 1 for ς = 0 (since |Ab|= |Bb|= |b|) and decrease monotonically (veriﬁed by elementary calculus) till their respective smallest zeros ς[b],0,ς[b],1. But then Eb,1(ς[b],0) = 2(ς[b],0)12Nϵ >0, which implies ς[b],0 <ς[b],1. Now, to show ς[b 0],1 <ς[b],0, note that E[b 0],1(ς) −E[b],0(ς) = 2ς12Nϵ + ς12iϵ−ς(12i−6)ϵ = ς(12i−6)ϵ(2ς(12(N−i)+6)ϵ+ς6ϵ−1) where 1 ≤i= |b|+1 <N . Since ς[b],0 < 3 4 1/ϵ , it follows thatE[b 0],1(ς[b],0) <0, which implies ς[b 0],1 <ς[b],0. Similarly, it is readily veriﬁed that ς[b],1 <ς[b 1],0, establishing (i). Finally, to show (ii), note thatE[b 0 c],1(ς)−E[b 0 d],0(ς) = 2ς12Nϵ+ς12iϵ−ς(12i−6)ϵ+ς12iϵ(F(Cc; ς)−F(Cd; ς)) = ς(12i−6)ϵ(2ς(12(N−i)+6)ϵ + ς6ϵ −1 + ς6ϵ(F(Cc; ς) −F(Cd; ς))). Again, similar to above, we use ς[b 0 d],0 < 3 4 1/ϵ in this expression to get E[b 0 c],1(ς[b 0 d],0) < 0. Since the exponential polynomials decay monotonically with ς till their ﬁrst roots, (ii) follows. Problem instances: We will now show the graph instances and witnesses to establish the pseudodimension bound. Our graphs will be Gi from the above construction (padded appropriately such that the min-cut intervals do not change, 31if we insist each instance has exactly nnodes), and the shattering family σb (b = (b1,...,b N) ∈{0,1}N) will be 2N values of σ corresponding to the 2N intervals of ς with distinct min-cuts in GN described above. To obtain the witnesses, we set the labels so that only the last pair of nodes (xi,yi) have different labels (i.e. labels are same for all (xj,yj),j < i) and therefore the loss function oscillates 2i times as (xi,yi) are correctly and incorrectly labeled in alternating intervals. The intervals of successive Gi are nested precisely so that σb shatter the instances for the above labelings/witnesses. Thus, we have shown that the pseudodimension is Ω(N) = Ω((n−4)/2) = Ω(n). C Active Learning Theorem 21. Let l1,...,l T : R →R denote an independent sequence of losses as a function of parameter ˜α, when the graph is created using a polynomial kernel w(u,v) = ( ˜d(u,v) + ˜α)d and labeled by Algorithm 5 followed by predicting the remaining labels by optimizing the quadratic objective ∑ u,vw(u,v)(f(u) −f(v))2. If ˜d(u,v) follows a κ-bounded distribution with a closed and bounded support, the sequence is 1 2 -dispersed. Proof. We ﬁrst determine values of the graph parameter ˜αfor which two candidate sets S,T have the same heuristic utilities ( uS = uT) in Algorithm 5. As noted in the proof of Theorem 11, f and fLS are rational polynomials in the graph parameter with degree at most nd. For ﬁxed LS, we observe that pLS is a rational polynomial in ˜α with degree O(ndℓ). Putting together, the equation uS = uT simpliﬁes to a polynomial equation in ˜αwith degree O(ndℓ2ℓ). Application of Lemma 24 implies that the coefﬁcients of the polynomial equation have bounded joint density, and therefore the roots are 1 2 -dispersed. Accounting for all possible subset pairs S,T and their respective labelings LS,LT, we obtain a maximum of (2n)2ℓ such equations. The discontinuities due to the active learning algorithm A′are therefore dispersed. At the same time, the discontinuities due to the semi-supervised procedure for label prediction correspond to soft label ﬂips for some fLS. This gives O((2n)ℓ) polynomial equations of degree at most ndwhose roots collectively contain the possible discontinuities in the loss function due to algorithm A. On any ϵ-interval we can apply Theorem 22 to bound the number of roots for each of the polynomial equations and apply Theorem 23 to show that this implies 1 2 -dispersion. 32",
      "references": [
        "Rapidly solving an online sequence of maximum flow problems.",
        "Data-Driven Algorithm Design.",
        "A discriminative model for semi-supervised learning.",
        "Person identification in webcam images: An application of semi-supervised learning.",
        "Learning-theoretic foundations of algorithm conﬁguration for combinatorial partitioning problems.",
        "Learning to branch.",
        "Dispersion for data-driven algorithm design, online learning, and private optimization.",
        "Learning to link.",
        "On the power of abstention and data-driven decision making for adversarial robustness.",
        "Semi-bandit optimization in the dispersed setting.",
        "Learning piecewise Lipschitz functions in changing environments.",
        "Data-driven clustering via parameterized lloyd’s families.",
        "Rademacher and gaussian complexities: Risk bounds and structural results.",
        "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples.",
        "Technical perspective: Algorithm selection as a learning problem.",
        "Learning from labeled and unlabeled data using graph mincuts.",
        "Semi-supervised learning using randomized mincuts.",
        "Semi-Supervised Learning.",
        "S2: An efficient graph based active learning algorithm with application to nonparametric classiﬁcation.",
        "Efficient non-parametric function induction in semi-supervised learning.",
        "The complexity of cylindrical algebraic decomposition with respect to polynomial degree.",
        "Human semi-supervised learning.",
        "A PAC approach to application-speciﬁc algorithm selection.",
        "Human-level concept learning through probabilistic program induction.",
        "Gradient-based learning applied to document recognition.",
        "Graph-based active learning based on label propagation.",
        "Convergence of stochastic processes.",
        "Online learning and online convex optimization.",
        "Normalized cuts and image segmentation.",
        "Beyond the point cloud: from transductive to semi- supervised learning.",
        "Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time.",
        "Going deeper with convolutions.",
        "Estimating approximate incentive compatibility.",
        "Learning from labeled and unlabeled data with label propagation.",
        "Proximity graphs for clustering and manifold learning.",
        "A scalable algorithm for graph-based active learning.",
        "Learning with local and global consistency.",
        "Introduction to semi-supervised learning.",
        "Semi-supervised learning using Gaussian fields and harmonic functions.",
        "Combining active learning and semi-supervised learning using gaussian fields and harmonic functions.",
        "Semi-supervised learning with graphs.",
        "Humans perform semi-supervised classiﬁcation too."
      ],
      "meta_data": {
        "arxiv_id": "2103.10547v4",
        "authors": [
          "Maria-Florina Balcan",
          "Dravyansh Sharma"
        ],
        "published_date": "2021-03-18T22:19:19Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Data-driven graph design for semi-supervised learning: learns problem-domain-specific graphs G(ρ) from multiple problem instances, providing principled online and distributional guarantees for graph-based SSL. Introduces dispersion-based analysis to handle non-Lipschitz losses, enables learning over one or more graph hyperparameters (threshold, polynomial, Gaussian, and multi-metric combinations), provides efficient semi-bandit algorithms, and shows how to combine multiple similarity metrics. Extends to active learning and multi-parameter tuning, with theoretical bounds (low regret in online setting, PAC-like generalization in distributional setting) and practical algorithms with collation across problem instances. Demonstrates that optimal graph hyperparameters vary by dataset and problem instance and that data-driven graph design can outperform fixed heuristics.",
        "methodology": "Formulates graph-based SSL as optimization over a family of graphs G(ρ) parameterized by hyperparameters ρ. Uses edge weights w(u,v) = g(d(u,v)) with g being a threshold, polynomial, or Gaussian kernel. The learned graph feeds into a quadratic objective (min-cut or harmonic) to assign labels to unlabeled nodes. Proposes online learning over ρ with full information or semi-bandit feedback; proves dispersion of the loss as a function of ρ under mild smoothness and randomness assumptions; develops algorithms (Algorithm 1 for online with dispersion; Algorithm 2 for efficient semi-bandit; Algorithm 3 and 4 for dynamic min-cut and harmonic objectives) to compute feedback sets and update weights, achieving O(√T) regret in online setting. Extends to multiple kernels (multi-metric) via algebraic geometry techniques (Tarski–Seidenberg, cylindrical algebraic decomposition) to select interpolation across metrics. Develops dispersion results for one-dimensional and multi-parameter settings (Theorem 5-7). Analyzes distributional setting via pseudodimension and Rademacher complexity to obtain uniform convergence and generalization guarantees (Theorems 15-20). Extends to active learning (Algorithm 5) and provides dispersion-based guarantees (Theorem 21).",
        "experimental_setup": "Datasets: MNIST, Omniglot, CIFAR-10; binary class setups (e.g., classes 0 and 1). Data sampling: random subset of size n = 100 with L labeled examples (L around 10-20). Distance metric: L2 distance in pixel space. Graph types: unweighted threshold graphs G(r) and weighted Gaussian kernels G(σ) (and polynomial kernel variants). SSL method: harmonic objective, label rounding. Evaluation: test accuracy on unlabeled points; online experiments measure average regret over T problem instances (T=50 in plots). Observations: optimal graph parameters vary across datasets and subsets; gaps of at least 10% between optimal and suboptimal graphs; online learning converges to near-optimal parameters; results show significance of data-driven graph design.",
        "limitations": "Theoretic bounds depend on assumptions like κ-bounded distance distributions, bounded supports, and dispersion properties which may not hold in all domains; weighted graph pseudodimension lower bound is Θ(n), implying potential hardness for worst-case; online guarantees rely on semi-bandit feedback and dispersion that may be challenging to ensure in practice; computational complexity for dynamic min-cut and related procedures can be high for large-scale graphs; experiments are limited to small pixel-based datasets and binary classification, with modest dataset sizes; generalization to multi-class and real-world large-scale SSL remains to be validated.",
        "future_research_directions": "Extend dispersion and learning guarantees to broader graph families and larger-scale graphs; improve multi-metric learning with more efficient algebraic geometry methods; develop scalable online/semi-bandit algorithms for graphs with millions of nodes; explore deeper active learning strategies and interactive graph refinement; extend to multi-class settings and real-world semi-supervised tasks; investigate robustness to label noise and distribution shifts; connect to other combinatorial optimization problems beyond SSL; study dynamic graphs where similarity evolves over time.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Uncertainty Aware Semi-Supervised Learning on Graph Data",
      "full_text": "Uncertainty Aware Semi-Supervised Learning on Graph Data Xujiang Zhao1, Feng Chen1, Shu Hu2, Jin-Hee Cho3 1The University of Texas at Dallas,{xujiang.zhao, feng,chen}@utdallas.edu 2University at Buffalo, SUNY ,shuhu@buffalo.edu 3Virginia Tech,jicho@vt.edu Abstract Thanks to graph neural networks (GNNs), semi-supervised node classiﬁcation has shown the state-of-the-art performance in graph data. However, GNNs have not considered different types of uncertainties associated with class probabilities to minimize risk of increasing misclassiﬁcation under uncertainty in real life. In this work, we propose a multi-source uncertainty framework using a GNN that reﬂects various types of predictive uncertainties in both deep learning and belief/evidence theory domains for node classiﬁcation predictions. By collecting evidence from the given labels of training nodes, the Graph-based Kernel Dirichlet distribution Estimation (GKDE) method is designed for accurately predicting node- level Dirichlet distributions and detecting out-of-distribution (OOD) nodes. We validated the outperformance of our proposed model compared to the state-of-the- art counterparts in terms of misclassiﬁcation detection and OOD detection based on six real network datasets. We found that dissonance-based detection yielded the best results on misclassiﬁcation detection while vacuity-based detection was the best for OOD detection. To clarify the reasons behind the results, we provided the theoretical proof that explains the relationships between different types of uncertainties considered in this work. 1 Introduction Inherent uncertainties derived from different root causes have realized as serious hurdles to ﬁnd effective solutions for real world problems. Critical safety concerns have been brought due to lack of considering diverse causes of uncertainties, resulting in high risk due to misinterpretation of uncertainties (e.g., misdetection or misclassiﬁcation of an object by an autonomous vehicle). Graph neural networks (GNNs) [ 16, 30] have received tremendous attention in the data science community. Despite their superior performance in semi-supervised node classiﬁcation and regression, they didn’t consider various types of uncertainties in the their decision process. Predictive uncertainty estimation [14] using Bayesian NNs (BNNs) has been explored for classiﬁcation prediction and regression in the computer vision applications, based on aleatoric uncertainty (AU) and epistemic uncertainty (EU). AU refers to data uncertainty from statistical randomness (e.g., inherent noises in observations) while EU indicates model uncertainty due to limited knowledge (e.g., ignorance) in collected data. In the belief or evidence theory domain, Subjective Logic (SL) [ 12] considered vacuity (or a lack of evidence or ignorance) as uncertainty in a subjective opinion. Recently other uncertainty types, such as dissonance, consonance, vagueness, and monosonance [ 12], have been discussed based on SL to measure them based on their different root causes. We ﬁrst considered multidimensional uncertainty types in both deep learning (DL) and belief and evi- dence theory domains for node-level classiﬁcation, misclassiﬁcation detection, and out-of-distribution (OOD) detection tasks. By leveraging the learning capability of GNNs and considering multi- dimensional uncertainties, we propose a uncertainty-aware estimation framework by quantifying 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2010.12783v2  [cs.LG]  24 Nov 2020different uncertainty types associated with the predicted class probabilities. In this work, we made the following key contributions: • A multi-source uncertainty framework for GNNs. Our proposed framework ﬁrst provides the estimation of various types of uncertainty from both DL and evidence/belief theory domains, such as dissonance (derived from conﬂicting evidence) and vacuity (derived from lack of evidence). In addition, we designed a Graph-based Kernel Dirichlet distribution Estimation (GKDE) method to reduce errors in quantifying predictive uncertainties. • Theoretical analysis: Our work is the ﬁrst that provides a theoretical analysis about the rela- tionships between different types of uncertainties considered in this work. We demonstrate via a theoretical analysis that an OOD node may have a high predictive uncertainty under GKDE. • Comprehensive experiments for validating the performance of our proposed framework : Based on the six real graph datasets, we compared the performance of our proposed framework with that of other competitive counterparts. We found that the dissonance-based detection yielded the best results in misclassiﬁcation detection while vacuity-based detection best performed in OOD detection. Note that we use the term ‘predictive uncertainty’ in order to mean uncertainty estimated to solve prediction problems. 2 Related Work DL research has mainly considered aleatoric uncertainty (AU) and epistemic uncertainty (EU) using BNNs for computer vision applications. AU consists of homoscedastic uncertainty (i.e., constant errors for different inputs) and heteroscedastic uncertainty (i.e., different errors for different inputs) [5]. A Bayesian DL framework was presented to simultaneously estimate both AU and EU in regression (e.g., depth regression) and classiﬁcation (e.g., semantic segmentation) tasks [14]. Later, distributional uncertainty was deﬁned based on distributional mismatch between testing and training data distributions [20]. Dropout variational inference [7] was used for an approximate inference in BNNs using epistemic uncertainty, similar to DropEdge [23]. Other algorithms have considered overall uncertainty in node classiﬁcation [ 3, 18, 32]. However, no prior work has considered uncertainty decomposition in GNNs. In the belief (or evidence) theory domain, uncertainty reasoning has been substantially explored, such as Fuzzy Logic [1], Dempster-Shafer Theory (DST) [27], or Subjective Logic (SL) [11]. Belief theory focuses on reasoning inherent uncertainty in information caused by unreliable, incomplete, deceptive, or conﬂicting evidence. SL considered predictive uncertainty in subjective opinions in terms of vacuity (i.e., a lack of evidence) and vagueness (i.e., failing in discriminating a belief state) [11]. Recently, other uncertainty types have been studied, such as dissonance caused by conﬂicting evidence[12]. In the deep NNs, [ 26] proposed evidential deep learning (EDL) model, using SL to train a deterministic NN for supervised classiﬁcation in computer vision based on the sum of squared loss. However, EDL didn’t consider a general method of estimating multidimensional uncertainty or graph structure. 3 Multidimensional Uncertainty and Subjective Logic This section provides an overview of SL and discusses multiple types of uncertainties estimated based on SL, called evidential uncertainty, with the measures of vacuity and dissonance. In addition, we give a brief overview of probabilistic uncertainty, discussing the measures of aleatoric uncertainty and epistemic uncertainty. 3.1 Subjective Logic A multinomial opinion of a random variable yis represented by ω = (b,u, a) where a domain is Y ≡{1,··· ,K}and the additivity requirement of ωis given as ∑ k∈Y bk + u= 1. To be speciﬁc, each parameter indicates, • b: belief mass distribution over Y and b= [b1,...,b K]T; • u: uncertainty mass representing vacuity of evidence; • a: base rate distribution over Y and a= [a1,...,a K]T. The projected probability distribution of a multinomial opinion can be calculated as: P(y= k) =bk + aku, ∀k∈Y. (1) 2A multinomial opinion ωdeﬁned above can be equivalently represented by aK-dimensional Dirichlet probability density function (PDF), where the special case with K = 2is the Beta PDF as a binomial opinion. Let αbe a strength vector over the singletons (or classes) in Y and p = [p1,··· ,pK]T be a probability distribution over Y. The Dirichlet PDF with p as a random vector K-dimensional variables is deﬁned by: Dir(p|α) = 1 B(α) ∏ k∈Y p(αk−1) k , (2) where 1 B(α) = Γ(∑ k∈Y αk)∏ k∈Y(αk) , αk ≥0, and pk ̸= 0, if αk <1. The term evidence is introduced as a measure of the amount of supporting observations collected from data that a sample should be classiﬁed into a certain class. Let ek be the evidence derived for the class k ∈Y. The total strength αk for the belief of each class k ∈Y can be calculated as: αk = ek + akW, where ek ≥0,∀k∈Y, and W refers to a non-informative weight representing the amount of uncertain evidence. Given the Dirichlet PDF as deﬁned above, the expected probability distribution over Y can be calculated as: E[pk] = αk ∑K k=1 αk = ek + akW W + ∑K k=1 ek . (3) The observed evidence in a Dirichlet PDF can be mapped to a multinomial opinion as follows: bk = ek S, u= W S , (4) where S = ∑K k=1 αk refers to the Dirichlet strength. Without loss of generality, we set ak = 1 K and the non-informative prior weight (i.e., W = K), which indicates that ak ·W = 1for each k∈Y. 3.2 Evidential Uncertainty In [12], we discussed a number of multidimensional uncertainty dimensions of a subjective opinion based on the formalism of SL, such as singularity, vagueness, vacuity, dissonance, consonance, and monosonance. These uncertainty dimensions can be observed from binomial, multinomial, or hyper opinions depending on their characteristics (e.g., the vagueness uncertainty is only observed in hyper opinions to deal with composite beliefs). In this paper, we discuss two main uncertainty types that can be estimated in a multinomial opinion, which are vacuity and dissonance. The main cause of vacuity is derived from a lack of evidence or knowledge, which corresponds to the uncertainty mass, u, of a multinomial opinion in SL as: vac(ω) ≡u= K/S,as estimated in Eq. (4). This uncertainty exists because the analyst may have insufﬁcient information or knowledge to analyze the uncertainty. The dissonance of a multinomial opinion can be derived from the same amount of conﬂicting evidence and can be estimated based on the difference between singleton belief masses (e.g., class labels), which leads to ‘inconclusiveness’ in decision making applications. For example, a four-state multinomial opinion is given as(b1,b2,b3,b4,u,a ) = (0.25,0.25,0.25,0.25,0.0,a) based on Eq. (4), although the vacuity uis zero, a decision can not be made if there are the same amounts of beliefs supporting respective beliefs. Given a multinomial opinion with non-zero belief masses, the measure of dissonance can be calculated as: diss(ω) = K∑ i=1 (bi ∑ j̸=ibjBal(bj,bi)∑ j̸=ibj ) , (5) where the relative mass balance between a pair of belief masses bj and bi is deﬁned as Bal(bj,bi) = 1−|bj−bi|/(bj+bi). We note that the dissonance is measured only when the belief mass is non-zero. If all belief masses equal to zero with vacuity being 1 (i.e., u= 1), the dissonance will be set to zero. 3.3 Probabilistic Uncertainty For classiﬁcation, the estimation of the probabilistic uncertainty relies on the design of an appropriate Bayesian DL model with parameters θ. Given input xand dataset G, we estimate a class proba- bility by P(y|x) = ∫ P(y|x; θ)P(θ|G)dθ, and obtain epistemic uncertaintyestimated by mutual information [2, 20]: I(y,θ|x,G)   Epistemic = H [ EP(θ|G)[P(y|x; θ)] ]    Entropy −EP(θ|G) [ H[P(y|x; θ)] ]    Aleatoric , (6) 3where H(·) is Shannon’s entropy of a probability distribution. The ﬁrst term indicatesentropy that represents the total uncertainty while the second term is aleatoric that indicates data uncertainty. By computing the difference between entropy and aleatoric uncertainties, we obtain epistemic uncertainty, which refers to uncertainty from model parameters. 4 Relationships Between Multiple Uncertainties Figure 1: Multiple uncertainties of different pre- diction. Let u = [uv,udiss,ualea,uepis,uen]. We use the shorthand notations uv, udiss, ualea, uepis, and uen to represent vacuity, dissonance, aleatoric, epistemic, and entropy, respectively. To interpret multiple types of uncertainty, we show three prediction scenarios of 3-class classiﬁcation in Figure 1, in each of which the strength parame- ters α= [α1,α2,α3] are known. To make a predic- tion with high conﬁdence, the subjective multinomial opinion, following a Dirichlet distribution, will yield a sharp distribution on one corner of the simplex (see Figure 1 (a)). For a prediction with conﬂicting evi- dence, called a conﬂicting prediction (CP), the multi- nomial opinion should yield a central distribution, representing conﬁdence to predict a ﬂat categorical distribution over class labels (see Figure 1 (b)). For an OOD scenario with α= [1,1,1], the multinomial opinion would yield a ﬂat distribution over the sim- plex (Figure 1 (c)), indicating high uncertainty due to the lack of evidence. The ﬁrst technical contribution of this work is as follows. Theorem 1. We consider a simpliﬁed scenario, where a multinomial random variableyfollows a K-class categorical distribution: y∼Cal(p), the class probabilities p follow a Dirichlet distribution: p ∼Dir(α), and αrefer to the Dirichlet parameters. Given a total Dirichlet strength S = ∑K i=1 αi, for any opinion ωon a multinomial random variable y, we have 1. General relations on all prediction scenarios. (a) uv + udiss ≤1; (b) uv >uepis. 2. Special relations on the OOD and the CP . (a) For an OOD sample with a uniform prediction (i.e., α= [1,..., 1]), we have 1 =uv = uen >ualea >uepis >udiss = 0 (b) For an in-distribution sample with a conﬂicting prediction (i.e.,α= [α1,...,α K] with α1 = α2 = ··· = αK, if S →∞), we have uen = 1, lim S→∞ udiss = lim S→∞ ualea = 1, lim S→∞ uv = lim S→∞ uepis = 0 with uen >ualea >udiss >uv >uepis. The proof of Theorem 1 can be found in Appendix A.1. As demonstrated in Theorem 1 and Figure 1, entropy cannot distinguish OOD (see Figure 1 (c)) and conﬂicting predictions (see Figure 1 (b)) because entropy is high for both cases. Similarly, neither aleatoric uncertainty nor epistemic uncertainty can distinguish OOD from conﬂicting predictions. In both cases, aleatoric uncertainty is high while epistemic uncertainty is low. On the other hand, vacuity and dissonance can clearly distinguish OOD from a conﬂicting prediction. For example, OOD objects typically show high vacuity with low dissonance while conﬂicting predictions exhibit low vacuity with high dissonance. This observation is conﬁrmed through the empirical validation via our extensive experiments in terms of misclassiﬁcation and OOD detection tasks. 5 Uncertainty-Aware Semi-Supervised Learning In this section, we describe our proposed uncertainty framework based on semi-supervised node classiﬁcation problem. It is designed to predict the subjective opinions about the classiﬁcation 4Figure 2: Uncertainty Framework Overview. Subjective Bayesian GNN (a) designed for estimating the different types of uncertainties. The loss function includes square error (d) to reduce bias, GKDE (b) to reduce errors in uncertainty estimation and teacher network (c) to reﬁne class probability. of testing nodes, such that a variety of uncertainty types, such as vacuity, dissonance, aleatoric uncertainty, and epistemic uncertainty, can be quantiﬁed based on the estimated subjective opinions and posterior of model parameters. As a subjective opinion can be equivalently represented by a Dirichlet distribution about the class probabilities, we proposed a way to predict the node-level subjective opinions in the form of node-level Dirichlet distributions. The overall description of the framework is shown in Figure 2. 5.1 Problem Deﬁnition Given an input graph G= (V,E,r,yL), where V = {1,...,N }is a ground set of nodes, E ⊆V×V is a ground set of edges, r = [r1,··· ,rN]T ∈RN×d is a node-level feature matrix, ri ∈Rd is the feature vector of node i, yL = {yi |i ∈L}are the labels of the training nodes L ⊂V, and yi ∈{1,...,K }is the class label of node i. We aim to predict : (1) the class probabilities of the testing nodes: pV\\L = {pi ∈[0,1]K |i ∈V \\L}; and (2) the associated multidimensional uncertainty estimates introduced by different root causes: uV\\L = {ui ∈[0,1]m |i ∈V \\L}, where pi,k is the probability that the class label yi = kand mis the total number of uncertainty types. 5.2 Proposed Uncertainty Framework Learning evidential uncertainty. As discussed in Section 3.1, evidential uncertainty can be derived from multinomial opinions or equivalently Dirichlet distributions to model a probability distribution for the class probabilities. Therefore, we design a Subjective GNN (S-GNN) f to form their multinomial opinions for the node-level Dirichlet distribution Dir(pi|αi) of a given node i. Then, the conditional probability P(p|A,r; θ) can be obtained by: P(p|A,r; θ) = ∏N i=1 Dir(pi|αi), αi = fi(A,r; θ), (7) where fi is the output of S-GNN for node i, θis the model parameters, and Ais an adjacency matrix. The Dirichlet probability function Dir(pi|αi) is deﬁned by Eq. (2). Note that S-GNN is similar to classical GNN, except that we use an activation layer (e.g., ReLU) instead of the softmax layer (only outputs class probabilities). This ensures that S-GNN would output non-negative values, which are taken as the parameters for the predicted Dirichlet distribution. Learning probabilistic uncertainty. Since probabilistic uncertainty relies on a Bayesian framework, we proposed a Subjective Bayesian GNN (S-BGNN) that adapts S-GNN to a Bayesian framework, with the model parameters θfollowing a prior distribution. The joint class probability of y can be estimated by: P(y|A,r; G) = ∫ ∫ P(y|p)P(p|A,r; θ)P(θ|G)dpdθ ≈ 1 M M∑ m=1 N∑ i=1 ∫ P(yi|pi)P(pi|A,r; θ(m))dpi, θ(m) ∼q(θ) (8) where P(θ|G) is the posterior, estimated via dropout inference, that provides an approximate solution of posterior q(θ) and taking samples from the posterior distribution of models [ 7]. Thanks to the 5beneﬁt of dropout inference, training a DL model directly by minimizing the cross entropy (or square error) loss function can effectively minimize the KL-divergence between the approximated distribution and the full posterior (i.e., KL[ q(θ)∥P(θ|G)]) in variational inference [ 7, 13]. For interested readers, please refer to more detail in Appendix B.8. Therefore, training S-GNN with stochastic gradient descent enables learning of an approximated distribution of weights, which can provide good explainability of data and prevent overﬁtting. We use a loss function to compute its Bayes risk with respect to the sum of squares loss ∥y −p∥2 2 by: L(θ) = ∑ i∈L ∫ ∥yi −pi∥2 2 ·P(pi|A,r; θ)dpi = ∑ i∈L ∑K k=1 ( yik −E[pik] )2 + Var(pik), (9) where yiis an one-hot vector encoding the ground-truth class withyij = 1and yik ̸= for all k̸= jand jis a class label. Eq. (9) aims to minimize the prediction error and variance, leading to maximizing the classiﬁcation accuracy of each training node by removing excessive misleading evidence. 5.3 Graph-based Kernel Dirichlet distribution Estimation (GKDE) Figure 3: Illustration of GKDE. Estimate prior Dirichlet distribution Dir(ˆα) for node j (red) based on training nodes (blue) and graph structure information. The loss function in Eq. (9) is designed to measure the sum of squared loss based on class labels of training nodes. However, it does not directly measure the quality of the predicted node-level Dirichlet distributions. To address this limitation, we proposed Graph-based Kernel Dirichlet distribution Estimation (GKDE) to better estimate node- level Dirichlet distributions by using graph structure in- formation. The key idea of the GKDE is to estimate prior Dirichlet distribution parameters for each node based on the class labels of training nodes (see Figure 3). Then, we use the estimated prior Dirichlet distribution in the training process to learn the following patterns: (i) nodes with a high vacuity will be shown far from training nodes; and (ii) nodes with a high dissonance will be shown near the boundaries of classes. Based on SL, let each training node represent one evidence for its class label. Denote the contribution of evidence estimation for node jfrom training node iby h(yi,dij) = [h1,...,h k,...,h K] ∈[0,1]K, where hk(yi,dij) is obtained by: hk(yi,dij) = {0 yi ̸= k g(dij) yi = k, (10) g(dij) = 1 σ √ 2π exp(− d2 ij 2σ2 ) is the Gaussian kernel function used to estimate the distribution effect between nodes iand j, and dij means the node-level distance (a shortest path between nodes i and j), and σ is the bandwidth parameter. The prior evidence is estimated based GKDE: ˆej =∑ i∈L h(yi,dij), where L is a set of training nodes and the prior Dirichlet distribution ˆαj = ˆej + 1. During the training process, we minimize the KL-divergence between model predictions of Dirichlet distribution and prior distribution: min KL[Dir(α)∥Dir( ˆα)]. This process can prioritize the extent of data relevance based on the estimated evidential uncertainty, which is proven effective based on the proposition below. Proposition 1. Given Ltraining nodes, for any testing nodes iand j, let di = [di1,...,d iL] be the vector of graph distances from nodes ito training nodes and dj = [dj1,...,d jL] be the graph distances from nodes jto training nodes, where dil is the node-level distance between nodes iand l. If for all l∈{1,...,L }, dil ≥djl, then we have ˆuvi ≥ˆuvj , where ˆuvi and ˆuvj refer to vacuity uncertainties of nodes iand jestimated based on GKDE. The proof for this proposition can be found in Appendix A.2. The above proposition shows that if a testing node is too far from training nodes, the vacuity will increase, implying that an OOD node is expected to have a high vacuity. In addition, we designed a simple iterative knowledge distillation method [10] (i.e., Teacher Network) to reﬁne the node-level classiﬁcation probabilities. The key idea is to train our proposed model 6(Student) to imitate the outputs of a pre-train a vanilla GNN (Teacher) by adding a regularization term of KL-divergence. This leads to solving the following optimization problem: minθ L(θ) +λ1KL[Dir(α)∥Dir( ˆα)] +λ2KL[P(y |A,r; G) ∥P(y|ˆp)], (11) where ˆp is the vanilla GNN’s (Teacher) output andλ1 and λ2 are trade-off parameters. 6 Experiments In this section, we conduct experiments on the tasks of misclassiﬁcation and OOD detections to answer the following questions for semi-supervised node classiﬁcation: Q1. Misclassiﬁcation Detection: What type of uncertainty is the most promising indicator of high conﬁdence in node classiﬁcation predictions? Q2. OOD Detection: What type of uncertainty is a key indicator of accurate detection of OOD nodes? Q3. GKDE with Uncertainty Estimates: How can GKDE help enhance prediction tasks with what types of uncertainty estimates? Through extensive experiments, we found the following answers for the above questions: A1. Dissonance (i.e., uncertainty due to conﬂicting evidence) is more effective than other uncertainty estimates in misclassiﬁcation detection. A2. Vacuity (i.e., uncertainty due to lack of conﬁdence) is more effective than other uncertainty estimates in OOD detection. A3. GKDE can indeed help improve the estimation quality of node-level Dirichlet distributions, resulting in a higher OOD detection. 6.1 Experiment Setup Datasets: We used six datasets, including three citation network datasets [25] (i.e., Cora, Citeseer, Pubmed) and three new datasets [28] (i.e., Coauthor Physics, Amazon Computer, and Amazon Photo). We summarized the description and experimental setup of the used datasets in Appendix B.21. Comparing Schemes: We conducted the extensive comparative performance analysis based on our proposed models and several state-of-the-art competitive counterparts. We implemented all models based on the most popular GNN model, GCN [16]. We compared our model (S-BGCN-T-K) against: (1) Softmax-based GCN [16] with uncertainty measured based on entropy; and (2) Drop-GCN that adapts the Monte-Carlo Dropout [ 7, 24] into the GCN model to learn probabilistic uncertainty; (3) EDL-GCN that adapts the EDL model [ 26] with GCN to estimate evidential uncertainty; (4) DPN-GCN that adapts the DPN [20] method with GCN to estimate probabilistic uncertainty. We evaluated the performance of all models considered using the area under the ROC (AUROC) curve and area under the Precision-Recall (AUPR) curve in both experiments [9]. 6.2 Results Misclassiﬁcation Detection. The misclassiﬁcation detection experiment involves detecting whether a given prediction is incorrect using an uncertainty estimate. Table 1 shows that S-BGCN-T-K outperforms all baseline models under the AUROC and AUPR for misclassiﬁcation detection. The outperformance of dissonance-based detection is fairly impressive. This conﬁrms that low dissonance (a small amount of conﬂicting evidence) is the key to maximize the accuracy of node classiﬁcation prediction. We observe the following performance order:Dissonance >Entropy ≈Aleatoric > Vacuity ≈Epistemic, which is aligned with our conjecture: higher dissonance with conﬂicting prediction leads to higher misclassiﬁcation detection. We also conducted experiments on additional three datasets and observed similar trends of the results, as demonstrated in Appendix C. OOD Detection. This experiment involves detecting whether an input example is out-of-distribution (OOD) given an estimate of uncertainty. For semi-supervised node classiﬁcation, we randomly selected one to four categories as OOD categories and trained the models based on training nodes of the other categories. Due to the space constraint, the experimental setup for the OOD detection is detailed in Appendix B.3. In Table 2, across six network datasets, our vacuity-based detection signiﬁcantly outperformed the other competitive methods, exceeding the performance of the epistemic uncertainty and other type of 1The source code and datasets are accessible at https://github.com/zxj32/uncertainty-GNN 7Table 1: AUROC and AUPR for the Misclassiﬁcation Detection. Data Model AUROC AUPR AccVa. Dis. Al. Ep. En. Va. Dis. Al. Ep. En. Cora S-BGCN-T-K 70.6 82.4 75.3 68.8 77.7 90.3 95.4 92.4 87.8 93.4 82.0 EDL-GCN 70.2 81.5 - - 76.9 90.0 94.6 - - 93.6 81.5 DPN-GCN - - 78.3 75.5 77.3 - - 92.4 92.0 92.4 80.8 Drop-GCN - - 73.9 66.7 76.9 - - 92.7 90.0 93.6 81.3 GCN - - - - 79.6 - - - - 94.1 81.5 Citeseer S-BGCN-T-K 65.4 74.0 67.2 60.7 70.0 79.8 85.6 82.2 75.2 83.5 71.0 EDL-GCN 64.9 73.6 - - 69.6 79.2 84.6 - - 82.9 70.2 DPN-GCN - - 66.0 64.9 65.5 - - 78.7 77.6 78.1 68.1 Drop-GCN - - 66.4 60.8 69.8 - - 82.3 77.8 83.7 70.9 GCN - - - - 71.4 - - - - 83.2 70.3 Pubmed S-BGCN-T-K 64.1 73.3 69.3 64.2 70.7 85.6 90.8 88.8 86.1 89.2 79.3 EDL-GCN 62.6 69.0 - - 67.2 84.6 88.9 - - 81.7 79.0 DPN-GCN - - 72.7 69.2 72.5 - - 87.8 86.8 87.7 77.1 Drop-GCN - - 67.3 66.1 67.2 - - 88.6 85.6 89.0 79.0 GCN - - - - 68.5 - - - - 89.2 79.0 Va.: Vacuity, Dis.: Dissonance, Al.: Aleatoric, Ep.: Epistemic, En.: Entropy Table 2: AUROC and AUPR for the OOD Detection. Data Model AUROC AUPR Va. Dis. Al. Ep. En. Va. Dis. Al. Ep. En. Cora S-BGCN-T-K 87.6 75.5 85.5 70.8 84.8 78.4 49.0 75.3 44.5 73.1 EDL-GCN 84.5 81.0 - 83.3 74.2 53.2 - - 71.4 DPN-GCN - - 77.3 78.9 78.3 - - 58.5 62.8 63.0 Drop-GCN - - 81.9 70.5 80.9 - - 69.7 44.2 67.2 GCN - - - - 80.7 - - - - 66.9 Citeseer S-BGCN-T-K 84.8 55.2 78.4 55.1 74.0 86.8 54.1 80.8 55.8 74.0 EDL-GCN 78.4 59.4 - - 69.1 79.8 57.3 - - 69.0 DPN-GCN - - 68.3 72.2 69.5 - - 68.5 72.1 70.3 Drop-GCN - - 72.3 61.4 70.6 - - 73.5 60.8 70.0 GCN - - - - 70.8 - - - - 70.2 Pubmed S-BGCN-T-K 74.6 67.9 71.8 59.2 72.2 69.6 52.9 63.6 44.0 56.5 EDL-GCN 71.5 68.2 - - 70.5 65.3 53.1 - - 55.0 DPN-GCN - - 63.5 63.7 63.5 - - 50.7 53.9 51.1 Drop-GCN - - 68.7 60.8 66.7 - - 59.7 46.7 54.8 GCN - - - - 68.3 - - - - 55.3 Amazon Photo S-BGCN-T-K 93.4 76.4 91.4 32.2 91.4 94.8 68.0 92.3 42.3 92.5 EDL-GCN 63.4 78.1 - - 79.2 66.2 74.8 - - 81.2 DPN-GCN - - 83.6 83.6 83.6 - - 82.6 82.4 82.5 Drop-GCN - - 84.5 58.7 84.3 - - 87.0 57.7 86.9 GCN - - - - 84.4 - - - - 87.0 Amazon Computer S-BGCN-T-K 82.3 76.6 80.9 55.4 80.9 70.5 52.8 60.9 35.9 60.6 EDL-GCN 53.2 70.1 - - 70.0 33.2 43.9 - - 45.7 DPN-GCN - - 77.6 77.7 77.7 - - 50.8 51.2 51.0 Drop-GCN - - 74.4 70.5 74.3 - - 50.0 46.7 49.8 GCN - - - - 74.0 - - - - 48.7 Coauthor Physics S-BGCN-T-K 91.3 87.6 89.7 61.8 89.8 72.2 56.6 68.1 25.9 67.9 EDL-GCN 88.2 85.8 - - 87.6 67.1 51.2 - - 62.1 DPN-GCN - - 85.5 85.6 85.5 - - 59.8 60.2 59.8 Drop-GCN - - 89.2 78.4 89.3 - - 66.6 37.1 66.5 GCN - - - - 89.1 - - - - 64.0 Va.: Vacuity, Dis.: Dissonance, Al.: Aleatoric, Ep.: Epistemic, En.: Entropy uncertainties. This demonstrates that vacuity-based model is more effective than other uncertainty estimates-based counterparts in increasing OOD detection. We observed the following performance order: Vacuity > Entropy ≈Aleatoric > Epistemic ≈Dissonance, which is consistent with the theoretical results as shown in Theorem 1. Ablation Study. We conducted additional experiments (see Table 3) in order to demonstrate the contributions of the key technical components, including GKDE, Teacher Network, and subjective Bayesian framework. The key ﬁndings obtained from this experiment are: (1) GKDE can enhance the OOD detection (i.e., 30% increase with vacuity), which is consistent with our theoretical proof about the outperformance of GKDE in uncertainty estimation, i.e., OOD nodes have a higher vacuity than other nodes; and (2) the Teacher Network can further improve the node classiﬁcation accuracy. 6.3 Why is Epistemic Uncertainty Less Effective than Vacuity? Although epistemic uncertainty is known to be effective to improve OOD detection [7, 14] in computer vision applications, our results demonstrate it is less effective than our vacuity-based approach. The ﬁrst potential reason is that epistemic uncertainty is always smaller than vacuity (From Theorem 1), which potentially indicates that epistemic may capture less information related to OOD. Another potential reason is that the previous success of epistemic uncertainty for OOD detection is limited to supervised learning in computer vision applications, but its effectiveness for OOD detection was not 8sufﬁciently validated in semi-supervised learning tasks. Recall that epistemic uncertainty (i.e., model uncertainty) is calculated based on mutual information (see Eq. (6)). In a semi-supervised setting, the features of unlabeled nodes are also fed to a model for training process to provide the model with a high conﬁdence on its output. For example, the model output P(y|A,r; θ) would not change too much even with differently sampled parameters θ, i.e., P(y|A,r; θ(i)) ≈P(y|A,r; θ(j)), which result in a low epistemic uncertainty. We also designed a semi-supervised learning experiment for image classiﬁcation and observed a consistent pattern with the results demonstrated in Appendix C.6. Table 3: Ablation study of our proposed models: (1) S-GCN: Subjective GCN with vacuity and dissonance estimation; (2) S-BGCN: S-GCN with Bayesian framework; (3) S-BGCN-T: S-BGCN with a Teacher Network; (4) S-BGCN-T-K: S-BGCN-T with GKDE to improve uncertainty estimation. Data Model AUROC (Misclassiﬁcation Detection) AUPR (Misclassiﬁcation Detection) AccVa. Dis. Al. Ep. En. Va. Dis. Al. Ep. En. Cora S-BGCN-T-K 70.6 82.4 75.3 68.8 77.7 90.3 95.4 92.4 87.8 93.4 82.0 S-BGCN-T 70.8 82.5 75.3 68.9 77.8 90.4 95.4 92.6 88.0 93.4 82.2 S-BGCN 69.8 81.4 73.9 66.7 76.9 89.4 94.3 92.3 88.0 93.1 81.2 S-GCN 70.2 81.5 - - 76.9 90.0 94.6 - - 93.6 81.5 AUROC (OOD Detection) AUPR (OOD Detection) Amazon Photo S-BGCN-T-K 93.4 76.4 91.4 32.2 91.4 94.8 68.0 92.3 42.3 92.5 - S-BGCN-T 64.0 77.5 79.9 52.6 79.8 67.0 75.3 82.0 53.7 81.9 - S-BGCN 63.0 76.6 79.8 52.7 79.7 66.5 75.1 82.1 53.9 81.7 - S-GCN 64.0 77.1 - - 79.6 67.0 74.9 - - 81.6 - Va.: Vacuity, Dis.: Dissonance, Al.: Aleatoric, Ep.: Epistemic, En.: Entropy 7 Conclusion In this work, we proposed a multi-source uncertainty framework of GNNs for semi-supervised node classiﬁcation. Our proposed framework provides an effective way of predicting node classiﬁcation and out-of-distribution detection considering multiple types of uncertainty. We leveraged various types of uncertainty estimates from both DL and evidence/belief theory domains. Through our extensive experiments, we found that dissonance-based detection yielded the best performance on misclassiﬁcation detection while vacuity-based detection performed the best for OOD detection, compared to other competitive counterparts. In particular, it was noticeable that applying GKDE and the Teacher network further enhanced the accuracy in node classiﬁcation and uncertainty estimates. Acknowledgments We would like to thank Yuzhe Ou for providing proof suggestions. This work is supported by the National Science Foundation (NSF) under Grant No #1815696 and #1750911. Broader Impact In this paper, we propose a uncertainty-aware semi-supervised learning framework of GNN for predicting multi-dimensional uncertainties for the task of semi-supervised node classiﬁcation. Our proposed framework can be applied to a wide range of applications, including computer vision, natural language processing, recommendation systems, trafﬁc prediction, generative models and many more [33]. Our proposed framework can be applied to predict multiple uncertainties of different roots for GNNs in these applications, improving the understanding of individual decisions, as well as the underlying models. While there will be important impacts resulting from the use of GNNs in general, our focus in this work is on investigating the impact of using our method to predict multi- source uncertainties for such systems. The additional beneﬁts of this method include improvement of safety and transparency in decision-critical applications to avoid overconﬁdent prediction, which can easily lead to misclassiﬁcation. We see promising research opportunities that can adopt our uncertainty framework, such as investi- gating whether this uncertainty framework can further enhance misclassiﬁcation detection or OOD detection. To mitigate the risk from different types of uncertainties, we encourage future research to understand the impacts of this proposed uncertainty framework to solve other real world problems. 9References [1] C. W. De Silva. Intelligent control: fuzzy logic applications. CRC press, 1995. [2] S. Depeweg, J.-M. Hernandez-Lobato, F. Doshi-Velez, and S. Udluft. Decomposition of uncertainty in bayesian deep learning for efﬁcient and risk-sensitive learning. In International Conference on Machine Learning, pages 1184–1193. PMLR, 2018. [3] D. Eswaran, S. Günnemann, and C. Faloutsos. The power of certainty: A dirichlet-multinomial model for belief propagation. In Proceedings of the 2017 SIAM International Conference on Data Mining, pages 144–152. SIAM, 2017. [4] T. Fawcett. An introduction to roc analysis. Pattern recognition letters, pages 861–874, 2006. [5] Y . Gal. Uncertainty in deep learning. University of Cambridge, 2016. [6] Y . Gal and Z. Ghahramani. Bayesian convolutional neural networks with bernoulli approximate variational inference. arXiv preprint arXiv:1506.02158, 2015. [7] Y . Gal and Z. Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050–1059, 2016. [8] X. Glorot and Y . Bengio. Understanding the difﬁculty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, pages 249–256, 2010. [9] D. Hendrycks and K. Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. [10] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. [11] A. Josang. Subjective logic. Springer, 2016. [12] A. Josang, J.-H. Cho, and F. Chen. Uncertainty characteristics of subjective opinions. In 2018 21st International Conference on Information Fusion (FUSION), pages 1998–2005. IEEE, 2018. [13] A. Kendall, V . Badrinarayanan, and R. Cipolla. Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding. arXiv preprint arXiv:1511.02680, 2015. [14] A. Kendall and Y . Gal. What uncertainties do we need in bayesian deep learning for computer vision? In Advances in neural information processing systems, pages 5574–5584, 2017. [15] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [16] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017. [17] D.-H. Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, 2013. [18] Z.-Y . Liu, S.-Y . Li, S. Chen, Y . Hu, and S.-J. Huang. Uncertainty aware graph gaussian process for semi-supervised learning. In AAAI, pages 4957–4964, 2020. [19] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning research, pages 2579–2605, 2008. [20] A. Malinin and M. Gales. Predictive uncertainty estimation via prior networks. In Advances in Neural Information Processing Systems, pages 7047–7058, 2018. [21] J. McAuley, C. Targett, Q. Shi, and A. Van Den Hengel. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pages 43–52, 2015. [22] T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, pages 1979–1993, 2018. 10[23] Y . Rong, W. Huang, T. Xu, and J. Huang. Dropedge: Towards deep graph convolutional networks on node classiﬁcation. In International Conference on Learning Representations, 2019. [24] S. Ryu, Y . Kwon, and W. Y . Kim. Uncertainty quantiﬁcation of molecular property prediction with bayesian neural networks. arXiv preprint arXiv:1903.08375, 2019. [25] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiﬁcation in network data. AI magazine, pages 93–93, 2008. [26] M. Sensoy, L. Kaplan, and M. Kandemir. Evidential deep learning to quantify classiﬁcation uncertainty. In Advances in Neural Information Processing Systems, pages 3179–3189, 2018. [27] K. Sentz, S. Ferson, et al. Combination of evidence in Dempster-Shafer theory , volume 4015. Sandia National Laboratories Albuquerque, 2002. [28] O. Shchur, M. Mumme, A. Bojchevski, and S. Günnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018. [29] A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in neural information processing systems, pages 1195–1204, 2017. [30] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y . Bengio. Graph Attention Networks. International Conference on Learning Representations, 2018. [31] Z. Yang, W. Cohen, and R. Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pages 40–48. PMLR, 2016. [32] Y . Zhang, S. Pal, M. Coates, and D. Ustebay. Bayesian graph convolutional neural networks for semi- supervised classiﬁcation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 5829–5836, 2019. [33] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun. Graph neural networks: A review of methods and applications. arXiv preprint arXiv:1812.08434, 2018. 11Appendix A Proofs A.1 Theorem 1’s Proof Theorem 1. We consider a simpliﬁed scenario, where a multinomial random variable y follows a K-class categorical distribution: y∼Cal(p), the class probabilities p follow a Dirichlet distribution: p ∼Dir(α), and αrefer to the Dirichlet parameters. Given a total Dirichlet strength S = ∑K i=1 αi, for any opinion ωon a multinomial random variable y, we have 1. General relations on all prediction scenarios. (a) uv + udiss ≤1; (b) uv >uepis. 2. Special relations on the OOD and the CP . (a) For an OOD sample with a uniform prediction (i.e., α= [1,..., 1]), we have 1 = uv = uen >ualea >uepis >udiss = 0 (b) For an in-distribution sample with a conﬂicting prediction (i.e.,α= [α1,...,α K] with α1 = α2 = ··· = αK, if S →∞), we have uen = 1, lim S→∞ udiss = lim S→∞ ualea = 1, lim S→∞ uv = lim S→∞ uepis = 0 with uen >ualea >udiss >uv >uepis. Interpretation. Theorem 1.1 (a) implies that increases in both uncertainty types may not happen at the same time. A higher vacuity leads to a lower dissonance, and vice versa (a higher dissonance leads to a lower vacuity). This indicates that a high dissonance only occurs only when a large amount of evidence is available and the vacuity is low. Theorem 1.1 (b) shows relationships between vacuity and epistemic uncertainty in which vacuity is an upper bound of epistemic uncertainty. Although some existing approaches [11, 26] treat epistemic uncertainty the same as vacuity, it is not necessarily true except for an extreme case where a sufﬁciently large amount of evidence available, making vacuity close to zero. Theorem 1.2 (a) and (b) explain how entropy differs from vacuity and/or dissonance. We observe that entropy is 1 when either vacuity or dissonance is 0. This implies that entropy cannot distinguish different types of uncertainty due to different root causes. For example, a high entropy is observed when an example is an either OOD or misclassiﬁed example. Similarly, a high aleatoric uncertainty value and a low epistemic uncertainty value are observed under both cases. However, vacuity and dissonance can capture different causes of uncertainty due to lack of information and knowledge and to conﬂicting evidence, respectively. For example, an OOD objects typically show a high vacuity value and a low dissonance value while a conﬂicting prediction exhibits a low vacuity and a high dissonance. Proof. 1. (a) Let the opinion ω= [b1,...,b K,uv], where Kis the number of classes, bi is the belief for class i, uv is the uncertainty mass (vacuity), and ∑K i=1 bi + uv = 1. Dissonance has a upper bound with udiss = K∑ i=1 (bi ∑K j=1,j̸=ibjBal(bi,bj) ∑K j=1,j̸=ibj ) (12) ≤ K∑ i=1 (bi ∑K j=1,j̸=ibj ∑K j=1,j̸=ibj ) , (since 0 ≤Bal(bi,bj) ≤1) = K∑ i=1 bi, where Bal(bi,bj) is the relative mass balance, then we have uv + udiss ≤ K∑ i=1 bi + uv = 1. (13) 1. (b) For the multinomial random variable y, we have y∼Cal(p), p ∼Dir(α), (14) where Cal(p) is the categorical distribution and Dir(α) is Dirichlet distribution. Then we have Prob(y|α) = ∫ Prob(y|p)Prob(p|α)dp, (15) 12and the epistemic uncertainty is estimated by mutual information, I[y,p|α] = H [ EProb(p|α)[P(y|p)] ] −EProb(p|α) [ H[P(y|p)] ] . (16) Now we consider another measure of ensemble diversity:Expected Pairwise KL-Divergencebetween each model in the ensemble. Here the expected pairwise KL-Divergence between two independent distributions, including P(y|p1) and P(y|p2), where p1 and p2 are two independent samples from Prob(p|α), can be computed, K[y,p|α] = EProb(p1|αProb(p2|α) [ KL[P(y|p1)∥P(y|p2)] ] (17) = − K∑ i=1 EProb(p1|α)[P(y|p1)]EProb(p2|α)[ln P(y|p2)] −EProb(p|α) [ H[P(y|p)] ] ≥ I[y,p|α], where I[y,p1|α] = I[y,p2|α]. We consider Dirichlet ensemble, the Expected Pairwise KL Divergence, K[y,p|α] = − K∑ i=1 αi S ( ψ(αi) −ψ(S) ) − K∑ i=1 −αi S ( ψ(αi + 1) −ψ(S+ 1) ) = K−1 S , (18) where S = ∑K i=1 αi and ψ(·) is the digamma Function, which is the derivative of the natural logarithm of the gamma function. Now we obtain the relations between vacuity and epistemic, K S Vacuity >K[y,p|α] = K−1 S ≥I[y,p|α]   Epistemic . (19) 2. (a) For an out-of-distribution sample, α= [1,..., 1], the vacuity can be calculated as uv = K∑K i=1 αi = K K = 1, (20) and the belief mass bi = (αi −1)/∑K i=1 αi = 0, we estimate dissonance, udiss = K∑ i=1 (bi ∑K j=1,j̸=ibjBal(bi,bj) ∑K j=1,j̸=ibj ) = 0. (21) Given the expected probability ˆp= [1/K,..., 1/K]⊤, the entropy is calculated based on logK, uen = H[ˆp] = − K∑ i=1 ˆpilogK ˆpi = − K∑ i=1 1 K logK 1 K = logK 1 K −1 = logKK = 1, (22) 13where H(·) is the entropy. Based on Dirichlet distribution, the aleatoric uncertainty refers to the expected entropy, ualea = Ep∼Dir(α)[H[p]] (23) = − K∑ i=1 Γ(S)∏K i=1 Γ(αi) ∫ SK pilogKpi K∏ i=1 pαi−1 i dp = − 1 ln K K∑ i=1 Γ(S)∏K i=1 Γ(αi) ∫ SK piln pi K∏ i=1 pαi−1 i dp = − 1 ln K K∑ i=1 αi S Γ(S+ 1) Γ(αi + 1)∏K i′=1,̸=iΓ(αi′) ∫ SK pαi i ln pi K∏ i′=1,̸=i p αi′−1 i′ dp = 1 ln K K∑ i=1 αi S ( ψ(S+ 1) −ψ(αi + 1) ) = 1 ln K K∑ i=1 1 K(ψ(K+ 1) −ψ(2)) = 1 ln K(ψ(K+ 1) −ψ(2)) = 1 ln K(ψ(2) + K∑ k=2 1 k −ψ(2)) = 1 ln K K∑ k=2 1 k < 1 ln K ln K = 1, where S = ∑K i=1 αi, p= [p1,...,p K]⊤, and K ≥2 is the number of category. The epistemic uncertainty can be calculated via the mutual information, uepis = H[Ep∼Dir(α)[p]] −Ep∼Dir(α)[H[p]] (24) = H[ˆp] −ualea = 1 − 1 ln K K∑ k=2 1 k <1. To compare aleatoric uncertainty with epistemic uncertainty, we ﬁrst prove that aleatoric uncertainty (Eq.(24)) is monotonically increasing and converging to 1 as Kincreases. Based on Lemma 1, we have ( ln(K+ 1) −ln K ) K∑ k=2 1 k < ln K K+ 1 ⇒ln(K+ 1) K∑ k=2 1 k <ln K ( K∑ k=2 1 k + 1 K+ 1 ) = ln K K+1∑ k=2 1 k ⇒ 1 ln K K∑ k=2 1 k < 1 ln(K+ 1) K+1∑ k=2 1 k. (25) Based on Eq. (25) and Eq. (24), we prove that aleatoric uncertainty is monotonically increasing with respect to K. So the minimum aleatoric can be shown to be 1 ln 2 1 2 , when K = 2. Similarly, for epistemic uncertainty, which is monotonically decreasing asKincreases based on Lemma 1, the maximum epistemic can be shown to be 1 − 1 ln 2 1 2 when K = 2. Then we have, ualea ≥ 1 ln 2 1 2 >1 − 1 2 ln 2 ≥uepis (26) Therefore, we prove that 1 = uv = uen >ualea >uepis >udiss = 0. 2. (b) For a conﬂicting prediction, i.e., α = [ α1,...,α K], with α1 = α2 = ··· = αK = C, and S =∑K i=1 αi = CK, the expected probability ˆp= [1/K,..., 1/K]⊤, the belief mass bi = (αi −1)/S, and the vacuity can be calculated as uv = K S S→∞ −−−−→0, (27) 14and the dissonance can be calculated as udiss = K∑ i=1 (bi ∑K j=1,j̸=ibjBal(bi,bj) ∑K j=1,j̸=ibj ) = K∑ i=1 bi (28) = K∑ i=1 ( ai −1∑K i=1 ai ) = ∑K i=1 ai −k∑K i=1 ai = 1 −K S S→∞ −−−−→1. Given the expected probability ˆp = [1 /K,..., 1/K]⊤, the entropy can be calculated based on Dirichlet distribution, uen = H[ˆp] = K∑ i=1 ˆpilogK ˆpi = 1, (29) and the aleatoric uncertainty is estimated as the expected entropy, ualea = Ep∼Dir(α)[H[p]] (30) = − K∑ i=1 Γ(S)∏K i=1 Γ(αi) ∫ SK pilogKpi K∏ i=1 pαi−1 i dp = − 1 ln K K∑ i=1 Γ(S)∏K i=1 Γ(αi) ∫ SK piln pi K∏ i=1 pαi−1 i dp = − 1 ln K K∑ i=1 αi S Γ(S+ 1) Γ(αi + 1)∏K i′=1,̸=iΓ(αi′) ∫ SK pαi i ln pi K∏ i′=1,̸=i p αi′−1 i′ dp = 1 ln K K∑ i=1 αi S ( ψ(S+ 1) −ψ(αi + 1) ) = 1 ln K K∑ i=1 1 K(ψ(S+ 1) −ψ(C+ 1)) = 1 ln K(ψ(S+ 1) −ψ(C+ 1)) = 1 ln K(ψ(C+ 1) + S∑ k=C+1 1 k −ψ(C+ 1)) = 1 ln K S∑ k=C+1 1 k S→∞ −−−−→1. The epistemic uncertainty can be calculated via mutual information, uepis = H[Ep∼Dir(α)[p]] −Ep∼Dir(α)[H[p]] (31) = H[ˆp] −ualea = 1 − 1 ln K S∑ k=C+1 1 k S→∞ −−−−→0. 15Now we compare aleatoric uncertainty with vacuity, ualea = 1 ln K S∑ k=C+1 1 k (32) = 1 ln K CK∑ k=C+1 1 k = ln(CK + 1) −ln(C+ 1) ln K = ln(K−K−1 C+1 ) ln K > ln(K−K−1 2 ) ln K = ln(4/K+ 4/K+ 1/2) ln K ≥ ln[3(4/K+ 4/K+ 1/2) 1 3 ] ln K = ln 3 + 1 3 ln(K2 32 ) ln K = ln 3 + 2 3 ln K−1 3 ln 32 ln K > 2 3. Based on Eq. (33), when C >3 2 , we have ualea > 2 3 > 1 C = uv (33) We have already proved that uv > uepis, when uen = 1, we have ualea > udiss Therefore, we prove that uen >ualea >udiss >uv >uepis with uen = 1,udiss →1,ualea →1,uv →0,uepis →0 Lemma 1. For all integerN ≥2, we have ∑N n=2 1 n < ln N (N+1) ln(N+1 N ) . Proof. We will prove by induction that, for all integerN ≥2, N∑ n=2 1 n < ln N (N + 1) ln(N+1 N ). (34) Base case: When N = 2, we have 1 2 < ln 2 3 ln 3 2 and Eq. (34) is true for N = 2. Induction step: Let the integer K ≥2 is given and suppose Eq. (34) is true for N = K, then K+1∑ k=2 1 k = 1 K+ 1 + K∑ k=2 1 k < 1 K+ 1 + ln K (K+ 1) ln(K+1 K ) = ln(K+ 1) (K+ 1) ln(K+1 K ). (35) Denote that g(x) = (x+ 1) ln(x+1 x ) with x> 2. We get its derivative, g′(x) = ln(1 + 1 x) −1 x <0, such that g(x) is monotonically decreasing, which results in g(K) >g(K+ 1). Based on Eq. (35) we have, K+1∑ k=2 1 k < ln(K+ 1) g(K) < ln(K+ 1) g(K+ 1) = ln(K+ 1) (K+ 2) ln(K+2 K+1 ). (36) Thus, Eq. (34) holds for N = K+ 1, and the proof of the induction step is complete. Conclusion: By the principle of induction, Eq. (34) is true for all integer N ≥2. A.2 Proposition 1’s Proof Proposition 1. Given Ltraining nodes, for any testing nodes iand j, let di = [di1,...,d iL] be the vector of graph distances from nodes ito training nodes and dj = [dj1,...,d jL] be the graph distances from nodes jto training nodes, where dil is the node-level distance between nodes iand l. If for all l∈{1,...,L }, dil ≥djl, then we have ˆuvi ≥ˆuvj , where ˆuvi and ˆuvj refer to vacuity uncertainties of nodes iand jestimated based on GKDE. 16Interpretation. From the above proposition, if a testing node is too distant (far away) from training nodes, the vacuity increases, indicating that an OOD node is expected to have a high vacuity value. Proof. Let y= [y1,...,y L] be the label vector for training nodes. Based on GKDE, the evidence contribution for the node iand a training node l∈{1,...,L |}is h(yl,dil) = [h1(yl,dil),...,h K(yl,dil)], where hk(yl,dil) = { 0 yl ̸= k g(dil) = 1 σ √ 2π exp(−dil2 2σ2 ) yl = k, (37) and the prior evidence can be estimated based GKDE: ˆei = L∑ m=1 K∑ k=1 hk(yl,dil), (38) where ˆei = [ei1,...,e iK]. Since each training node only contributes the same evidence based on its label based on Eq. (37), the total evidence is estimated by all the contributing evidence as K∑ k=1 eik = L∑ m=1 1 σ √ 2π exp(−dil 2 2σ2 ), K∑ k=1 ejk = L∑ m=1 1 σ √ 2π exp(−djl 2 2σ2 ), (39) where the vacuity values for node iand node jbased on GKDE are, ˆuvi = K∑K k=1 eik + K , ˆuvj = K∑K k=1 ejk + K . (40) Now, we prove Eq. (40) above. If dil ≥djl for ∀l∈{1,...,L }, we have K∑ k=1 eik = L∑ m=1 1 σ √ 2π exp(−dil 2 2σ2 ) (41) ≤ L∑ m=1 1 σ √ 2π exp(−djl 2 2σ2 ) = K∑ k=1 ejk, such that ˆuvi = K∑K k=1 eik + K ≥ K∑K k=1 ejk + K = ˆuvj . (42) B Additional Experimental Details B.1 Source code The source code and datasets are accessible at https://github.com/zxj32/uncertainty-GNN B.2 Description of Datasets Table 4: Description of datasets and their experimental setup for the node classiﬁcation prediction. Cora Citeseer Pubmed Co. Physics Ama.Computer Ama.Photo #Nodes 2,708 3,327 19,717 34, 493 13, 381 7, 487 #Edges 5,429 4,732 44,338 282, 455 259, 159 126, 530 #Classes 7 6 3 5 10 8 #Features 1,433 3,703 500 8,415 767 745 #Training nodes 140 120 60 100 200 160 #Validation nodes 500 500 500 500 500 500 #Test nodes 1,000 1,000 1,000 1000 1,000 1000 Cora, Citeseer, and Pubmed [25]: These are citation network datasets, where each network is a directed network in which a node represents a document and an edge is a citation link, meaning that there exists an 17edge when Adocument cites Bdocument, or vice-versa with a direction. Each node’s feature vector contains a bag-of-words representation of a document. For simplicity, we don’t discriminate the direction of links and treat citation links as undirected edges and construct a binary, symmetric adjacency matrix A. Each node is labeled with the class to which it belongs. Coauthor Physics, Amazon Computers, and Amazon Photo [28]: Coauthor Physics is the dataset for co- authorship graphs based on the Microsoft Academic Graph from the KDD Cup 2016 Challenge2. In the graphs, a node is an author and an edge exists when two authors co-author a paper. A node’s features represent the keywords of its papers and the node’s class label indicates its most active ﬁeld of study. Amazon Computers and Amazon Photo are the segments of an Amazon co-purchase graph [21], where a node is a good (i.e., product), an edge exists when two goods are frequently bought together. A node’s features are bag-of-words representation of product reviews and the node’s class label is the product category. For all the used datasets, we deal with undirected graphs with 20 training nodes for each category. We chose the same dataset splits as in [ 31] with an additional validation node set of 500 labeled examples for the hyperparameter obtained from the citation datasets, and followed the same dataset splits in [28] for Coauthor Physics, Amazon Computer, and Amazon Photo datasets, for the fair comparison3. Metric: We used the following metrics for our experiments: • Area Under Receiver Operating Characteristics (AUROC): AUROC shows the area under the curve where FPR (false positive rate) is in x-axis and TPR (true positive rate) is in y-axis. It can be interpreted as the probability that a positive example is assigned a higher detection score than a negative example[4]. A perfect detector corresponds to an AUROC score of 100%. • Area Under Precision-Prediction Curve (AUPR): The PR curve is a graph showing the precision=TP/(TP+FP) and recall=TP/(TP+FN) against each other,and AUPR denotes the area under the precision-recall curve. The ideal case is when Precision is 1 and Recall is 1. B.3 Experimental Setup for Out-of-Distribution (OOD) Detection For OOD detection on semi-supervised node classiﬁcation, we randomly selected 1-4 categories as OOD categories and trained the models only based on training nodes of the other categories. In this setting, we still trained a model for semi-supervised node classiﬁcation task, but only part of node categories were not used for training. Hence, we suppose that our model only outputs partial categories (as we don’t know the OOD category), see Table 5. For example, Cora dataset, we trained the model with 80 nodes (20 nodes for each category) with the predictions of 4 categories. Positive ratio is the ratio of out-of-distribution nodes among on all test nodes. Table 5: Description of datasets and their experimental setup for the OOD detection. Dataset Cora Citeseer Pubmed Co.Physics Ama.Computer Ama.Photo Number of training categories 4 3 2 3 5 4 Training nodes 80 60 40 60 100 80 Test nodes 1000 1000 1000 1000 1000 1000 Positive ratio 38% 55% 40.4% 45.1% 48.1% 51.1% B.4 Baseline Setting In experiment part, we considered 4 baselines. For GCN, we used the same hyper-parameters as [ 16]. For EDL-GCN, we used the same hyper-parameters as GCN, and replaced softmax layer to activation layer (Relu) with squares loss [26]. For DPN-GCN, we used the same hyper-parameters as GCN, and changed the softmax layer to activation layer (exponential). Note that as we can not generate OOD node, we only used in-distribution loss of (see Eq.12 in [20]) and ignored the OOD part loss. For Drop-GCN, we used the same hyper-parameters as GCN, and set Monte Carlo sampling times M = 100, dropout rate equal to 0.5. B.5 Time Complexity Analysis S-BGCN has a similar time complexity with GCN while S-BGCN-T has the double complexity of GCN. For a given network where |V|is the number of nodes, |E|is the number of edges, Cis the number of dimensions of the input feature vector for every node, F is the number of features for the output layer, and M is Monte Carlo sampling times. 2KDD Cup 2016 Dataset: Online Available at https://kddcup2016.azurewebsites.net/ 3The source code and datasets are accessible at https://github.com/zxj32/uncertainty-GNN 18Table 6: Big-O time complexity of our method and baseline GCN. Dataset GCN S-GCN S-BGCN S-BGCN-T S-BGCN-T-K Time Complexity (Train) O(|E|CF) O(|E|CF) O(2|E|CF) O(2|E|CF) O(2|E|CF) Time Complexity (Test) O(|E|CF) O(|E|CF) O(M|E|CF) O(M|E|CF) O(M|E|CF) B.6 Model Setups for semi-supervised node classiﬁcation Our models were initialized using Glorot initialization [8] and trained to minimize loss using the Adam SGD optimizer [15]. For the S-BGCN-T-K model, we used the early stopping strategy [28] on Coauthor Physics, Amazon Computer and Amazon Photo datasets while non-early stopping strategy was used in citation datasets (i.e., Cora, Citeseer and Pubmed). We set bandwidthσ= 1 for all datasets in GKDE, and set trade off parameters λ1 = 0.001 for misclassiﬁcation detection, λ1 = 0.1 for OOD detection and λ2 = min(1,t/200) (where tis the index of a current training epoch) for both task; other hyperparameter conﬁgurations are summarized in Table 7. For semi-supervised node classiﬁcation, we used 50 random weight initialization for our models on Citation network datasets. For Coauthor Physics, Amazon Computer and Amazon Photo datasets, we reported the result based on 10 random train/validation/test splits. In both effect of uncertainty on misclassiﬁcation and the OOD detection, we reported the AUPR and AUROC results in percent averaged over 50 times of randomly chosen 1000 test nodes in all of test sets (except training or validation set) for all models tested on the citation datasets. For S-BGCN-T-K model in these tasks, we used the same hyperparameter conﬁgurations as in Table 7, except S-BGCN-T-K Epistemic using 10,000 epochs to obtain the best result. Table 7: Hyperparameter conﬁgurations of S-BGCN-T-K model Cora Citeseer Pubmed Co.Physics Ama.Computer Ama.Photo Hidden units 16 16 16 64 64 64 Learning rate 0.01 0.01 0.01 0.01 0.01 0.01 Dropout 0.5 0.5 0.5 0.1 0.2 0.2 L2 reg.strength 0.0005 0.0005 0.0005 0.001 0.0001 0.0001 Monte-Carlo samples 100 100 100 100 100 100 Max epoch 200 200 200 100000 100000 100000 B.7 Pseudo code for Our Algorithms Algorithm 1: S-BGCN-T-K Input: G = (V,E,r) and yL Output: pV\\L, uV\\L 1 ℓ= 0; 2 Set hyper-parameters η,λ1,λ2; 3 Initialize the parameters γ,β; 4 Calculate the prior Dirichlet distribution Dir(ˆα); 5 Pretrain the teacher network to get Prob(y|ˆp); 6 repeat 7 Forward pass to compute α, Prob(pi|A,r; G) for i∈V; 8 Compute joint probability Prob(y|A,r; G); 9 Backward pass via the chain-rule the calculate the sub-gradient gradient: g(ℓ) = ∇ΘL(Θ) 10 Update parameters using step size ηvia Θ(ℓ+1) = Θ(ℓ) −η·g(ℓ) 11 ℓ= ℓ+ 1; 12 until convergence 13 Calculate pV\\L, uV\\L 14 return pV\\L, uV\\L B.8 Bayesian Inference with Dropout The marginalization in Eq.(8) (in main paper) is generally intractable. A dropout technique is used to obtain an approximate solution and use samples from the posterior distribution of models [7]. Hence, we adopted a dropout technique in [6] for variational inference in Bayesian convolutional neural networks where Bernoulli distributions are assumed over the network’s weights. This dropout technique allows us to perform probabilistic 19inference over our Bayesian DL framework using GNNs. For Bayesian inference, we identified a posterior distribution over the network’s weights, given the input graphDand observed labels yL by Prob(θ|D), where θ= {W1,..., WL,b1,...,b L}, Lis the total number of layers and Wi refers to the GNN’s weight matrices of dimensions Di ×Di−1, and bi is a bias vector of dimensions Di for layer i= 1,··· ,L. Since the posterior distribution is intractable, we used a variational inference to learn q(θ), a distribution over matrices whose columns are randomly set to zero, approximating the intractable posterior by minimizing the Kullback-Leibler (KL)-divergence between this approximated distribution and the full posterior, which is given by: KL(q(θ)∥Prob(θ|D)) (43) We deﬁne Wi in q(θ) by: Wi = Midiag([zij]Di j=1), z ij ∼Bernoulli(di) for i= 1,...,L,j = 1,...,D i−1 (44) where γ = {M1,..., ML,m1,..., mL}are the variational parameters, Mi ∈RDi×Di−1 , mi ∈RDi, and d = {d1,...,d L}is the dropout probabilities with zij of Bernoulli distributed random variables. The binary variable zij = 0 corresponds to unit jin layer i−1 being dropped out as an input to layer i. We can obtain the approximate model of the Gaussian process from [ 6]. The dropout probabilities, di’s, can be optimized or ﬁxed [13]. For simplicity, we ﬁxed di’s in our experiments, as it is beyond the scope of our study. In [6], the minimization of the cross entropy (or square error) loss function is proven to minimize the KL-divergence (see Eq. (43)). Therefore, training the GNN model with stochastic gradient descent enables learning of an approximated distribution of weights, which provides good explainability of data and prevents overﬁtting. For the dropout inference, we performed training on a DL model with dropout before every weight layer and dropout at a test time to sample from the approximate posterior (i.e., stochastic forward passes, a.k.a. Monte Carlo dropout; see Eq. (45)). At the test stage, we infer the joint probability by: p(y|A,r; D) = ∫ ∫ Prob(y|p)Prob(p|A,r; θ)Prob(θ|D)dpdθ ≈ 1 M ∑M m=1 ∫ Prob(y|p)Prob(p|A,r; θ(m))dp, θ(m) ∼q(θ), (45) where M is Monte Carlo sampling times. We can also infer the Dirichlet parameters αas: α≈ 1 M ∑M m=1 f(A,r,θ(m)), θ(m) ∼q(θ). (46) C Additional Experimental Results In addition to the uncertainty analysis in Section 5, we also conducted additional experiments. First, we conducted an ablation experiment for each component (such as GKDE, Teacher network, Subjective framework and Bayesian framework) we proposed. Second, we provide additional uncertainty visualization results in network node classiﬁcations for Citeseer dataset. To clearly understand the effect of different types of uncertainty in classiﬁcation accuracy and OOD, we used the AUROC and AUPR curves for all types of models considered in this work. C.1 Ablation Experiments We conducted an additional experiments in order to clearly demonstrate the contributions of the key technical components, including a teacher Network, Graph kernel Dirichlet Estimation (GKDE) and subjective Bayesian framework. The key ﬁndings obtained from this experiment are: (1) The teacher Network can further improve node classiﬁcation accuracy (i.e., 0.2% - 1.5% increase, as shown in Table 8); and (2) GKDE (Graph-Based Kernel Dirichlet Distribution Estimation) using the uncertainty estimates can enhance OOD detection (i.e., 4% - 30% increase, as shown in Table 9). C.2 Experiment based on GAT model We also conducted the semi-supervised node classiﬁcation based on GAT model [ 30]).Model setup: The S- BGAT-T-K model has two dropout probabilities, which are a dropout on features and a dropout on attention coefﬁcients, as shown in Table 10. We changed the dropout on attention coefﬁcients to 0.4 at the test stage and set trade off parameters λ= min(1,t/50), using the same early stopping strategy [ 30]. The result are shown in Table 11. C.3 Misclassiﬁcation Detection For Amazon Photo, Amazon Computer and Coauthor Physics dataset, the misclassiﬁcation detection results are shown in Tabel 12. 20Table 8: Ablation experiment on AUROC and AUPR for the Misclassiﬁcation Detection. Data Model AUROC AUPR Va. Dis. Al. Ep. En. Va. Dis. Al. Ep. En. Acc Cora S-BGCN-T-K 70.6 82.4 75.3 68.8 77.7 90.3 95.4 92.4 87.8 93.4 82.0 S-BGCN-T 70.8 82.5 75.3 68.9 77.8 90.4 95.4 92.6 88.0 93.4 82.2 S-BGCN 69.8 81.4 73.9 66.7 76.9 89.4 94.3 92.3 88.0 93.1 81.2 S-GCN 70.2 81.5 - - 76.9 90.0 94.6 - - 93.6 81.5 Citeseer S-BGCN-T-K 65.4 74.0 67.2 60.7 70.0 79.8 85.6 82.2 75.2 83.5 71.0 S-BGCN-T 65.4 73.9 67.1 60.7 70.1 79.6 85.5 82.1 75.2 83.5 71.3 S-BGCN 63.9 72.1 66.1 58.9 69.2 78.4 83.8 80.6 75.6 82.3 70.6 S-GCN 64.9 71.9 - - 69.4 79.5 84.2 - - 82.5 71.0 Pubmed S-BGCN-T-K 63.1 69.9 66.5 65.3 68.1 85.6 90.8 88.8 86.1 89.2 79.3 S-BGCN-T 63.2 69.9 66.6 65.3 64.8 85.6 90.9 88.9 86.0 89.3 79.2 S-BGCN 62.7 68.1 66.1 64.4 68.0 85.4 90.5 88.6 85.6 89.2 78.8 S-GCN 62.9 69.5 - - 68.0 85.3 90.4 - - 89.2 79.1 Amazon Photo S-BGCN-T-K 66.0 89.3 83.0 83.4 83.2 95.4 98.9 98.4 98.1 98.4 92.0 S-BGCN-T 66.1 89.3 83.1 83.5 83.3 95.6 99.0 98.4 98.2 98.4 92.3 S-BGCN 68.6 93.6 90.6 83.6 90.6 90.4 98.1 97.3 95.8 97.3 81.0 S-GCN - - - - 86.7 - - - - - 98.4 Amazon Computer S-BGCN-T-K 65.0 87.8 83.3 79.6 83.6 89.4 96.3 95.0 94.2 95.0 84.0 S-BGCN-T 65.2 88.0 83.4 79.7 83.6 89.4 96.5 95.0 94.5 95.1 84.1 S-BGCN 63.7 89.1 84.3 76.1 84.4 84.9 95.7 93.9 91.4 93.9 76.1 S-GCN - - - - 81.5 - - - - - 95.2 Coauthor Physics S-BGCN-T-K 80.2 91.4 87.5 81.7 87.6 98.3 99.4 99.0 98.4 98.9 93.0 S-BGCN-T 80.4 91.5 87.6 81.7 87.6 98.3 99.4 99.0 98.6 99.0 93.2 S-BGCN 79.6 90.5 86.3 81.2 86.4 98.0 99.2 98.8 98.3 98.8 92.9 S-GCN 89.1 89.0 - - 89.2 99.0 99.0 - - 99.0 92.9 Va.: Vacuity, Dis.: Dissonance, Al.: Aleatoric, Ep.: Epistemic, En.: Entropy Table 9: Ablation experiment on AUROC and AUPR for the OOD Detection. Data Model AUROC AUPR Va. Dis. Al. Ep. En. Va. Dis. Al. Ep. En. Cora S-BGCN-T-K 87.6 75.5 85.5 70.8 84.8 78.4 49.0 75.3 44.5 73.1 S-BGCN-T 84.5 81.2 83.5 71.8 83.5 74.4 53.4 75.8 46.8 71.7 S-BGCN 76.3 79.3 81.5 70.5 80.6 61.3 55.8 68.9 44.2 65.3 S-GCN 75.0 78.2 - - 79.4 60.1 54.5 - - 65.3 Citeseer S-BGCN-T-K 84.8 55.2 78.4 55.1 74.0 86.8 54.1 80.8 55.8 74.0 S-BGCN-T 78.6 59.6 73.9 56.1 69.3 79.8 57.4 76.4 57.8 69.3 S-BGCN 72.7 63.9 72.4 61.4 70.5 73.0 62.7 74.5 60.8 71.6 SGCN 72.0 62.8 - - 70.0 71.4 61.3 - - 70.5 Pubmed S-BGCN-T-K 74.6 67.9 71.8 59.2 72.2 69.6 52.9 63.6 44.0 56.5 S-BGCN-T 71.8 68.6 70.0 60.1 70.8 65.7 53.9 61.8 46.0 55.1 S-BGCN 70.8 68.2 70.3 60.8 68.0 65.4 53.2 62.8 46.7 55.4 S-GCN 71.4 68.8 - - 69.7 66.3 54.9 - - 57.5 Amazon Photo S-BGCN-T-K 93.4 76.4 91.4 32.2 91.4 94.8 68.0 92.3 42.3 92.5 S-BGCN-T 64.0 77.5 79.9 52.6 79.8 67.0 75.3 82.0 53.7 81.9 S-BGCN 63.0 76.6 79.8 52.7 79.7 66.5 75.1 82.1 53.9 81.7 S-GCN 64.0 77.1 - - 79.6 67.0 74.9 - - 81.6 Amazon Computer S-BGCN-T-K 82.3 76.6 80.9 55.4 80.9 70.5 52.8 60.9 35.9 60.6 S-BGCN-T 53.7 70.5 70.4 69.9 70.1 33.6 43.9 46.0 46.8 45.9 S-BGCN 56.9 75.3 74.1 73.7 74.1 33.7 46.2 48.3 45.6 48.3 S-GCN 56.9 75.3 - - 74.2 33.7 46.2 - - 48.3 Coauthor Physics S-BGCN-T-K 91.3 87.6 89.7 61.8 89.8 72.2 56.6 68.1 25.9 67.9 S-BGCN-T 88.7 86.0 87.9 70.2 87.8 67.4 51.9 64.6 29.4 62.4 S-BGCN 89.1 87.1 89.5 78.3 89.5 66.1 49.2 64.6 35.6 64.3 S-GCN 89.1 87.0 - - 89.4 -66.2 49.2 - - 64.3 Va.: Vacuity, Dis.: Dissonance, Al.: Aleatoric, Ep.: Epistemic, D.En.: Differential Entropy, En.: Entropy C.4 Graph Embedding Representations of Different Uncertainty Types To better understand different uncertainty types, we used t-SNE (t-Distributed Stochastic Neighbor Embed- ding [19]) to represent the computed feature representations of a pre-trained BGCN-T model’s ﬁrst hidden layer on the Cora dataset and the Citeseer dataset. Seven Classes on Cora Dataset: In Figure 4, (a) shows the representation of seven different classes, (b) shows our model prediction and (c)-(f) present the extent of uncertainty for respective uncertainty types, including vacuity, dissonance, and aleatoric uncertainty, respectively. Six Classes on Citeseer Dataset: In Figure 5 (a), a node’s color denotes a class on the Citeseer dataset where 6 different classes are shown in different colors. Figure 5 (b) is our prediction result. 21Table 10: Hyper-parameters of S-BGAT-T-K model Cora Citeseer Pubmed Hidden units 64 64 64 Learning rate 0.01 0.01 0.01 Dropout 0.6/0.6 0.6/0.6 0.6/0.6 L2 reg.strength 0.0005 0.0005 0.001 Monte-Carlo samples 100 100 100 Max epoch 100000 100000 100000 Table 11: Semi-supervised node classiﬁcation accuracy based on GAT Cora Citeseer Pubmed GAT 83.0 ±0.7 72.5 ±0.7 79.0 ±0.3 GAT-Drop 82.8 ±0.8 72.6 ±0.7 79.0 ±0.3 S-GAT 83.0 ±0.7 72.6 ±0.6 79.0 ±0.3 S-BGAT 82.9 ±0.7 72.4 ±0.7 78.9 ±0.3 S-BGAT-T 83.7 ±0.6 73.2 ±0.5 79.1 ±0.2 S-BGAT-T-K 83.8 ±0.7 73.0 ±0.7 79.1 ±0.2 Table 12: AUROC and AUPR for the Misclassiﬁcation Detection. Data Model AUROC AUPR AccVa. Dis. Al. Ep. En. Va. Dis. Al. Ep. En. Amazon Photo S-BGCN-T-K 66.0 89.3 83.0 83.4 83.2 95.4 98.9 98.4 98.1 98.4 92.0 EDL-GCN 65.1 88.5 - - 82.2 94.6 98.1 - - 98.0 91.2 DPN-GCN - - 81.8 80.8 81.3 - - 98.1 98.0 98.0 92.0 Drop-GCN - - 84.5 84.4 84.6 - - 98.2 98.1 98.2 91.3 GCN - - - - 86.8 - - - - 98.5 91.2 Amazon Computer S-BGCN-T-K 65.0 87.8 83.3 79.6 83.6 89.4 96.3 95.0 94.2 95.0 84.0 EDL-GCN 64.1 86.5 - - 82.2 93.6 97.1 - - 97.0 79.7 DPN-GCN - - 76.8 76.0 76.3 - - 94.5 94.3 94.4 84.8 Drop-GCN - - 79.1 75.9 79.2 - - 95.1 94.5 95.1 79.6 GCN - - - - 81.7 - - - - 95.4 82.6 Coauthor Physics S-BGCN-T-K 80.2 91.4 87.5 81.7 87.6 98.3 99.4 99.0 98.4 98.9 93.0 EDL-GCN 78.8 89.5 - - 86.2 96.6 97.2 - - 97.0 92.7 DPN-GCN - - 87.0 86.4 86.8 - - 99.1 99.0 99.0 92.5 Drop-GCN - - 87.6 84.1 87.7 - - 98.9 98.6 98.9 93.0 GCN - - - - 88.7 - - - - 99.0 92.8 Va.: Vacuity, Dis.: Dissonance, Al.: Aleatoric, Ep.: Epistemic, En.: Entropy Figure 4: Graph embedding representations of the Cora dataset for classes and the extent of uncer- tainty: (a) shows the representation of seven different classes; (b) shows our model prediction; and (c)-(f) present the extent of uncertainty for respective uncertainty types, including vacuity, dissonance, aleatoric, epistemic. Eight Classes on Amazon Photo Dataset: In Figure 6, a node’s color denotes vacuity uncertainty value, and the big node represent training node. These results are based on OOD detection experiment. Compare Figure 6 (a) and (b), we found that GKDE can indeed improve the OOD detection. 22Figure 5: Graph embedding representations of the Citeseer dataset for classes and the extent of uncertainty: (a) shows the representation of seven different classes, (b) shows our model prediction and (c)-(f) present the extent of uncertainty for respective uncertainty types, including vacuity, dissonance, and aleatoric uncertainty, respectively. For Figures 5 (c)-(f), the extent of uncertainty is presented where a blue color refers to the lowest uncertainty (i.e., minimum uncertainty) while a red color indicates the highest uncertainty (i.e., maximum uncertainty) based on the presented color bar. To examine the trends of the extent of uncertainty depending on either training nodes or test nodes, we draw training nodes as bigger circles than test nodes. Overall we notice that most training nodes (shown as bigger circles) have low uncertainty (i.e., blue), which is reasonable because the training nodes are the ones that are already observed. Now we discuss the extent of uncertainty under each uncertainty type. Vacuity: In Figure 6 (b), most training nodes show low uncertainty, we observe majority of OOD nodes in the button cluster show high uncertainty as appeared in red. Dissonance: In Figure 5 (d), similar to vacuity, training nodes have low uncertainty. But unlike vacuity, test nodes are much less uncertain. Recall that dissonance represents the degree of conﬂicting evidence (i.e., discrepancy between each class probability). However, in this dataset, we observe a fairly low level of dissonance and the obvious outperformance of Dissonance in node classiﬁcation prediction. Aleatoric uncertainty: In Figure 5 (e), a lot of nodes show high uncertainty with larger than 0.5 except a small amount of training nodes with low uncertainty. Epistemic uncertainty: In Figure 5 (f), most nodes show very low epistemic uncertainty values because uncertainty derived from model parameters can disappear as they are trained well. C.5 PR and ROC Curves AUPR for the OOD Detection : Figure 8 shows the AUPRC for the OOD detection when S-BGCN-T-K is used to detect OOD in which test nodes are considered based on their high uncertainty level, given a different uncertainty type, such as vacuity, dissonance, aleatoric, epistemic, or entropy (or total uncertainty). Also to check the performance of the proposed models with a baseline model, we added S-BGCN-T-K with test nodes randomly selected (i.e., Random). Obviously, in Random baseline, precision was not sensitive to increasing recall while in S-BGCN-T-K (with test nodes being selected based on high uncertainty) precision decreases as recall increases. But although most S-BGCN-T-K models with various uncertainty types used to select test nodes shows sensitive precision to increasing recall (i.e., proving uncertainty being an indicator of improving OOD detection). In addition, unlike AUPR in misclassiﬁcation detection, which showed the best performance in S-BGCN-T-K Dissonance (see Figure 7), S-BGCN-T-K Dissonance showed the second worst performance among the proposed S-BGCN-T-K models with other uncertainty types. This means that less conﬂicting information does not help OOD detection. On the other hand, overall we observed Vacuity performs the best among all. From this ﬁnding, we can claim that to improve OOD detection, less information with a high vacuity value can help boost the accuracy of the OOD detection. AUROC for the OOD Detection: First, we investigated the performance of our proposed S-BGCN-T-K models when test nodes are selected based on seven different criteria (i.e., uncertainty measures). For AUROC in Figure 9, we observed much better performance in most S-BGCN-T-K models with all uncertainty types except epistemic uncertainty. 23(a) S-BGCN-T  (b) S-BGCN-T-K Figure 6: Graph embedding representations of the Amazon Photo dataset for the extent of vacuity uncertainty based on OOD detection experiment. (a) PR curves on Cora  (b) PR curves on Citeseer  (c) PR curves on Pubmed Figure 7: PR curves of misclassiﬁcation detection for S-BGCN-T-K and other baselines, GCN-Drop and GCN. (a) Amazon Photo  (b) Amazon Computers  (c) Coauthor Physics Figure 8: PR cuves of OOD detection for S-BGCN-T-K with uncertainties. C.6 Analysis for Epistemic Uncertainty in OOD Detection Although epistemic uncertainty is known to be effective to improve OOD detection [7, 14] in computer vision applications, our results demonstrate it is less effective than our vacuity-based approach. One potential reason is that the previous success of epistemic in computer vision applications are only applied in supervised learning, but they are not sufﬁciently validated in semi-supervised learning. To back up our conclusion, designe a image classiﬁcation experiment based on MC-Drop[7] method to do the following experiment: 1) supervised learning on MNIST dataset with 50 labeled images; 2) semi-supervised learning (SSL) on MNIST dataset with 50 labeled images and 49950 unlabeled images, while there are 50% OOD images (24975 FashionMNIST images) in unlabeled set. For both experiment, we test the epistemic uncertainty on 49950 unlabeled set (50% In-distribution (ID) images and 50% OOD images). We conduct the experiment the 24(a) Amazon Photo  (b) Amazon Computers  (c) Coauthor Physics Figure 9: ROC curves of OOD detection for S-BGCN-T-K with uncertainties. experiment based on three popular SSL methods, V AT [22], Mean Teacher [29] and pseudo label [17]. Table 13 Table 13: Epistemic uncertainty for semi-supervised image classiﬁcation. Epistemic Supervised V AT Mean Teacher Pseudo Label In-Distribution 0.140 0.116 0.105 0.041 Out-of-Distribution 0.249 0.049 0.076 0.020 shows the average epistemic uncertainty value for in-distribution samples and OOD samples. The result shows the same pattern with [14, 13] in a supervised setting, but an opposite pattern in a semi-supervised setting that low epistemic of OOD samples, which is less effective top detect OOD. Note that the SSL setting is similar to our semi-supervised node classiﬁcation setting, which feed the unlabeled sample to train the model. C.7 Compare with Bayesian GCN baseline Compare with a (Bayesian) GCN baseline, Dropout+DropEdge [ 23]. As shown in the table 14 below, our proposed method performed better than Dropout+DropEdge on the Cora and Citeer datasets for misclassiﬁcaiton detection. A similar trend was observed for OOD detection. Table 14: Compare with DropEdge on Misclassiﬁcation Detection . Dataset Model AUROC AUPR Va. Dis. Al. Ep. En. Va. Dis. Al. Ep. En. Cora S-BGCN-T-K 70.6 82.4 75.3 68.8 77.7 90.3 95.4 92.4 87.8 93.4 DropEdge - - 76.6 56.1 76.6 - - 93.2 85.4 93.2 Citeseer S-BGCN-T-K 65.4 74.0 67.2 60.7 70.0 79.8 85.6 82.2 75.2 83.5 DropEdge - - 71.1 51.2 71.1 - - 84.0 70.3 84.0 Va.: Vacuity, Dis.: Dissonance, Al.: Aleatoric, Ep.: Epistemic, En.: Entropy D Derivations for Joint Probability and KL Divergence D.1 Joint Probability At the test stage, we infer the joint probability by: 25p(y|A,r; G) = ∫ ∫ Prob(y|p)Prob(p|A,r; θ)Prob(θ|G)dpdθ ≈ ∫ ∫ Prob(y|p)Prob(p|A,r; θ)q(θ)dpdθ ≈ 1 M M∑ m=1 ∫ Prob(y|p)Prob(p|A,r; θ(m))dp, θ(m) ∼q(θ) ≈ 1 M M∑ m=1 ∫ N∑ i=1 Prob(yi|pi)Prob(pi|A,r; θ(m))dpi, θ(m) ∼q(θ) ≈ 1 M M∑ m=1 N∑ i=1 ∫ Prob(yi|pi)Prob(pi|A,r; θ(m))dpi, θ(m) ∼q(θ) ≈ 1 M M∑ m=1 N∏ i=1 ∫ Prob(yi|pi)Dir(pi|α(m) i )dpi, α(m) = f(A,r,θ(m)),q θ(m) ∼q(θ), where the posterior over class label pwill be given by the mean of the Dirichlet: Prob(yi = p|θ(m)) = ∫ Prob(yi = p|pi)Prob(pi|A,r; θ(m))dpi = α(m) ip ∑K k=1 α(m) ik . The probabilistic form for a speciﬁc node iby using marginal probability, Prob(yi|A,r; G) = ∑ y\\yi Prob(y|A,r; G) = ∑ y\\yi ∫ ∫ N∏ j=1 Prob(yj|pj)Prob(pj|A,r; θ)Prob(θ|G)dpdθ ≈ ∑ y\\yi ∫ ∫ N∏ j=1 Prob(yj|pj)Prob(pj|A,r; θ)q(θ)dpdθ ≈ M∑ m=1 ∑ y\\yi ∫ N∏ j=1 Prob(yj|pj)Prob(pj|A,r; θ(m))dp, θ(m) ∼q(θ) ≈ M∑ m=1 [∑ y\\yi ∫ N∏ j=1 Prob(yj|pj)Prob(pj|A,r; θ(m))dpj ] , θ(m) ∼q(θ) ≈ M∑ m=1 [∑ y\\yi N∏ j=1,j̸=i Prob(yj|A,rj; θ(m)) ] Prob(yi|A,r; θ(m)), θ(m) ∼q(θ) ≈ M∑ m=1 ∫ Prob(yi|pi)Prob(pi|A,r; θ(m))dpi, θ(m) ∼q(θ). To be speciﬁc, the probability of label pis, Prob(yi = p|A,r; G) ≈ 1 M M∑ m=1 α(m) ip ∑K k=1 α(m) ik , α(m) = f(A,r,θ(m)), θ(m) ∼q(θ). 26D.2 KL-Divergence KL-divergence between Prob(y|r; γ,G) and Prob(y|ˆp) is given by KL[Prob(y|A,r; G)||Prob(y|ˆp))] = EProb(y|A,r;G) [ log Prob(y|A,r; G) Prob(y|ˆp) ] ≈ EProb(y|A,r;G) [ log ∏N i=1 Prob(yi|A,r; G)∏N i=1 Prob(y|ˆp) ] ≈ EProb(y|A,r;G) [ N∑ i=1 log Prob(yi|A,r; G) Prob(y|ˆp) ] ≈ N∑ i=1 EProb(y|A,r;G) [ log Prob(yi|A,r; G) Prob(y|ˆp) ] ≈ N∑ i=1 K∑ j=1 Prob(yi = j|A,r; G) ( log Prob(yi = j|A,r; G) Prob(yi = j|ˆp) ) The KL divergence between two Dirichlet distributions Dir(α) and Dir(ˆα) can be obtained in closed form as, KL[Dir(α)∥Dir(ˆα)] = ln Γ(S) −ln Γ(ˆS) + K∑ c=1 ( ln Γ(ˆαc) −ln Γ(αc) ) + K∑ c=1 (αc −ˆαc)(ψ(αc) −ψ(S)), where S = ∑K c=1 αc and ˆS = ∑K c=1 ˆαc. 27",
      "references": [
        "Intelligent control: fuzzy logic applications",
        "Decomposition of uncertainty in bayesian deep learning for efﬁcient and risk-sensitive learning.",
        "The power of certainty: A dirichlet-multinomial model for belief propagation",
        "An introduction to roc analysis",
        "Uncertainty in deep learning",
        "Bayesian convolutional neural networks with bernoulli approximate variational inference",
        "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
        "Understanding the difﬁculty of training deep feedforward neural networks",
        "A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks",
        "Distilling the knowledge in a neural network",
        "Subjective logic",
        "Uncertainty characteristics of subjective opinions",
        "Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding",
        "What uncertainties do we need in bayesian deep learning for computer vision?",
        "Adam: A method for stochastic optimization",
        "Semi-supervised classification with graph convolutional networks",
        "Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks",
        "Uncertainty aware graph gaussian process for semi-supervised learning",
        "Visualizing data using t-sne",
        "Predictive uncertainty estimation via prior networks",
        "Image-based recommendations on styles and substitutes",
        "Virtual adversarial training: a regularization method for supervised and semi-supervised learning",
        "Dropedge: Towards deep graph convolutional networks on node classiﬁcation",
        "Uncertainty quantification of molecular property prediction with bayesian neural networks",
        "Collective classification in network data",
        "Evidential deep learning to quantify classification uncertainty",
        "Combination of evidence in Dempster-Shafer theory",
        "Pitfalls of graph neural network evaluation",
        "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
        "Graph Attention Networks",
        "Revisiting semi-supervised learning with graph embeddings",
        "Bayesian graph convolutional neural networks for semi- supervised classification",
        "Graph neural networks: A review of methods and applications"
      ],
      "meta_data": {
        "arxiv_id": "2010.12783v2",
        "authors": [
          "Xujiang Zhao",
          "Feng Chen",
          "Shu Hu",
          "Jin-Hee Cho"
        ],
        "published_date": "2020-10-24T04:56:46Z",
        "github_url": "https://github.com/zxj32/uncertainty-GNN"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper tackles uncertainty in semi-supervised node classification on graphs by integrating multi-source uncertainties from two domains: (i) deep learning based probabilistic uncertainty (aleatoric and epistemic) and (ii) belief/evidence theory based uncertainty (vacuity and dissonance) via Subjective Logic. The key contributions are: (a) a unified uncertainty-aware framework for Graph Neural Networks that predicts node-level Dirichlet distributions (i.e., node-level opinions) to quantify diverse uncertainties, (b) Graph-based Kernel Dirichlet Distribution Estimation (GKDE) to estimate prior Dirichlet parameters from graph structure and training labels, aiding robust uncertainty estimation and OOD detection, (c) theoretical analysis establishing relationships among different uncertainty types and showing, for instance, that OOD nodes tend to have high vacuity while in-distribution conflicting predictions have high dissonance, and (d) extensive empirical evaluation on six real-world graphs demonstrating that dissonance excels for misclassification detection and vacuity excels for OOD detection; GKDE and a Teacher Network further improve performance. A theoretical proof clarifies the interactions between uncertainty types, and ablation studies validate the contributions of GKDE, Teacher Network, and the overall Bayesian framework.",
        "methodology": "The authors combine graph neural networks with evidential reasoning by representing predictions per node as multinomial opinions, equivalently Dirichlet distributions Dir(α) over K classes. Key components: (i) Subjective GNN (S-GNN) that outputs Dirichlet concentration αi for each node i, using non-negative activations (e.g., ReLU) instead of softmax to model a full Dirichlet, (ii) Subjective Bayesian GNN (S-BGNN) that places a variational Bayesian treatment on network weights via dropout, enabling posterior samples θ(m) and approximate Bayesian model averaging to estimate P(y|A,r;G). Loss combines prediction error and uncertainty terms: L(θ)=sum over labeled nodes of squared error plus Var(pik) (i.e., reduces bias and variance). (iii) Graph-based Kernel Dirichlet distribution Estimation (GKDE): uses training labels as evidence and Gaussian kernels over graph distances to estimate prior Dirichlet αˆ for each node j, αˆj= eˆj+1 with eˆj derived from kernel-weighted evidence from training nodes. Minimizes KL(Dir(α) || Dir(αˆ)). Proposition links GKDE vacuity to distance from labeled data, implying higher vacuity for distant (likely OOD) nodes. (iv) Teacher Network distillation: a pre-trained teacher GNN provides target probabilities pˆ; the student S-BGNN-T-K minimizes KL between Dir(α) and Dir(αˆ) and KL between P(y|A,r;G) and P(y|ˆp). Training pipeline: end-to-end optimization of α and θ with dropout-based Bayesian inference and GKDE regularization.  (v) Theoretical framework for uncertainties: interprets vacuity (lack of evidence) and dissonance (conflicting evidence) and contrasts them with aleatoric and epistemic uncertainties; a set of relationships (Theorem 1) clarifies how these uncertainties interact and how entropy cannot distinguish OOD from misclassification, whereas vacuity and dissonance can prune those cases. GKDE acts as a principled prior that ties structural proximity to evidential strength.",
        "experimental_setup": "Datasets: six real networks: Cora, Citeseer, Pubmed (citation networks) and Coauthor Physics, Amazon Computers, Amazon Photo (co-authorship/product graphs). Datasets are standard splits with 20 training nodes per class on the citation graphs, plus a 500-node validation set and 1000 test nodes; for Amazon-related graphs, multiple splits were used (see Appendix). OOD setup: for OOD detection, 1–4 categories are withheld from training as OOD categories; test nodes come from the remaining categories; evaluation uses AUROC and AUPR. Baselines include Softmax-GCN (entropy), Drop-GCN (Monte Carlo dropout), EDL-GCN (evidential deep learning), and DPN-GCN (probabilistic uncertainty). Metrics: AUROC and AUPR for both misclassification detection and OOD detection. Implementation and experiments follow the GCN backbone with various uncertainty augmentations; time complexity and ablation analyses are provided. Hyperparameters include GKDE bandwidth σ=1, and training with dropout-based Bayesian inference; teacher network regularization terms; and loss coefficients λ1, λ2 for KL terms. Ablation studies quantify GKDE, Teacher Network, and Bayesian components across datasets. 6 datasets are used with reported improvements in misclassification and OOD tasks; supplementary materials provide extended results and GAT-based variants.",
        "limitations": "Limitations and scope considerations include: (1) dependence on GKDE hyperparameters (e.g., bandwidth σ, kernel choices) and the assumption that short_path distances capture evidential similarity; (2) reliance on labeled training data to provide evidence; (3) semi-supervised setup is validated on six medium-sized graphs; scalability concerns on very large graphs or dynamic/heterogeneous graphs are not deeply explored; (4) approximate Bayesian inference via dropout provides an inexpensive estimator but may introduce bias in uncertainty estimates; (5) the framework focuses on vacuity and dissonance, with limited exploration of other uncertainty dimensions such as consonance or monosonance; (6) theoretical results rely on a simplified Dirichlet-multinomial setting (Theorem 1) and may not capture all complexities of real-world graphs; (7) OOD evaluation involves artificially constructed OOD categories via label withholding which may not reflect all real-world OOD scenarios.",
        "future_research_directions": "Potential extensions include: (i) exploring more uncertainty dimensions (e.g., consonance, vagueness), (ii) applying GKDE to other graph tasks or heterogeneous/dynamic graphs, (iii) integrating active learning strategies to query uncertain nodes and improve evidence, (iv) scaling GKDE to larger graphs with efficient kernel estimations, (v) evaluating on more diverse domains (e.g., biology, social networks, traffic) and real-world safety-critical settings, (vi) combining with other GNN architectures (e.g., GAT, GraphSAGE) and training regimes, (vii) enhancing theoretical guarantees for uncertainty decomposition in broader Bayesian graph models, (viii) studying the interaction of multi-source uncertainty with adversarial robustness and model explainability.",
        "experimental_code": "# GKDE experimental code fragments (Kernel_Graph.py)\n\ndef kernel_distance(x, sigma=1.0):\n    coffit = 1.0/ (2*sigma*np.sqrt(2 * math.pi))\n    k_dis = np.exp(-np.square(x)/(2 * np.square(sigma)))\n    return k_dis\n\n\ndef all_kde(sigma):\n    datasets = ['cora', 'citeseer', 'pubmed']\n    for dataset in datasets[:]:\n        adj, idx_train, y_train, train_mask = get_data(dataset)\n        node_num = len(y_train)\n        class_num = y_train.shape[1]\n        G = nx.from_numpy_array(adj)\n        alpha = np.zeros_like(y_train)\n\n        for i in range(len(y_train)):\n            graph_dis_i = np.zeros(node_num)\n            for j in idx_train:\n                try:\n                    graph_dis_i_j = nx.shortest_path_length(G, i, j)\n                except nx.NetworkXNoPath:\n                    graph_dis_i_j = 1e10\n                graph_dis_i[j] = graph_dis_i_j\n            kernel_dis = kernel_distance(graph_dis_i, sigma=sigma)\n            kernel_alpha_i = np.reshape(kernel_dis, [-1, 1]) * y_train\n            alpha_i = np.sum(kernel_alpha_i, axis=0)\n            alpha[i] = alpha_i\n        train_mask = True\n        acc = masked_accuracy_numpy(alpha, y_train, train_mask)\n        print(acc)\n        np.save('data/prior/all_prior_alpha_{}_sigma_{}.npy'.format(dataset, sigma), alpha + 1)\n    print('Xujiang Zhao')\n\n\ndef all_kde_ood(sigma):\n    datasets = ['cora', 'citeseer', 'pubmed']\n    for dataset in datasets[0:3]:\n        adj, idx_train, y_train, train_mask = get_data_ood(dataset)\n        node_num = len(y_train)\n        class_num = y_train.shape[1]\n        G = nx.from_numpy_array(adj)\n        alpha = np.zeros_like(y_train)\n\n        for i in range(len(y_train)):\n            graph_dis_i = np.zeros(node_num)\n            for j in idx_train:\n                try:\n                    graph_dis_i_j = nx.shortest_path_length(G, i, j)\n                except nx.NetworkXNoPath:\n                    graph_dis_i_j = 1e10\n                graph_dis_i[j] = graph_dis_i_j\n            kernel_dis = kernel_distance(graph_dis_i, sigma=sigma)\n            kernel_alpha_i = np.reshape(kernel_dis, [-1, 1]) * y_train\n            alpha_i = np.sum(kernel_alpha_i, axis=0)\n            alpha[i] = alpha_i\n        train_mask = True\n        acc = masked_accuracy_numpy(alpha, y_train, train_mask)\n        print(acc)\n        np.save('data/prior/all_prior_alpha_{}_sigma_{}_ood.npy'.format(dataset, sigma), alpha + 1)\n    print('Xujiang Zhao')\n\n\ndef all_kde_ood_npy(sigma):\n    datasets = ['amazon_electronics_photo', 'amazon_electronics_computers', 'ms_academic_phy']\n    for dataset in datasets[2:3]:\n        adj, idx_train, y_train, train_mask = load_npz_data_ood(dataset, 223)\n        node_num = len(y_train)\n        class_num = y_train.shape[1]\n        G = nx.from_numpy_array(adj)\n        alpha = np.zeros_like(y_train)\n\n        for i in range(len(y_train)):\n            graph_dis_i = np.zeros(node_num)\n            for j in idx_train:\n                try:\n                    graph_dis_i_j = nx.shortest_path_length(G, i, j)\n                except nx.NetworkXNoPath:\n                    graph_dis_i_j = 1e10\n                graph_dis_i[j] = graph_dis_i_j\n            kernel_dis = kernel_distance(graph_dis_i, sigma=sigma)\n            kernel_alpha_i = np.reshape(kernel_dis, [-1, 1]) * y_train\n            alpha_i = np.sum(kernel_alpha_i, axis=0)\n            alpha[i] = alpha_i\n        acc = masked_accuracy_numpy(alpha, y_train, train_mask)\n        print(acc)\n        np.save('data/prior/all_prior_alpha_{}_sigma_{}_ood.npy'.format(dataset, sigma), alpha + 1)\n    print('Xujiang Zhao')\n\n\nif __name__ == '__main__':\n    all_kde(sigma=1)\n    all_kde_ood_npy(sigma=1)\n    all_kde_ood(sigma=1)\n",
        "experimental_info": "Experimental settings extracted from code implementing GKDE priors for a semi-supervised GNN with evidential reasoning. Key points:\n- Datasets used for KDE priors: cora, citeseer, pubmed; with optional extra datasets for out-of-distribution priors (amazon_electronics_photo, amazon_electronics_computers, ms_academic_phy).\n- Kernel distance: Gaussian kernel with bandwidth sigma (kernel_distance function). Default usage in code uses sigma=1.\n- Prior construction: For each node i, compute graph distance to a set of training nodes idx_train; convert to kernel distances; multiply kernel distances by training labels y_train to accumulate evidence; alpha_i = sum(kernel_dis * y_train) across training nodes; then save prior alpha as alpha + 1 for Dirichlet prior.\n- Output: saves priors to data/prior/all_prior_alpha_<dataset>_sigma_<sigma>.npy or _ood variants; prints acc as quick sanity check.\n- In main, the all_kde pipeline is executed when run as a script. The code provides three functions for standard KDE priors, OOD priors, and OOD priors for specific NPZ-based datasets.\n- Relationship to method: GKDE provides the prior Dirichlet parameters for each node, which are later used by the S_BGCN_T_K model as prior_alpha in Dirichlet-based evidential learning with a teacher network."
      }
    },
    {
      "title": "Entropy Minimization In Emergent Languages",
      "full_text": "Entropy Minimization In Emergent Languages Eugene Kharitonov 1 Rahma Chaabouni 1 2 Diane Bouchacourt 1 Marco Baroni 1 3 Abstract There is growing interest in studying the lan- guages that emerge when neural agents are jointly trained to solve tasks requiring communication through a discrete channel. We investigate here the information-theoretic complexity of such lan- guages, focusing on the basic two-agent, one- exchange setup. We ﬁnd that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the com- municating agent’s inputs and the messages is minimized, within the range afforded by the need for successful communication. That is, emer- gent languages are (nearly) as simple as the task they are developed for allow them to be. This pressure is ampliﬁed as we increase communica- tion channel discreteness. Further, we observe that stronger discrete-channel-driven entropy min- imization leads to representations with increased robustness to overﬁtting and adversarial attacks. We conclude by discussing the implications of our ﬁndings for the study of natural and artiﬁcial communication systems. 1. Introduction There has recently been much interest in the analysis of the communication systems arising when deep network agents that interact to accomplish a goal are allowed to exchange language-like discrete messages (Lazaridou et al., 2016; Havrylov & Titov, 2017; Choi et al., 2018; Lazaridou et al., 2018; Li & Bowling, 2019; Chaabouni et al., 2020). Under- standing the emergent protocol is important if we want to eventually develop agents capable of interacting with each other and with us through language (Mikolov et al., 2016; 1Facebook AI Research, Paris, France 2Cognitive Machine Learning (ENS - EHESS - PSL - CNRS - INRIA) 3Catalan Insti- tute for Research and Advanced Studies, Barcelona, Spain. Corre- spondence to: Eugene Kharitonov <kharitonov@fb.com>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Chevalier-Boisvert et al., 2019). The pursuit might also provide comparative evidence about how core properties of human language have evolved (Kirby, 2002; Hurford, 2014; Harding Graesser et al., 2019). While earlier stud- ies reported ways in which deep agent protocols radically depart from human language (Kottur et al., 2017; Boucha- court & Baroni, 2018; Chaabouni et al., 2019; Lowe et al., 2019), we show here that emergent communication shares an important property of the latter, namely a tendency to- wards entropy minimization. Converging evidence shows that efﬁciency pressures are at work in language and other biological communication systems (Ferrer i Cancho et al., 2013; Gibson et al., 2019). One particular aspect of communicative efﬁciency, robustly observed across many semantic domains, is the tendency to minimize lexicon entropy, to the extent allowed by the coun- teracting need for accuracy (Zaslavsky et al., 2018; 2019). For example, while most languages distinguish grandmoth- ers from grandfathers, few have separate words for mother- and father-side grandmothers, as the latter distinction makes communication only slightly more accurate at the cost of an increase in lexicon complexity (Kemp & Regier, 2012). We show here, in two separate games designed to precisely mea- sure such property, that the protocol evolved by interacting deep agents is subject to the same complexity minimiza- tion pressure. Entropy minimization in natural language has been con- nected to the Information Bottleneck principle (Tishby et al., 1999). In turn, complexity reduction due to the Information Bottleneck provides a beneﬁcial regularization effect on learned representations (Fischer, 2019; Alemi et al., 2016; Achille & Soatto, 2018a;b). It is difﬁcult to experimentally verify the presence of such effect in human language, but we can look for it in our computational simulations. We conﬁrm that, when relaxing channel discreteness, the en- tropy minimization property no longer holds, and the system becomes less robust against overﬁtting and adversarial noise. This in turn raises intriguing questions about the origin of discreteness in human language, that we return to in the conclusion. arXiv:1905.13687v3  [cs.CL]  26 Jun 2020Entropy Minimization In Emergent Languages 2. General framework We establish our results in the context of signaling games (Lewis, 1969), as introduced to the current language emergence literature by Lazaridou et al. (2016) and adopted in several later studies (Havrylov & Titov, 2017; Boucha- court & Baroni, 2018; Lazaridou et al., 2018). There are two agents, Sender and Receiver, provided with individual in- puts at the beginning of each episode. Sender sends a single message to Receiver, and Receiver has to perform an action based on its own input and the received message. Impor- tantly, there is no direct supervision on the message protocol. We consider agents that are deterministic functions of their inputs (after training). As an example, consider the task of communicating a n-bit number, sampled uniformly at random from 0...2n −1. The full number is shown to Sender, and its k (0 ≤k ≤n) least-signiﬁcant bits are also revealed to Receiver. Receiver has to output the full number, based on the message from Sender and its own input. Would Sender transmit the en- tire number through its message? In this case, the protocol would be “complex,” encodingnbits. Alternatively, Sender could only encode the bits that Receiver does not know, and let Receiver ﬁll in the rest by itself. This emergent protocol would be “simple,” encoding only strictly necessary infor- mation. We ﬁnd experimentally that, once the agents are successfully trained to jointly solve the task, the emergent protocol minimizes the entropy of the messages or, equiva- lently in our setup, the mutual information between Sender’s input and messages. In other words, the agents consistently approximate the simplest successful protocol (in the current example, the one transmitting ≈n−kbits). We can connect the entropies of Sender and Receiver in- puts is and ir, messages m, Receiver’s output (the chosen action) o, and ground-truth outputs lby standard inequali- ties (Cover & Thomas, 2012).1 Denoting Sender’s computa- tion as a function S : S(is) =m, and Receiver as function R: R(m,ir) =o, we obtain: H(is) ≥H(S(is)) =H(m) ≥H(m|ir) ≥ ≥H(R(m,ir)|ir) =H(o|ir) ≈H(l|ir), (1) where the last relation stems from the fact that after success- ful training o≈l. Note that, since agents are deterministic after training, H(m) = I(is; m). We can then use these quantities interchangeably. Our empirical measurements indicate that the entropy of the messages min the emergent protocol tends to approach the lower bound: H(m) →H(l|ir), even if the upper bound H(is) is far. that Receiver needs is reduced without chang- ing other parameters, the emergent protocol becomes sim- 1We also use the fact that that H(x) ≥ H(g(x)) for any dis- crete r.v. xand function g. pler (lower entropy). In other words, the emergent protocol adapts to minimize the information that passes through it. Code for our experiments is publicly available at github.com/facebookresearch/EGG/ as a part of the EGG framework (Kharitonov et al., 2019). 3. Methodology 3.1. Games We study two signaling games. In Guess Number, the agents are trained to recover an integer-representing vector with uniform Bernoulli-distributed components. This simple setup gives us full control over the amount of information needed to solve the task. The second game, Image Classi- ﬁcation, employs more naturalistic data, as the agents are jointly trained to classify pairs of MNIST digits (LeCun et al., 1998b). Guess Number We draw an 8-bit integer 0 ≤z ≤255 uniformly at random, by sampling its 8 bits independently from the uniform Bernoulli distribution. All bits are revealed to Sender as an 8-dimensional binary vector is. The last k bits are revealed to Receiver ( 0 ≤k ≤8) as its input ir. Sender outputs a single-symbol message mto Receiver. In turn, Receiver outputs a vector othat recovers all the bits of zand should be equal to is. In this game, Sender has a linear layer that maps the input vector is to a hidden representation of size 10, followed by a leaky ReLU activation. Next is a linear layer followed by a softmax over the vocabulary. Receiver linearly maps both its input ir and the message to 10-dimensional vectors, concatenates them, applies a fully connected layer with output size 20, followed by a leaky ReLU. Finally, another linear layer and a sigmoid nonlinearity are applied. When training with REINFORCE and the Stochastic Computation graph approach (see Sec. 3.2), we increase the hidden layer sizes threefold, as this leads to a more robust convergence. Image Classiﬁcation In this game, the agents are jointly trained to classify 28x56 images of two MNIST digits, stacked side-by-side (more details in Supplementary). Un- like Guess Number, Receiver has no side input. Instead, we control the informational complexity of Receiver’s task by controlling the size of its output space, i.e., the number of labels we assign to the images. To do so, we group all two- digit sequences 00..99 into Nl ∈{2,4,10,20,25,50,100} equally-sized classes. In Sender, input images are embedded by a LeNet-1 in- stance (LeCun et al., 1990) into 400-dimensional vectors. These embedded vectors are passed to a fully connected layer, followed by a softmax selecting a vocabulary sym- bol. Receiver embeds the received messages into 400- dimensional vectors, passed to a fully connected layer withEntropy Minimization In Emergent Languages a softmax activation returning the class probabilities. We report hyperparameter grids in Supplementary. In the following experiments, we ﬁx vocabulary to 1024 symbols (experiments with other vocabulary sizes, multi-symbol mes- sages, and larger architectures are reported in Supplemen- tary). No parts of the agents are pre-trained or shared. The loss being optimized depends on the chosen gradient estima- tion method (see Sec. 3.2). We denote it L(o,l), and it is a function of Receiver’s outputoand the ground-truth output l. When training in Guess Number with REINFORCE, we use a 0/1 loss: the agents get zero loss only when all bits of zare correctly recovered. When training with Gumbel- Softmax relaxation or the Stochastic Computation Graph approach, we use binary cross-entropy (Guess Number) and negative log-likelihood (Image Classiﬁcation). 3.2. Training with discrete channel Training to communicate with discrete messages is non- trivial, as we cannot back-propagate through the messages. Current language emergence work mostly uses Gumbel- Softmax relaxation (e.g., Havrylov & Titov, 2017) or RE- INFORCE (e.g., Lazaridou et al., 2016) to get gradient esti- mates. We also explore the Stochastic Computation Graph optimization approach. We plug the obtained gradient esti- mates into Adam (Kingma & Ba, 2014). Gumbel-Softmax relaxation Samples from the Gumbel- Softmax distribution (a) are reperameterizable, hence allow gradient-based training, and (b) approximate samples from the corresponding Categorical distribution (Maddison et al., 2016; Jang et al., 2016). To get a sample that approximates an n-dimensional Categorical distribution with probabilities pi, we draw ni.i.d. samples gi from Gumbel(0,1) and use them to calculate a vector ywith components: yi = exp[(gi + logpi)/τ]∑ j exp[(gj + logpj)/τ], (2) where τ is the temperature hyperparameter. As τ tends to 0, the samples yget closer to one-hot samples; as τ →+∞, the components yi become uniform. During training, we use these relaxed samples as messages from Sender, making the entire Sender/Receiver setup differentiable. REINFORCE by Williams (1992) is a standard reinforce- ment learning algorithm. In our setup, it estimates the gradi- ent of the expectation of the loss L(o,l) w.r.t. the parameter vector θas follows: Eis,ir Em∼S(is),o∼R(m,ir) [(L(o; l) −b)∇θlog Pθ(m,o)] (3) The expectations are estimated by sampling mfrom Sender and, after that, sampling ofrom Receiver. We use the run- ning mean baseline b(Greensmith et al., 2004; Williams, 1992) as a control variate. We adopt the common trick to add an entropy regularization term (Williams & Peng, 1991; Mnih et al., 2016) that favors higher entropy. We impose entropy regularization on the outputs of the agents with coefﬁcients λs (Sender) and λr (Receiver). Stochastic Computation Graph (SCG) In our setup, the gradient estimate approach of Schulman et al. (2015) re- duces to computing the gradient of the surrogate function: Eis,ir Em∼S(is) [L(o; l) +sg(L(o; l) −b) logPθ(m)] , (4) where sgdenotes stop-gradient operation. We do not sample Receiver actions: Its parameter gradients are obtained with standard backpropagation (ﬁrst term in Eq. 4). Sender’s messages are sampled, and its gradient is calculated akin to REINFORCE (second term in Eq. 4). Again, we apply entropy-favoring regularization on Sender’s output (with coefﬁcient λs) and use the mean baseline. Role of entropy regularization As we mentioned above, when training with REINFORCE and SCG, we include a (standard) entropy regularization term in the loss which explicitly maximizes entropy of Sender’s output. Clearly, this term is at odds with the entropy minimization effect we observe. In our experiments, we found that high values ofλs (the parameter controlling Sender’s entropy regularization) prevent communication success; on the other hand, a small non-zero λs is crucial for successful training. In Sec. 4 we investigate the effect of λs on entropy minimization.2 3.3. Experimental protocol In Guess Number, we use all 28 possible inputs for train- ing, early stopping and analysis. In Image Classiﬁcation, we train on random image pairs from the MNIST training data, and use image pairs from the MNIST held-out set for validation. We select the runs that achieved a high level of performance (training accuracy above 0.99 for Guess Number and validation accuracy above 0.98 for Image Clas- siﬁcation), thus studying typical agent behavior provided they succeeded at the game. At test time, we select the Sender’s message symbol greed- ily, hence the messages are discrete and Sender represents a (deterministic) function S of its input is, m = S(i). Calculating the entropy H(m) of the distribution of dis- crete messages mis straightforward. In Guess Number, we enumerate all 256 possible values of zas inputs, obtain messages and calculate entropy H(m). For Image Classiﬁ- cation, we sample image pairs from the held-out set. The upper bound on H(m) is as follow: Hmax = 8bits (bounded by H(is)) in Guess Number, and Hmax = 10 2The parameter λr, that controls Receiver’s entropy regulariza- tion, does not inﬂuence the observed effect.Entropy Minimization In Emergent Languages bits (bounded by vocabulary size) in Image Classiﬁcation. Its lower bound is equal to Hmin = H(l|ir) = 8−kbits for Guess number. In Image Classiﬁcation, communica- tion can only succeed if H(m) is not less than H(l), i.e., Hmin = H(l) = log2 Nl, with Nl the number of equally- sized classes we split the images into. 4. Experiments 4.1. Entropy minimization Guess Number In Figure 1, the horizontal axes span the number of bits of z that Receiver lacks, 8 −k. The ver- tical axis reports the information content of the protocol, measured by messages entropy H(m). Each integer on the horizontal axis corresponds to a game conﬁguration, and for each such conﬁguration we aggregate multiple (suc- cessful) runs with different hyperparameters and random seeds. Hmin indicates the minimal amount of bits Sender has to send in a particular conﬁguration for the task to be solvable. The upper bound (not shown) is Hmax = 8bits. Across hyperparameters and random seeds, trainings with Gumbel-Softmax and SCG have success rate above 50%. With REINFORCE success rate is approximately 20%. Consider ﬁrst the conﬁgurations where Receiver’s input is insufﬁcient to answer correctly (at least one binary digit hidden, k≤7). From Figure 1a, we observe that the trans- mitted information is strictly monotonically increasing with the number of binary digits hidden from Receiver. Thus, even if Sender sees the very same input in all conﬁgura- tions, a more nuanced protocol is only developed when it is necessary. Moreover, the entropy H(m) (equivalently: the transmitted information) stays close to the lower bound. This entropy minimization property holds for all the consid- ered training approaches across all conﬁgurations. Consider next the conﬁguration where Receiver is getting the whole integer zas its input (k= 8, the leftmost conﬁgu- ration in Figure 1, corresponding to 0 on x axis). Based on the observations above, one would expect that the protocol would approach zero entropy in this case (as no informa- tion needs to be transmitted). However, the measurements indicate that the protocol is encoding considerably more information. It turns out that this information is entirely ignored by Receiver. To demonstrate this, we fed all pos- sible distinct inputs to Sender, retrieved the corresponding messages, and shufﬂed them to destroy any information about the inputs they might carry. The shufﬂed messages were then passed to Receiver alongside its own (un-shufﬂed) inputs. The overall performance was not affected by this manipulation, conﬁrming the hypothesis that Receiver ig- nores the messages. We conclude that in this case there is no entropy minimization pressure on Sender simply be- cause there is no communication. The full experiment is in Supplementary. We further consider the effect of various hyperparameters. In Figure 1b, we split the results obtained with Gumbel- Softmax by relaxation temperature. As discussed in Sec. 3.2, lower temperatures more closely approximate discrete com- munication, hence providing a convenient control of the level of discreteness imposed during training (recall that at test time we enforce full discreteness by selecting the symbol greedily). The ﬁgure shows that lower tempera- tures consistently lead to lower H(m). This implies that, as we increase the “level of discreteness” at training, we get stronger entropy minimization pressure. In Figures 1c & 1d, we report H(m) when training with Stochastic Graph Optimization and REINFORCE across degrees of entropy regularization. We report curves corre- sponding to λs values which converged in more than three conﬁgurations. With REINFORCE, we see a weak tendency for a higher λs to trigger a higher entropy in the protocol. However, message entropy stays generally close to the lower bound even in presence of strong exploration, which favors higher entropy in Sender’s output distribution. Image Classiﬁcation As the models are more complex, we only had consistent success when training with Gumbel- Softmax (success rate is approximately 80%). In Figure 2a we aggregate all successful runs. The information encoded by the protocol grows as Receiver’s output requires more information. However, in all conﬁgurations, the transmit- ted information stays well below the 10-bit upper bound and tends to be close to Hmin. A natural interpretation is that Sender prefers to take charge of image classiﬁcation and directly pass information about the output label, rather than sending along a presumably more information-heavy description of the input. In Figure 2b, we split the runs by temperature. Again, we see that lower temperatures consis- tently lead to stronger entropy minimization pressures. Summarizing, when communicating through a discrete chan- nel, there is consistent pressure for the emergent protocol to encode as little information as necessary. This holds across games, training methods and hyperparameters. When train- ing with Gumbel-Softmax, temperature controls the strength of this pressure, conﬁrming the relation between entropy minimization and discreteness. 4.2. Evolution of message entropy during training To gain further insights into the minimization trend, we studied the evolution of message entropy during training. We observed that the initial entropy of Sender can be both higher and lower than the minimum entropy Hmin required for solving the task. Further, we measured how the en- tropy of the messages changes after each training epoch by applying the same procedure as above, i.e., feeding theEntropy Minimization In Emergent Languages 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits Gumbel-Softmax Stoch. computation REINFORCE Hmin (a) All three training approaches. 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits  = 1.0  = 1.25  = 1.5 Hmin (b) Training with Gumbel-Softmax relaxation. 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 Hmin (c) Training with Stochastic Computation Graph. 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 s = 0.5 Hmin (d) Training with REINFORCE. Figure 1.Guess Number: entropy of the messages m. Shaded regions represent one standard error of the mean (SEM). entire dataset to Sender and selecting the message symbol greedily. When message entropy starts higher than Hmin, it falls close to it during the training. Similarly, when it starts lower than Hmin, it increases during training. This experiment is reported in Supplementary. Thus, information minimization is not simply due to the difﬁculty of discov- ering a higher-entropy protocol during learning, but also due to the complexity of maintaining mutual coordination between the agents. 4.3. Representation discreteness and robustness The entropy minimization effect indicates that a discrete rep- resentation will only store as much information as necessary to solve the task. This emergent behavior resembles the In- formation Bottleneck principle (Tishby et al., 1999; Achille & Soatto, 2018a). The fact that lower training-time temper- atures in Gumbel-Softmax optimization correlate with both higher discreteness and a tighter bottleneck (see Sec. 3.3) makes us further conjecture that discreteness is causally connected to the emergent bottleneck. The Information Bottleneck principle has also been claimed to govern en- tropy minimization in natural language (Zaslavsky et al., 2018; 2019). Bottleneck effects in neural agents and natural language might be due to the same cause, namely communi- cation discreteness. Further, we hypothesize that the emergent discrete bottle- neck might have useful properties, since existing (continu- ous) architectures that explicitly impose a bottleneck pres- sure are more robust to overﬁtting (Fischer, 2019) and ad- versarial attacks (Alemi et al., 2016; Fischer, 2019). We test whether similar regularization properties also emerge in our computational simulations (without any explicit pressure imposed through the cost function), and whether they are correlated with communication channel discreteness. If this connection exists, it also suggests that discreteness might be “beneﬁcial” to human languages for the same reasons. 4.3.1. R OBUSTNESS TO OVER -FITTING To assess our hypotheses, we consider the Image Classiﬁ- cation game (Nl = 10) in presence of randomly-shufﬂed training labels (the test set is untouched) (Zhang et al., 2016). This task allows us to explore whether the discrete commu- nication bottleneck is associated to robustness to overﬁtting, and whether the latter depends on discreteness level (con- trolled by the temperature τ of Gumbel-Softmax). We use the same architecture as above. The agents are trained withEntropy Minimization In Emergent Languages 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits All runs Hmin (a) Successful runs pooled together. 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits =0.75 =1.0 =1.5 =2.0 Hmin (b) Successful runs grouped by temperature. Figure 2.Image Classiﬁcation: entropy of the messages min function of log number of target classes, Nl. Shaded regions mark SEM. Gumbel-Softmax relaxation; at test-time the communication is fully discrete. We also consider two baseline architectures without the discrete channel. In Linear, the fully connected output layer of Sender is directly connected to the linear embedding input of Receiver. Softmax (SM) places a softmax activation (with temperature) after Sender’s output layer and passes the result to Receiver. We vary temperature and proportion of training examples with shufﬂed labels. We use temperatures τ = 1.0 and τ = 10.0 (the agents reach a test accuracy of 0.98 when trained with these temperatures on the original training set). SM with τ = 1.0 and τ = 10.0 behave similarly, hence we only report SM with τ = 1.0. Figure 3a shows training accuracy when all labels are shuf- ﬂed. Linear and SM ﬁt the random labels almost perfectly within the ﬁrst 150 epochs. With τ = 10.0, GS achieves 0.8 accuracy within 200 epochs. When GS with τ = 1.0 is considered, the agents only start to improve over random guessing after 150 epochs, and accuracy is well below 0.2 after 200 epochs. As expected, test set performance is at chance level (Figure 3b). In the next experiment, we shufﬂe labels for a randomly selected half of the training instances. Train and test accuracies are shown in Figures 3c and 3d, respectively. All models initially ﬁt the true-label exam- ples (train accuracy ≈0.5, test accuracy ≈0.97). With more training, the baselines and GS with τ = 10.0 start (over)ﬁtting the random labels, too: train accuracy grows, while test accuracy falls. In contrast, GS with τ = 1.0 does not ﬁt random labels, and its test accuracy stays high. Note that SM patterns with Linear and high-temperature GS, showing that the training-time discretization noise in GS is instrumental for robustness to over-ﬁtting. We interpret the results as follows. To fully exploit their joint capacity for “successful” over-ﬁtting, the agents need to coordinate label memorization. This requires passing large amounts of information through the channel. With a low temperature (more closely approximating a discrete channel), this is hard, due to a stronger entropy minimiza- tion pressure. To test the hypothesis, we run an experiment where all labels are shufﬂed and a layer of size 400x400 is either added to Sender (just before the channel) or to Re- ceiver (just after the channel). We predict that, with higherτ (less discrete, less entropy minimization pressure), the train- ing curves will be close, as the extra capacity can be used for memorization equally easy in both cases. With lower τ (more discrete, more pressure), the accuracy curves will be more distant, as the extra capacity can only be successfully exploited for memorization when placed before the channel. Figures 3e & 3f bear out the prediction. 4.3.2. R OBUSTNESS TO ADVERSARIAL EXAMPLES We study next robustness of agents equipped with a relaxed discrete channel against adversarial attacks. We use the same architectures as in the preceding experiment. We train agents with different random seeds and imple- ment white-box attacks on the trained models, varying tem- perature τ and the allowed perturbation norm, ϵ. We use the standard Fast Gradient Sign Method of (Goodfellow et al., 2014). The original image is is perturbed to i∗ s along the direction that maximizes the loss of Receiver’s output o= R(S(is)) w.r.t. the ground-truth class l: i∗ s = clip[is + ϵ·sign[∇is L(o,l)] ,0,1] , (5) where ϵcontrols the L∞norm of the perturbation. Under an attack with a ﬁxed ϵ, a more robust method will have a higher accuracy. To avoid numerical stability issues akin to those reported by (Carlini & Wagner, 2016), all computa- tions are done in 64-bit ﬂoats. We experiment with two approaches of getting gradients forEntropy Minimization In Emergent Languages 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Train accuracy GS, =1.0 GS, =10.0 Linear SM, =1.0 (a) All train labels are shufﬂed. 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Test accuracy GS, =1.0 GS, =10.0 Linear SM, =1.0  (b) All train labels are shufﬂed. 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Train accuracy GS, =1.0 GS, =10.0 Linear SM, =1.0 (c) Half of train labels are shufﬂed. 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Test accuracy GS, =1.0 GS, =10.0 Linear SM, =1.0  (d) Half of train labels are shufﬂed. 0 200 400 600 800 1000 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Train accuracy =10.0, before channel =10.0, after channel (e) All labels shufﬂed; Additional layer before channel vs. after channel 0 200 400 600 800 1000 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Train accuracy =1.0, before channel =1.0, after channel (f) All labels shufﬂed; Additional layer before channel vs. after chan- nel Figure 3.Learning in presence of random labels. GS (SM) denotes models trained with Gumbel-Softmax (Softmax) channel. Linear are models with the channel removed.Entropy Minimization In Emergent Languages 0.00 0.05 0.10 0.15 0.20 0.25 Perturbation norm,  0.2 0.4 0.6 0.8 1.0Accuracy GS, =0.1 GS, =1.0 GS, =10.0 (a) Robustness vs. temp. τ. 0.00 0.05 0.10 0.15 0.20 0.25 Perturbation norm,  0.0 0.2 0.4 0.6 0.8 1.0Accuracy GS, =0.1 SM, =1.0 Linear (b) Comparison to the baselines. Figure 4.Robustness to adversarial examples: higher accuracy given ﬁxed ϵimplies more robustness. the attack. Under the ﬁrst approach, the gradient ∇is L(o,l) is estimated using the standard Gumbel-Softmax relaxation. It is possible, however, that the randomization that Gumbel- Softmax uses internally reduces the usefulness of gradients used for the attack. Hence we also experiment with a setup that is easier for an adversary: after training (and during the attack), we replace the Gumbel-Softmax by a softmax non-linearity with the same temperature. We found that performance in these two setups is virtually the same, indi- cating that the obtained robustness results are independent from the randomization in the channel. Rather, they are due to emergence of well-separated “categories” during training. As in the preceding experiment, SM behaves similarly with different temperatures (we experimented with τ ∈ {0.1,1.0,10.0}): we only report results with τ = 1.0. Fig- ure 4a shows that, as temperature decreases, the accuracy drop also decreases. The highest robustness is achieved with τ = 0.1. Comparison with the baselines (Figure 4b) conﬁrms that relaxed discrete training with τ = 0.1 im- proves robustness. In sum, increased channel discreteness makes it harder to transmit large amounts of information, and leads to in- creased robustness against over-ﬁtting and adversarial ex- amples. Discreteness brings about a bottleneck that has beneﬁcial properties, which might ultimately provide a mo- tivation for why an emergent communication system should evolve towards discreteness. 5. Related Work We brieﬂy reviewed studies of emergent deep agent commu- nication and entropy minimization in human language in the introduction. We are not aware of earlier work that looks for this property in emergent communication, although Evti- mova et al. (2018) used information theory to study protocol development during learning, and, closer to us, K˚ageb¨ack et al. (2018) studied the effect of explicitly adding a com- plexity minimization term to the cost function of an emer- gent color-naming system. Discrete representations are explored in many places (e.g., van den Oord et al., 2017; Jang et al., 2016; Rolfe, 2016). However, these works focus on ways to learn good discrete representations, rather than analyzing the properties of rep- resentations that are independently emerging on the side. Furthermore, our study extends to agents communicating with variable-length messages, produced and consumed by GRU (Cho et al., 2014) and Transformer (Vaswani et al., 2017) cells (see Supplementary). The sequential setup is speciﬁc to language, clearly distinguished from the settings studied in generic sparse-representation work. Other studies, inspired by the Information Bottleneck prin- ciple, control the complexity of neural representations by regulating their information content (Strouse & Schwab, 2017; Fischer, 2019; Alemi et al., 2016; Achille & Soatto, 2018a;b). While they externally impose the bottleneck, we observe that the latter is an intrinsic feature of learning to communicate through a discrete channel. 6. Discussion Entropy minimization is pervasive in human language, where it constitutes a speciﬁc facet of the more general pressure towards communication efﬁciency. We found that the same property consistently characterizes the protocol emerging in simulations where two neural networks learn to solve a task jointly through a discrete communication code. In a comparative perspective, we hypothesize that entropy minimization is a general property of discrete communica- tion, independent of speciﬁc biological constraints humans are subject to. In particular, our analysis tentatively estab- lishes a link between this property and the inherent difﬁculty of encoding information in discrete form (cf. the effect of adding a layer before or after the communication bottleneck in the over-ﬁtting experiment).Entropy Minimization In Emergent Languages Exploring entropy minimization in computational simula- tions provides a ﬂexibility we lack when studying humans. For example, we uncovered here initial evidence that the communication bottleneck is acting as a good regularizer, making the joint agent system more robust to noise and adversarial examples. This leads to an intriguing conjec- ture on the origin of language. Its discrete nature is often traced back to the fact that it allows us to produce an in- ﬁnite number of expressions by combining a ﬁnite set of primitives (e.g., Berwick & Chomsky, 2016). However, it is far from clear that the need to communicate an inﬁnite number of concepts could have provided the initial pressure to develop a discrete code. More probably, once such code independently emerged, it laid the conditions to develop an inﬁnitely expressive language (Bickerton, 2014; Collier et al., 2014). Our work suggests that, because of its inherent regularizing effect, discrete coding is advantageous already when communication is about a limited number of concepts, providing an alternative explanation for its origin. In the future, we would like to study more continuous seman- tic domains, such as color maps, where perfect accuracy is not easily attainable, nor desirable. Will the networks ﬁnd an accuracy/complexity trade-off similar to those at- tested in human languages? Will other core language prop- erties claimed to be related to this trade-off, such as Zipﬁan frequency distributions (Ferrer i Cancho & D ´ıaz-Guilera, 2007), concurrently emerge? We would also like to compare the performance of human subjects equipped with novel con- tinuous vs. discrete communication protocols, adopting the methods of experimental semiotics (Galantucci, 2009). We expect discrete protocols to be more general and robust. Our results have implications for the efforts to evolve agents interacting with each other and with humans through a dis- crete channel. First, because of entropy minimization, we should not agents to develop a richer protocol than the sim- plest one ensuring accurate communication. For example, Bouchacourt & Baroni (2018) found that agents trained to discriminate pairs of natural images depicting instances of about 500 high-level categories, such as cats and dogs, de- veloped a lexicon that does not denote such categories, but low-level properties of the images themselves. This makes sense from an entropy-minimization perspective, as talking about the 500 high-level categories demands log2 500 bits of information, whereas many low-level strategies (e.g., dis- criminating average pixel intensity in the images) will only require transmitting a few bits. To have agents developing rich linguistic protocols, we must face them with varied challenges that truly demand them. Second, the focus on a discrete protocol is typically moti- vated by the goal to develop machines eventually able to communicate with humans. Indeed, discrete messages are not required in multi-agent scenarios where no human in the loop is foreseen (Sukhbaatar et al., 2016). Our results sug- gest that, long before agents reach the level of complexity necessary to converse with humans, there are independent reasons to encourage discreteness, as it leads to simpler protocols and it provides a source of robustness in a noisy world. An exciting direction for future applied work will be to test the effectiveness of discrete communication as a general form of representation learning. Acknowledgements The authors thank Emmanuel Dupoux for discussions and the anonymous reviewers for their feed- back. References Achille, A. and Soatto, S. Information dropout: Learning optimal representations through noisy computation. IEEE TPAMI, 40(12):2897–2905, 2018a. Achille, A. and Soatto, S. Emergence of invariance and disentanglement in deep representations. The Journal of Machine Learning Research, 19(1):1947–1980, 2018b. Alemi, A. A., Fischer, I., Dillon, J. V ., and Murphy, K. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016. Berwick, R. and Chomsky, N. Why Only Us: Language and Evolution. MIT Press, Cambridge, MA, 2016. Bickerton, D. More than Nature Needs: Language, Mind, and Evolution. Harvard University Press, Cambridge, MA, 2014. Bouchacourt, D. and Baroni, M. How agents see things: On visual representations in an emergent language game. In EMNLP, 2018. Carlini, N. and Wagner, D. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016. Chaabouni, R., Kharitonov, E., Dupoux, E., and Baroni, M. Anti-efﬁcient encoding in emergent communication. In NeurIPS, 2019. Chaabouni, R., Kharitonov, E., Bouchacourt, D., Dupoux, E., and Baroni, M. Compositionality and generalization in emergent languages. In ACL, 2020. Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y . BabyAI: A platform to study the sample efﬁciency of grounded language learning. In ICLR, 2019. Cho, K., Van Merri ¨enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y . Learn- ing phrase representations using rnn encoder-decoderEntropy Minimization In Emergent Languages for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. Choi, E., Lazaridou, A., and de Freitas, N. Compositional obverter communication learning from raw visual input. arXiv preprint arXiv:1804.02341, 2018. Collier, K., Bickel, B., van Schaik, C., Manser, M., and Townsend, S. Language evolution: Syntax before phonol- ogy? Proceedings of the Royal Society B: Biological Sciences, 281(1788):1–7, 2014. Cover, T. M. and Thomas, J. A. Elements of Information Theory. John Wiley & Sons, 2012. Evtimova, K., Drozdov, A., Kiela, D., and Cho, K. Emergent communication in a multi-modal, multi-step referential game. In ICLR, 2018. Ferrer i Cancho, R. and D´ıaz-Guilera, A. The global minima of the communicative energy of natural communication systems. Journal of Statistical Mechanics: Theory and Experiment, 2007(06):P06009, 2007. Ferrer i Cancho, R., Hern´andez-Fern´andez, A., Lusseau, D., Agoramoorthy, G., Hsu, M., and Semple, S. Compression as a universal principle of animal behavior. Cognitive Science, 37(8):1565–1578, 2013. Fischer, I. The conditional entropy bottleneck, 2019. URL https://openreview.net/forum? id=rkVOXhAqY7. Galantucci, B. Experimental semiotics: A new approach for studying communication as a form of joint action. Topics in Cognitive Science, 1(2):393–410, 2009. Gibson, E., Piantadosi, R. F. S., Dautriche, I., Mahowald, K., Bergen, L., and Levy, R. How efﬁciency shapes human language. Trends in Cognitive Science, 2019. In press. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Greensmith, E., Bartlett, P. L., and Baxter, J. Variance reduc- tion techniques for gradient estimates in reinforcement learning. JMLR, 5(Nov):1471–1530, 2004. Harding Graesser, L., Cho, K., and Kiela, D. Emergent lin- guistic phenomena in multi-agent communication games. In EMNLP, 2019. Havrylov, S. and Titov, I. Emergence of language with multi- agent games: Learning to communicate with sequences of symbols. In NIPS, 2017. Hurford, J. The Origins of Language. Oxford University Press, Oxford, UK, 2014. Jang, E., Gu, S., and Poole, B. Categorical repa- rameterization with Gumbel-Softmax. arXiv preprint arXiv:1611.01144, 2016. K˚ageb¨ack, M., Dubhashi, D., and Sayeed, A. DeepColor: Reinforcement learning optimizes information efﬁciency and well-formedness in color name partitioning. In Pro- ceedings of CogSci, pp. 1895–1900, Austin, TX, 2018. Kemp, C. and Regier, T. Kinship categories across lan- guages reﬂect general communicative principles. Science, 336(6084):1049–1054, 2012. Kharitonov, E., Chaabouni, R., Bouchacourt, D., and Ba- roni, M. EGG: a toolkit for research on Emergence of lanGuage in Games. In EMNLP: System Demonstrations, 2019. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kirby, S. Natural language from artiﬁcial life. Artiﬁcial life, 8(2):185–215, 2002. Kottur, S., Moura, J. M., Lee, S., and Batra, D. Natural lan- guage does not emerge “naturally” in multi-agent dialog. arXiv preprint arXiv:1706.08502, 2017. Lazaridou, A., Peysakhovich, A., and Baroni, M. Multi- agent cooperation and the emergence of (natural) lan- guage. arXiv preprint arXiv:1612.07182, 2016. Lazaridou, A., Hermann, K. M., Tuyls, K., and Clark, S. Emergence of linguistic communication from referential games with symbolic and pixel input. arXiv preprint arXiv:1804.03984, 2018. LeCun, Y ., Boser, B. E., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. E., and Jackel, L. D. Hand- written digit recognition with a back-propagation network. In NIPS, 1990. LeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278–2324, 1998a. LeCun, Y ., Bottou, L., Bengio, Y ., Haffner, P., et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998b. Lewis, D. Convention harvard university press. Cambridge, MA, 1969. Li, F. and Bowling, M. Ease-of-teaching and language structure from emergent communication. In NeurIPS. 2019.Entropy Minimization In Emergent Languages Lowe, R., Foerster, J., Boureau, Y ., Pineau, J., and Dauphin, Y . On the pitfalls of measuring emergent communica- tion. In Proceedings of AAMAS, pp. 693–701, Montreal, Canada, 2019. Maddison, C. J., Mnih, A., and Teh, Y . W. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. Mikolov, T., Joulin, A., and Baroni, M. A roadmap towards machine intelligence. In International Conference on In- telligent Text Processing and Computational Linguistics, pp. 29–61. Springer, 2016. Mnih, V ., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn- chronous methods for deep reinforcement learning. In ICML, 2016. Rolfe, J. T. Discrete variational autoencoders.arXiv preprint arXiv:1609.02200, 2016. Schulman, J., Heess, N., Weber, T., and Abbeel, P. Gradient estimation using stochastic computation graphs. In NIPS, 2015. Strouse, D. and Schwab, D. J. The deterministic information bottleneck. Neural computation, 29(6):1611–1630, 2017. Sukhbaatar, S., Szlam, A., and Fergus, R. Learning mul- tiagent communication with backpropagation. In NIPS. 2016. Tishby, N., Pereira, F., and Bialek, W. The information bot- tleneck method. In Proceedings of the 37th Annual Aller- ton Conference on Communication, Control and Comput- ing. University of Illinois Press, 1999. van den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. In NIPS, 2017. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin, I. Attention is all you need. In NIPS, 2017. Williams, R. J. Simple statistical gradient-following algo- rithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992. Williams, R. J. and Peng, J. Function optimization using connectionist reinforcement learning algorithms. Con- nection Science, 3(3):241–268, 1991. Zaslavsky, N., Kemp, C., Regier, T., and Tishby, N. Ef- ﬁcient compression in color naming and its evolution. Proceedings of the National Academy of Sciences , 115 (31):7937–7942, 2018. Zaslavsky, N., Regier, T., Tishby, N., and Kemp, C. Se- mantic categories of artifacts and animals reﬂect efﬁcient coding. In Proceedings of CogSci, pp. 1254–1260, Mon- treal, Canada, 2019. Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking general- ization. arXiv preprint arXiv:1611.03530, 2016. 7. How much does Receiver rely on messages in Guess Number? We supplement the experiments of Section 3 of the main text by studying the degree to which Receiver relies on messages in Guess Number. In particular, we show that when Receiver has the full input (is = ir), it ignores the messages. We measure the degree to which Receiver relies on the messages from Sender by constructing a setup where we break communication, but still let Receiver rely on its own input. More precisely, we ﬁrst enumerate all test inputs for Sender is and Receiver ir. We obtain messages that correspond to Sender’s inputs, and shufﬂe them. Next, we feed the shufﬂed messages alongside Receiver’s own (un- shufﬂed) inputs and compute accuracy, as a measure of Receiver’s dependence on the messages. This procedure preserves the marginal distribution of Sender’s messages, but destroys all the information Sender transmits. Without messages, Receiver, given k input bits, can only reach an accuracy of 28−k. In Figure 5, we report results ag- gregated by training method. Receiver is extremely close to the accuracy’s higher bound in all conﬁgurations. Moreover, when Receiver gets the entire input, the drop in accuracy after shufﬂing is tiny, proving that Receiver’s reliance on the message is minimal in that setting. 8. Inﬂuence of architecture choices 8.1. Does vocabulary size affect the results? We repeat the same experiments as in Section 3 of the main text while varying vocabulary size. Note that, to make Guess Number solvable across each conﬁguration, the vocabulary has to contain at least 256 symbols. Similarly, for Image Classiﬁcation, vocabulary size must be of at least 100. We tried vocabulary sizes of 256, 1024, 4096 for Guess Number, and 512, 1024, 2048 for Image Classiﬁcation. The results are reported in Figures 6 (Guess Number) and 7 (Image Classiﬁcation). We observe that there is little qualitative variation over vocabulary size, hence the conclusions we had in Section 3 of the main paper are robust to variations of this parameter.Entropy Minimization In Emergent Languages 0 2 4 6 8 Binary digits hidden 0.0 0.2 0.4 0.6 0.8 1.0Accuracy Gumbel-Softmax Stoch. computation REINFORCE upper bound Figure 5.Guess Number: Receiver’s dependence on messages, measured as performance drop under message intervention. 8.2. Does Receiver’s capacity affect the results? One potential confounding variable is the capacity of Re- ceiver. Indeed, if Receiver is very simple, then, for the task to be solved, Sender would have to calculate the answer itself and feed it to Receiver. To investigate this, we repeat the Image Classiﬁcation experiment from Section 4.1 of the main paper while controlling the power of Receiver’s ar- chitecture: we put two additional fully-connected 400x400 hidden layers between the input embedding and the output layer, while in Section 4, Receiver had a single hidden layer. In Figure 8, we compare the results obtained with these two variations of Receiver. The reported entropy minimiza- tion effect holds: even in presence of additional layers, the entropy of messages H(m) is far from the upper-bound Hmax = 10 bits and closely follows the lower bound, Hmin = log2 Nl. Thus, again, a more nuanced protocol only appears when it is needed. Finally, we see that results for both architectures are close, although in three out of seven task setups (the number of classes Nl is 2, 10, and 20) a deeper model results in a slightly higher entropy of the protocol, on average. Overall, we conclude that Receiver’s capacity does not play a major role in the entropy mini- mization effect and the latter also takes place with a more powerful Receiver. 8.3. What if communication takes place through sequences of symbols? We also experiment with Guess Number in a setup where the agents communicate via variable-length messages. The general architecture of the agents is same as in Section 3, but we append GRU agents (Cho et al., 2014). Sender GRU is unrolled to generate the message. The message is produced until the GRU outputs a special eos token or until the max- imal length is reached. In the latter case, eos is appended to the message. The produced message is consumed by a Receiver’s GRU unit and the hidden state corresponding to eos is used by Receiver as input to further processing. When Receiver has additional inputs (in the Guess Num- ber game), these inputs are used as initial hidden state of the GRU cell. We use the Stochastic Computation Graph estimator as described in Section 3.2, as it provided fastest convergence. We consider the entire variable-length message as the real- ization of a random variablemwhen calculating the entropy of the messages, H(m). The results are reported in Fig- ure 9, arranged in function of maximal message length and vocabulary size. As before, we aggregate the successful runs according to the entropy regularization coefﬁcient λs applied to Sender’s output layer. From Figure 9 we observe that the results are in line with those obtained in the one-symbol scenario. Entropy mini- mization still holds: a more nuanced (high-entropy) protocol only develops when more digits are hidden from Receiver, which hence requires more information to perform the task. The approximation to the lower bound is however less tight as the overall number of possible messages grows (higher maximum length and/or vocabulary size). There is also a weak tendency for lower λs to encourage a tighter bottle- neck. In preliminary experiments, we have similar results when the variable-length communication is performed via Trans- former cells (Vaswani et al., 2017) instead of GRUs (not reported here). 9. Two-digit MNIST dataset As discussed in Section 3, to ensure high output informa- tional complexity in the Image Classiﬁcation task, we use a two-digit variant of the MNIST dataset (LeCun et al., 1998a). We construct it as follows. When iterating over the original MNIST dataset, we take a batch band (a) select the ﬁrst |b|/2 and last |b|/2 images, refer to them as b1 and b2, respectively; (b) create a new batch where the ith image from b1 is placed to the left of the ith image from b2 and then vice versa. As a result, we obtain a new stream of images, where each MNIST digit is seen twice, on the left and on the right side. Note that not all possible pairwise combinations of the original images are generated (there are 600002 of those in the training set alone) and the exact combinations change across epochs. As labels, we use the depicted two-digit number modulo Nl, where Nl is the re- quired number of classes. All pixels are scaled into [0, 1]. We use this same process to generate training and test sets, based on the training and test images of the original MNIST dataset, respectively.Entropy Minimization In Emergent Languages 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits  = 1.0  = 1.25  = 1.5 Hmin (a) V ocab. size: 256, Gumbel-Softmax 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits  = 1.0  = 1.25  = 1.5 Hmin (b) V ocab. size: 1024, Gumbel-Softmax 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits  = 1.0  = 1.25  = 1.5 Hmin (c) V ocab. size: 4096, Gumbel-Softmax 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 Hmin (d) V ocab. size: 256, Stoch. Computation Graph approach 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 Hmin (e) V ocab. size: 1024, Stoch. Computation Graph approach 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 Hmin (f) V ocab. size: 4096, Stoch. Computation Graph approach 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 s = 0.5 Hmin (g) V ocab. size: 256, REINFORCE 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 s = 0.5 Hmin (h) V ocab. size: 1024, REINFORCE 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 s = 0.5 Hmin (i) V ocab. size: 4096, REINFORCE Figure 6.Guess Number: Entropy of the messages m, depending on vocabulary size, training method, and relaxation temperature τ (when trained with Gumbel-Softmax) or Sender’s entropy regularization coefﬁcientλs. Shaded regions mark standard deviation. 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits =0.75 =1.0 =1.5 =2.0 Hmin (a) V ocab. size: 512 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits =0.75 =1.0 =1.5 =2.0 Hmin (b) V ocab. size: 1024 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits =0.75 =1.0 =1.5 =2.0 Hmin (c) V ocab. size: 2048 Figure 7.Image Classiﬁcation: entropy of the messages H(m) across vocabulary sizes. Successful runs are pooled together. Shaded regions mark standard deviation.Entropy Minimization In Emergent Languages 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits One hidden layer Three hidden layers Hmin Figure 8.Image Classiﬁcation: entropy of the messages H(m) across Receiver model sizes. Successful runs are pooled together. Shaded regions mark standard deviation. 10. Hyperparameters In our experiments, we used the following hyperparameter grids. Guess Number (Gumbel-Softmax) V ocab. size: [256, 1024, 4096]; temperature, τ: [0.5, 0.75, 1.0, 1.25, 1.5]; learning rate: [0.001, 0.0001]; max. number of epochs: 250; random seeds: [0, 1, 2, 3]; batch size: 8; early stopping thr.: 0.99; bits shown to Receiver: [0, 1, 2, 3, 4, 5, 6, 7, 8]. Guess Number (REINFORCE) V ocab. size: [256, 1024, 4096]; Sender entropy regularization coef., λs: [0.01, 0.025, 0.05, 0.1, 0.5, 1.0]; Receiver entropy regularization coef., λr: [0.01, 0.1, 0.5, 1.0]; learning rate: [0.0001, 0.001, 0.01]; max. number of epochs: 1000; random seeds: [0, 1, 2, 3]; batch size: 2048; early stopping thr.: 0.99; bits shown to Receiver: [0, 1, 2, 3, 4, 5, 6, 7, 8]. Guess Number (Stochastic Computation Graph ap- proach): V ocab. size: [256, 1024, 4096]; Sender entropy regularization coef., λs: [0.01, 0.05, 0.1, 0.25]; learning rate: [0.0001, 0.001]; max. number of epochs: 1000; ran- dom seeds: [0, 1, 2, 3]; batch size: 2048; early stopping thr.: 0.99; bits shown to Receiver: [0, 1, 2, 3, 4, 5, 6, 7, 8]. Image Classiﬁcation experiments V ocab. size: [512, 1024, 2048]; temperature, τ: [0.5, 0.75, 1.0, 1.5, 2.0]; learning rate: [0.001], max. number of epochs: 100; random seeds: [0, 1, 2]; batch size: 32; early stopping thr.: 0.98; number of classes: [2, 4, 10, 20, 25, 50, 100]. Fitting random labels experiments V ocab. size: 1024; temperature, τ: [1.0, 10.0]; learning rate: 0.0001, max. number of epochs: 200; random seeds: [0, 1, 2, 3, 4]; batch size: 32; early stopping thr.: ∞; prob. of label corruption: [0.0, 0.5, 1.0]. Adversarial attack experiments V ocab. size: 1024; tem- perature, τ: [0.1, 1.0, 10.0]; learning rate: 0.0001, max. number of epochs: 200; random seeds: [0, 1, 2, 3, 4]; batch size: 32; early stopping thr.: 0.98. 11. Evolution of message entropy during training In this Section, we aim to gain additional insight into de- velopment of the communication protocol by measuring its entropy during training. We concentrate on Guess Number and use the same experimental runs summarized in Figure 1 of the main text. For each game conﬁguration (that is, number of bits hidden from Receiver), we randomly select one successful run and plot the evolution of Sender message entropy and accuracy over training epochs.3 We also plot entropy and accuracy curves for a randomly selected failed run, to verify to what extent entropy development depends on task success. We report results for runs where training was performed with Gumbel-Softmax relaxation and with the Stochastic Graph Computation approach in Figures 10 and 11, respectively. The reported entropy and accuracy values are calculated in evaluation mode, where Sender’s output is selected greedily, without sampling. A higher entropy of such deterministic Sender indicates that the latter can encode more information about inputs in its messages. From these results, we ﬁrstly observe that the initial entropy of Sender’s messages (before training) can be both higher than required for communication success (Figures 10a and 11a) and lower (the rest). When it starts higher than needed, it generally falls closer to the minimum level required for the solution. When the initial value is low, it increases during training. The failed runs can have message entropy above (Figures 10a, 10b & 11a) and below (e.g. Figures 10c, 10d & 11d) successful runs, suggesting that there is no systematic relation between degree of entropy and task success. The fact that the entropy can be reduced with no decrease in accuracy or even with accuracy growth (e.g. Figure 10a, red line, epochs 5..30) indicates that the tendency to discover new messages (increasing entropy) is counter-balanced by the complexity of mutual coordination with Receiver when entropy is larger. In our interpretation, it is this interplay that serves as a source of the natural bottleneck. Finally, while in some runs the entropy is effectively in- creased w.r.t. its initialization level, the resulting protocol’s entropy is at, or slightly above the lower bound of what the task allows. In this sense, we argue that the reported effect 3We exclude the conﬁguration in which Receiver sees the entire input, as it is a degenerate case of non-communication, as discussed in Section 4 of the main text.Entropy Minimization In Emergent Languages 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits =0.05 =0.1 =0.01 Hmin (a) Max length: 5, vocabulary size: 16 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits =0.05 =0.1 =0.01 Hmin (b) Max length: 10, vocabulary size: 16 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits =0.05 =0.1 =0.01 Hmin (c) Max length: 5, vocabulary size: 64 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits =0.05 =0.1 =0.01 Hmin (d) Max length: 10, vocabulary size: 64 Figure 9.Guess Number: Entropy of the emergent protocol when communication is performed with variable-length messages. Shaded regions mark standard deviation.Entropy Minimization In Emergent Languages can be correctly denoted as a “minimization” result. References Achille, A. and Soatto, S. Information dropout: Learning optimal representations through noisy computation. IEEE TPAMI, 40(12):2897–2905, 2018a. Achille, A. and Soatto, S. Emergence of invariance and disentanglement in deep representations. The Journal of Machine Learning Research, 19(1):1947–1980, 2018b. Alemi, A. A., Fischer, I., Dillon, J. V ., and Murphy, K. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016. Berwick, R. and Chomsky, N. Why Only Us: Language and Evolution. MIT Press, Cambridge, MA, 2016. Bickerton, D. More than Nature Needs: Language, Mind, and Evolution. Harvard University Press, Cambridge, MA, 2014. Bouchacourt, D. and Baroni, M. How agents see things: On visual representations in an emergent language game. In EMNLP, 2018. Carlini, N. and Wagner, D. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016. Chaabouni, R., Kharitonov, E., Dupoux, E., and Baroni, M. Anti-efﬁcient encoding in emergent communication. In NeurIPS, 2019. Chaabouni, R., Kharitonov, E., Bouchacourt, D., Dupoux, E., and Baroni, M. Compositionality and generalization in emergent languages. In ACL, 2020. Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y . BabyAI: A platform to study the sample efﬁciency of grounded language learning. In ICLR, 2019. Cho, K., Van Merri ¨enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y . Learn- ing phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. Choi, E., Lazaridou, A., and de Freitas, N. Compositional obverter communication learning from raw visual input. arXiv preprint arXiv:1804.02341, 2018. Collier, K., Bickel, B., van Schaik, C., Manser, M., and Townsend, S. Language evolution: Syntax before phonol- ogy? Proceedings of the Royal Society B: Biological Sciences, 281(1788):1–7, 2014. Cover, T. M. and Thomas, J. A. Elements of Information Theory. John Wiley & Sons, 2012. Evtimova, K., Drozdov, A., Kiela, D., and Cho, K. Emergent communication in a multi-modal, multi-step referential game. In ICLR, 2018. Ferrer i Cancho, R. and D´ıaz-Guilera, A. The global minima of the communicative energy of natural communication systems. Journal of Statistical Mechanics: Theory and Experiment, 2007(06):P06009, 2007. Ferrer i Cancho, R., Hern´andez-Fern´andez, A., Lusseau, D., Agoramoorthy, G., Hsu, M., and Semple, S. Compression as a universal principle of animal behavior. Cognitive Science, 37(8):1565–1578, 2013. Fischer, I. The conditional entropy bottleneck, 2019. URL https://openreview.net/forum? id=rkVOXhAqY7. Galantucci, B. Experimental semiotics: A new approach for studying communication as a form of joint action. Topics in Cognitive Science, 1(2):393–410, 2009. Gibson, E., Piantadosi, R. F. S., Dautriche, I., Mahowald, K., Bergen, L., and Levy, R. How efﬁciency shapes human language. Trends in Cognitive Science, 2019. In press. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Greensmith, E., Bartlett, P. L., and Baxter, J. Variance reduc- tion techniques for gradient estimates in reinforcement learning. JMLR, 5(Nov):1471–1530, 2004. Harding Graesser, L., Cho, K., and Kiela, D. Emergent lin- guistic phenomena in multi-agent communication games. In EMNLP, 2019. Havrylov, S. and Titov, I. Emergence of language with multi- agent games: Learning to communicate with sequences of symbols. In NIPS, 2017. Hurford, J. The Origins of Language. Oxford University Press, Oxford, UK, 2014. Jang, E., Gu, S., and Poole, B. Categorical repa- rameterization with Gumbel-Softmax. arXiv preprint arXiv:1611.01144, 2016. K˚ageb¨ack, M., Dubhashi, D., and Sayeed, A. DeepColor: Reinforcement learning optimizes information efﬁciency and well-formedness in color name partitioning. In Pro- ceedings of CogSci, pp. 1895–1900, Austin, TX, 2018. Kemp, C. and Regier, T. Kinship categories across lan- guages reﬂect general communicative principles. Science, 336(6084):1049–1054, 2012.Entropy Minimization In Emergent Languages 0 50 100 150 200 250 epoch 0.0 0.5 1.0 1.5 2.0 2.5H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (a) Binary digits hidden: 2 0 50 100 150 200 250 epoch 0 1 2 3 4 5H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (b) Binary digits hidden: 4 0 50 100 150 200 250 epoch 0 2 4 6H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (c) Binary digits hidden: 6 0 50 100 150 200 250 epoch 2 4 6 8H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (d) Binary digits hidden: 8 Figure 10.Evolution of H(m) over training epochs. Gumbel Softmax-based optimization, Guess Number. For each game conﬁguration, speciﬁed by the number of bits Receiver lacks, we sample one successful (black line) and one failed (red line) training trajectory. The blue line marks Hmin, minimal entropy for a successful solution.Entropy Minimization In Emergent Languages 0 50 100 150 200 250 epoch 0 2 4 6H(m), bits 0 50 100 150 200 250 epoch 0.2 0.4 0.6 0.8 1.0Accuracy (a) Binary digits hidden: 2 0 50 100 150 200 250 epoch 2 4 6H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (b) Binary digits hidden: 4 0 50 100 150 200 250 epoch 2 3 4 5 6H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (c) Binary digits hidden: 6 0 50 100 150 200 250 epoch 4 6 8H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (d) Binary digits hidden: 8 Figure 11.Evolution of H(m) over training epochs. Stochastic Computation Graph-based optimization, Guess Number. For each game conﬁguration, speciﬁed by the number of bits Receiver lacks, we sample one successful (black line) and one failed (red line) training trajectory. The blue line marks Hmin, minimal entropy for a successful solution.Entropy Minimization In Emergent Languages Kharitonov, E., Chaabouni, R., Bouchacourt, D., and Ba- roni, M. EGG: a toolkit for research on Emergence of lanGuage in Games. In EMNLP: System Demonstrations, 2019. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kirby, S. Natural language from artiﬁcial life. Artiﬁcial life, 8(2):185–215, 2002. Kottur, S., Moura, J. M., Lee, S., and Batra, D. Natural lan- guage does not emerge “naturally” in multi-agent dialog. arXiv preprint arXiv:1706.08502, 2017. Lazaridou, A., Peysakhovich, A., and Baroni, M. Multi- agent cooperation and the emergence of (natural) lan- guage. arXiv preprint arXiv:1612.07182, 2016. Lazaridou, A., Hermann, K. M., Tuyls, K., and Clark, S. Emergence of linguistic communication from referential games with symbolic and pixel input. arXiv preprint arXiv:1804.03984, 2018. LeCun, Y ., Boser, B. E., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. E., and Jackel, L. D. Hand- written digit recognition with a back-propagation network. In NIPS, 1990. LeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278–2324, 1998a. LeCun, Y ., Bottou, L., Bengio, Y ., Haffner, P., et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998b. Lewis, D. Convention harvard university press. Cambridge, MA, 1969. Li, F. and Bowling, M. Ease-of-teaching and language structure from emergent communication. In NeurIPS. 2019. Lowe, R., Foerster, J., Boureau, Y ., Pineau, J., and Dauphin, Y . On the pitfalls of measuring emergent communica- tion. In Proceedings of AAMAS, pp. 693–701, Montreal, Canada, 2019. Maddison, C. J., Mnih, A., and Teh, Y . W. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. Mikolov, T., Joulin, A., and Baroni, M. A roadmap towards machine intelligence. In International Conference on In- telligent Text Processing and Computational Linguistics, pp. 29–61. Springer, 2016. Mnih, V ., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn- chronous methods for deep reinforcement learning. In ICML, 2016. Rolfe, J. T. Discrete variational autoencoders.arXiv preprint arXiv:1609.02200, 2016. Schulman, J., Heess, N., Weber, T., and Abbeel, P. Gradient estimation using stochastic computation graphs. In NIPS, 2015. Strouse, D. and Schwab, D. J. The deterministic information bottleneck. Neural computation, 29(6):1611–1630, 2017. Sukhbaatar, S., Szlam, A., and Fergus, R. Learning mul- tiagent communication with backpropagation. In NIPS. 2016. Tishby, N., Pereira, F., and Bialek, W. The information bot- tleneck method. In Proceedings of the 37th Annual Aller- ton Conference on Communication, Control and Comput- ing. University of Illinois Press, 1999. van den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. In NIPS, 2017. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin, I. Attention is all you need. In NIPS, 2017. Williams, R. J. Simple statistical gradient-following algo- rithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992. Williams, R. J. and Peng, J. Function optimization using connectionist reinforcement learning algorithms. Con- nection Science, 3(3):241–268, 1991. Zaslavsky, N., Kemp, C., Regier, T., and Tishby, N. Ef- ﬁcient compression in color naming and its evolution. Proceedings of the National Academy of Sciences , 115 (31):7937–7942, 2018. Zaslavsky, N., Regier, T., Tishby, N., and Kemp, C. Se- mantic categories of artifacts and animals reﬂect efﬁcient coding. In Proceedings of CogSci, pp. 1254–1260, Mon- treal, Canada, 2019. Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking general- ization. arXiv preprint arXiv:1611.03530, 2016.",
      "references": [
        "Information dropout: Learning optimal representations through noisy computation.",
        "Emergence of invariance and disentanglement in deep representations.",
        "Deep variational information bottleneck.",
        "Why Only Us: Language and Evolution.",
        "More than Nature Needs: Language, Mind, and Evolution.",
        "How agents see things: On visual representations in an emergent language game.",
        "Defensive distillation is not robust to adversarial examples.",
        "Anti-efficient encoding in emergent communication.",
        "Compositionality and generalization in emergent languages.",
        "BabyAI: A platform to study the sample efficiency of grounded language learning.",
        "Learning phrase representations using rnn encoder-decoder for statistical machine translation.",
        "Compositional obverter communication learning from raw visual input.",
        "Language evolution: Syntax before phonology?",
        "Elements of Information Theory.",
        "Emergent communication in a multi-modal, multi-step referential game.",
        "The global minima of the communicative energy of natural communication systems.",
        "Compression as a universal principle of animal behavior.",
        "The conditional entropy bottleneck.",
        "Experimental semiotics: A new approach for studying communication as a form of joint action.",
        "How efficiency shapes human language.",
        "Explaining and harnessing adversarial examples.",
        "Variance reduction techniques for gradient estimates in reinforcement learning.",
        "Emergent linguistic phenomena in multi-agent communication games.",
        "Emergence of language with multi-agent games: Learning to communicate with sequences of symbols.",
        "On the pitfalls of measuring emergent communication.",
        "DeepColor: Reinforcement learning optimizes information efficiency and well-formedness in color name partitioning.",
        "Kinship categories across languages reflect general communicative principles.",
        "Handwritten digit recognition with a back-propagation network.",
        "Gradient-based learning applied to document recognition.",
        "Ease-of-teaching and language structure from emergent communication.",
        "The concrete distribution: A continuous relaxation of discrete random variables.",
        "A roadmap towards machine intelligence.",
        "Asynchronous methods for deep reinforcement learning.",
        "Understanding deep learning requires rethinking generalization."
      ],
      "meta_data": {
        "arxiv_id": "1905.13687v3",
        "authors": [
          "Eugene Kharitonov",
          "Rahma Chaabouni",
          "Diane Bouchacourt",
          "Marco Baroni"
        ],
        "published_date": "2019-05-31T15:54:41Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Deep Learning From Crowdsourced Labels: Coupled Cross-Entropy Minimization, Identifiability, and Regularization",
      "full_text": "Published as a conference paper at ICLR 2023 DEEP LEARNING FROM CROWDSOURCED LABELS : COUPLED CROSS -ENTROPY MINIMIZATION , I DENTI - FIABILITY , AND REGULARIZATION Shahana Ibrahim, Tri Nguyen, and Xiao Fu ∗ School of Electrical Engineering and Computer Science Oregon State University Corvallis, OR 97330, USA ABSTRACT Using noisy crowdsourced labels from multiple annotators, a deep learning-based end-to-end (E2E) system aims to learn the label correction mechanism and the neural classifier simultaneously. To this end, many E2E systems concatenate the neural classifier with multiple annotator-specific “label confusion” layers and co-train the two parts in a parameter-coupled manner. The formulated coupled cross-entropy minimization (CCEM)-type criteria are intuitive and work well in practice. Nonetheless, theoretical understanding of the CCEM criterion has been limited. The contribution of this work is twofold: First, performance guarantees of the CCEM criterion are presented. Our analysis reveals for the first time that the CCEM can indeed correctly identify the annotators’ confusion characteristics and the desired “ground-truth” neural classifier under realistic conditions, e.g., when only incomplete annotator labeling and finite samples are available. Second, based on the insights learned from our analysis, two regularized variants of the CCEM are proposed. The regularization terms provably enhance the identifiability of the target model parameters in various more challenging cases. A series of synthetic and real data experiments are presented to showcase the effectiveness of our approach. 1 I NTRODUCTION The success of deep learning has escalated the demand for labeled data to an unprecedented level. Some learning tasks can easily consume millions of labeled data (Najafabadi et al., 2015; Goodfellow et al., 2016). However, acquiring data labels is a nontrivial task—it often requires a pool of annotators with sufficient domain expertise to manually label the data items. For example, the popular Microsoft COCO dataset contains 2.5 million images and around 20,000 work hours aggregated from multiple annotators were used for its category labeling (Lin et al., 2014). Crowdsourcing is considered an important working paradigm for data labeling. In crowdsourcing platforms, e.g., Amazon Mechanical Turk (Buhrmester et al., 2011), Crowdflower (Wazny, 2017), and ClickWork (Vakharia & Lease, 2013), data items are dispatched and labeled by many annotators; the annotations are then integrated to produce reliable labels. A notable challenge is that annotator-output labels are sometimes considerably noisy. Training machine learning models using noisy labels could seriously degrade the system performance (Arpit et al., 2017; Zhang et al., 2016a). In addition, the labels provided by individual annotators are often largely incomplete, as a dataset is often divided and dispatched to different annotators. Early crowdsourcing methods often treat annotation integration and downstream operations, e.g., classification, as separate tasks; see, (Dawid & Skene, 1979; Karger et al., 2011a; Whitehill et al., 2009; Snow et al., 2008; Welinder et al., 2010; Liu et al., 2012; Zhang et al., 2016b; Ibrahim et al., 2019; Ibrahim & Fu, 2021). This pipeline estimates the annotators’ confusion parameters (e.g., the confusion matrices under the Dawid & Skene (DS) model (Dawid & Skene, 1979)) in the first stage. ∗Contact Information: Shahana Ibrahim, Tri Nguyen, and Xiao Fu: {ibrahish, nguyetr9, xiao.fu}@oregonstate.edu 1 arXiv:2306.03288v1  [cs.LG]  5 Jun 2023Published as a conference paper at ICLR 2023 Then, the corrected and integrated labels along with the data are used for training the downstream tasks’ classifiers. However, simultaneously learning the annotators’ confusions and the classifier in an end-to-end (E2E) manner has shown substantially improved performance in practice. (Raykar et al., 2010; Khetan et al., 2018; Tanno et al., 2019; Rodrigues & Pereira, 2018; Chu et al., 2021; Guan et al., 2018; Cao et al., 2019; Li et al., 2020; Wei et al., 2022; Chen et al., 2020). To achieve the goal of E2E-based learning under crowdsourced labels, a class of methods concatenate a “confusion layer” for each annotator to the output of a neural classifier, and jointly learn these parts via a parameter-coupled manner. This gives rise to a coupled cross entropy minimization (CCEM) criterion (Rodrigues & Pereira, 2018; Tanno et al., 2019; Chen et al., 2020; Chu et al., 2021; Wei et al., 2022). In essence, the CCEM criterion models the observed labels as annotators’ confused outputs from the “ground-truth” predictor (GTP)—i.e., the classifier as if being trained using the noiseless labels and with perfect generalization—which is natural and intuitive. A notable advantage of the CCEM criteria is that they often lead to computationally convenient optimization problems, as the confusion characteristics are modeled as just additional structured layers added to the neural networks. Nonetheless, the seemingly simple CCEM criterion and its variants have shown promising performance. For example, the crowdlayer method is a typical CCEM approach, which has served as a widely used benchmark for E2E crowdsourcing since its proposal (Rodrigues & Pereira, 2018). In (Tanno et al., 2019), a similar CCEM-type criterion is employed, with a trace regularization added. More CCEM-like learning criteria are seen in (Chen et al., 2020; Chu et al., 2021; Wei et al., 2022), with some additional constraints and considerations. Challenges. Apart from its empirical success, understanding of the CCEM criterion has been limited. Particularly, it is often unclear if CCEM can correctly identify the annotators’ confusion characteristics (that are often modeled as “confusion matrices”) and the GTP under reasonable settings (Rodrigues & Pereira, 2018; Chu et al., 2021; Chen et al., 2020; Wei et al., 2022), but identifiability stands as the key for ensured performance. The only existing identifiability result on CCEM was derived under restricted conditions, e.g., the availability of infinite samples and the assumption that there exists annotators with diagonally-dominant confusion matrices (Tanno et al., 2019). To our best knowledge, model identifiability of CCEM has not been established under realistic conditions, e.g., in the presence of incomplete annotator labeling and non-experts under finite samples. We should note that a couple of non-CCEM approaches proposed identifiability-guaranteed solu- tions for E2E crowdsourcing. The work in (Khetan et al., 2018) showed identifiability under their expectation maximization (EM)-based learning approach, but the result is only applicable to binary classification. An information-theoretic loss-based learning approach proposed in (Cao et al., 2019) presents some identifiability results, but under the presence of a group of independent expert annota- tors and infinite samples. These conditions are hard to meet or verify. In addition, the computation of these methods are often more complex relative to the CCEM-based methods. Contributions. Our contributions are as follows: • Identifiability Characterizations of CCEM-based E2E Crowdsourcing. In this work, we show that the CCEM criterion can indeed provably identify the annotators’ confusion matrices and the GTP up to inconsequential ambiguities under mild conditions. Specifically, we show that, if the number of annotator-labeled items is sufficiently large and some other reasonable assumptions hold, the two parts can be recovered with bounded errors. Our result is the first finite-sample identifiability result for CCEM-based crowdsourcing. Moreover, our analysis reveals that the success of CCEM does not rely on conditional independence among the annotators. This is favorable (as annotator independence is a stringent requirement) and surprising, since conditional independence is often used to derive the CCEM criterion; see, e.g., (Tanno et al., 2019; Rodrigues & Pereira, 2018; Chu et al., 2021). • Regularization Design for CCEM With Provably Enhanced Identfiability. Based on the key insights revealed in our analysis, we propose two types of regularizations that can provide enhanced identifiability guarantees under challenging scenarios. To be specific, the first regularization term ensures that the confusion matrices and the GTP can be identified without having any expert annotators if one has sufficiently large amount of data. The second regularization term ensures identifiability when class specialists are present among annotators. These identifiability-enhanced approaches demonstrate promising label integration performance in our experiments. Notation. The notations are summarized in the supplementary material. 2Published as a conference paper at ICLR 2023 2 B ACKGROUND Crowdsourcing is a core working paradigm for labeling data, as individual annotators are often not reliable enough to produce quality labels. Consider a set of N items {xn}N n=1. Each item belongs to one of the K classes. Here, xn ∈ RD denote the feature vector of nth data item. Let {yn}N n=1 denote the set of ground-truth labels, where yn ∈ [K], ∀n. The ground-truth labels {yn}N n=1 are unknown. We ask M annotators to provide their labels for their assigned items. Consequently, each item xn is labeled by a subset of annotators indexed by Sn ⊆ [M]. Let {by(m) n }(m,n)∈S denote the set of labels provided by all annotators, where S = {(m, n) | m ∈ Sn, n∈ [N]} and by(m) n ∈ [K]. Note that |S| ≪NM usually holds and that the annotators may often incorrectly label the items, leading to an incomplete and noisy {by(m) n }(m,n)∈S. Here, “incomplete” means that S does not cover all possible combinations of (m, n)—i.e., not all annotators label all items. The goal of an E2E crowdsourcing system is to train a reliable machine learning model (often a classifier) using such noisy and incomplete annotations. This is normally done via blindly estimating the confusion characteristics of the annotators from the noisy labels. Classic Approaches. Besides the naive majority voting method, one of the most influential crowd- sourcing model is by Dawid & Skene (Dawid & Skene, 1979). Dawid & Skene models the confusions of the annotators using the conditional probabilities Pr(by(m) n |yn). Consequently, annotator m’s confusion is fully characterized by a matrix defined as follows: Am(k, k′) := Pr(by(m) n = k|yn = k′), ∀k, k′ ∈ [K]. Dawid & Skene also proposed an EM algorithm that estimate Am’s under a naive Bayes generative model. Many crowdsourcing methods follow Dawid & Skene’s modeling ideas (Ghosh et al., 2011; Dalvi et al., 2013; Karger et al., 2013; Zhang et al., 2016b; Traganitis et al., 2018; Ibrahim et al., 2019; Ibrahim & Fu, 2021). Like (Dawid & Skene, 1979), many of these methods do not exploit the data features xn while learning the confusion matrices of the annotators. They often treat confusion estimation and classifier training as two sequential tasks. Such two-stage approaches may suffer from error propagation. E2E Crowdsourcing and The Identifiability Challenge. The E2E approaches in (Rodrigues & Pereira, 2018; Tanno et al., 2019; Khetan et al., 2018) model the probability of mth annotator’s response to the data item xn as follows: Pr(by(m) n = k|xn) = KX k′=1 Pr(by(m) n = k|yn = k′)Pr(yn = k′|xn), k∈ [K], (1) where the assumption that the annotator confusion is data-independent has been used to derive the right-hand side. In the above, the distribution Pr(by(m) n |yn) models the confusions in annotator responses given the true label yn and Pr(yn|xn) denotes the true label distribution of the data item xn. The distribution Pr(yn|xn) can be represented using a mapping f : RD → [0, 1]K, [f(xn)]k = Pr(yn = k|xn). Define a probability vector p(m) n ∈ [0, 1]K for every m and n, such that [p(m) n ]k = Pr(by(m) n = k|xn). With these notations, the following model holds: p(m) n = Amf(xn), ∀m, n. (2) In practice, one does not observe p(m) n . Instead, the annotations by(m) n are categorical realizations of the distribution p(m) n ; i.e., by(m) n ∼ Cart(p(m) n ). (3) The goal of E2E crowdsourcing is to identify Am and f in (2) from noisy labels {by(m) n }(m,n)∈S and data {xn}N n=1. Note that even if p(m) n in (2) is observed, Am and f(·) are in general not identifiable. The reason is that one can easily find nonsingular matrices Q ∈ RK×K such that p(m) n = eAm ef(xn), where eAm = AmQ and ef(xn) = Q−1f(xn). If an E2E approach does not ensure the identifiability of Am and f, but outputs ef(xn) = Q−1f(xn), then the learned predictor is not likely to work reasonably. Identifying the ground-truth f not only identifies the true labels for the training data (data for which the annotations have been collected), but is useful to predict the true labels for unseen test data items. In addition, correctly identifying Am’s helps quantify how reliable each annotator is. 3Published as a conference paper at ICLR 2023 CCEM-based Approaches and Challenges. A number of existing E2E methods are inspired by the model in (2) to formulate CCEM-based learning losses. • Crowdlayer (Rodrigues & Pereira, 2018): For each (m, n) ∈ S, the generative model in (2) naturally leads to a cross-entropy based learning loss, i.e., CE(Amf(xn), by(m) n ). As some annotators co-label items, the components f(xn) associated with the co-labeled items are shared by these annotators. With such latent component coupling and putting the cross-entropy losses for all (m, n) ∈ Stogether, the work in (Rodrigues & Pereira, 2018) formulate the following CCEM loss: minimize f∈F,{Am∈RK×K} 1 |S| X (m,n)∈S CE(softmax(Amf(xn)), by(m) n ), (4) where F ⊆ {f(x) ∈ RK| f(x) ∈ ∆K, ∀x} is a function class and the softmax operator is applied as the output of Amf(xn) is supposed to be a probability mass function (PMF). The method based on the formulation in (4) was called crowdlayer (Rodrigues & Pereira, 2018). The name comes from the fact that the confusion matrices of the “crowd” Am’s can be understood as additional layers if f is represented by a neural network. The loss function in (4) is relatively easy to optimize, as any off-the-shelf neural network training algorithms can be directly applied. This seemingly natural and simple CCEM approach demonstrated substantial performance improvement upon classic methods. However, there is no performance characterization for (4). • TraceReg (Tanno et al., 2019): The work in (Tanno et al., 2019) proposed a similar loss function— with a regularization term in order to establish identifiability of the Am’s and f: minimize f∈F,{Am∈A} 1 |S| X (m,n)∈S CE(Amf(xn), by(m) n ) + λ MX m=1 trace(Am), (5) where A is the constrained set {A ∈ RK×K|A ≥ 0, 1⊤A = 1⊤}. It was shown that (i) if p(m) n instead of by(m) n is observed for every m and n and (ii) if the mean of the Am’s are diagonally dominant, then solving (5) with CE(Amf(xn), by(m) n ) replaced by enforcing p(m) n = Amf(xn) ensures identifying Am for all m, up to column permutations. Even though, the result provides some justification for using CCEM criterion under trace regularization, the result is clearly unsatisfactory, as it requires many stringent assumptions to establish identifiability. • SpeeLFC (Chen et al., 2020), Union-Net (Wei et al., 2022), CoNAL (Chu et al., 2021): A number of other variants of the CCEM criterion are also proposed for E2E learning with crowdsourced labels. The work in (Chen et al., 2020) uses a similar criterion as in (4), but with the softmax operator applied on the columns of Am’s, instead on the output of Amf(xn). The work in (Wei et al., 2022) concatenates Am’s vertically and applies the softmax operator on the columns of such concatenated matrix, while formulating the CCEM criterion. The method in (Chu et al., 2021) models a common confusion matrix in addition to the annotator-specific confusion matrices Am’s and employs a CCEM-type criterion to learn both in a coupled fashion. Nonetheless, none of these approaches have successfully tackled the identifiability aspect. The work (Wei et al., 2022) claims theoretical support for their approach, but the proof is flawed. In fact, understanding to the success of CCEM-based methods under practical E2E learning settings is still unclear. Other Related Works. A couple of non-CCEM approaches have also considered the identifiability aspects (Khetan et al., 2018; Cao et al., 2019). The work in (Khetan et al., 2018) presents an EM- inspired alternating optimization strategy. Such procedure involves training the neural classifier multiple times, making it a computationally intensive approach. In addition, its identifiability guarantees are under certain strict assumptions, e.g., binary classification, identical confusions and conditional independence for annotators—which are hard to satisfy in practice. The approach in (Cao et al., 2019) employs an information-theoretic loss function to jointly train a predictor network and annotation aggregation network. The identifiability guarantees are again under restrictive assumptions, e.g., infinite data, no missing annotations and existence of mutually independent expert annotators. We should also note that there exist works considering data-dependent annotator confusions as well, e.g., (Zhang et al., 2020) (multiple annotator case) and (Cheng et al., 2021; Xia et al., 2020; Zhu et al., 2022) (single annotator case)—leveraging more complex models and learning criteria. In our work, we focus on the criterion designed using a model in (1) as it is shown to be effective in practice. 4Published as a conference paper at ICLR 2023 3 I DENTIFIABILITY OF CCEM-B ASED E2E C ROWDSOURCING In this section, we offer identifiability analysis of the CCEM learning loss under realistic settings. We consider the following re-expressed objective function of CCEM: minimize f,{Am} − 1 |S| X (m,n)∈S KX k=1 I[by(m) n = k] log[Amf(xn)]k (6a) subject to f ∈ F, Am ∈ A, ∀m, (6b) where F ⊆ {f(x) ∈ RK| f(x) ∈ ∆K, ∀x} is a function class and A is the constrained set {A ∈ RK×K|A ≥ 0, 1⊤A = 1⊤}, since each column of Am’s are conditional probability distributions. The rationale is that the confusion matrices Am’s act as correction terms for annotator’s labeling noise to output a true label classifier f. Intuitively, the objective in (6a) encourages the output estimates bf, { bAm} to satisfy the relation p(m) n = bAm bf(xn), ∀m, n. One can easily note that the identifiability guarantees for Am and f ( i.e., when does the ground-truth Am and the ground-truth f can be identified) are not so straightforward since there exists an infinite number of nonsingular matrices Q ∈ RK×K such that p(m) n = ( bAmQ)(Q−1 bf(xn)). To proceed, we make the following assumptions: Assumption 1 Each data item xn, n∈ [N] is drawn from a distributionD independently at random and has a bounded ℓ2 norm. The observed index pairs (m, n) are included in S uniformly at random. Assumption 2 The neural network function class F has a complexity measure denoted as RF. Here, we use the sensitive complexity parameter introduced in (Lin & Zhang, 2019) as the complexity measure RF. In essence, RF gets larger as the neural network function class F gets deeper and wider—also see supplementary material Sec. K for more discussion. Assumption 3 There exists a GTP f♮ : RD → [0, 1]K, such that [f♮(xn)]k = Pr(yn = k|xn), where yn denotes the true label of the data item xn. In addition, there exists ef ∈ Fsuch that ∥ ef(xn) − f♮(xn)∥2 ≤ ν for all xn ∼ D, where 0 ≤ ν <∞. Assumption 4 (Near-Class Specialist Assumption) LetA♮ m denote the ground-truth confusion matrix for annotator m. For each class k, there exists a near-class specialist, indexed by mk, such that ∥A♮ mk (k, :) − e⊤ k ∥2 ≤ ξ1, where 0 ≤ ξ1 < ∞. Assumption 5 (Near-Anchor Point Assumption) For each classk, there exists a near-anchor point, xnk , such that ∥f♮(xnk ) − ek∥2 ≤ ξ2), where 0 ≤ ξ2 < ∞. Assumptions 1-3 are standard conditions for analyzing performance of neural models under finite sample settings. The Near-Class Specialist Assumption (NCSA) implies that there exist annotators for each class k ∈ [K] who can correctly distinguish items belonging to class k from those belonging to other classes. It is important to note that NCSA assumption is much milder compared to the commonly used assumptions such as the existence of all-class expert annotators (Cao et al., 2019) or diagonally dominant A♮ m’s (Tanno et al., 2019). Instead, the NCSA only requires that for each class, there exists one annotator who is specialized for it (i.e., class-specialist)—but the annotator needs not be an all-class expert; see Fig. 1 in the supplementary material for illustration. The Near-Anchor Point Assumption (NAPA) is the relaxed version of the exact anchor point assumption commonly used in provable label noise learning methods (Xia et al., 2019; Li et al., 2021). Under Assumptions 1-5, we have the following result: Theorem 1 Assume that each [A♮ mf♮(xn)]k and [Amf(xn)]k, ∀Am ∈ A, ∀f ∈ Fare at least (1/β). Also assume that σmax(A♮ m) ≤ σ, ∀m, for a certain σ > 0. Then, for any α > 0, with 5Published as a conference paper at ICLR 2023 probability greater than 1 − K/Nα, any optimal solution bAm and bf of the problem (6) satisfies the following relations: min Π ∥ bAm − A♮ mΠ∥2 F = Kσ2(η + ξ1 + ξ2), ∀m ∈ [M], (7a) E x∼D h min Π ∥ bf(x) − Π⊤f♮(x)∥2 2 i = K(η + ξ1 + ξ2), (7b) where η2 = O \u0010 βMN α / √ S \u0010√M log S + (∥X∥FRF) 1 4 \u0011 + β √ KMN αν \u0011 , Π ∈ {0, 1}K is a permutation matrix, and X = [ xn1 , . . . ,xnS ], (ms, ns) ∈ S, if conditions ξ1, ξ2 ≤ 1/K, ν ≤ 1/βK2M2Nα, and S = |S| = Ω \u0010 β2M2N2αK2 max \u0010 M log S, p ∥X∥FRF \u0011\u0011 hold. The proof of Theorem 1 is relegated to the supplementary material in Sec. B. The takeaways are as follows: First, the CCEM can provably identify the A♮ m’s and the GTP f♮ under finite samples—and such finite-sample identifiability of CCEM was not established before. Second, some restrictive conditions (e.g., the existence of all-class experts (Tanno et al., 2019; Cao et al., 2019)) used in the literature for establishing CCEM’s identifiability are actually not needed. Third, many works derived the CCEM criterion under the maximum likelihood principle via assuming that the annotators are conditionally independent; see (Rodrigues & Pereira, 2018; Chen et al., 2020; Tanno et al., 2019; Chu et al., 2021). Interestingly, as we revealed in our analysis, the identifiability of A♮ m and f♮ under the CCEM criterion does not rely on the annotators’ independence. These new findings support and explain the effectiveness of CCEM in a wide range of scenarios observed in the literature. 4 E NHANCING IDENTIFIABILITY VIA REGULARIZATION Theorem 1 confirms that CCEM is a provably effective criterion for identifyingA♮ m’s and f♮. The caveats lie in Assumptions 4 and 5. Essentially, these assumptions require that some rows of the collection {A♮ m}M m=1 and some vectors of the collection {f♮(xn)}N n=1 are close to the canonical vectors. These are reasonable assumptions, but satisfying them simultaneously may not always be possible. A natural question is if these assumptions can be relaxed, at least partially. In this section, we propose variants of CCEM that admits enhanced identifiability. To this end, we use a condition that is often adopted in the nonnegative matrix factorization (NMF) literature: Definition 1 (SSC)(Fu et al., 2019; Gillis, 2020) Consider the second-order cone C = {x ∈ RK| √ K − 1∥x∥2 ≤ 1⊤x}. A nonnegative matrix Z ∈ RL×K + satisfies the sufficiently scattered condition (SSC) if (i)C ⊆cone(Z⊤) and (ii) cone{Z⊤} ⊆cone{Q} does not hold for any orthogonal Q ∈ RK×K except for the permutation matrices. Geometrically, the matrix Z satisfying the SSC means that its rows span a large “area” within the nonnegative orthant, so that the conic hull spanned by the rows contains the second order cone C as a subset. Note that the SSC condition subsumes the the NCSA (Assumption (4) where ξ1 = 0) and the NAPA (Assumption (5) whereξ1 = 0) as special cases. Using SSC, we first show that the CCEM criterion in (6) attains identifiability of A♮ m’s and f♮ without using the NCSA or the NAPA, when the problem size grows sufficiently large. Theorem 2 Suppose that the incomplete labeling paradigm in Assumption 1 holds with S = Ω(t) and N = O(t3), for a certain t >0. Also, assume that there exists Z = {n1, . . . , nT } such that F♮ Z = [f♮(xn1 ), . . . ,f♮(xnT )] satisfies the SSC and W♮ = [A♮⊤ 1 , . . . ,A♮⊤ M ] satisfies the SSC as well. Then, at the limit of t → ∞, if f♮ ∈ F, the optimal solutions ({ bAm}, bf) of (6) satisfies bAm = A♮ mΠ for all m and bf(x) = Π⊤f♮(x) for all x ∼ D, where Π is a permutation matrix. The proof is via connecting the CCEM problem to the classic nonnegative matrix factorization (NMF) problem at the limit of t → ∞; see Sec. D. It is also worth to note that Theorem 2 does not require complete observation of the annotations to guarantee identifiability. Another remark is that W♮ and F♮ satisfying the SSC is more relaxed compared to that they satisfying the NCSA and NAPA, 6Published as a conference paper at ICLR 2023 respectively. For example, one may need class specialists with much higher expertise to satisfy NCSA than to satisfy SSC on W♮—also see geometric illustration in Sec. L. Compared to the result in Theorem 1, Theorem 2 implies that when N is very large, the CCEM should work under less stringent conditions relative to the NCSA and NAPA. Nonetheless, the assumption that both the GTP and the annotators’ confusion matrices should meet certain requirements may not always hold. In the following, we further show that the SSC on either W♮ or F♮ can be relaxed: Theorem 3 Suppose that the incomplete labeling paradigm in Assumption 1 holds with S = Ω(t) and N = O(t3), for a certain t >0. Then, at the limit of t → ∞, if f♮ ∈ F, we have A∗ m = A♮ mΠ for all m and f∗(x) = Π⊤f♮(x) for all x ∼ D, when any of the following conditions hold: (a) If there exists Z = {n1, . . . , nT } such that (F♮ Z)⊤ = [f♮(xn1 ), . . . ,f♮(xnT )]⊤ satisfies the SSC, rank(W♮) = K, and ({A∗ m}, f∗) is the optimal solution of (6) with the maximal log det(F∗F∗ ⊤). (b) If W♮ = [A♮⊤ 1 , . . . ,A♮⊤ M ]⊤satisfies the SSC, rank(F♮) = K, and (A∗ m, f∗) is the optimal solution of (6) with the maximal log det((W∗)⊤W∗). Theorem 3(a) shows that if the number of annotated data items is large and the GTP-outputs associated with xn are diverse enough to satisfy the SSC, then f♮ and A♮ m are identifiable even if there are no class specialists available. Compared to Theorem 2, the identifiability is clearly enhanced, as the conditions on the annotators have been relaxed. Theorem 3(b) implies that if W♮ satisfies SSC, identifiability can be established irrespective of the geometry of F♮. Intuitively, when there are more annotators available, W♮ is more likely to satisfy SSC (the relation between the size of W♮ and SSC was shown in (Ibrahim et al., 2019, Theorem 4) under reasonable conditions). Hence, in practice, Theorem 3(b) may offer ensured performance in cases where more annotators are available. Similar to the proof in Theorem 2, the proof here connects the CCEM-based E2E crowdsourcing problem to the simplex-structured matrix factorization (SSMF) problem (see (Fu et al., 2015; 2018)) at the limit of t → ∞. The detailed proof is provided in Sec. E. We should mention that various connections between matrix factorization models and data labeling problems were also observed in related prior works, e.g, the classic approaches based on Dawid & Skene model (Dawid & Skene, 1979; Zhang et al., 2016b; Ibrahim et al., 2019; Ibrahim & Fu, 2021) and E2E learning under noisy labels (Li et al., 2021). Nonetheless, the former does not consider cross-entropy losses or involve any feature extractor, whereas, the latter does not consider multiple annotators or incomplete labeling. Implementation via Regularization Theorem 3 naturally leads to regularized versions of CCEM. Correspondingly, we have the following two cases: Following Theorem 3(a), when the GTP is believed to be diverse over differentxn’s, we propose to employ the following regularized CCEM: minimize Am∈A,∀m, f∈F − 1 |S| X (m,n)∈S KX k=1 I[by(m) n = k] log[Amf(xn)]k − λ log detFF ⊤, (8) where F = [ f(x1), . . . ,f(xN )]⊤. In similar spirit, following Theorem 3(b), we propose the following regularized CCEM when M is large (so that W♮ likely satisfies SSC): minimize Am∈A,∀m, f∈F − 1 |S| X (m,n)∈S KX k=1 I[by(m) n = k] log[Amf(xn)]k − λ log detW⊤W, (9) where W = [A⊤ 1, . . . ,A⊤ M ]⊤. The implementations for (8) and (9) are called as geometry-regularized crowdsourcing network, abbreviated as GeoCrowdNet(F) and GeoCrowdNet(W), respectively. Remark 1 Based on our analyses, our suggested rule of thumb for choosing regularization is as fol- lows: When M is large but N is relatively small, using GeoCrowdNet(W) is recommended, as W is more likely to satisfy the SSC compared to F⊤. However, when N is large, using GeoCrowdNet(F) is expected to have a better performance as F⊤ is likely to satisfy the SSC in this case. Under big data settings, N is often large and the GTP outputs of xn’s are often reasonably diverse. Hence, (8) oftentimes works well, as will be seen in our experiments. Nonetheless, for cases where the geometry of F⊤ violates the SSC, the employment of more annotators can help via using (9)—which is also an intuitive advantage of crowdsourcing. 7Published as a conference paper at ICLR 2023 Table 1: Average test accuracy ( ± std) of the proposed methods and the baselines on MNIST & Fashion-MNIST dataset under various(N, M)’s; labels are produced by machine annotators;p = 0.1. Methods MNIST Fashion-MNISTCase 1 Case 2 Case 1 Case 2(1000,15)(1000,20)(5000,5) (10000,5)(1000,15)(1000,30)(5000,5)(10000,5)GeoCrowdNet(F)79.89±3.0882.18±3.4885.92±2.7387.21±2.4778.98±2.8384.47±1.6480.60±0.4683.68±2.17GeoCrowdNet(W)80.97±1.3183.69±2.3777.79±8.9782.37±9.1879.80±4.2385.56±1.9172.36±3.8474.03±7.41GeoCrowdNet(λ= 0)71.15±6.7369.17±2.6171.66±4.4860.29±7.9170.92±4.1481.88±4.4169.31±4.7773.04±7.56TraceReg70.14±6.9378.09±3.5263.71±10.7664.45±10.1475.06±3.4383.79±2.1569.82±6.4672.21±8.44CrowdLayer65.72±4.8172.90±2.3151.25±6.2453.12±14.0263.91±2.1176.50±3.6864.73±6.8963.18±3.55MBEM37.24±8.5433.39±5.7228.14±6.2426.90±2.0123.14±2.3625.43±5.6837.34±3.7338.62±5.45CoNAL51.33±4.2755.59±5.3244.79±7.0139.41±9.2159.76±3.3471.44±6.2252.15±6.0854.61±7.54Max-MIG81.32±1.3183.34±3.3683.71±1.2381.05±0.6570.95±5.6181.27±5.3773.45±5.2173.62±3.12NN-MV78.98±3.2182.78±1.4583.44±1.9883.10±2.8662.61±6.3873.61±3.8861.95±3.7371.64±2.25NN-DSEM73.95±4.9077.57±4.0665.81±2.7469.09±3.2155.89±5.9773.64±4.4431.40±3.8635.10±4.89 5 E XPERIMENTS Baselines. The proposed methods are compared with a number of existing E2E crowdsourcing methods, namely, TraceReg(Tanno et al., 2019), MBEM (Khetan et al., 2018), CrowdLayer (Rodrigues & Pereira, 2018), CoNAL (Chu et al., 2021), and Max-MIG (Cao et al., 2019). In addition to these baselines, we also learn neural network classifiers using the labels aggregated from majority voting and the classic EM algorithm proposed by Dawid &S kene (Dawid & Skene, 1979), denoted as NN-MV and NN-DSEM, respectively. Real-Data Experiments with Machine Annotations. The settings are as follows: Dataset. We use the MNIST dataset (Deng, 2012) and Fashion-MNIST dataset (Xiao et al., 2017); see more details in Sec. N. Noisy Label Generation. We train a number of machine classifiers to produce noisy labels. Five types of classifiers, including support vector machines (SVM), k-Nearest Neighbour (kNN), logistic regression, convolutional neural network (CNN), and fully connected neural network are considered to act as annotators. To create more annotators, we change some of their parameters (e.g., the number of nearest neighbour parameter k of kNN and the number of epochs for training the CNN). We consider two different cases: (i) Case 1: NCSA (Assumption 4) holding: Each annotator is chosen to be an all-class expert with probability 0.1. If an annotator is chosen to be an all-class expert, it is trained carefully so that its classification accuracy exceeds a certain threshold; see details in Sec. N. Note that the existence of an expert implies that the NCSA and the SSC are likely satisfied by W♮, but the reverse is not necessarily true. We use this strategy to enforce NCSA only for validation purpose. It does not mean that the CCEM criterion needs the existence of experts to work. (ii) Case 2: NCSA not holding: Each machine annotator is trained by randomly choosing a small subset of the training data (with 100 to 500 samples) so that the accuracy of these machine annotators are low. This way, any all-class expert or class-specialist is unlikely to exist. We use the two cases to validate our theorems. Under our analyses, GeoCrowdNet(W) is expected to work better under Case 1, as NCSA approximately implies SSC. In addition, GeoCrowdNet(F) does not rely on the geometry of W♮, and thus can work under both cases, if N is reasonably large (which implies that (F♮)⊤is more likely to satisfy the SSC). Once the machine annotators are trained, we let them label unseen data items of size N. To evaluate the methods under incomplete labeling, an annotator labels any data item with probability p = 0.1. Under this labeling strategy, every data item is only labeled by a small subset of the annotators. Settings. The neural network classifier architecture for MNIST dataset is chosen to be Lenet-5 (Lecun et al., 1998), which consists of two sets of convolutional and max pooling layers, followed by a flattening convolutional layer, two fully-connected layers and finally a softmax layer. For Fashion-MNIST dataset, we use the ResNet-18 architecture (He et al., 2016). Adam (Kingma & Ba, 2015) is used an optimizer with weight decay of 10−4 and batch size of 128. The regularization parameter λ and the initial learning rate of the Adam optimizer are chosen via grid search method using the validation set from {0.01, 0.001, 0.0001} and {0.01, 0.001}, respectively. We choose the same neural network structures for all the baselines. The confusion matrices are initialized with identity matrices of size K for proposed methods and the baselines TraceReg and CrowdLayer. Results. Table 1 presents the average label prediction accuracy on the testing data of the MNIST and the Fashion-MNIST over 5 random trials, for various cases. One can observe thatGeoCrowdNet(F) 8Published as a conference paper at ICLR 2023 Table 2: Average test accuracy of the proposed methods and the baselines on LabelMe (M = 59, K= 8) and Music (M = 44, K= 10) datasets. Methods LabelMe Music GeoCrowdNet(F) 85 .85 ± 0.33 67 .13 ± 1.27 GeoCrowdNet(W) 81 .59 ± 0.73 66 .46 ± 1.77 GeoCrowdNet (λ=0) 80 .35 ± 0.40 65 .93 ± 1.81 TraceReg 80 .89 ± 0.73 66 .40 ± 0.90 Crowdlayer 81 .41 ± 0.90 65 .53 ± 1.74 MBEM 80 .05 ± 3.31 69 .00 ± 2.15 CoNAL 81 .68 ± 2.72 60 .93 ± 0.61 Max-MIG 85 .03 ± 0.44 64 .33 ± 2.31 NN-MV 79 .57 ± 1.24 63 .40 ± 2.68 NN-DSEM 78 .26 ± 1.18 64 .66 ± 2.34 performs the best when there are more number of annotated items, even when there are no class specialist annotators present (i.e., case 2). On the other hand, GeoCrowdNet(W) starts showing its advantage over GeoCrowdNet(F) when there are more number of annotators and class spe- cialists among them. Both are consistent with our theorems. In addition, the baseline MaxMIG performs competitively when there are all-class experts—which is consistent with their identifiability analysis. However, when there are no such experts (case 2), its performance drops compared to the proposed methods, especially GeoCrowdNet(F) whose identifiability does not rely on the existence of all-class experts or class specialists. Overall, GeoCrowdNet(F) exhibits consistently good performance. Real-Data Experiments with Human Annotators. The settings are as follows: Datasets. We consider two different datasets, namely, LabelMe (M = 59) (Rodrigues et al., 2017; Russell et al., 2007) and Music (M = 44) (Rodrigues et al., 2014), both having noisy and incomplete labels provided by human workers from AMT. Settings. For LabelMe, we employ the pretrained VGG-16 embeddings followed by a fully connected layer as used in (Rodrigues & Pereira, 2018). For Music, we choose a similar architecture, but with batch normalization layers. We use a batch size of 128 for LabelMe and a batch size of 100 for Music dataset. All other settings are the same as the machine annotator case. More details of the datasets and architecture can be seen in Sec. N. Results. Table 2 shows the average label prediction accuracies on the test data for 5 random trials. One can see that GeoCrowdNet (λ=0) works reasonably well relative to the machine annotation case in the previous experiment. This may be because the number of annotators for both LabelMe and Music are reasonably large (M = 59 and M = 44, respectively), which makes W♮ satisfying Assumption 4 become easier than before. This validates our claims in Theorem 1, but also shows the limitations of the plain-vanilla CCEM—that is, it may require a fairly large number of annotators to start showing promising results. Similar to the previous experiment, both GeoCrowdNet(F) and GeoCrowdNet(W) outperform the unregularized version GeoCrowdNet (λ=0), showing the advantages of the identifiability-enhancing regularization terms. More experiments and details are presented in Sec. N, due to page limitations. 6 C ONCLUSION In this work, we revisited the CCEM criterion—one of the most popular E2E learning criteria for crowdsourced label integration. We provided the first finite-sample identifiability characterization of the confusion matrices and the neural classifier which are learned using the CCEM criterion. Compared to many exiting identifiability results, our guarantees are under more relaxed and more realistic settings. In particular, a take-home point revealed in our analysis is that CCEM can provably identify the desired model parameters even if the annotators are dependent, which is a surprising but favorable result. We also proposed two regularized variants of the CCEM, based on the insights learned from our identifiability analysis for the plain-vanilla CCEM. The regularized CCEM criteria provably enhance the identifiability of the confusion matrices and the neural classifier under more challenging scenarios. We evaluated the proposed approaches on various synthetic and real datasets. The results corroborate our theoretical claims. 9Published as a conference paper at ICLR 2023 Acknowledgement. This work is supported in part by the National Science Foundation under Project NSF IIS-2007836. REFERENCES Devansh Arpit, Stanisław Jastrzundefinedbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. A closer look at memorization in deep networks. In Proceedings of International Conference on Machine Learning, pp. 233–242, 2017. Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, volume 30, 2017. Michael Buhrmester, Tracy Kwang, and Samuel Gosling. Amazon’s Mechanical Turk: A new source of inexpensive, yet high-quality, data? Perspectives on Psychological Science, 6:3–5, 02 2011. Peng Cao, Yilun Xu, Yuqing Kong, and Yizhou Wang. Max-MIG: An information theoretic ap- proach for joint learning from crowds. In Proceedings of International Conference on Learning Representations, 2019. Zhijun Chen, Huimin Wang, Hailong Sun, Pengpeng Chen, Tao Han, Xudong Liu, and Jie Yang. Structured probabilistic end-to-end learning from crowds. In Proceedings of International Joint Conference on Artificial Intelligence, pp. 1512–1518, 7 2020. Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu. Learning with instance- dependent label noise: A sample sieve approach. In Proceedings of International Conference on Learning Representations, 2021. Zhendong Chu, Jing Ma, and Hongning Wang. Learning from crowds by modeling common confusions. Proceedings of the AAAI Conference on Artificial Intelligence, 35(7):5832–5840, May 2021. Nilesh Dalvi, Anirban Dasgupta, Ravi Kumar, and Vibhor Rastogi. Aggregating crowdsourced binary ratings. In Proceedings of International Conference on World Wide Web, pp. 285–294, 2013. Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied statistics, pp. 20–28, 1979. Li Deng. The MNIST database of handwritten digit images for machine learning research. IEEE Signal Process. Mag., 29(6):141–142, 2012. A.A. Fedotov, P. Harremoes, and F. Topsoe. Refinements of pinsker’s inequality.IEEE Trans. Inf. Theory, 49(6):1491–1498, 2003. Xiao Fu, Wing-Kin Ma, Kejun Huang, and Nicholas D. Sidiropoulos. Blind separation of quasi- stationary sources: Exploiting convex geometry in covariance domain.IEEE Trans. Signal Process., 63(9):2306–2320, 2015. Xiao Fu, Kejun Huang, and Nicholas D Sidiropoulos. On identifiability of nonnegative matrix factorization. IEEE Signal Process. Lett., 25(3):328–332, 2018. Xiao Fu, Kejun Huang, Nicholas D Sidiropoulos, and Wing-Kin Ma. Nonnegative matrix factorization for signal and data analytics: Identifiability, algorithms, and applications. IEEE Signal Process. Mag., 36(2):59–80, 2019. Arpita Ghosh, Satyen Kale, and Preston McAfee. Who moderates the moderators?: Crowdsourcing abuse detection in user-generated content. In Proceedings of the ACM conference on Electronic commerce, pp. 167–176, 2011. Nicolas Gillis. Nonnegative Matrix Factorization. Society for Industrial and Applied Mathematics, Philadelphia, PA, 2020. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. 10Published as a conference paper at ICLR 2023 Melody Guan, Varun Gulshan, Andrew Dai, and Geoffrey Hinton. Who said what: Modeling individual labelers improves classification. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), 2018. Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016. K. Huang, N. Sidiropoulos, and A. Swami. Non-negative matrix factorization revisited: Uniqueness and algorithm for symmetric decomposition. IEEE Trans. Signal Process., 62(1):211–224, 2014. Kejun Huang, Nicholas D. Sidiropoulos, Evangelos E. Papalexakis, Christos Faloutsos, Partha Pratim Talukdar, and Tom M. Mitchell. Principled neuro-functional connectivity discovery. InProceedings of the SIAM International Conference on Data Mining, 2015. Kejun Huang, Xiao Fu, and Nikolaos D Sidiropoulos. Anchor-free correlated topic modeling: Identifiability and algorithm. In Advances in Neural Information Processing Systems, pp. 1786– 1794, 2016. Shahana Ibrahim and Xiao Fu. Crowdsourcing via annotator co-occurrence imputation and provable symmetric nonnegative matrix factorization. In Proceedings of International Conference on Machine Learning, volume 139, pp. 4544–4554, 2021. Shahana Ibrahim, Xiao Fu, Nikos Kargas, and Kejun Huang. Crowdsourcing via pairwise co- occurrences: Identifiability and algorithms. In Advances in Neural Information Processing Systems, volume 32, pp. 7847–7857, 2019. David Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing systems. In Advances in Neural Information Processing Systems, volume 24, pp. 1953–1961, 2011a. David R Karger, Sewoong Oh, and Devavrat Shah. Efficient crowdsourcing for multi-class labeling. ACM Sigmetrics Performance Evaluation Review, 41(1):81–92, 2013. Ashish Khetan, Zachary Chase Lipton, and Anima Anandkumar. Learning from noisy singly-labeled data. In Proceedings of International Conference on Learning Representations, 2018. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of International Conference on Learning Representations, 2015. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical Report, University of Toronto, 2009. Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Shikun Li, Shiming Ge, Yingying Hua, Chunhui Zhang, Hao Wen, Tengfei Liu, and Weiqiang Wang. Coupled-view deep classifier learning from multiple noisy annotators. Proceedings of the AAAI Conference on Artificial Intelligence, 34:4667–4674, 2020. Xuefeng Li, Tongliang Liu, Bo Han, Gang Niu, and Masashi Sugiyama. Provably end-to-end label- noise learning without anchor points. In Proceedings of International Conference on Machine Learning, pp. 6403–6413, 2021. Shan Lin and Jingwei Zhang. Generalization bounds for convolutional neural networks. arXiv preprint, arXiv:1910.01487, 2019. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In European Conference on Computer Vision, 2014. Qiang Liu, Jian Peng, and Alexander T Ihler. Variational inference for crowdsourcing. In Advances in Neural Information Processing Systems, volume 25, pp. 692–700, 2012. Shuying Liu and Weihong Deng. Very deep convolutional neural network based image classification using small training sample size. InAsian Conference on Pattern Recognition (ACPR), pp. 730–734, 2015. 11Published as a conference paper at ICLR 2023 Andreas Maurer. A vector-contraction inequality for rademacher complexities. In International Conference on Algorithmic Learning Theory, pp. 7–13, 2016. Maryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar, Naeem Seliya, Randall Wald, and Edin Muharemagic. Deep learning applications and challenges in big data analytics. Journal of Big Data, 2(1):1–21, 2015. Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda Moy. Learning from crowds. Journal of Machine Learning Research , 11: 1297–1322, 2010. Filipe Rodrigues and Francisco Pereira. Deep learning from crowds. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), 2018. Filipe Rodrigues, Francisco Pereira, and Bernardete Ribeiro. Gaussian process classification and active learning with multiple annotators. In Proceedings of International Conference on Machine Learning, volume 32, pp. 433–441, 2014. Filipe Rodrigues, Mariana Lourenco, Bernardete Ribeiro, and Francisco C. Pereira. Learning supervised topic models for classification and regression from crowds. IEEE Trans. Pattern Anal. Mach. Intell., 39(12):2409–2422, 2017. Bryan C. Russell, Antonio Torralba, Kevin P. Murphy, and William T. Freeman. Labelme: A database and web-based tool for image annotation. International Journal of Computer Vision, 77:157–173, 2007. Pinsker M S. The information stability of gaussian random variables and processes (in russian). Dokl. Akad. Nauk SSSR, 133:28–30, 1960. Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014. Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y Ng. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 254–263, 2008. Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C. Alexander, and Nathan Sil- berman. Learning from noisy labels by regularized estimation of annotator confusion. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11236–11245, 2019. Panagiotis A Traganitis, Alba Pages-Zamora, and Georgios B Giannakis. Blind multiclass ensemble classification. IEEE Trans. Signal Process., 66(18):4737–4752, 2018. Donna Vakharia and Matthew Lease. Beyond AMT: An analysis of crowd work platforms.Computing Research Repository, 2013. Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices, pp. 210–268. Cambridge University Press, 2012. Kerri Wazny. “Crowdsourcing” ten years in: A review. Journal of Global Health, 7:020602, 2017. Hongxin Wei, Renchunzi Xie, Lei Feng, Bo Han, and Bo An. Deep learning from multiple noisy annotators as a union. IEEE Trans. Neural Netw. Learn. Syst., pp. 1–11, 2022. Peter Welinder, Steve Branson, Pietro Perona, and Serge J Belongie. The multidimensional wisdom of crowds. In Advances in Neural Information Processing Systems, pp. 2424–2432, 2010. Jacob Whitehill, Ting fan Wu, Jacob Bergsma, Javier R. Movellan, and Paul L. Ruvolo. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Advances in Neural Information Processing Systems, volume 22, pp. 2035–2043. 2009. Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama. Are anchor points really indispensable in label-noise learning? In Advances in Neural Information Processing Systems, volume 32, 2019. 12Published as a conference paper at ICLR 2023 Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instance-dependent label noise. In Advances in Neural Information Processing Systems, volume 33, pp. 7597–7610, 2020. Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-MNIST: a novel image dataset for bench- marking machine learning algorithms. arXiv preprint, arXiv:1708.07747, 2017. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In Proceedings of International Conference on Learning Representations, 2016a. Le Zhang, Ryutaro Tanno, Mou-Cheng Xu, Chen Jin, Joseph Jacob, Olga Cicarrelli, Frederik Barkhof, and Daniel Alexander. Disentangling human error from ground truth in segmentation of medical images. In Advances in Neural Information Processing Systems, volume 33, pp. 15750–15762, 2020. Yuchen Zhang, Xi Chen, Dengyong Zhou, and Michael I. Jordan. Spectral methods meet EM: A provably optimal algorithm for crowdsourcing. Journal of Machine Learning Research, 17(102): 1–44, 2016b. Zhaowei Zhu, Jialu Wang, and Yang Liu. Beyond images: Label noise transition matrix estimation for tasks with lower-quality features. In Proceedings of International Conference on Machine Learning, volume 162, pp. 27633–27653, 2022. 13Published as a conference paper at ICLR 2023 Supplementary Material of “ Deep Learning From Crowdsourced Labels: Coupled Cross-entropy Minimization, Identifiability, and Regularization” A N OTATION x, x, and X represent a scalar, a vector, and a matrix, respectively. [x]i and x(i) both denote the ith entry of the vector x. X(:, j) denotes the jth column vector of X and X(i, :) denotes the ith row vector of X. [X]i,j and X(i, j) both mean the (i, j)th entry of X. ∥x∥2 and ∥X∥F mean the Euclidean (Frobenius) norm of the augment.[I] means an integer set{1, 2, . . . , I}. ⊤denote transpose. ∆K = {x ∈ RK : P i x(i) = 1, x ≥ 0} denotes the probability simplex. X ≥ 0 implies that all the entries of the matrix X are nonnegative. I[A] denotes an indicator function for the event A such that I[A] = 1 if the event A happens, otherwise I[A] = 0. CE(x, y) = −PK k=1 I[y = k] log(x(k)) denotes the cross entropy function. IK denotes an identity matrix of size K × K. σmax(X) and σmin(X) denote the largest and the smallest singular values of the matrix X, respectively. cone(X) denotes the conic hull formed by the columns of the matrix X. det(X) represents the determinant of X. vec(X) denotes the vectorization operation that concatenates the columns of X. trace(X) outputs the trace (the sum of the diagonal elements) of X. B P ROOF OF THEOREM 1 If the vectors p(m) n for all m, nare available, then one can represent the model in (2) as a low-rank nonnegative matrix factorization (NMF) model as follows:   p(1) 1 . . . p(1) N ... ... ... p(M) 1 . . . p(M) N   | {z } P∈RMK×N =   A1 ... AM   | {z } W∈RMK×K [f(x1) . . . f(xN )]| {z } F∈RK×N , (10) where the factors W and F are both nonnegative per their physical meaning. Let P♮ denote the ground-truth and p♮(m) n ∈ RK the (m, n)th block in P♮, following the representa- tion in (10). Note that we do not observe the entire P♮ but only the by(m) n sampled from p♮(m) n for (m, n) ∈ S. We first show that the CCEM criterion in (6) implicitly estimates P♮ from incomplete labels indexed by S. To this end, we consider the objective function in (6a): DS(P; bY) ≜ − 1 S X (m,n)∈S KX k=1 I[by(m) n = k] logP((m − 1)K + k, n), (11) where bY denotes the set of observed noisy labels, i.e., {by(m) n }(m,n)∈S. Let { bAm}M m=1 and bf denote the estimates given by the learning criterion in (6). Then, the criterion (6) helps define the following term: bP ≜ arg min P∈P DS(P; bY), (12) where bP = cW \u0002 bf(x1) . . . bf(xN ) \u0003 , cW = [ bA⊤ 1 , . . . ,bA⊤ M ]⊤, P ≜ {P ∈ RMK ×N | P = W [f(x1) . . . f(xN )] , W ∈ W, f ∈ F}, W ≜ {W = [A⊤ 1 , . . . ,A⊤ M ]⊤ ∈ RMK ×K | 1⊤Am = 1⊤, Am ≥ 0, ∀m}, and F⊂ {f(x) ∈ RK| f(x) ∈ ∆K, ∀x ∈ RD} is the neural network function class. Our main goal is to characterize the estimation errors of { bAm}M m=1 and bf. This can be done via first bounding the estimation errors ∥p♮(m) n − bp(m) n ∥2 2, where p♮(m) n is given by: p♮(m) n = A♮ mf♮(xn), ∀m, n, in which A♮ m and f♮ denote the mth ground-truth confusion matrix and the GTP, respectively, and bp(m) n denote the (m, n)th block in bP. 14Published as a conference paper at ICLR 2023 B.1 E STIMATING ∥p♮(m) n − bp(m) n ∥2 2 We first show the following proposition: Proposition 1 Under the assumptions in Theorem 1, the following result hold with probability at least 1 − δ: 1 NM NX n=1 MX m=1 ∥p♮(m) n − bp(m) n ∥2 2 ≤ 4 √ 2KβRS(P) + 10 log (β) p 2 log (4/δ)√ S + 2β √ Kν, (13) where RS(P) = 16√ S r MK log \u0010 4S √ K \u0011 + (2∥X∥FRF) 1 2 and is related to the empirical Rademacher complexity of the set P under the observed samples S. The proof is provided in Sec. C. Proposition 1 provides an upper-bound for ∥P♮ − bP∥2 F. To achieve the main goal, i.e., characterizing the estimation errors of { bAm}M m=1 and bf, we require an upper- bound for PM m=1 ∥p♮(m) n − bp(m) n ∥2 2 as well. The following result gives a tighter upper bound forPM m=1 ∥p♮(m) n − bp(m) n ∥2 2: Proposition 2 Under the assumptions in Theorem 1, for any α >0 and δ ∈ (0, 1), the following result holds with probability at least 1 − 1 Nα : 1 M MX m=1 ∥p♮(m) n − bp(m) n ∥2 2 ≤ Nα   4 √ 2KβRS(P) + 10 log (β) p 2 log (4/δ)√ S + 2β √ Kν + 4δ ! , ∀n, (14) where RS(P) = 16√ S r MK log \u0010 4S √ K \u0011 + (2∥X∥FRF) 1 2 . The proof is provided in Sec. I. Note that the bound in (14) looks divergent at the first glance, as the second term in the R.H.S. could go to infinity. However, given a large enough S, one can choose appropriate α and δ such that the bound in (14) does not explode. B.2 E STIMATION ERRORS FOR bAm AND bf In this section, we characterize the estimation error of the confusion matrices and the GTP. We start with using simplified notations to represent the results in Propositions 1-2, i.e., ∥P♮ − bP∥2 F ≤ ζ2, (15) MX m=1 ∥p♮(m) n − bp(m) n ∥2 2 ≤ φ2, ∀n. (16) with probability greater than 1 − δ and 1 − 1 Nα , respectively. Assumptions 4-5 imply that there exists index sets Λ = { ˜m1, . . . ,˜mK} and Ψ = {˜n1, . . . ,˜nK} such that: W♮(Λ, :) = IK + NW F♮(:, Ψ) = IK + NF , where ∥NW ∥F ≤ √ Kξ1, and ∥NF ∥F ≤ √ Kξ2. Note that the conditions do not require any confusion matrices to be near identity; i.e., we do not need any annotators to be all-class specialists. 15Published as a conference paper at ICLR 2023 Nonetheless, for notation simplicity, we let Λ = {1, . . . , K} and Ψ = {1, . . . , K}, which is without loss of generality for our proof in the sequel. Under this simplification, the following holds: W♮ =   A♮ 1 A♮ 2 ... A♮ M  , F♮ = \u0002 f♮(x1) . . . f♮(xN ) \u0003 = \u0002 F♮ 1 F♮ 2 \u0003 , where A♮ 1 = IK + NW , F♮ 1 = IK + NF , and F♮ 2 ∈ RK×(N−K), ∥NW ∥F ≤ √ Kξ1, and ∥NF ∥F ≤ √ Kξ2. Let bAm and bf denote the estimates of A♮ m and f♮, respectively, using the learning criterion (6) and construct cW =   bA1 bA2 ... bAM  , bF = \u0002 bf(x1) . . . bf(xN ) \u0003 = \u0002 bF1 bF2 \u0003 , where bAm ∈ RK×K, bF1 ∈ RK×K, and bF2 ∈ RK×(N−K). Then, we have: ∥P♮ − bP∥2 F = ∥W♮F♮ − cW bF∥2 F = ∥IK + NW + NF + NW NF − bA1 bF1∥2 F + ∥F♮ 2 + NW F♮ 2 − bA1 bF2∥2 F + MX m=2 ∥A♮ m + A♮ mNF − bAm bF1∥2 F + MX m=2 ∥A♮ mF♮ 2 − bAm bF2∥2 F. (17) Let us define the error matrices as below: E(1) A ≜ bA1 − (IK + NW )Π| {z } A♮ 1Π , E(m) A ≜ bAm − A♮ mΠ, ∀m >1, (18) E(1) F ≜ bF1 − Π⊤(IK + NF )| {z } Π⊤F♮ 1 , E(2) F ≜ bF2 − Π⊤F♮ 2 , (19) where Π ∈ [0, 1]K×K is a column permutation matrix and is the same across all the error blocks. We hope to characterize the norm of these error matrices. Upper bound for ∥E(1) A ∥F and ∥E(1) F ∥F. We start by considering the first term on the R.H.S. of (17), i.e., ∥IK + NW + NF + NW NF − bA1 bF1∥2 F. From (16), we have the following with probability greater than 1 − K Nα : φ2 ≥ ∥IK + NW + NF + NW NF − bA1 bF1∥2 F = KX k=1 |1 + fN(k, k) − bA1(k, :) bF1(:, k)|2 + KX j=1 X k̸=j |fN(j, k) − bA1(j, :) bF1(:, k)|2, (20) where fN = NW + NF + NW NF . The absolute value of the largest entry in fN can be bounded by ξ1 + ξ2 + √ Kξ1ξ2. Let us denote κ = φ + ξ1 + ξ2 + √ Kξ1ξ2 for conciseness. Then, from (20), we get the following conditions: 1 − κ ≤ |bA1(k, :) bF1(:, k)| ≤1 + κ, ∀k ∈ [K] (21a) | bA1(k, :) bF1(:, j)| ≤κ, ∀k ̸= j. (21b) From the conditions (21a) and (21b), we have the following result: 16Published as a conference paper at ICLR 2023 Lemma 1 Assume that the conditions in (21a) and (21b) hold and that κ ≤ 1 K+1 . Then, we have the following relations satisfied: arg maxℓ bA1(k, ℓ) ̸= arg maxℓ bA1(j, ℓ), ∀k ̸= j (22a) arg maxq bF1(q, k) ̸= arg maxq bF1(q, j), ∀k ̸= j (22b) arg maxℓ bA1(k, ℓ) = arg maxq bF1(q, k), ∀k. (22c) The proof of the lemma is given in Sec. F. From the lower bound condition in (21a), we have the following result: KX k=1 (1 − κ) ≤ KX k=1 bA1(k, 1) bF1(1, k) + ··· + bA1(k, K) bF1(K, k) ≤ KX k=1 maxℓ∈[K] bA1(k, ℓ)( bF1(1, k) + ··· + bF1(K, k)) = KX k=1 maxℓ∈[K] bA1(k, ℓ) (23) where the last equality is obtained due to the probability simplex constraints on the columns of bF1. From the lower bound condition in (21a), we have the below result as well: KX k=1 (1 − κ) ≤ KX k=1 \u0010 bA1(k, 1) bF1(1, k) + ··· + bA1(k, K) bF1(K, k) \u0011 ≤ KX k=1 maxℓ bF1(k, ℓ)( bA1(1, k) + ··· + bA1(K, k)) = KX k=1 maxℓ bF1(k, ℓ). (24) where the last equality is obtained due to the probability simplex constraints on all the columns of bA1. Next, we characterize the term ∥E(1) A ∥2 F. To achieve this, by employing the result (22a) in Lemma 1, we fix the column permutation Π as follows: Π(j, k) = ( 1, j = arg maxℓ bA1(k, ℓ) 0, otherwise. (25) 17Published as a conference paper at ICLR 2023 We get the following set of relations: ∥ bA1 − IKΠ∥2 F = KX k=1 ∥ bA1(k, :) − e⊤ k Π∥2 2 = KX k=1 ∥ bA1(k, :)∥2 2 + K − 2 KX k=1 bA1(k, :)Π⊤ek = ∥ bA1∥2 F + K − 2 KX k=1 bA1(k, :)Π⊤ek ≤ 2K − 2 KX k=1 bA1(k, :)Π⊤ek (a) = 2K − 2 KX k=1 max ℓ bA1(k, ℓ) (b) ≤ 2K − 2 KX k=1 (1 − κ) = 2Kκ, where the last inequality (b) is by (23) and the relation (a) is obtained by choosing Π as defined in (25). Hence, we have the following with probability greater than 1 − K Nα : ∥E(1) A ∥F ≤ ∥bA1 − IKΠ∥F + ∥NW ∥F ≤ √ 2Kκ + √ Kξ1. (26) We proceed to consider the error term ∥E(1) F ∥2 F. To achieve this, consider the following: ∥ bF1 − Π⊤IK∥2 F = KX k=1 ∥ bF1(k, :) − e⊤ k Π⊤∥2 2 = KX k=1 ∥ bF1(k, :)∥2 2 + K − 2 KX k=1 bF1(k, :)Π⊤ek = ∥ bF1∥2 F + K − 2 KX k=1 bF1(k, :)Π⊤ek ≤ 2K − 2 KX k=1 bF1(k, :)Π⊤ek (a) = 2K − 2 KX k=1 max ℓ bF1(k, ℓ) (b) ≤ 2K − 2 KX k=1 (1 − κ) = 2Kκ, where the last inequality (b) is by (24) and the relation (a) is by combining the definition of Π in (25) and (22c) in Lemma 1. Hence, we get the following with probability greater than 1 − K Nα : ∥E(1) F ∥F ≤ ∥bF1 − IKΠ∥F + ∥NF ∥F ≤ √ 2Kκ + √ Kξ2. (27) 18Published as a conference paper at ICLR 2023 Upper bound for∥E(2) F ∥F. Next, we consider the second term in(17), i.e., ∥F♮ 2 +NW F♮ 2 − bA1 bF2∥F. From (15), we have the following with probability greater than 1 − δ: ζ ≥ ∥F♮ 2 + NW F♮ 2 − bA1 bF2∥F = ∥F♮ 2 + NW F♮ 2 − (IKΠ + NW Π + E(1) A )(E(2) F + Π⊤F♮ 2 )∥F = ∥(IKΠ + NW Π + E(1) A )E(2) F + E(1) A Π⊤F♮ 2 ∥F = ∥ bA1E(2) F + E(1) A Π⊤F♮ 2 ∥F (a) ≥ \f\f\f∥ bA1E(2) F ∥F − ∥E(1) A Π⊤F♮ 2 ∥F \f\f\f (b) ≥ \f\f\fσmin( bA1)∥E(2) F ∥F − ∥E(1) A ∥F∥F♮ 2 ∥F \f\f\f (c) ≥ \f\f\fσmin( bA1)∥E(2) F ∥F − ( √ 2Kκ + √ Kξ1)∥F♮ 2 ∥F \f\f\f (28) where the inequality (a) is by using the triangle inequality, (b) is obtained by applying the following two relations for any two matrices A ∈ RK×K, B ∈ RK×L, L≥ K : ∥AB∥F ≥ σmin(A)∥B∥F (29) ∥AB∥F ≤ ∥A∥F∥B∥F. (30) The inequality (c) is obtained by applying (26). Hence, (28) combined with the fact that∥F2∥F ≤ ∥F♮∥F gives the following relation with probability greater than 1 − δ − K Nα : ∥E(2) F ∥F ≤ ζ + ( √ 2Kκ + √ Kξ1)∥F♮∥F σmin( bA1) . (31) Next, we use the following lemma to characterize σmin( bA1): Lemma 2 Suppose that the matrix X ∈ RK×K takes the form X = IK + E1 + E2. Assume that ∥E1∥F ≤ υ1 and ∥E2∥F ≤ υ2, for a certain υ1, υ2 > 0. Then, we have σmin(X) ≥ |1 − υ1 − υ2|. The proof is relegated to Sec. G. Applying Lemma 2, we get the following with probability greater than 1 − δ − K Nα : ∥E(2) F ∥F ≤ ζ + ( √ 2Kκ + √ Kξ1)∥F♮∥F |1 − √ 2Kκ − 2 √ Kξ1| ≤ c1(ζ + √ Kκ∥F♮∥F) |1 − √ Kκ| (32) for certain constant c1 > 0. where we used the fact that ξ1 ≤ √κ since κ = φ + ξ1 + ξ2 + √ Kξ1ξ2 and and κ ≤ 1. Upper bound for ∥E(m) A ∥F, m >1. We consider the third term on the R.H.S. of (17). From (16), for each m, we have the following with probability greater than 1 − K Nα : √ Kφ ≥ ∥A♮ m + A♮ mNF − bAm bF1∥F = ∥A♮ m + A♮ mNF − (A♮ mΠ + E(m) A )(Π⊤IK + Π⊤NF + E(1) F )∥F = ∥E(m) A (Π⊤IK + Π⊤NF + E(1) F ) + A♮ mΠE(1) F ∥F = ∥E(m) A bF1 + A♮ mΠE(1) F ∥F (a) ≥ \f\f\f∥E(m) A bF1∥F − ∥A♮ mΠE(1) F ∥F \f\f\f (b) ≥ \f\f\fσmin( bF1)∥E(m) A ∥F − σmax(A♮ m)∥E(1) F ∥F \f\f\f (c) ≥ \f\f\fσmin( bF1)∥E(m) A ∥F − ( √ 2Kκ + √ Kξ2)σmax(A♮ m) \f\f\f, where the relation (a) by the triangle inequality, (b) is by applying (29) and (30), and (c) is via (27). 19Published as a conference paper at ICLR 2023 Hence, ∀m >1, we get the following with probability greater than 1 − 2K Nα : ∥E(m) A ∥F ≤ √ Kφ + ( √ 2Kκ + √ Kξ2)σmax(A♮ m) |1 − √ 2Kκ − 2 √ Kξ2| ≤ c2 √ Kκσmax(A♮ m) |1 − √ Kκ| (33) for certain constant c2 > 0, where we used the fact that φ ≤ √κ and ξ2 ≤ √κ since κ = φ + ξ1 + ξ2 + √ Kξ1ξ2 and κ ≤ 1. In the above, we have also applied σmin( bF1) ≥ |1 − √ 2Kκ − 2 √ Kξ2| following Lemma 2 and (27). Putting Together.From (33), we have the following with probability greater than 1 − 2K Nα : ∥E(m) A ∥2 F = ∥ bAm − A♮ mΠ∥2 F ≤ c2 2K2κ |1 − √ Kκ|2 , ∀m. (34) where we have used the fact that σmax(A♮ m) ≤ ∥A♮ m∥F ≤ √ K. Similarly, by combining(27) and (32), we have the following with probability greater than1−δ− 2K Nα ∥ bF − Π⊤F♮∥2 F = ∥E(1) F ∥2 F + ∥E(2) F ∥2 F ≤ ( √ 2Kκ + √ Kξ2)2 + c2 1(ζ + √ Kκ∥F♮∥F)2 |1 − √ Kκ|2 ≤ c3(ζ + √ NKκ )2 |1 − √ Kκ|2 (35) for certain constant c3 > 0, where we have used the fact that ∥F♮∥F ≤ √ N. We hope to characterize the generalization performance of the predicted function bf from (35). Towards this, we have the following result: Lemma 3 Under Assumptions 1 and 2, the following holds with probability greater than 1 − δ: Ex∼D h ∥ bf(x) − Π⊤f♮(x)∥2 2 i ≤ 1 N ∥ bF − Π⊤F♮∥2 F + 64N−5/8 (2∥X∥FRF) 1 4 + 16 r 2 log(4/δ) N . (36) The proof is given in Sec. H. Hence, combining Lemma 3 with (35), we have the following with probability greater than 1 − 2δ − 2K Nα Ex∼D h ∥ bf(x) − Π⊤f♮(x)∥2 2 i ≤ c3(ζ + √ NKκ )2 N|1 − √ Kκ|2 + 64N−5/8 (2∥X∥FRF) 1 4 + 16 r 2 log(4/δ) N , (37) where κ ≤ φ + 2 √ K(ξ1 + ξ2), ζ2 ≤ 4 √ 2NMKβ RS(P) + 10NM log (β) p 2 log (4/δ)√ S + 2NMβ √ Kν φ2 ≤ 4 √ 2NαMKβ RS(P) + 10NαM log (β) p 2 log (4/δ)√ S + 2NαMβ √ Kν + 4NαMδ, RS(P) = 16√ S r MK log \u0010 4S √ K \u0011 + (2∥X∥FRF) 1 2 . Also note that the final bounds in (34) and (37) are satisfied only if κ ≤ 1 K+1 as given by Lemma 1. This gives the final conditions on ξ1, ξ2, ν, and S. By letting δ = 1 S , we obtain the final results in Theorem 1. 20Published as a conference paper at ICLR 2023 C P ROOF OF PROPOSITION 1 Let us consider the following notation: P(ω) = p(m) n , (38) where ω = (m, n) ∈ [M] × [N]; i.e., P(ω) “reads out” the (m, n)th block in (10). We also define bY(ω) = by(m) n . With this notation, we have DS(P; bY) = 1 S SX s=1 CE(P(ωs), bY(ωs)), where CE(x, y) = −PK k=1 I[y = k] log(x(k)), S = |S|, and ωs = (ms, ns) ∈ S. Under Assump- tion 1, we can define: DΠ(P, bY) ≜ ES∼Π[CE(P(ω), bY(ω))] = MX m=1 NX n=1 π(m) n CE(p(m) n , by(m) n ), where Π denotes the uniform distribution and π(m) n = 1 NM denotes the probability of observing the annotation for the index pair (m, n). Eq. (12) implies DS( bP; bY) ≤ DS( eP; bY), (39) where eP is defined using the following construction: eP = W♮ \u0002 ef(x1) . . . ef(xN ) \u0003 , and ef is a learning function constructed under Assumption 3. To be specific, ef satisfies ∥ ef(x) − f♮(x)∥2 ≤ ν, ∀x ∼ D. Hence, by taking expectation w.r.t. bY we have E[DΠ( bP; bY) − DΠ(P♮; bY)] = E[DΠ( bP; bY)] − DS( bP; bY) + DS(P♮; bY) − E[DΠ(P♮; bY)] + DS( bP; bY) − DS( eP; bY) + DS( eP; bY) − DS(P♮; bY) ≤ E[DΠ( bP; bY)] − DS( bP; bY) + DS(P♮; bY) − E[DΠ(P♮; bY)] + DS( eP; bY) − DS(P♮; bY) ≤ sup P∈P \f\f\fDS(P; bY) − E[DΠ(P; bY)] \f\f\f + \f\f\fDS(P♮; bY) − E[DΠ(P♮; bY)] \f\f\f + \f\f\fDS( eP; bY) − DS(P♮; bY) \f\f\f, (40) where the first inequality is obtained from (39). Let us consider the L.H.S. of (40): E h DΠ( bP; bY) − DΠ(P♮; bY) i = NX n=1 MX m=1   π(m) n KX k=1 −P♮((m − 1)K + k, n) log bP((m − 1)K + k, n) +P♮((m − 1)K + k, n) logP♮((m − 1)K + k, n) \u0001 = NX n=1 MX m=1 π(m) n KX k=1 P♮((m − 1)K + k, n) log P♮((m − 1)K + k, n) bP((m − 1)K + k, n) = DKL \u0010 P♮, bP \u0011 , (41) 21Published as a conference paper at ICLR 2023 where expectation is taken w.r.t. bY (while taking expectation, we used the uniform probability π(m) n = 1 NM , ∀, n, mfor observing each annotation by(m) n and used the ground-truth probability P♮((m − 1)K + k, n) for each event I[by(m) n = k]) and DKL(P♮, bP) is the average Kullback–Leibler (KL) divergence between the entries of the matrices P♮ and bP ∈ RMK ×N , which is given by DKL \u0010 P♮, bP \u0011 = 1 NM NX n=1 MX m=1 DKL \u0010 p♮(m) n , bp(m) n \u0011 . Upper-bounding the first term on the R.H.S of (40). Next, we characterize the first term on the R.H.S. of (40). To achieve this, we invoke the following theorem (Theorem 26.5 in (Shalev-Shwartz & Ben-David, 2014)) : Theorem 4 (Shalev-Shwartz & Ben-David, 2014, Theorem 26.5) Assume that for all y and for all x, we have |CE(x; y)| ≤zmax. Then for any P ∈ P, the following holds with probability greater than 1 − δ: \f\f\fDS(P; bY) − E[DΠ(P; bY)] \f\f\f ≤ 2RS(ℓ ◦ P ◦ S) + 4zmax r 2 log(4/δ) S , (42) where ℓ ◦ P ◦ Sdenotes the set ℓ ◦ P ◦ S≜ n\u0010 CE(P(ω1); bY(ω1)), . . . ,CE(P(ωS); bY(ωS)) \u0011 | P ∈ P o and RS(X) denotes the empirical Rademacher complexity of the set X. To apply Theorem 4, we will characterizeRS(ℓ ◦P ◦S) which is defined as follows (Shalev-Shwartz & Ben-David, 2014): RS(ℓ ◦ P ◦ S) ≜ 1 S E \" sup P∈P SX s=1 σsCE(P(ωs); bY(ωs)) # , (43) where expectation is w.r.t. the independent Rademacher random variablesσs ∈ {−1, 1}. Note that P(ω) is a vector-valued [see Eq. (38)]. Hence, we invoke the following contraction result to upper bound RS(ℓ ◦ P ◦ S): Lemma 4 (Maurer, 2016) Let P be a class of mappings {P : X →RK}, where X be any set and (ω1, . . . , ωS) ∈ XS. Also assume that ℓ : RK → R has the Lipschitz constant L. Then E \" sup P∈P SX s=1 σsℓ(P(ωs)) # ≤ √ 2LE   sup P∈P X s,k σskPk(ws)   where Pk(w) denotes the kth component of P(ω), σsk is an independent (doubly indexed) Rademacher random variable and the expectations are taken w.r.t. the Rademacher random variables. Let us define a vector z ≜ (P1(ω1), P2(ω1), . . . ,PK−1(ωs), PK(ωs)) ∈ RSK and the set Z ≜ {z = (P1(ω1), . . . ,PK(ωs)) | P ∈ P}. With these definitions, we apply Lemma 4 in (43) and obtain RS(ℓ ◦ P ◦ S) ≤ √ 2β S E   sup P∈P X s,k σs,kPk(ws)   = √ 2β S E \" sup z∈Z X i σiz(i) # = √ 2βK 1 SK E \" sup z∈Z X i σiz(i) # = √ 2βKRS(Z), (44) where β is an upper bound of the Lipschitz constant of the cross entropy loss function CE(x; y) = −PK k=1 I[y = k] logx(k) when x ∈ ∆K with x(k) > (1/β). ∀k. Next, we will characterize RS(Z) using the covering number of the set Z. 22Published as a conference paper at ICLR 2023 Definition 2 (Vershynin, 2012) The ϵ-net covering of the set Z (denoted as Z) is a finite subset of Z (i.e.,Z ⊆ Z) such that for any z ∈ Z, there exists an z ∈ Z satisfying ∥z − z∥2 2 ≤ ϵ. The smallest cardinality of the ϵ-nets of Z is known as the covering number of Z, which is denoted as N(ϵ, Z). Let us consider a pair of vectors z, z ∈ Zas below: z = (P1(ω1), . . . ,PK(ωs)) ∈ RSK , P(ωs) = AmsF(:, ns), z = \u0000 P1(ω1), . . . ,PK(ωs) \u0001 ∈ RSK , P(ωs) = AmsF(:, ns), where ωs = (ms, ns) and all Am, Am, F, and F satisfy the nonnegativity constraints and have unit ℓ1-norm on the columns. Then, we have ∥z − z∥2 = SX s=1 ∥AmsF(:, ns) − AmsF(:, ns)∥2 2 ≤ SX s=1 ∥AmsF(:, ns) − AmsF(:, ns)∥2 = SX s=1 ∥AmsF(:, ns) − AmsF(:, ns) + AmsF(:, ns) − AmsF(:, ns)∥2 ≤ SX s=1 ∥Ams − Ams∥F∥F(:, ns)∥2 + ∥Ams∥F∥F(:, ns) − F(:, ns)∥2 ≤ SX s=1 ∥Ams − Ams∥F + √ K SX s=1 ∥F(:, ns) − F(:, ns)∥2 where the first inequality is by ∥x∥2 2 ≤ ∥x∥2 if the entries of x are smaller than 1. The second inequality is by triangle inequality. The last inequality uses the fact the Frobenius norm of Am’s are bounded by √ K and the ℓ2 norm of any column of F is bounded by 1. Hence, to obtain an ε-net covering for the set Z (i.e., ∥z − z∥ ≤ε), we only need to show that there exists a ε2 2 √ K -net covering for F ◦ Sand a ε2 2S -net covering for each Am’s since SX s=1 ε2 2S + √ K ε2 2 √ K = ε2. Here F ◦ Sdenotes F ◦ S= {[f(xn1 ), . . . ,f(xnS )] ∈ RK×S | f ∈ F}, where (ms, ns) = ωs ∈ S. Note that the full rank matrix Am ∈ RK×K can be represented as a K2-dimensional vector whose Euclidean norm is bounded by √ K. Hence, the cardinality of the ε2 2S -net covering for Am ∈ RK×K is at most \u0010 4SK √ K ε2 \u0011K2 (Shalev-Shwartz & Ben-David, 2014). Next, we consider the covering number corresponding to the function class F ◦ S. Using Lemma 14 of (Lin & Zhang, 2019), we get the cardinality of the ε2 2 √ K -net covering for F ◦ Sas below: N \u0012 ε2 2 √ K , F ◦ S \u0013 ≤ exp   s 2 √ K∥X∥FRF ε2  , where X = [xn1 , . . . ,xnS ] ∈ Rd×S and the parameter RF is from Assumption 2. Using the covering number results, the cardinality of the ε-net covering of set Z is bounded by the following: N(ε, Z) ≤   4SK √ K ε2 !MK 2 × exp   s 2 √ K∥X∥FRF ε2  . (45) Now that we have characterized N(ϵ, Z), we invoke the below lemma to obtain the Rademacher complexity RS(Z): 23Published as a conference paper at ICLR 2023 Lemma 5 (Bartlett et al., 2017, Lemma A.5) The empirical Rademacher complexity of the set Z with respect to the observed set S having size SK is upper bounded as follows: RS(Z) ≤ inf a>0   4a√ SK + 12 SK Z √ SK a q log N(µ, Z)dµ ! . (46) We apply (45) in Lemma 5 and obtain RS(Z) (a) ≤ inf a>0 \u0012 4a√ SK + 12 SK √ SK q log N(a, Z) \u0013 (b) ≤ inf a>0   4a√ SK + 12√ SK vuutMK 2 log   4SK √ K a2 ! +   2 √ K∥X∥FRF a2 !1 2   (c) ≤ 4√ S + 12√ SK r MK 2 log \u0010 4S √ K \u0011 + (2∥X∥FRF) 1 2 ≤ 16√ S r MK log \u0010 4S √ K \u0011 + (2∥X∥FRF) 1 2 . (47) In the above, the first inequality (a) is obtained by using the relation Z √ SK a q log N(µ, Z)dµ ≤ √ SK q log N(a, Z), which holds because q log N(µ, Z) decreases monotonically as µ increases. The inequality (b) is obtained by applying (45), and (c) is obtained by fixing a = √ K which is smaller than √ SK. Combining the upperbound of RS(Z) given by (47) with the upper bound of RS(ℓ ◦ P ◦ S) as given by (44) and with the result in (42), we get that with probability greater than 1 − δ, \f\f\fDS(P; bY) − E[DΠ(P; bY)] \f\f\f ≤ 2 √ 2βKRS(Z) + 4zmax r 2 log(4/δ) S , (48) where RS(Z) is upper bounded by (47) and zmax is the upperbound of the value of the function CE(x, y) which can be characterized as below: zmax = max x(k)> 1 β y∈[K] CE(x, y) ≤ max x(k)> 1 β y∈[K] − KX k=1 I[y = k] logx(k) ≤ max u> 1 β − log u = log(β). (49) Upper-bounding the second term on the R.H.S of (40). Next, we proceed to upper bound the second term on the R.H.S of (40). Let us consider the Hoeffding’s inequality Lemma 6 Let Z1, . . . , ZS be independent bounded random variables with Zs ∈ [zmin, zmax] for all s where −∞ < zmin ≤ zmax < ∞. Then for all t ≥ 0, Pr   1 S SX s=1 (Zs − E[Zs]) ≥ t ! ≤ exp \u0012 − 2St2 (zmax − zmin)2 \u0013 . To use Lemma 6, let us define the random variable Z(m) n as follows: Z(m) n ≜ CE(p♮(m) n , by(m) n ), where p♮(m) n = A♮ mf♮(xn). The maximum and minimum values of Z(m) n are zmax = log(β) (see (49)) and zmin = 0, respectively. Then, invoking Lemma 6, one can obtain Pr \u0010 DS(P♮; bY)] − E[DΠ(P♮; bY)] ≥ t \u0011 ≤ exp \u0012 − 2St2 (log(β))2 \u0013 . (50) 24Published as a conference paper at ICLR 2023 Hence, by substituting t = log (β) q log(1 δ ) 2S , where δ ∈ (0, 1) in (50), we get that with probability greater than 1 − δ DS(P♮; bY)] − E[DΠ(P♮; bY)] ≤ log (β) s log \u00001 δ \u0001 2S . (51) Upper-bounding the third term on the R.H.S of (40). \f\f\fDS( eP; bY) − DS(P♮; bY) \f\f\f = \f\f\f\f\f\f − 1 S X (m,n)∈S KX k=1 I[by(m) n = k] log eP((m − 1)K + k, n) + 1 S X (m,n)∈S KX k=1 I[by(m) n = k] logP♮((m − 1)K + k, n) \f\f\f\f\f\f ≤ 1 S X (m,n)∈S KX k=1 I[by(m) n = k] \f\f\flog[A♮ mf♮(xn)]k − log[A♮ m ef(xn)]k \f\f\f ≤ 1 S X (m,n)∈S KX k=1 I[by(m) n = k]β \f\f\f[A♮ mf♮(xn)]k − [A♮ m ef(xn)]k \f\f\f ≤ 1 S X (m,n)∈S KX k=1 I[by(m) n = k]β∥A♮ m(k, :)∥2∥f♮(xn) − ef(xn)∥2 ≤ β √ Kν, (52) where the first inequality uses the triangle inequality, the second inequality uses the Lipschitz continuity of log function, the third inequality is via Cauchy Schwartz inequality, and the last inequality employs Assumption 3. Putting Together. Hence, by combining the result in (48) with (41), (40), (51), and (52), we get that with probability greater than 1 − 2δ, DKL \u0010 P♮, bP \u0011 ≤ 2 √ 2βKRS(Z) + 4 log(β) r 2 log(4/δ) S + log (β) s log \u00001 δ \u0001 2S + β √ Kν. (53) Using Pinsker’s inequality (S, 1960; Fedotov et al., 2003), we get DKL \u0010 P♮, bP \u0011 = 1 NM NX n=1 MX m=1 DKL \u0010 p♮(m) n , bp(m) n \u0011 ≥ 1 2NM NX n=1 MX m=1 ∥p♮(m) n − bp(m) n ∥2 1 ≥ 1 2NM NX n=1 MX m=1 ∥p♮(m) n − bp(m) n ∥2 2 where the last inequality uses the fact that ∥x∥1 ≥ ∥x∥2. The above relation combined with (53), implies that with probability greater than 1 − 2δ: 1 NM NX n=1 MX m=1 ∥p♮(m) n − bp(m) n ∥2 2 ≤ 4 √ 2KβRS(Z) + 10 log (β) p 2 log (4/δ)√ S + 2β √ Kν, (54) where RS(Z) is upper-bounded in (47) D P ROOF OF THEOREM 2 The proof of Theorem 2 utilizes some key results from the proof of Theorem 1. We start by employing the result from Proposition 1. Let us fix δ = 1 S and α = 1 8 in the result of Proposition 1. Then, it 25Published as a conference paper at ICLR 2023 implies that when f♮ ∈ F, i.e., ν = 0, S ≥ C1t, N ≤ C2t3, for certain constants C1, C2 > 0 and at the limit of t → ∞, we get the following: ∥P♮ − bP∥2 F = 0, with probability 1, i.e., bP = W♮F♮. (55) On the other hand, the matrix bP can be constructed using the estimates of the CCEM criterion (6) via bP = cW bF. (56) Hence, in order to identify W♮ and F♮ from the NMF model in (55), we invoke the following result: Lemma 7 (Huang et al., 2014) Consider the matrix factorization model Z = XY , where X ∈ RI×K, Y ∈ RK×J and rank(X) = rank(Y ) = K. If X, Y ≥ 0 and both X and Y satisfy SSC, then any cX and bY that satisfy Z = cX bY must have the following form: cX = XΠΣ, bY = Σ−1Π⊤Y , where Π is a column permutation matrix and Σ is a diagonal nonnegative and scaling matrix. Next, we show that Lemma 7 can be applied given the conditions in Theorem 2. The matrix F♮ Z = [f♮(xn1 ), . . . ,f♮(xnT )] includes a subset of columns of F♮, i.e., cone(F♮ Z) ⊆ cone(F♮). Hence, we have C ⊆cone(F♮ Z) =⇒ C ⊆cone(F♮), (57) where C = {x ∈ RK| √ K − 1∥x∥2 ≤ 1⊤x} is the second-order cone. Also for any orthogonal matrix Q ∈ RK×K except for the permutation matrices, the below holds: cone(F♮ Z) ̸⊂ cone{Q} =⇒ cone(F♮) ̸⊂ cone{Q}. (58) Eqs (57) and (58) imply that F♮ satisfies SSC, given the assumption thatF♮ Z satisfies SSC. In addition, the column scaling does not affect the conic hull, i.e, cone(F♮) = cone(Σ−1F♮), cone(W♮⊤) = cone(ΣW♮⊤). Since both W♮ and F♮ satisfy SSC, rank(F♮) = rank(W♮) = K hold (see (Huang et al., 2016)). Hence, the conditions in Lemma 7 holds for (55). Comparing (55) and (56) and invoking Lemma 7, we get cW = W♮Π, (59a) bF = Π⊤F♮, (59b) where the scaling Σ is automatically removed due to the sum-to-one constraints on the columns of bAm’s and bF. The first result (59a) implies that bAm = A♮ mΠ, ∀m. Applying the second result (59b) in Lemma 3 along with the assumption that N → ∞, we get Ex∼D h ∥ bf(x) − Π⊤f♮(x)∥2 2 i = 0. (60) Fact 1 Let X be a nonnegative random variable with E[X] = 0. Then, X is zero almost surely, i.e., Pr(X = 0) = 1. Employing Fact 1 in (60), we get that ∥ bf(x) − Π⊤f♮(x)∥2 2 = 0, ∀x ∼ D, =⇒ bf(x) = Π⊤f♮(x), ∀x ∼ D, due to the nonnegativity of f♮ and bf. 26Published as a conference paper at ICLR 2023 E P ROOF OF THEOREM 3 To prove Theorem 3, let us first consider the CCEM criterion in(6). Let cW and bF be any optimal solution of (6) and bP = cW bF. (61) From Proposition 1, by fixing δ = 1 S and α = 1 8 and using the conditions in the statement of Theorem 3, i.e., f♮ ∈ Fimplying ν = 0, S ≥ C1t, N ≤ C2t3, for certain constants C1, C2 > 0, we get the following at the limit of t → ∞: ∥P♮ − bP∥2 F = 0, with probability 1, i.e., bP = W♮F♮. (62) Proof of Theorem 3(a): We consider the following result which is distilled and summarized from the proof of Theorem 1 in (Fu et al., 2015): Lemma 8 Suppose a matrix Y ∈ RK×J satisfies Y ≥ 0, 1⊤Y = 1⊤ , rank(Y ) = K, and SSC. Then, for any bY = QY satisfying bY ≥ 0, 1⊤ bY = 1⊤, the following holds: |det(Q)| ≤1, The equality holds only if Q is a permutation matrix. Let us start by considering the following criterion: maximize W,F det(FF ⊤) (63a) s.t. bP = WF (63b) F ≥ 0, 1⊤F = 1⊤ (63c) where bP is the optimal solution from the CCEM criterion (6) and hence, as shown before, bP satisfies (62). Let W∗ and F∗ be optimal solutions of (63), then the following holds: det(FF ∗⊤) ≥ det(F♮F♮⊤). (64) From (62), we observe that W∗ and F∗ satisfies W∗ = W♮Q−1, F∗ = QF♮, for a certain invertible matrix Q. Let us assume that Q is not a permutation matrix, Then, we have det(F∗F∗⊤) = det(Q⊤F♮F♮⊤Q) = |det(Q)|2 det(F♮F♮⊤) < det(F♮F♮⊤) where the last inequality is from Lemma 8 using the SSC condition on F♮. Note that the result is a contradiction from (64). Hence, Q must be a permutation matrix. This implies that the optimal solution W∗ and F∗ of Problem 63 satisfies the following: W∗ = W♮Π, (65a) F∗ = Π⊤F♮, (65b) Following the last part of Theorem 2, by using Lemma 3 and Fact 1, we have A∗ m = A♮ mΠ, ∀m. f∗(x) = Π⊤f♮(x), ∀x ∼ D. 27Published as a conference paper at ICLR 2023 Note that since log is a monotonically increasing function, by using log det(FF ⊤) in (63) in place of det(FF )⊤ does not change the optimal solution, yet it keeps the objective function in differentiable domain (this is because, det(X) becomes zero if X is singular). Hence, we can conclude that the following criterion maximize W,F log det(FF ⊤) (66a) s.t. bP = WF (66b) F ≥ 0, (66c) also results in the same optimal solutions W∗ and F∗ satisfying (65). Proof of Theorem 3(b): We consider the following result which is a modified version of Lemma A.1 from (Huang et al., 2015): Lemma 9 Suppose a matrix X ∈ RI×K satisfies X ≥ 0, 1⊤X = ρ1⊤, ρ >0, rank(X) = K, and SSC. Then, for any cX = XQ satisfying cX ≥ 0, 1⊤ cX = ρ1⊤, the following holds: |det(Q)| ≤1, The equality holds only if Q is a permutation matrix. Note that, in Lemma 9, we are given that 1⊤X = ρ1⊤. Then, by multiplying Q on both sides, we obtain 1⊤XQ = ρ1⊤Q. (67) Since 1⊤XQ = ρ1⊤ also holds by the assumption in Lemma 9, combining with (67), we get ρ1⊤Q = ρ1⊤ =⇒ 1⊤Q = 1⊤. (68) The result in (68) is used in order to conclude Lemma 9 from Lemma A.1 of (Huang et al., 2015). We consider the following criterion: maximize W,F det(W⊤W) (69a) s.t. bP = WF (69b) W ≥ 0, 1⊤W = M1⊤, (69c) where we used the constraint 1⊤W = M1⊤ since 1⊤Am = 1⊤ for all m. Let W∗ and F∗ be optimal solutions of (69), then we have: det(W∗⊤W∗) ≥ det(W♮⊤W♮). (70) Applying (62), W∗ and F∗ satisfies W∗ = W♮Q, F∗ = Q−1F♮, for a certain invertible matrix Q. Let us assume that Q is not a permutation matrix, Then, we have det(W∗⊤W∗) = det(Q⊤W♮⊤W♮Q) = |det(Q)|2 det(W♮⊤W♮) < det(W♮⊤W♮) where the last inequality is from Lemma 9 using the SSC condition on W♮ which is a contradiction from (70). Hence, Q must be a permutation matrix. This implies that the optimal solution W∗ and F∗ of Problem 69 satisfies the following: W∗ = W♮Π, (71a) F∗ = Π⊤F♮, (71b) Similar to the last part of Theorem 2, by using Lemma 3 and Fact 1, we have A∗ m = A♮ mΠ, ∀m. f∗(x) = Π⊤f♮(x), ∀x ∼ D. In this case as well, the optimal solutions do not change if we employ log det(W⊤W) in (69) in place of det(W⊤W). 28Published as a conference paper at ICLR 2023 Remark 2 Geometrically, Theorem 3 seeks maximum volume solutions w.r.t. the conic hulls ofF and W⊤, respectively. One can also note that in both cases of Theorem 3, the corresponding optimal solutions do not change if we minimize log det(W⊤W) in (63) and minimize log det(FF ⊤) in (69). However, relying on the SSC of F♮ (W♮) and minimizing the volume of conic hull of W⊤ (F) may be inefficient since P♮ = W♮F♮ does not hold exactly in practice. F P ROOF OF LEMMA 1 Let us define some notations: ℓ∗ k = arg maxℓ∈[K] bA1(k, ℓ) q∗ k = arg maxq∈[K] bF1(q, k). Consider the following conditions given in Lemma 1 1 − κ ≤ |bA1(k, :) bF1(:, k)| ≤1 + κ, ∀k ∈ [K] (72a) | bA1(k, :) bF1(:, j)| ≤κ, ∀k ̸= j. (72b) F.1 P ROVING ℓ∗ k ̸= q∗ j , ∀k ̸= j We begin by noticing the below relation from (72a): (1 − κ) ≤ bA1(k, 1) bF1(1, k) + ··· + bA1(k, K) bF1(K, k) ≤ maxℓ∈[K] bA1(k, ℓ)( bF1(1, k) + ··· + bF1(K, k)) = maxℓ∈[K] bA1(k, ℓ) = bA1(k, ℓ∗ k), ∀k, (73) where the first equality employs the probability simplex constraints on the columns of bF. We further proceed to prove by using contradiction. First, assume the below for certain k ̸= j, ℓ∗ k = arg maxℓ bA1(k, ℓ) = arg maxq bF1(q, j) = q∗ j . (74) Under the assumption (74), consider (72b). For (72b) to hold, we need to satisfy the below for a certain k ̸= j: bA1(k, ℓ∗ k) bF1(q∗ j , j) < κ =⇒ bF1(q∗ j , j) < κ 1 − κ where the last relation is obtained from (73). This also implies that 1 = KX q=1 bF1(q, k) < Kκ 1 − κ =⇒ κ > 1 K + 1. However, Lemma 1 assumes that κ ≤ 1 K+1 , Hence, the assumption (74) does not hold and we get arg maxℓ bA1(k, ℓ) ̸= arg maxq bF1(q, j), k̸= j. (75) F.2 P ROVING ℓ∗ k = q∗ k, ∀k From the result in (75), we consider (72b) for a certain k ̸= j. bA1(k, ℓ∗ k) bF1(q, j) ≤ κ =⇒ bF1(q, j) ≤ κ 1 − κ, q̸= q∗ j . (76) 29Published as a conference paper at ICLR 2023 where the last relation is obtained from (73). Since P q bF1(q, j) = 1, we get bF1(q∗ j , j) ≥ 1 − Kκ 1 − κ , ∀j. (77) From the result in (75) and the condition in (72b), we also have the following for a certain k ̸= j bA1(k, ℓ) bF1(q∗ j , j) ≤ κ =⇒ bA1(k, ℓ) ≤ κ(1 − κ) 1 − Kκ , ℓ̸= ℓ∗ k, (78) where the last relation by (77). Next, we proceed to prove by contradiction. First, assume the below for certain k ℓ∗ k ̸= q∗ k. (79) Hence, for each k, we have | bA1(k, :) bF1(:, k)| = bA1(k, 1) bF1(1, k) + ··· + bA1(k, K) bF1(K, k) ≤ X q̸=q∗ k bF1(q, k) + bA1(k, ℓ), ℓ̸= ℓ∗ k ≤ (K − 1)κ 1 − κ + κ(1 − κ) 1 − Kκ (80) where we have used the fact that all the entries of bA1 and bF1 are smaller than 1, in the first inequality and have applied (76) and (78) in the last inequality. The lower-bound condition in (72a) gives that for each k, | bA1(k, :) bF1(:, k)| ≥1 − κ. (81) Then, comparing both (80) and (81), we hope to have (K − 1)κ 1 − κ ≥ 1 − κ ⇒ κ ≥ K + 1 − p (K + 1)2 − 4 2 However, K+1− √ (K+1)2−4 2 ≥ 1 K+1 , which leads to a contradiction to our assumption thatκ ≤ 1 K+1 . Hence, the assumption (79) does not hold and we get arg maxℓ bA1(k, ℓ) = arg maxq bF1(q, k), ∀k. (82) Combining both (82) and (82), we get arg maxℓ bA1(k, ℓ) ̸= arg maxℓ bA1(j, ℓ), ∀k ̸= j arg maxq bF1(q, k) ̸= arg maxq bF1(q, j), ∀k ̸= j. G P ROOF OF LEMMA 2 We have σmin(X) = σmin(IK + E1 + E2) = min ∥x∥2=1 ∥(IK + E1 + E2)x∥2 = min ∥x∥2=1 ∥IKx + (E1 + E2)x∥2 (a) ≥ min ∥x∥2=1 |∥IKx∥2 − ∥(E1 + E2)x∥2| ≥ \f\f\f\f min ∥x∥2=1 ∥IKx∥2 − max ∥x∥2=1 ∥(E1 + E2)x∥2 \f\f\f\f = \f\f\f\f1 − max ∥x∥2=1 ∥(E1 + E2)x∥2 \f\f\f\f = |1 − ∥E1 + E2∥2| (b) ≥ |1 − ∥E1∥F − ∥E2∥F| (c) ≥ |1 − υ1 − υ2| (83) 30Published as a conference paper at ICLR 2023 where the inequality (a) employs the triangle inequality, (b) is obtained via the matrix norm equiva- lence relation ∥E1 + E2∥2 ≤ ∥E1 + E2∥F ≤ ∥E1∥F + ∥E2∥F, (c) is obtained by the assumption that ∥E1∥F ≤ υ1 and ∥E2∥F ≤ υ2. H P ROOF OF LEMMA 3 First, let us define the following w.r.t the loss functiong(f, y) = ∥f − y∥2 2, ∀f ∈ F, y ∈ RK and the input data X = (x1, . . . ,xN ) where each xn is sampled i.i.d. from the distribution D under Assumption 1: LX( bf) ≜ 1 N NX n=1 g( bf(xn), f♮(xn)) = 1 N NX n=1 ∥ bf(xn) − Π⊤f♮(xn)∥2 2 = 1 N ∥ bF − Π⊤F♮∥2 F, (84a) LD( bf) ≜ Ex∼D h g( bf(x), f♮(x)) i = Ex∼D h ∥ bf(x) − Π⊤f♮(x)∥2 2 i . (84b) We invoke Theorem 26.5 in (Shalev-Shwartz & Ben-David, 2014) and get that with probability greater than 1 − δ: LD( bf) ≤ LX( bf) + 2RN (g ◦ F) + 4¯c r 2 log(4/δ) N =⇒ Ex∼D h ∥ bf(x) − Π⊤f♮(x)∥2 2 i ≤ 1 N ∥ bF − Π⊤F♮∥2 F + 4RN (F) + 16 r 2 log(4/δ) N (85) where the last inequality utilizes the definitions in (84), the contraction lemma (Lemma 26.9 from (Shalev-Shwartz & Ben-David, 2014)) and also applied ¯c = 4 since |g(f, y)| ≤4 in our case. The term RN (F) denotes the empirical Rademacher complexity of the neural network function class F which is upperbounded via the sensitive complexity parameter RF as follows (Lin & Zhang, 2019): RN (F) ≤ 16N−5/8 (2∥X∥FRF) 1 4 . I P ROOF OF PROPOSITION 2 Our goal is to bound 1 M PM m=1 ∥p♮(m) n − bp(m) n ∥2 2 for each n. To achieve this, let us define the random variable U := 1 NM PN n=1 PM m=1 ∥p♮(m) n − bp(m) n ∥2 2. From Proposition 1, the following result hold: Pr(U ≤ ϑ(δ)) ≥ 1 − δ, where randomness is due to all xn’s, by(m) n ’s, and S and ϑ(δ) is given by the R.H.S. of (54). Then we have E[U] = Z umax 0 h(u)udu = Z ϑ(δ) 0 h(u)udu + Z umax ϑ(δ) h(u)udu ≤ ϑ(δ) Z ϑ(δ) 0 h(u)du + umax Z umax ϑ(δ) h(u)du ≤ ϑ(δ) + umaxδ ≤ ϑ(δ) + 4δ (86) where umax denotes the maximum value of the random variable U and is given by umax ≤ 4 and h(u) denotes the probability density function of the random variable U. Also, we have E[U] = 1 N NX n=1 E [Un] (87) where Un ≜ 1 M PM m=1 ∥p♮(m) n − bp(m) n ∥2 2. To proceed, we have the following lemma: 31Published as a conference paper at ICLR 2023 Lemma 10 Let Un ≜ 1 M PM m=1 ∥p♮(m) n − bp(m) n ∥2 2. Then E[Un] = E[Un′], ∀, n, n′, where expectation is taken w.r.t. allxn’s, by(m) n ’s, and S. The proof is provided in Sec. J. Combining Lemma 10 with (86) and (87), we get E[Un] ≤ ϑ(δ) + 4δ. Applying Markov inequality, we get the following for anyτ >0 Pr(Un ≤ τE[Un]) ≥ 1 − 1 τ =⇒ Pr(Un ≤ τϑ(δ) + 4τδ ) ≥ 1 − 1 τ Letting τ = Nα, where 0 < α <1, we get that with probability greater than 1 − 1 Nα 1 M MX m=1 ∥p♮(m) n − bp(m) n ∥2 2 ≤ Nαϑ(δ) + 4Nαδ. J P ROOF OF LEMMA 10 Let us define the following: u(xn, bθv1,...,vn,...,vn′,...,vN ) ≜ 1 M MX m=1 ∥p♮(m) n − bp(m) n ∥2 2 = 1 M MX m=1 ∥A♮ mf♮(xn) − bAm bf(xn)∥2 2 where vn = {xn, {by(m) n }m∈Sn, Sn} and bθ denotes estimates { bAm} and bf which are obtained using the data {v1, . . . ,vn, . . . ,vn′, . . . ,vN } = {x1, . . . ,xN , bY, S}. One can observe that for any n′ ̸= n, bθv1,...,vn,...,vn′,...,vN = bθv1,...,vn′,...,vn,...,vN (88) since the order of vn does not affect the value of the estimates. Then we have Exn,xn′, xj, j̸=n,n′ [u(xn, bθv1,...,vn,...,vn′,...,vN )] = Exn,xn′, xj, j̸=n,n′ [u(xn, bθv1,...,vn′,...,vn,...,vN )] = Exn,xn′, xj, j̸=n,n′ [u(xn′, bθv1,...,vn′,...,vn,...,vN )] (89) where the first equality is by (88) and the last equality is obtained since xn and xn′ are identically distributed. From (89), we can obtain that E x1,...,xN [u(xn, bθv1,...,vn,...,vn′,...,vN )] = E x1,...,xN [u(xn′, bθv1,...,vn′,...,vn,...,vN )] =⇒ EbY \u0014 E x1,...,xN [u(xn, bθv1,...,vn,...,vn′,...,vN )] \u0015 = EbY \u0014 E x1,...,xN [u(xn′, bθv1,...,vn′,...,vn,...,vN )] \u0015 =⇒ ES \u0014 EbY \u0014 E x1,...,xN [u(xn, bθv1,...,vn,...,vn′,...,vN )] \u0015\u0015 = ES \u0014 EbY \u0014 E x1,...,xN [u(xn′, bθv1,...,vn′,...,vn,...,vN )] \u0015\u0015 =⇒ E v1,...,vN [u(xn, bθv1,...,vn,...,vn′,...,vN )] = E v1,...,vN [u(xn′, bθv1,...,vn′,...,vn,...,vN )]. 32Published as a conference paper at ICLR 2023 Figure 1: (left) W♮ = [A♮⊤ 1 , . . . ,A♮⊤ M ]⊤ satisfying NCSA with K = 3, meaning that there exists specialists for each class k ∈ [K] and (right) all-class expert annotator m∗ among M annotators, meaning A♮ m∗ ≈ IK. Figure 2: The dots denote the rows of W♮ and the circle denote the second-order cone C. K M ORE DETAILS ON THE COMPLEXITY MEASURE RF The work in (Lin & Zhang, 2019) introduces a complexity measure for measuring the expressiveness of different family of neural network classes. For example, consider a general convolution neural network (CNN) architecture with L layers such that f(x) = σL(γL(HL)σL−1(γL−1(HL−1) . . .σ1(γ1(H1)x) . . .)), ∀f ∈ F, where σi(·) = [σi(·), . . . , σi(·)] ∈ Rdi represents an element-wise activation function at ith layer satisfying σi(0) = 0 and σi being ρ-Lipschitz continuous for all i. Let GF denote the set of layer indices with fully connected layers and GC denote the layer indices with convolutional layers. If i ∈ GF, we haveγi(Hi) = Hi ∈ Rdi×di−1 . Suppose i ∈ GC, then we haveγi(Hi)zi−1 = Hi⊛zi−1, where the matrix Hi ∈ Rci×ri contains ci convolutional filters each of which having dimension ri, zi−1 denotes the output of the (i − 1)th layer, and ⊛ denotes the convolution operation. Here d0 = d is the dimension of the of the input data items xn’s. Then, its sensitive complexity RF is defined as follows: RF = 2ρLL2  X i∈GF d2 i d2 i−1 + X i∈GC c2 i r2 i p di/ci ! . Note that such as a general CNN architecture covers many popular neural networks, e.g., fully connected neural networks, CNNs such as Lenet-5 (Lecun et al., 1998), VGG-16 (Liu & Deng, 2015), and so on. L N EAR -CLASS SPECIALIST ASSUMPTION AND SSC NCSA is a relaxed condition compared to having all-class expert annotators or having diagonally dominant annotators–see Fig. 1. It can be understood that NCSA assumption does not require any single annotator to be specialists with respect to all classes. Nonetheless, having an all-class expert annotator m∗ satisfies NCSA, but not vice versa. 33Published as a conference paper at ICLR 2023 Algorithm 1: GeoCrowdNet(F) input : Data {xn, by(m) n , m∈ Sn}N n=1 1 Initialize Am’s to identity matrices IK; 2 Initialize the parameters θ of the neural network function class F; 3 ψ = [vec(A1)⊤, . . . ,vec(AM)⊤, vec(θ)⊤]⊤; 4 while stopping criterion is not reached do 5 while stopping criterion is not reached do 6 Draw a random batch B; 7 Compute ∇ψLF (ψ, B); 8 ψ ← AdamOptimizer(ψ, ∇ψLF (ψ, B)); 9 for m = 1to M do 10 Am ← softmax(Am) ▷ softmax operation on the columns of Am 11 end 12 end 13 end output :Estimates bAm, ∀m, bθ The SSC (Definition 1) is a relaxed condition compared to the NCSA (Assumption 4). To illustrate this, we consider the geometry of the matrix W♮ satisfying the NCSA and the SSC as shown in Fig. 2. M A LGORITHM DESCRIPTION In this section, we detail the implementation of the regularized criteria in (8) and (9). The neural network predictor function f ∈ Fis parameterized using θ and can be denoted as fθ : Rd → ∆. Let ψ = [vec(A1)⊤, . . . ,vec(AM )⊤, vec(θ)⊤]⊤. Let us also define the following for certain batch B ⊂[N]: LF (ψ, B) = − 1 |B| X n∈B X m∈Sn KX k=1 I[by(m) n = k] log[Amfθ(xn)]k − λ log det eF eF⊤, LW (ψ, B) = − 1 |B| X n∈B X m∈Sn KX k=1 I[by(m) n = k] log[Amfθ(xn)]k − λ log detW⊤W, where eF = [f(xn1 ), . . . ,f(xn|B|)], n1, . . . , n|B| ∈ Band W = [A⊤ 1 , . . . ,A⊤ M ]⊤. Let ∇ψLF (ψ, B) denote the stochastic gradient of LF (ψ, B) w.r.t. ψ and ∇ψLW (ψ, B) de- note the stochastic gradient of LW (ψ, B) w.r.t. ψ. Under these notations, the proposed methods GeoCrowdNet(F) and GeoCrowdNet(W) are described in Algorithm ?? and ??, respectively. The Python implementation of the algorithms is available in GitHub1. N A DDITIONAL EXPERIMENTS AND IMPLEMENTATION DETAILS N.1 S YNTHETIC ANNOTATORS Dataset. We consider the CIFAR-10 (Krizhevsky, 2009) dataset for synthetic noisy label experiments. CIFAR-10 consists of 60, 000 labeled color images of animals, vehicles and so on, each having a size of 32 × 32 and belonging K = 10 different classes. We use 45, 000 images for training, 5, 000 images for validation, and 10, 000 images for testing. Noisy Label Generation. In order to produce noisy annotations for the images of the training data, we simulate M = 5 synthetic annotators. We randomly choose an annotatorm∗ among M annotators and its confusion matrix is generated by A♮ m∗ = IK + γrand(K, K), followed by normalization w.r.t. the ℓ1 norm of the corresponding columns. Here, IK denotes the identity matrix of size K and 1https://github.com/shahanaibrahimosu/end-to-end-crowdsourcing 34Published as a conference paper at ICLR 2023 Algorithm 2: GeoCrowdNet(W) input : Data {xn, by(m) n , m∈ Sn}N n=1 1 Initialize Am’s to identity matrices IK; 2 Initialize the parameters θ of the neural network function class F; 3 ψ = [vec(A1)⊤, . . . ,vec(AM)⊤, vec(θ)⊤]⊤; 4 while stopping criterion is not reached do 5 while stopping criterion is not reached do 6 Draw a random batch B; 7 Compute ∇ψLW (ψ, B); 8 ψ ← AdamOptimizer(ψ, ∇ψLW (ψ, B)); 9 for m = 1to M do 10 Am ← softmax(Am) ▷ softmax operation on the columns of Am 11 end 12 end output :Estimates bAm, ∀m, bθ 13 end rand(K, K) denotes the K ×K matrix with its entries randomly chosen from a uniform distribution between 0 and 1. The parameter γ controls how well the annotator m∗ correctly identifies the ground-truth labels. The confusion matrices of the remaining M − 1 annotators are generated such that they provide labels uniformly at random—i.e., the M − 1 annotators are all unreliable. This type of modeling for the confusion matrices resembles the hammer-spammer model as employed in the works (Rodrigues & Pereira, 2018; Tanno et al., 2019). Using the ground-truth label yn’s provided by the dataset and the generated confusion matrices, the noisy annotations by(m) n ’s are produced. Each annotation by(m) n is observed independently at random such that only 20% of the total annotations are available for training. Note that, the true labels are not accessible by any of the methods. Neural Network Architecture and Settings. For the CIFAR-10 dataset, we choose the ResNet-9 architecture (He et al., 2016). Adam (Kingma & Ba, 2015) is used as the optimizer with weight decay of 10−4 and a batch size of 128. The regularization parameter λ and the initial learning rate of the Adam optimizer are chosen via grid search method over the validation set from{0.01, 0.001, 0.0001} and {0.01, 0.001}, respectively. We choose the same neural network structures for all the baselines. The confusion matrices are initialized with identity matrices for the proposed method and the baselines TraceReg and CrowdLayer. Results. Table 3 presents the results under various values of γ—when γ is smaller, the chance of annotator m∗ correctly labeling the data items is better. One can see that the proposed approach, namely, GeoCrowdNet(F), outperforms the baselines in all the scenarios under test. When annotator m∗’s labeling accuracy drops drastically (i.e., whenγ becomes larger), GeoCrowdNet(F) performs the best. This implies that even if there are no class specialists, GeoCrowdNet(F) works well under the large N cases. Figs. 3, 4 and 5 show the ground-truth confusion matrices and the corresponding estimates when γ = 0.01 for the GeoCrowdNet methods. One can see all methods estimate the confusion matrices reasonably well, corroborating our identifiability claims. N.2 R EAL DATA EXPERIMENTS - MACHINE ANNOTATIONS Here, we provide additional details of the real data experiments. For experiments with machine annotations, we consider the MNIST and the Fashion-MNIST datasets. Each image in these datasets is of size 28 × 28 in the grey-scale format. We consider 57, 000 images as training data, 3000 images for validation, and 10, 000 images for testing. For machine annotation, we train a number of machine classifiers to act as annotators. In Table 1, under Case 2, we choose M = 5 annotators. Specifically, Linear SVM, logistic regression, k-NN with k = 5, CNN with two convolution layers followed by a max pooling layer, a fully connected layer and a softmax layer, and a fully connected neural network (FCNN) with 1 hidden layer and 128 hidden units are trained. The CNN is trained for 5 epochs and the FCNN is trained for 10 epochs. The classifiers are intentionally not well trained, so that they can mimic error-proning annotators. 35Published as a conference paper at ICLR 2023 Table 3: Average test accuracy of the proposed methods and the baselines on the CIFAR-10 dataset (K = 10), labeled by M = 5 synthetic annotators. Methods γ = 0.01 γ = 0.2 γ = 0.3 GeoCrowdNet(F) 84.82 ± 0.43 71.66 ± 1.08 67.97 ± 1.44 GeoCrowdNet(W) 82.76 ± 0.46 69.20 ± 0.59 63.86 ± 1.64 GeoCrowdNet (λ=0) 82.50 ± 0.38 69.14 ± 1.29 63.73 ± 2.64 TraceReg 82.54 ± 0.34 70.01 ± 1.16 65.01 ± 2.30 Crowdlayer 83.01 ± 0.23 69.91 ± 1.48 65.70 ± 2.32 MBEM 81.01 ± 0.32 69.45 ± 1.56 64.15 ± 2.21 CoNAL 81.41 ± 0.43 68.56 ± 1.78 63.21 ± 1.98 Max-MIG 82.01 ± 0.78 69.96 ± 1.23 62.01 ± 2.46 NN-MV 50.96 ± 0.83 37.90 ± 1.19 33.24 ± 1.17 NN-DSEM 52.02 ± 0.80 37.95 ± 1.41 33.64 ± 0.69 0 1 2 3 4 5 6 7 8 9 0123456789 0.95 1.1e-06 0.0029 0.0014 0.00089 0.0018 0.0033 0.0038 0.0051 0.004 0.0064 0.96 0.0083 0.00026 0.0064 0.0039 0.0053 0.0014 0.0019 0.0077 0.0091 0.003 0.95 0.0085 0.0086 0.0008 0.00037 0.0016 0.0083 0.00094 0.004 0.0092 0.005 0.97 0.003 0.0065 0.008 0.00018 0.0071 0.0094 0.007 0.0027 0.0075 0.001 0.96 0.0086 0.0028 0.0028 0.0012 0.00019 0.0064 0.002 0.0025 0.0048 0.00051 0.95 0.0014 0.0057 0.0066 0.00098 0.0039 0.0067 0.0039 0.00048 0.0051 0.0063 0.96 0.0091 0.0055 0.0086 0.0013 0.0013 0.0076 0.0038 0.0016 0.0088 0.0033 0.97 0.0069 0.0084 0.0059 0.0072 0.0033 0.0026 0.0086 0.004 0.0092 0.0064 0.95 0.0011 0.0089 0.0043 0.0055 0.0039 0.0023 0.0085 0.0055 2.8e-05 0.0058 0.96 0.0 0.2 0.4 0.6 0.8 1.0 0 1 2 3 4 5 6 7 8 9 0123456789 0.9 0.011 0.017 0.01 0.012 0.009 0.0072 0.0071 0.01 0.013 0.0075 0.91 0.0075 0.012 0.0088 0.007 0.0095 0.0076 0.013 0.013 0.012 0.0086 0.89 0.015 0.015 0.013 0.013 0.012 0.0089 0.011 0.0068 0.0063 0.013 0.89 0.011 0.039 0.011 0.01 0.0068 0.0086 0.0066 0.0087 0.014 0.013 0.91 0.01 0.012 0.011 0.0085 0.0063 0.0073 0.0075 0.014 0.04 0.014 0.87 0.0098 0.016 0.0084 0.01 0.0078 0.008 0.014 0.015 0.012 0.0085 0.91 0.0082 0.011 0.0089 0.0082 0.0075 0.0086 0.0084 0.017 0.013 0.009 0.91 0.0089 0.007 0.016 0.009 0.012 0.011 0.0071 0.011 0.0097 0.0082 0.9 0.012 0.0089 0.015 0.0072 0.0095 0.0065 0.0067 0.0079 0.0089 0.0071 0.92 0.0 0.2 0.4 0.6 0.8 1.0 0 1 2 3 4 5 6 7 8 9 0123456789 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.0 0.2 0.4 0.6 0.8 1.0 0 1 2 3 4 5 6 7 8 9 0123456789 0.089 0.089 0.12 0.11 0.092 0.099 0.1 0.11 0.1 0.095 0.1 0.096 0.11 0.082 0.099 0.096 0.12 0.1 0.11 0.088 0.11 0.084 0.13 0.11 0.094 0.091 0.1 0.11 0.087 0.092 0.11 0.087 0.11 0.11 0.083 0.11 0.1 0.093 0.11 0.087 0.1 0.1 0.1 0.1 0.11 0.1 0.1 0.11 0.075 0.097 0.1 0.075 0.099 0.083 0.12 0.12 0.11 0.1 0.087 0.1 0.085 0.095 0.082 0.11 0.11 0.11 0.12 0.098 0.094 0.093 0.1 0.098 0.092 0.11 0.094 0.11 0.1 0.1 0.098 0.098 0.084 0.09 0.092 0.11 0.1 0.11 0.1 0.096 0.11 0.1 0.097 0.1 0.077 0.093 0.095 0.1 0.11 0.097 0.11 0.11 0.0 0.2 0.4 0.6 0.8 1.0 Figure 3: The illustration of the ground-truth confusion matrices and the estimated confusion matrices by the proposed approach GeoCrowdNet(F) for CIFAR-10 dataset, withM = 5 machine annotators and γ = 0.01. The ground-truth A♮ m∗ (top left) and the estimate bAm∗ (top right). The ground-truth A♮ m (bottom left) and the estimate bAm (bottom right), m ̸= m∗. The average mean squared error (MSE) of the confusion matrices is 0.088. For example, for the MNIST dataset, the individual label prediction accuracy of these annotators on unseen data items of size N ranges from 15.88% to 82.23% (averaged over 5 random trials) and 2 annotators have less than 50% accuracy on average. Also note that our algorithm does not know which annotator is more accurate. For Case 2, we train more annotators by changing some parameters of the above mentioned machine classifiers. Specifically, we use linear SVM, polynomial kernel-based SVM, and Gaussian SVM as different variants of SVM. For k-NN, we choose k from {3, 5, 7, 10}. FCNN, CNN, and logistic regression are trained with different number of epochs{10, 15, 20, 25} and with random initializations for each case. Under case 1, if an annotator is chosen to be a specialist, it is trained with more number 36Published as a conference paper at ICLR 2023 0 1 2 3 4 5 6 7 8 9 0123456789 0.95 1.1e-06 0.0029 0.0014 0.00089 0.0018 0.0033 0.0038 0.0051 0.004 0.0064 0.96 0.0083 0.00026 0.0064 0.0039 0.0053 0.0014 0.0019 0.0077 0.0091 0.003 0.95 0.0085 0.0086 0.0008 0.00037 0.0016 0.0083 0.00094 0.004 0.0092 0.005 0.97 0.003 0.0065 0.008 0.00018 0.0071 0.0094 0.007 0.0027 0.0075 0.001 0.96 0.0086 0.0028 0.0028 0.0012 0.00019 0.0064 0.002 0.0025 0.0048 0.00051 0.95 0.0014 0.0057 0.0066 0.00098 0.0039 0.0067 0.0039 0.00048 0.0051 0.0063 0.96 0.0091 0.0055 0.0086 0.0013 0.0013 0.0076 0.0038 0.0016 0.0088 0.0033 0.97 0.0069 0.0084 0.0059 0.0072 0.0033 0.0026 0.0086 0.004 0.0092 0.0064 0.95 0.0011 0.0089 0.0043 0.0055 0.0039 0.0023 0.0085 0.0055 2.8e-05 0.0058 0.96 0.0 0.2 0.4 0.6 0.8 1.0 0 1 2 3 4 5 6 7 8 9 0123456789 0.96 0.0052 0.0047 0.0047 0.0046 0.0049 0.004 0.0042 0.0042 0.0057 0.004 0.96 0.0043 0.0063 0.0049 0.0042 0.005 0.0043 0.0051 0.0046 0.0042 0.0046 0.96 0.004 0.0039 0.0038 0.0041 0.0043 0.0043 0.0056 0.0035 0.0036 0.0038 0.97 0.0036 0.0043 0.0036 0.0036 0.0036 0.0042 0.0037 0.0045 0.0041 0.0037 0.96 0.0038 0.0039 0.004 0.0045 0.0037 0.0039 0.0041 0.0039 0.0049 0.0044 0.96 0.0038 0.0044 0.0041 0.005 0.0043 0.0045 0.004 0.0046 0.0039 0.0039 0.96 0.0043 0.0053 0.0043 0.0043 0.0043 0.0039 0.0039 0.0045 0.0041 0.00510.96 0.0052 0.0039 0.0052 0.0045 0.0053 0.0054 0.0041 0.0056 0.0051 0.00450.95 0.0056 0.0039 0.0044 0.004 0.0044 0.0038 0.0039 0.0045 0.0043 0.0039 0.96 0.0 0.2 0.4 0.6 0.8 1.0 0 1 2 3 4 5 6 7 8 9 0123456789 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.0 0.2 0.4 0.6 0.8 1.0 0 1 2 3 4 5 6 7 8 9 0123456789 0.14 0.1 0.074 0.1 0.093 0.11 0.099 0.096 0.087 0.098 0.083 0.099 0.13 0.11 0.094 0.098 0.1 0.11 0.099 0.083 0.096 0.07 0.26 0.095 0.065 0.066 0.087 0.11 0.077 0.076 0.074 0.097 0.075 0.21 0.092 0.053 0.086 0.074 0.13 0.11 0.076 0.098 0.058 0.099 0.17 0.11 0.12 0.086 0.086 0.095 0.11 0.06 0.072 0.04 0.11 0.24 0.098 0.093 0.076 0.1 0.1 0.11 0.086 0.09 0.11 0.1 0.13 0.11 0.07 0.1 0.1 0.097 0.11 0.073 0.083 0.081 0.13 0.14 0.11 0.084 0.11 0.081 0.1 0.12 0.078 0.11 0.1 0.096 0.13 0.077 0.095 0.098 0.07 0.084 0.092 0.1 0.11 0.097 0.1 0.15 0.0 0.2 0.4 0.6 0.8 1.0 Figure 4: The illustration of the ground-truth confusion matrices and the estimated confusion matrices by the proposed approach GeoCrowdNet(W) for CIFAR-10 dataset, withM = 5 machine annotators and γ = 0.01. The ground-truth A♮ m∗ (top left) and the estimate bAm∗ (top right). The ground-truth A♮ m (bottom left) and the estimate bAm (bottom right), m ̸= m∗. The average mean squared error (MSE) of the confusion matrices is 0.102. Table 4: Average test accuracy (± std) of the proposed methods and the baselines on Fashion-MNIST under various (N, M)’s on Case 1; labels are produced by machine annotators; p = 0.1. Methods (1000, 25) (750, 30) GeoCrowdNet(F) 82.81 ± 2.05 82.40 ± 1.51 GeoCrowdNet(W) 83.33 ± 3.39 83.65 ± 4.11 GeoCrowdNet(λ = 0) 79.43 ± 4.07 79.07 ± 2.91 TraceReg 80.79 ± 3.87 81.12 ± 4.03 CrowdLayer 73.54 ± 6.58 75.56 ± 8.19 MBEM 26.19 ± 3.45 26.67 ± 3.45 CoNAL 63.13 ± 8.53 66.02 ± 5.65 Max-MIG 77.05 ± 3.06 72.59 ± 8.38 NN-MV 68.48 ± 3.15 71.74 ± 2.21 NN-DSEM 68.84 ± 2.46 73.08 ± 4.09 of samples (10, 000 samples) with more number of epochs such that its label prediction accuracy is higher than 90%. Under this strategy, in Table 1 for the case M = 15, the individual label prediction accuracy of the annotators on unseen data items of sizeN ranges from 3.15% to 95.27% (2 annotators with more than 90% accuracy) . For the case with M = 20, the individual label prediction accuracy of the annotators on unseen data items of size N ranges from 3.24% to 95.73% (3 annotators with more than 90% accuracy). Additional experiment results using the MNIST and Fashion-MNIST datasets can be found in Tables 4-6. As expected, GeoCrowdNet(W) and GeoCrowdNet(F) offer the best performance in Case 1 and Case 2, respectively. . 37Published as a conference paper at ICLR 2023 0 1 2 3 4 5 6 7 8 9 0123456789 0.95 1.1e-06 0.0029 0.0014 0.00089 0.0018 0.0033 0.0038 0.0051 0.004 0.0064 0.96 0.0083 0.00026 0.0064 0.0039 0.0053 0.0014 0.0019 0.0077 0.0091 0.003 0.95 0.0085 0.0086 0.0008 0.00037 0.0016 0.0083 0.00094 0.004 0.0092 0.005 0.97 0.003 0.0065 0.008 0.00018 0.0071 0.0094 0.007 0.0027 0.0075 0.001 0.96 0.0086 0.0028 0.0028 0.0012 0.00019 0.0064 0.002 0.0025 0.0048 0.00051 0.95 0.0014 0.0057 0.0066 0.00098 0.0039 0.0067 0.0039 0.00048 0.0051 0.0063 0.96 0.0091 0.0055 0.0086 0.0013 0.0013 0.0076 0.0038 0.0016 0.0088 0.0033 0.97 0.0069 0.0084 0.0059 0.0072 0.0033 0.0026 0.0086 0.004 0.0092 0.0064 0.95 0.0011 0.0089 0.0043 0.0055 0.0039 0.0023 0.0085 0.0055 2.8e-05 0.0058 0.96 0.0 0.2 0.4 0.6 0.8 1.0 0 1 2 3 4 5 6 7 8 9 0123456789 0.92 0.0091 0.0099 0.0083 0.0093 0.0084 0.0067 0.0068 0.008 0.011 0.0069 0.92 0.0074 0.011 0.0087 0.007 0.0089 0.0075 0.01 0.0088 0.0078 0.0084 0.92 0.0081 0.0091 0.0072 0.0082 0.0084 0.0082 0.011 0.0053 0.0053 0.0068 0.94 0.006 0.011 0.006 0.0064 0.0056 0.0066 0.0061 0.0085 0.0077 0.0068 0.93 0.0069 0.0072 0.0075 0.0083 0.0061 0.0064 0.0068 0.007 0.015 0.01 0.92 0.007 0.0087 0.0073 0.0092 0.0073 0.0077 0.0077 0.0099 0.007 0.0065 0.93 0.0075 0.0099 0.0078 0.0073 0.007 0.0066 0.0065 0.0091 0.0077 0.0088 0.93 0.0088 0.0064 0.01 0.0082 0.01 0.0099 0.007 0.011 0.0091 0.0082 0.92 0.011 0.0069 0.0094 0.0067 0.008 0.0064 0.0066 0.0078 0.0078 0.0066 0.93 0.0 0.2 0.4 0.6 0.8 1.0 0 1 2 3 4 5 6 7 8 9 0123456789 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.0 0.2 0.4 0.6 0.8 1.0 0 1 2 3 4 5 6 7 8 9 0123456789 0.085 0.089 0.12 0.11 0.094 0.1 0.1 0.11 0.098 0.092 0.082 0.089 0.096 0.12 0.097 0.1 0.1 0.088 0.12 0.11 0.11 0.079 0.14 0.11 0.083 0.084 0.099 0.12 0.084 0.087 0.11 0.084 0.11 0.12 0.081 0.11 0.1 0.095 0.11 0.084 0.098 0.11 0.1 0.11 0.11 0.1 0.1 0.1 0.072 0.096 0.11 0.072 0.09 0.073 0.13 0.14 0.1 0.1 0.083 0.11 0.083 0.097 0.075 0.11 0.11 0.11 0.13 0.095 0.092 0.095 0.1 0.1 0.11 0.077 0.098 0.092 0.12 0.11 0.11 0.085 0.11 0.1 0.088 0.11 0.097 0.11 0.098 0.1 0.097 0.097 0.099 0.1 0.075 0.09 0.093 0.1 0.11 0.097 0.11 0.12 0.0 0.2 0.4 0.6 0.8 1.0 Figure 5: The illustration of the ground-truth confusion matrices and the estimated confusion matrices by GeoCrowdNet(λ = 0) for CIFAR-10 dataset, with M = 5 machine annotators and γ = 0.01. The ground-truth A♮ m∗ (top left) and the estimate bAm∗ (top right). The ground-truth A♮ m (bottom left) and the estimate bAm (bottom right), m ̸= m∗. The average mean squared error (MSE) of the confusion matrices is 0.097. Table 5: Average test accuracy (± std) of the proposed methods and the baselines on MNIST under various (N, M)’s on Case 2; labels are produced by machine annotators; p = 0.1. Methods (5000, 25) (10000, 25) GeoCrowdNet(F) 87.26 ± 0.70 87.66 ± 1.23 GeoCrowdNet(W) 84.64 ± 4.04 86.53 ± 3.24 GeoCrowdNet(λ = 0) 80.08 ± 3.14 85.51± 2.35 TraceReg 78.06 ± 6.72 86.85 ± 5.45 CrowdLayer 57.39 ± 6.93 56.66 ± 6.34 MBEM 35.56 ± 7.92 21.82 ± 6.54 CoNAL 46.35 ± 6.75 46.86 ± 5.43 Max-MIG 83.34 ± 1.54 86.84 ± 2.13 NN-MV 83.46 ± 0.67 83.32 ± 1.34 NN-DSEM 84.37 ± 0.70 83.78 ± 1.11 N.3 R EAL DATA EXPERIMENTS - HUMAN ANNOTATORS For experiments with annotations collected from AMT, we use the LabelMe dataset (Rodrigues et al., 2017; Russell et al., 2007) and the Music dataset (Rodrigues et al., 2014). The LabelMe dataset (Rodrigues et al., 2017; Russell et al., 2007) is an image classification dataset consisting of 2688 images from K = 8 different classes, namely, highway, inside city, tall building, street, forest, coast, mountain, and open country. From the available images, 1000 images are annotated by M = 59 AMT workers. In total, about 2547 image annotations are obtained by the AMT workers whose labeling accuracy ranges from 0% to 100% with a mean accuracy of 69.2%. In 38Published as a conference paper at ICLR 2023 Table 6: Average test accuracy (± std) of the proposed methods and the baselines on Fashion-MNIST under various (N, M)’s on Case 2; labels are produced by machine annotators; p = 0.1. Methods (5000, 25) (10000, 25) GeoCrowdNet(F) 87.47 ± 0.86 88.39 ± 1.92 GeoCrowdNet(W) 85.80 ± 1.26 87.09 ± 2.71 GeoCrowdNet(λ = 0) 85.58 ± 2.23 85.37 ± 2.59 TraceReg 85.20 ± 2.36 84.02 ± 2.74 CrowdLayer 70.07 ± 7.19 74.92 ± 6.19 MBEM 42.15 ± 3.65 44.34 ± 3.48 CoNAL 65.13 ± 4.94 62.65 ± 7.02 Max-MIG 85. 33 ± 2.09 85.94 ± 1.93 NN-MV 79.85 ± 2.88 83.53 ± 1.53 NN-DSEM 81.34 ± 1.65 84.67 ± 1.15 order to enrich the training dataset, standard augmentation techniques such as rescaling, cropping, horizontal flips, etc., are employed and accordingly, the training dataset consists of 10, 000 images annotated by 59 workers; see more details in (Rodrigues & Pereira, 2018; Chu et al., 2021). The validation set consists of 500 images and the remaining 1188 images are used for testing. The Music dataset (Rodrigues et al., 2014) consists of audio samples of 10 different genres of music such as classical, country, disco, hiphop, jazz, rock, blues, reggae, pop, and metal. The dataset has about 1000 samples of songs each having a duration of 30 seconds. About 700 samples are annotated by 44 AMT workers (overall, about 2946 annotations are observed) and the remaining 300 are allocated for testing. Out of the annotated 700 samples, we consider 595 samples for training, and the remaining 105 samples are used for validation. For the LabelMe dataset, we employ the settings used in (Rodrigues & Pereira, 2018). The pretrained VGG-16 embeddings for the images are given as inputs to a fully connected (FC) neural network with one hidden layer having 128 hidden units and ReLU activation functions. A dropout layer with rate 50% is also used, followed by a softmax layer. For the Music dataset, we choose the same FC layer and the softmax layer as employed in the LabelMe settings, but with batch normalization layers before each of these layers. 39",
      "references": [
        "A closer look at memorization in deep networks.",
        "Spectrally-normalized margin bounds for neural networks.",
        "Amazon’s Mechanical Turk: A new source of inexpensive, yet high-quality, data?",
        "Max-MIG: An information theoretic approach for joint learning from crowds.",
        "Structured probabilistic end-to-end learning from crowds.",
        "Gaussian process classification and active learning with multiple annotators.",
        "Learning from noisy singly-labeled data.",
        "Deep learning from crowds.",
        "Crowdsourcing via annotator co-occurrence imputation and provable symmetric nonnegative matrix factorization.",
        "Crowdsourcing via pairwise co-occurrences: Identifiability and algorithms.",
        "Iterative learning for reliable crowdsourcing systems.",
        "Efficient crowdsourcing for multi-class labeling.",
        "Learning from crowds.",
        "Labelme: A database and web-based tool for image annotation.",
        "The information stability of gaussian random variables and processes (in russian).",
        "Microsoft COCO: Common objects in context.",
        "The MNIST database of handwritten digit images for machine learning research.",
        "Refinements of pinsker’s inequality.",
        "Blind separation of quasi-stationary sources: Exploiting convex geometry in covariance domain.",
        "On identifiability of nonnegative matrix factorization.",
        "Nonnegative matrix factorization for signal and data analytics: Identifiability, algorithms, and applications.",
        "Maximum likelihood estimation of observer error-rates using the EM algorithm.",
        "Learning supervised topic models for classification and regression from crowds.",
        "Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.",
        "Learning from noisy labels by regularized estimation of annotator confusion.",
        "Blind multiclass ensemble classification.",
        "Beyond AMT: An analysis of crowd work platforms.",
        "Introduction to the non-asymptotic analysis of random matrices.",
        "“Crowdsourcing” ten years in: A review.",
        "Deep learning from multiple noisy annotators as a union.",
        "The multidimensional wisdom of crowds.",
        "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise.",
        "Are anchor points really indispensable in label-noise learning?",
        "Part-dependent label noise: Towards instance-dependent label noise.",
        "Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms.",
        "Understanding deep learning requires rethinking generalization.",
        "Disentangling human error from ground truth in segmentation of medical images.",
        "Spectral methods meet EM: A provably optimal algorithm for crowdsourcing.",
        "Beyond images: Label noise transition matrix estimation for tasks with lower-quality features.",
        "Provably end-to-end label-noise learning without anchor points.",
        "Learning with instance-dependent label noise: A sample sieve approach.",
        "Variational inference for crowdsourcing.",
        "Understanding Machine Learning: From Theory to Algorithms.",
        "Label noise transition matrix estimation for tasks with lower-quality features.",
        "Dawid & Skene: Maximum likelihood estimation of observer error-rates using the EM algorithm."
      ],
      "meta_data": {
        "arxiv_id": "2306.03288v1",
        "authors": [
          "Shahana Ibrahim",
          "Tri Nguyen",
          "Xiao Fu"
        ],
        "published_date": "2023-06-05T22:21:26Z",
        "github_url": "https://github.com/shahanaibrahimosu/end-to-end-crowdsourcing"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper provides the first finite-sample identifiability analysis for coupled cross-entropy minimization (CCEM) in end-to-end crowdsourcing, showing that CCEM can identifiably recover annotator confusion matrices and the ground-truth predictor (GTP) under realistic settings with incomplete labels and finite samples. It proves identifiability without requiring annotator independence and introduces two regularized CCEM variants, GeoCrowdNet(F) and GeoCrowdNet(W), that enhance identifiability in more challenging scenarios. The work also offers practical implementations and empirical validation on synthetic and real datasets, demonstrating improved label integration and classifier performance relative to existing baselines. Theoretical contributions are complemented by regularization design and extensive experiments on MNIST, Fashion-MNIST, LabelMe, and Music.",
        "methodology": "The core idea is to model each annotator m with a confusion matrix A_m that maps the ground-truth distribution f(x) to the observed label distribution. The end-to-end CCEM objective minimizes the cross-entropy between the observed annotator labels and Am f(x) across the labeled pairs, with f belonging to a neural-network function class F and A_m constrained to be row-stochastic. The paper analyzes identifiability under relaxed conditions, introducing Assumptions 1-5 (including near-class specialist and near-anchor-point relaxations) and develops Theorems 1-3 showing recovery up to permutation. To enhance identifiability, two regularizations are proposed: log det(FF^T) (GeoCrowdNet(F)) and log det(W^T W) where W stacks all A_m (GeoCrowdNet(W)). The SSC (sufficiently scattered condition) is used to generalize identifiability beyond all-class experts. Implementation details include initializing A_m as I_K, using softmax on columns to enforce stochasticity, and training with Adam; for regularized variants, the regularizers are incorporated into the CCEM loss. Theoretical analysis connects CCEM to low-rank NMF and simplex-structured matrix factorization (SSMF) in the large-data limit.",
        "experimental_setup": "Datasets and settings include both synthetic and real crowdsourcing scenarios. Real-data: MNIST and Fashion-MNIST with machine annotators (five classifiers) labeled at p=0.1 incomplete rate; two cases: Case 1 with all-class experts (NCSA holds) and Case 2 without experts; evaluation on test sets. CNN-based architectures (Lenet-5 for MNIST, ResNet-18 for Fashion-MNIST) are used with Adam optimizer and grid-searched regularization λ. Synthetic data: CIFAR-10 with M=5 synthetic annotators, using a hammer-spammer-type setup; images labeled with probabilistic confusion; 10k test samples; Gaussian noise initialized; model uses ResNet-9, same baselines. For MNIST and Fashion-MNIST, additional baselines include TraceReg, CrowdLayer, MBEM, CoNAL, Max-MIG, NN-MV, and NN-DSEM. Real crowdsourcing: LabelMe (M=59, K=8) and Music (M=44, K=10) with AMT workers; VGG-16 embeddings for images in LabelMe; training and evaluation splits as in prior work; results reported as average test accuracy with standard deviation across trials. Evaluation focuses on label-aggregation quality and downstream classifier performance under incomplete labeling and varying numbers of annotators.",
        "limitations": "Identifiability guarantees rely on assumptions such as near-anchor/or near-class-specialist conditions (NAPA/NCSA) and the SSC; some results require large numbers of labeled items and data samples (S and N) and may degrade if these assumptions fail. The identifiability is up to permutation, implying learned components may be permuted without changing predictive performance. The models assume nonnegative CCEM factors and cross-entropy losses; practical settings may deviate from these assumptions. Computationally, training CCEM with multiple annotator-specific layers can incur overhead, and performance depends on hyperparameter choices (e.g., regularization λ, learning rate). The guarantees do not extend to all possible labeling regimes, and baseline comparisons show sensitivity to annotator quality and label sparsity.",
        "future_research_directions": "Explore weaker or alternative identifiability conditions beyond SSC and NCSA/NAPA, including settings with even sparser labeling and more diverse annotator behaviors. Extend CCEM to regression or multi-task settings, multi-label problems, and non-stationary annotators. Investigate active or adaptive labeling strategies to improve identifiability with limited labeled data, and integrate with semi-supervised or self-supervised learning to exploit unlabeled data. Develop more scalable regularizers and optimization techniques to reduce computational overhead for very large-scale crowdsourcing. Apply the framework to domains beyond vision/audio (e.g., medical imaging, natural language processing) and assess robustness to concept drift and annotator dynamics.",
        "experimental_code": "# Relevant experimental implementation excerpts (CCEM with regularized variants)\n\n# File: algorithms/trainer_proposed.py\n\ndef trainer_proposed(args,alg_options,logger):\n\n\n\tlamda_list=alg_options['lamda_list']\n\tlearning_rate_list=alg_options['learning_rate_list']\n\t\n\tbest_val_acc = -100\n\t# Perform training and validation\n\tfor l in range(len(lamda_list)):\n\t\tfor j in range(len(learning_rate_list)):\n\t\t\targs.lam = lamda_list[l]\n\t\t\targs.learning_rate = learning_rate_list[j]\n\t\t\tlogger.info('Training with lambda='+str(args.lam)+' learning_rate = '+str(args.learning_rate))\n\t\t\tout=train_val(args,alg_options,logger)\n\t\t\tif out['best_val_acc'] >= best_val_acc:\n\t\t\t\tbest_model = out['final_model_f_dict']\n\t\t\t\tbest_val_acc=out['best_val_acc']\n\t\t\t\tbest_lam, best_lr = lamda_list[l], learning_rate_list[j]\n\t\n\t# Perform testing\n\tlogger.info('Testing with lambda='+str(best_lam)+' learning_rate = '+str(best_lr))\n\ttest_acc = test(args,alg_options,logger,best_model)\n\treturn test_acc\n\n\ndef train_val(args,alg_options,logger):\n\n\ttrain_loader = alg_options['train_loader']\n\tval_loader = alg_options['val_loader']\n\tdevice = alg_options['device']\n\tannotations_list = alg_options['annotations_list_maxmig']\t\t  \n\t\n\tif args.dataset=='synthetic':\n\t\t# Instantiate the model f and the model for confusion matrices \n\t\thidden_layers=1\n\t\thidden_units=10\n\t\tmodel_f = FCNN(args.R,args.K,hidden_layers,hidden_units)\n\t\tmodel_A = confusion_matrices(args.device,args.M,args.K)\n\t\t\t\n\t\t# The optimizers\n\t\toptimizer_f = optim.Adam(model_f.parameters(),lr=args.learning_rate,weight_decay=1e-5)\n\t\toptimizer_A = optim.Adam(model_A.parameters(),lr=args.learning_rate,weight_decay=1e-5)\n\t\t\t\n\telif args.dataset=='mnist':\n\t\t# Instantiate the model f and the model for confusion matrices \t\n\t\tif args.proposed_init_type=='mle_based':\n\t\t\tA_init=confusion_matrix_init_mle_based(annotations_list,args.M,args.K)\n\t\telif args.proposed_init_type=='identity':\n\t\t\tA_init=[]\n\t\telse:\n\t\t\tA_init=[] \t\t\n\t\tmodel_f = CrowdNetwork(args.R,args.M,args.K,'lenet',args.proposed_init_type,A_init)\n\t\t\n\t\t# The optimizers\t\t\n\t\toptimizer_f = optim.Adam(model_f.parameters(), lr=args.learning_rate, weight_decay=1e-4)\n\t\n\telif args.dataset=='labelme':\n\t\tif args.proposed_init_type=='mle_based':\n\t\t\tA_init=confusion_matrix_init_mle_based(annotations_list,args.M,args.K)\n\t\telif args.proposed_init_type=='identity':\n\t\t\tA_init=[]\n\t\telse:\n\t\t\tA_init=[] \t\t\t\n\t\t# Instantiate the model f and the model for confusion matrices \t\t\n\t\tmodel_f = CrowdNetwork(args.R,args.M,args.K,'fcnn_dropout',args.proposed_init_type,A_init)\n\t\t\n\t\t# The optimizers\t\t\n\t\toptimizer_f = optim.Adam(model_f.parameters(), lr=args.learning_rate, weight_decay=1e-4)\n\t\n\telif args.dataset=='music':\n\t\tif args.proposed_init_type=='mle_based':\n\t\t\tA_init=confusion_matrix_init_mle_based(annotations_list,args.M,args.K)\n\t\telif args.proposed_init_type=='identity':\n\t\t\tA_init=[]\n\t\telse:\n\t\t\tA_init=[] \t\t \n\t\tmodel_f = CrowdNetwork(args.R,args.M,args.K,'fcnn_dropout_batchnorm',args.proposed_init_type,A_init)\n\t\t\n\t\t# The optimizers\t\t\n\t\toptimizer_f = optim.Adam(model_f.parameters(), lr=args.learning_rate, weight_decay=1e-4)\n\telif args.dataset=='cifar10':\n\t\tif args.proposed_init_type=='mle_based':\n\t\t\tA_init=confusion_matrix_init_mle_based(annotations_list,args.M,args.K)\n\t\telif args.proposed_init_type=='identity':\n\t\t\tA_init=[]\n\t\telse:\n\t\t\tA_init=[] \t\t\n\t\tmodel_f = CrowdNetwork(args.R,args.M,args.K,args.classifier_NN,args.proposed_init_type,A_init)\n\t\t# Instantiate the model f and the model for confusion matrices \t\t\n\t\t# The optimizers\n\t\t#optimizer_f = optim.SGD(model_f.parameters(), lr=args.learning_rate, weight_decay=1e-4,momentum=0.9)\n\t\toptimizer_f = optim.Adam(model_f.parameters(), lr=args.learning_rate, weight_decay=1e-4)\n\t\t#scheduler_f = MultiStepLR(optimizer_f, milestones=alg_options['milestones'], gamma=0.1)\n\t\tscheduler_f = optim.lr_scheduler.OneCycleLR(optimizer_f, args.learning_rate, epochs=args.n_epoch, steps_per_epoch=len(train_loader))\n\telse:\n\t\tlogger.info('Incorrect choice for dataset')\n\tif torch.cuda.is_available:\n\t\tmodel_f = model_f.to(device)\n\t\n\t# Loss function\n\tloss_function = torch.nn.NLLLoss(ignore_index=-1, reduction='mean')\n\t\n\t\n\n\tA_true = alg_options['A_true']\n\tflag_lr_scheduler=alg_options['flag_lr_scheduler']\n\n\t\n\t#Start training\n\tval_acc_list=[]\n\ttrain_acc_list=[]\n\tA_est_error_list=[]\n\tlen_train_data=len(train_loader.dataset)\n\tlen_val_data=len(val_loader.dataset)\n\ttrain_soft_labels=np.zeros((args.N,args.K))\n\tbest_val_score = 0\n\tbest_f_model = copy.deepcopy(model_f)\n\tfor epoch in range(args.n_epoch):\n\t\tmodel_f.train()\n\t\t\n\t\ttotal_train_loss=0\n\t\tce_loss=0\n\t\treg_loss=0\n\t\tn_train_acc=0\n\t\t#for i, data_t in enumerate(train_loader):\n\t\tfor batch_x, batch_annotations, batch_annot_onehot, batch_annot_mask, batch_annot_list, batch_y in train_loader:\n\t\t\tflag=0\n\t\t\tif torch.cuda.is_available:\n\t\t\t\tbatch_x=batch_x.to(device)\n\t\t\t\tbatch_annotations=batch_annotations.to(device)\n\t\t\t\tbatch_y = batch_y.to(device)\n\t\t\t\toptimizer_f.zero_grad()\n\t\t\tf_x, Af_x,A = model_f.forward(batch_x.float())\n\t\t\tAf_x = Af_x.view(-1,args.K)\n\t\t\tbatch_annotations=batch_annotations.view(-1)\n\t\t\tcross_entropy_loss =loss_function(Af_x.log(), batch_annotations.long())\t\n\t\t\tregularizer_loss = reg_loss_function(A,f_x,args)\n\t\t\tif(np.isnan(regularizer_loss.item()) or np.isinf(regularizer_loss.item()) or regularizer_loss.item() > 100)  :\n\t\t\t\tflag=1\t\n\t\t\t\tregularizer_loss=torch.tensor(0.0)\n\t\t\t#regularizer_loss=torch.tensor(0.0)\n\t\t\tloss = cross_entropy_loss+args.lam *regularizer_loss\n\t\t\ttotal_train_loss+=loss.item()\n\t\t\treg_loss+=regularizer_loss.item()\n\t\t\tce_loss+=cross_entropy_loss.item() \t\n\t\t\tloss.backward()\n\t\t\toptimizer_f.step()\n\t\t\tif alg_options['flag_lr_scheduler']:\n\t\t\t\tscheduler_f.step()\n\t\t\t\t\n\t\t\n\t\t# Training error\n\t\t y_hat = torch.max(f_x,1)[1]\n\t\t u = (y_hat == batch_y).sum()\n\t\t n_train_acc += u.item()\t\n\t\t\n\n\t\t\n\t# Validation error\n\t\twith torch.no_grad():\n\t\t\tmodel_f.eval()\n\t\t\tn_val_acc=0\n\t\t\tfor batch_x,batch_y in val_loader:\n\t\t\t\tif torch.cuda.is_available:\n\t\t\t\t\tbatch_x=batch_x.to(device)\n\t\t\t\t\tbatch_y = batch_y.to(device)\n\t\t\t\tf_x,Af_x,A = model_f(batch_x.float())\n\t\t\t\ty_hat = torch.max(f_x,1)[1]\n\t\t\t\tu = (y_hat == batch_y).sum()\n\t\t\t\tn_val_acc += u.item()\n\t\t\tval_acc_list.append(n_val_acc /len_val_data )\n\t\t\ttrain_acc_list.append(n_train_acc/len_train_data)\n\t\t\t\t\n\t\t\t# A_est error\n\t\t\tA_est = A\n\t\t\tA_est = A_est.detach().cpu().numpy()\n\t\t\tA_est_error = get_estimation_error(A_est,A_true)\n\t\t\tA_est_error_list.append(A_est_error)\n\n\t\tlogger.info('epoch:{}, Total train loss: {:.4f}, ' \\\n\t\t\t\t'  Val. Acc: {:.4f}, ' \\\n\t\t\t\t' Estim. error: {:.4f}'\\\n\t\t\t\t' '.format(epoch+1, total_train_loss / len_train_data*args.batch_size, \\\n\t\t\t\tn_val_acc / len_val_data,\\\n\t\t\t\tA_est_error))\n\t\t\t\t\n\t\t\t\n\t\tif val_acc_list[epoch] > best_val_score: \t\n\t\t\tbest_val_score = val_acc_list[epoch]\n\t\t\tbest_f_model = copy.deepcopy(model_f)\n\t\n\tval_acc_array = np.array(val_acc_list)\n\tepoch_best_val_score = np.argmax(val_acc_array)\n\tlogger.info(\"Best epoch based on validation: %d\" % epoch_best_val_score)\n\tlogger.info(\"Final train accuracy : %f\" % train_acc_list[epoch_best_val_score])\n\tlogger.info(\"Best val accuracy : %f\" % val_acc_list[epoch_best_val_score])\n\t\n\tout={}\n\tout['epoch_best_val_score']= epoch_best_val_score\n\tout['best_train_acc']= train_acc_list[epoch_best_val_score]\n\tout['best_val_acc']= val_acc_list[epoch_best_val_score]\n\tout['best_train_soft_labels']=train_soft_labels\n\tout['final_model_f_dict']=best_f_model\n\tmethod=alg_options['method']\n\tlog_file_identifier='_noreg'\n\tif method=='NOREGEECS':\n\t\tfor m in range(5):\n\t\t\tdf_cm = pd.DataFrame(A_true[m], range(args.K), range(args.K))\n\t\t\tsn.set(font_scale=1.4) # for label size\n\t\t\tsn.heatmap(df_cm, vmin=0, vmax=1, annot=True,  annot_kws={\"size\": 8}) # font size\n\t\t\tplt.savefig(args.log_folder+'A_true_'+str(m)+log_file_identifier+'.eps')\n\t\t\tplt.savefig(args.log_folder+'A_true_'+str(m)+log_file_identifier+'.png')\n\t\t\tplt.close()\n\t\t\t\n\t\t\tdf_cm = pd.DataFrame(A_est[m], range(args.K), range(args.K))\n\t\t\tsn.set(font_scale=1.4) # for label size\n\t\t\tsn.heatmap(df_cm, vmin=0, vmax=1, annot=True,  annot_kws={\"size\": 8}) # font size\n\t\t\tplt.savefig(args.log_folder+'A_est_'+str(m)+log_file_identifier+'.eps')\n\t\t\tplt.savefig(args.log_folder+'A_est_'+str(m)+log_file_identifier+'.png')\n\t\t\tplt.close()\n\t#log_file_name = args.log_folder+'log_'+str(args.learning_rate\n\treturn out\n\n\ndef test(args,alg_options,logger, best_model):\n\ttest_loader = alg_options['test_loader']\n\tmodel=best_model\n\tdevice=alg_options['device']\n\t#Start testing\n\tn_test_acc=0\n\tlen_test_data=len(test_loader.dataset)\n\twith torch.no_grad():\n\t\tmodel.eval()\n\t\tfor batch_x,batch_y in test_loader:\n\t\t\tif torch.cuda.is_available:\n\t\t\t\tbatch_x=batch_x.to(device)\n\t\t\t\tbatch_y = batch_y.to(device)\n\t\t\tf_x,Af_x,_ = model(batch_x.float())\n\t\t\ty_hat = torch.max(f_x,1)[1]\n\t\t\tu = (y_hat == batch_y).sum()\n\t\t\tn_test_acc += u.item()\n\tlogger.info('Final test accuracy : {:.4f}'.format(n_test_acc/len_test_data))\n\treturn (n_test_acc/len_test_data)\n\n\ndef regularization_loss_logdeth(A,f_x,args):\n\tHH = torch.mm(f_x.t(),f_x)\t\n\tregularizer_loss = -torch.log(torch.linalg.det(HH))\n\treturn regularizer_loss\n\n\ndef regularization_loss_logdetw(A,f_x,args):\n\tW = A.view(args.M*args.K,args.K)\n\tWW = torch.mm(W.t(),W)\n\tregularizer_loss = -torch.log(torch.linalg.det(WW))\n\treturn regularizer_loss\n\n\ndef regularization_loss_logdeth_min(A,f_x,args):\n\tHH = torch.mm(f_x.t(),f_x)\t\n\t#regularizer_loss = torch.log(torch.linalg.det(HH+0.0001*torch.eye(args.K).to(args.device)))\n\tregularizer_loss = torch.log(torch.linalg.det(HH+0.001*torch.eye(args.K).to(args.device)))\n\treturn regularizer_loss\n",
        "experimental_info": "- Method and objectives:\n  - The method studied is end-to-end coupled cross-entropy minimization (CCEM) with regularized variants GeoCrowdNet(F) (log det of FF^T) and GeoCrowdNet(W) (log det of W^T W) as regularizers, as implemented in the codebase.\n  - The training objective combines cross-entropy loss on annotator labels with a regularization term: loss = cross_entropy_loss + lambda * regularizer_loss, where lambda is confusion-regularization strength (args.lam). Different regularizers are available: log det of H = FF^T or of WW = W^T W depending on the variant.\n  - The identifiability analysis and training are evaluated under synthetic data, MNIST, LabelMe, Music, CIFAR-10 datasets with a CrowdNetwork that jointly learns the ground-truth predictor f and annotator confusion matrices A (or P in some models).\n\n- Implementation sections (key components):\n  - train_val and test workflows for the proposed method (trainer_proposed.py): trainer_proposed, train_val, and test functions, including the integration of regularization losses via reg_loss_function(A,f_x,args) and the lambda parameter for regularization.\n  - Regularization loss implementations:\n    - regularization_loss_logdeth(A,f_x,args): uses HH = f_x^T f_x and regularization = -log(det(HH)).\n    - regularization_loss_logdetw(A,f_x,args): constructs W from A, WW = W^T W, and regularization = -log(det(WW)).\n    - regularization_loss_logdeth_min(A,f_x,args): variant with log(det(HH + epsilon I)).\n\n- Datasets and experimental setups (data and models):\n  - Datasets: synthetic data (M annotators, K classes, N samples, R-dimensional inputs) and real datasets: MNIST, CIFAR-10, LabelMe, Music. Train/val/test splits are created using the provided loaders.\n  - Real data handling includes possibly IID or realistic annotator patterns, and A_true is estimated or loaded depending on the dataset and init_type.\n\n- Architectural and optimization details:\n  - Model f is selected according to dataset: Lenet for MNIST, fCNN variants for other datasets (e.g., 'fcnn_dropout' or 'fcnn_dropout_batchnorm' in CrowdNetwork).\n  - The annotator confusion matrices A are learned as part of the model; initialization options include 'close_to_identity', 'mle_based', and 'identity' (for different experiments).\n  - Optimizers are Adam with learning rate tuned per experiment; optional learning-rate scheduling is enabled for CIFAR-10 experiments (OneCycleLR or MultiStepLR depending on config).\n\n- Hyperparameters and experimental settings:\n  - lamda_list and learning_rate_list are explored depending on flag_hyperparameter_tuning. If tuning, lamdas include values like 0.2, 0.1 for CIFAR-10; otherwise a single lambda pow selected by args.lam is used.\n  - args.n_epoch controls the total number of epochs; args.n_epoch_maxmig is used for Max-MMI variants; batch_size is dataset dependent (e.g., 64 for MNIST, 100 for labelme, 400 for CIFAR-10 synthetic).\n  - Regularization-specific methods: VOLMINEECS_LOGDETH and VOLMINEECS_LOGDETW correspond to log-determinant regularizers; NOREGEECS corresponds to a baseline with no regularizer.\n\n- Evaluation and outputs:\n  - Validation accuracy is tracked per epoch; the best validation epoch is selected; final test accuracy is reported on the test set.\n  - A_estimation_error is computed comparing learned A against A_true via a permutation alignment (linear_sum_assignment) and normalized L1 error.\n  - Outputs include per-epoch val accuracy, training accuracy, A estimation error, and final test accuracy. Heatmaps of A_true and A_est are optionally saved if NOREGEECS is used.\n\n- Experimental files and data flow:\n  - main training and evaluation flow is implemented in algorithms/trainer_proposed.py and integrated via helpers/algorithm_wrapper.py with the function algorithmwrapperEECS, which routes to trainer_proposed for VOLMINEECS_* methods and trainer_noregeecs for NOREGEECS.\n  - Data pipelines are implemented in helpers/data_load.py (Dataset classes for MNIST, CIFAR-10, LabelMe, Music), helpers/transformer.py for transforms, and helpers/model.py defining CrowdNetwork and related networks.\n"
      }
    },
    {
      "title": "The Entropy Enigma: Success and Failure of Entropy Minimization",
      "full_text": "The Entropy Enigma: Success and Failure of Entropy Minimization Ori Press 1 Ravid Shwartz-Ziv 2 Yann LeCun2 3 Matthias Bethge 1 Abstract Entropy minimization (EM) is frequently used to increase the accuracy of classification models when they’re faced with new data at test time. EM is a self-supervised learning method that op- timizes classifiers to assign even higher proba- bilities to their top predicted classes. In this pa- per, we analyze why EM works when adapting a model for a few steps and why it eventually fails after adapting for many steps. We show that, at first, EM causes the model to embed test im- ages close to training images, thereby increasing model accuracy. After many steps of optimiza- tion, EM makes the model embed test images far away from the embeddings of training images, which results in a degradation of accuracy. Build- ing upon our insights, we present a method for solving a practical problem: estimating a model’s accuracy on a given arbitrary dataset without hav- ing access to its labels. Our method estimates accuracy by looking at how the embeddings of input images change as the model is optimized to minimize entropy. Experiments on 23 challenging datasets show that our method sets the SoTA with a mean absolute error of 5.75%, an improvement of 29.62% over the previous SoTA on this task. Our code is available at: https://github. com/oripress/EntropyEnigma 1. Introduction Practitioners commonly employ model adaptation strategies to enhance classifier performance on real-world data, which often differs significantly from training data. Unsupervised losses play a crucial role in adapting models to images corrupted by noise, such as snow or motion blur, or images from domains not seen in training, such as paintings or computer rendered images. Entropy minimization (EM) is a 1University of T ¨ubingen, T ¨ubingen AI Center, Germany 2New York University 3Meta AI, FAIR. Correspondence to: <ori.press@bethgelab.org>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). Test Time Adaptation (TTA) method that can improve the accuracy of a model on new datasets, without the need for additional labeled training data. EM adapts classifiers by iteratively increasing the probabilities assigned to the most likely classes while diminishing those of the others, and is an integral part of many recent TTA methods (Wang et al., 2020; Mummadi et al., 2021; Rusak et al., 2022b; Goyal et al., 2022; Niu et al., 2022; Cho et al., 2023; Niu et al., 2023; Press et al., 2023; D¨obler et al., 2024; Marsden et al., 2024). In this paper, we analyze EM to understand how it works, when and why it fails, and how to use it to predict model accuracy. The initial intuition behind using entropy minimization, given by Wang et al. (2020) was based on the observation that models tend to be more accurate on images for which they make predictions with higher confidence. The logi- cal extension of this observation was to encourage models to bolster their confidence on such images. However, our analysis reveals this intuition to be only partly true. Remark- ably, even when we construct datasets by excluding samples initially classified correctly — effectively creating datasets with a 100% classification error rate at the start — entropy minimization performance remains largely intact. Our analysis uncovers that during entropy minimization, embeddings of images from the input dataset tend to form distinct clusters. The distances between samples within each cluster diminish, creating more defined groupings, while the centers of these clusters gradually move apart, a phe- nomenon akin to neural collapse (Papyan et al., 2020; Han et al., 2021; Ben-Shaul et al., 2023). At first, embeddings of the input images not only cluster, but also stay close to the embeddings of original training images. Only after nu- merous optimization steps do these embeddings begin to diverge, distancing themselves from the embeddings of the clean training data (Fig. 1). We show this divergence to be intricately tied to a reduction in the model’s accuracy. Drawing from our insights, we present a method designed to estimate the accuracy of a given model on any dataset, with- out labels. This task is notably difficult, because in some cases in-distribution accuracy is tied to out-of-distribution (OOD) accuracy (Miller et al., 2021), while in other cases it is not (Teney et al., 2022). Our approach, termed Weighted Flips (WF), works in conjunction with TTA methods as they 1 arXiv:2405.05012v2  [cs.CV]  12 May 2024The Entropy Enigma: Success and Failure of Entropy Minimization Figure 1.Understanding the successes and failures of EM through clustering embedding dynamics. After a few iterations of adaptation (left), EM improves the accuracy of pretrained classifiers by embedding the input test data near mean embeddings of classes from the training data, marked by stars. Eventually, after many iterations (right), EM fails, because it embeds input test data far from where training data is embedded. We show the t-SNE embeddings of 16-class-Imagenet (Geirhos et al., 2018), throughout adaptation to Gaussian Noise 3 (Hendrycks & Dietterich, 2019). adapt to input data, with minimal added overhead. Using approximations of cluster consistency, WF estimates the ac- curacy of the network by measuring how the predictions of a fixed set of images change: the more they change, the lower the consistency of the clusters and the lower the predicted accuracy. We validate the efficacy of our method across an extensive array of 23 ImageNet-scale (Deng et al., 2009) datasets, encompassing diverse challenges, such as random adversarial noises, hard images, and datasets featuring OOD classes. WF surpasses the prior state-of-the-art methods by a substantial margin of 29.62%, setting a new benchmark in the accuracy estimation domain. 2. The Mystery of Entropy Minimization EM has been validated as effective in semi-supervised settings, with pioneering work by (Grandvalet & Ben- gio, 2004) and subsequent advancements, such as Tent (Wang et al., 2020), which demonstrated EM’s ability to enhance the accuracy of pre-trained classifiers on unlabeled ImageNet-scale (Deng et al., 2009) datasets. EM operates by iteratively optimizing the model to minimize the en- tropy of the output classification probabilities, denoted by H(ˆy) = −P c p(ˆyc) logp(ˆyc), where ˆy is the logits vector and p(ˆyc) is the probability assigned to class c. This ap- proach inherently boosts the likelihood of the most probable classes while diminishing that of the others. Wang et al. (2020) observed a correlation between lower output entropy and accuracy, indicating that images with low entropy out- puts are more likely to be classified correctly. Subsequent studies, including (Niu et al., 2022; Press et al., 2023; Mars- den et al., 2024), have built on this foundation, assigning more weight to lower-entropy samples, and even ignoring high-entropy samples entirely. To assess the influence of correctly classified images on EM’s effectiveness, we tested the effects of omitting im- ages that were initially correctly classified by the model. If such images are pivotal in EM’s ability to enhance classi- fier performance, we expect a notable decline in the EM efficacy. For this purpose, we utilized ImageNet-C (Hendrycks & Dietterich, 2019) Gaussian Noise level 3, dividing it into training and holdout sets. The training set was replicated seven times, systematically omitting images for which the ground truth label lay somewhere in the pre-trained model’s top-k predictions, for ( k ∈ [1, 2, 3, 5, 10, 20, 50]). Con- cretely, for k = 1 , all accurately classified images were excluded, and for k = 2, images whose label ranked within the top two predictions were removed, and so forth. Each altered training set was used to adapt a Tented model. The model’s accuracy was then evaluated on the holdout set, with evaluations every ten iterations, spanning a total of 1,000 iterations. The experiment results (see Figure 2) are revealing, under- scoring the robustness of EM. Notably, EM’s effectiveness endures even when images initially classified correctly are excluded. For instance, removing all initially correctly classified im- ages before adaptation produces an increase in accuracy comparable to not removing any images, with gains of 10.50% and 12.38%, respectively. Even more remarkable is the persistence of this trend: with k = 10, the model still 2The Entropy Enigma: Success and Failure of Entropy Minimization Figure 2.EM remains effective even when initially correctly classified images are excluded. Accuracy gain per iteration on a holdout set, as Tent adapts to its inputs. Surprisingly, the per- formance gain on the holdout set is high, even when we exclude top-k samples from the training set. When top-k = 0, no images are excluded. registers a notable accuracy improvement of 7.88%. This observation is particularly striking given the nature of the excluded images – they are not just numerous, but also rep- resent the highest quality, being those the network is most certain about. Specifically, images excluded atk = 1, which constitute 45% of the dataset, have an average entropy of 1.85, markedly lower than the original dataset’s average entropy of 2.84. Additionally, we also tested the effects of removing images according on their initial entropy level, and found similar results (see Appendix G). These findings intriguingly sug- gest that the model’s accuracy and entropy on individual images may not be as pivotal to EM’s success in enhancing classifier performance as previously thought. It reveals a nuanced dimension of EM’s functionality and hints at the presence of deeper mechanisms, which we will investigate next. 3. Phases of Entropy Minimization: Clustering Dynamics and Embedding Alignment We analyze the evolution of input data embeddings as EM progresses through its iterations. At first, EM causes the model to increase in accuracy, which we refer to as the first phase, followed by a decrease in accuracy, which we refer to as the second phase. The number of EM iterations needed for the model to reach its maximum accuracy (the end of the first phase, and the beginning of the second) is varied and depends on the input data. In the first phase, these embeddings align closely with the embeddings of samples from the original training distribu- tion. However, in the second phase, this alignment starts to deteriorate; the embeddings drift progressively further from the training distribution, disrupting the initial alignment, as conceptually depicted in Figure 3. Figure 3.The two-phase clustring paradigm explains EM be- havior. Intuitive visualization of EM’s phases. In the first phase (success), input test data becomes more clustered, aligning closely with the mean embeddings of corresponding classes from the train- ing data (the colored stars). In the second phase (failure), these clusters diverge from the mean embeddings. To examine the clustering process across the two phases of the EM, we focus on two measures: (1) the quality of the clusters and (2) their alignment with the original training data distribution. For evaluating cluster quality, we ran k-means on the embeddings and computed the Silhouette score (Rousseeuw, 1987), a widely recognized metric for measuring cluster quality. The Silhouette score gauges how closely an embedding corresponds to its own cluster in contrast to neighboring clusters, with a high score indicating distinct and well-separated clusters. To quantify the alignment between clusters and embeddings of the original training distribution, we looked at mean embeddings for the classes in the ImageNet validation set, alongside the centroids of clusters found by k-means. We use the Hungarian method (Kuhn, 1955) to find a matching between mean class embeddings and centroids, which mini- mizes the average distance between each assigned pair of (class embedding, centroid). Henceforth, we refer to this average of distances as “Shift distance”. As ImageNet contains many similar fine-grained classes, we restrict our analyses to the 16 classes outlined in (Geirhos et al., 2018), which represent approximately 20% of the total images. Consequently, we use k = 16 when we cluster the embeddings using k-means. This focused approach allowed for a detailed and controlled examination of clustering be- haviors within the framework of EM. We now examine changes in the Silhouette score and Shift distance as Tent adapts to the input data, over 50,000 it- erations using a ResNet-50 (He et al., 2016). Figure 4 showcases the comparative Silhouette scores and Shift dis- 3The Entropy Enigma: Success and Failure of Entropy Minimization Figure 4.Two-phase behavior during the EM adaption predicts accuracy.Differences in Silhouette score, Shift distance, and accuracy for Tent adaptation. Each point corresponds to a test dataset; each dataset appears twice: once in blue, corresponding to phase 1 (success, ∆ Acc ≥ 0), and once in orange, corresponding to phase 2 (failure, ∆ Acc < 0). Left: In both phases, and across almost all datasets, the Silhouette score of embeddings increases, corresponding to a better-clustered embedding space. Right: In the first phase, input data embeddings are kept close to training image embeddings, while in the second phase, they drift away, exhibiting large Shift distance changes. The datasets used are IN-C, IN-C and IN-3DCC. tances for both phases, incorporating findings from three diverse datasets: IN-C (Hendrycks & Dietterich, 2019), IN- C (Mintun et al., 2021), and IN-3DCC (Kar et al., 2022). Our findings distill into two primary insights: First, a pos- itive change in Silhouette score, indicative of enhanced clustering, is observed in both phases for more than 98% of cases. Notably, during the initial phase, a positive cor- relation exists between changes in Silhouette score and ac- curacy (ρ = 0.70, significant at α = 0.05). Second, Shift distances minimally change (and sometimes diminish, sig- nifying closer proximity to training data embeddings) in the first phase, they notably grow larger in the second phase. During this latter phase, a substantial negative correlation emerges between changes in Shift distance and accuracy (ρ = −0.79, significant at α = 0.05). Synthesizing these results reveals a nuanced picture: EM bolsters accuracy by clustering the embedded data into more concentrated clusters. This strategy remains efficacious as long as these embeddings align closely with the embeddings of the training data. However, as input data embeddings diverge from the training distribution, the classifier’s ac- curacy diminishes. This intricate interplay offers a deeper understanding of EM’s operation and its dependency on the spatial dynamics of data embeddings. We discuss the connection between EM and clustering in more detail in Appendix A. 4. Estimating Dataset Accuracy Leveraging our understanding of EM, we tackle a critical challenge in TTA settings: estimating the accuracy of a classification model on a given dataset. Ideally, one might resort to the metrics used in this paper, namely Silhouette score or Shift distance, for this purpose. However, these metrics encounter practical hurdles: the Silhouette score depends on clustering, which varies across datasets due to differences in class distributions or the total number of classes, and calculating the Shift distance is impossible, as accessing the training data (in order to calculate mean embedding vectors per class) is forbidden in most TTA settings (Wang et al., 2020; Niu et al., 2022; Yuan et al., 2023). 4.1. Label Flipping Due to the difficulties of measuring these scores in prac- tice, we take a different approach. We look at the number of images for which the model’s prediction changes some- where between the initial and the final iteration of the EM (“label flips”). According to our hypothesis, the number of label flips is correlated with the pre-trained model’s ac- curacy on the dataset. Our reasoning is as follows: there exists a tight correlation between accuracy and Silhouette score at iteration 0 — the higher the accuracy, the better clustered the input data, shown in Figure 5. Therefore, we do not expect EM, which works by clustering its inputs, to significantly change an already well-clustered set of embed- dings. It follows that there will likely be only a few label flips. Conversely, given a dataset with a low accuracy, its 4The Entropy Enigma: Success and Failure of Entropy Minimization Figure 5.Label flips are strongly correlated with Silhouette score. Silhouette score at the initial iteration and the total number of label flips at the final iteration are correlated for datasets in IN-C, IN-C, and IN-3DCC. Both metrics are correlated with accuracy, but measuring label flips is easier and more practical. image embeddings will likely be badly clustered initially, which leads EM to change them significantly, resulting in many label flips. We demonstrate the validity of this reasoning by adapt- ing the state-of-the-art TTA method, Rdumb (Press et al., 2023), to IN-C, IN-C, and IN-3DCC. Initially, we used the pre-trained model to classify 1,000 input images and then recorded the total number of label flips after adaptation. The model is adapted for 1,000 iterations because Rdumb resets itself every 1,000 iterations. We find a strong correlation between accuracy and label flips, seen in Figure 5. 4.2. Weighted Flips We now describe the Weighted Flips (WF) method of con- verting the count of label flips into a dataset accuracy es- timate. Instead of just counting the number of flips, we additionally consider the classifier’s initial confidence in its predictions for each image; images initially classified with high confidence that later flip should contribute more significantly than those with lower initial confidence. We then compute the WF as: W F= X i 1{flip}(i) · ci where 1flip (i) is 1 if image i’s label flipped and0 otherwise, and ci is the confidence percentile of imagei. Utilizing pairs of weighted flips and accuracy ((WF, accuracy)k) from IN- Validation and ImageNet-C holdout noises, we interpolate the weighted-flips-to-accuracy function, f (refer to Figure 6). To estimate the accuracy of a model on an unfamiliar dataset, we adapt the model to it using RDumb (for details, see Appendix J), measuring flips on the first 1,000 input images. After adaptation, we count and weigh the flips, estimating the model’s accuracy as f(WF). Importantly, WF is versatile and can work with a range of TTA methods (see Appendix E), and f can be interpolated in a variety of different ways (see Appendix B). In Appendix D.1, D.2, we present ablation studies on the effects of varying end iterations and holdout set sizes on performance. 4.3. Experimental Setting Accuracy estimation methods must yield robust estimates across diverse and challenging datasets to be considered reliable. In our evaluation, we probe the effectiveness of our proposed method using an extensive selection of popular ImageNet-scale classification datasets. This includes all classification datasets from the Shift-Happens benchmark1. Our chosen datasets encompass a wide spectrum, from various types of noise (IN-C, IN-C, IN-3DCC, CCC) and domain shifts (IN-R, IN-V2, IN-D), to adversarial noises (Patch-IN, BG Challenge, IN-Obfuscations), and even im- ages featuring classes not present in ImageNet (NINCO). Several datasets provide multiple splits of a similar nature, the results of which we average, except for ImageNet-D (Rusak et al., 2022a), which encompasses a variety of dis- tinct domains. The CCC dataset (Press et al., 2023) is par- ticularly expansive, containing 27 splits with 7.5M images each; for practicality, we only include the initial 25k images from each split in our analysis. Altogether, our evaluation spans 326 individual dataset splits. We briefly describe the other methods tested alongside ours: • AC (Hendrycks & Gimpel, 2016): Computes the dataset-wide average confidence for the top-predicted class in each image. • DoC (Guillory et al., 2021): Builds upon AC by as- sessing the variance in mean confidence between the validation and OOD sets, demonstrating consistent en- hancements in performance. • ATC (Garg et al., 2022): Estimates accuracy by deter- mining the fraction of unlabeled data samples where the model’s confidence exceeds a learned threshold. • COT (Lu et al., 2023): Estimates accuracy by applying Optimal Transport to quantify the disparity between OOD and in-distribution model outputs. 1https://github.com/ shift-happens-benchmark/icml-2022 5The Entropy Enigma: Success and Failure of Entropy Minimization Figure 6.Fitting and accuracy prediction using the WF method. Left: Fitting f: using the noises in IN-C Holdout and ImageNet- Validation, we fit pairs of (weighted flips, accuracy), shown in red. The black curve shows the function resulting from interpolating the points, f(x) = 0.00036x2 − 0.32x + 75.66. Right: With our weighted-flips-to-accuracy function f, we can estimate the accuracy of a model across the six splits from (Rusak et al., 2022a). We use the same f function and show that it works across different architectures, without refitting. 4.4. Results Looking at Table 1 reveals that our WF method consistently stands out as the best estimator across a broad spectrum of ImageNet-scale datasets. WF sets a new benchmark by achieving an average estimation error of just 5.75%, signifi- cantly outperforming the nearest competitor, COT, reducing the relative error by 29.62%. This exemplary performance of WF is not limited to average cases; even in the most challenging scenarios of worst-case performance, WF main- tains its superiority, cutting the error by 29.74% compared to COT. Furthermore, WF demonstrates remarkable consis- tency as an estimator. In 18 of the 23 datasets evaluated, it either leads the pack or comes a close second. This is in stark contrast to the performance of COT, which, despite being second-best, only achieves top-two rankings in 12 datasets. The persistent effectiveness of WF across diverse conditions underscores its reliability and superiority in ac- curacy estimation. Practicality of WF: Beyond its top-tier performance, WF stands out for its practicality. It operates concurrently with the EM process, requiring only three parameters that define the weighted-flips-to-accuracy function, f. This process adds minimal computational overhead, requiring only 20 additional forward passes for every 1,000 Rdumb iteration steps. Lastly, WF is effective even when only a small num- ber of samples are available, see Appendix C. Versatility across Models and Architectures: To demon- strate the adaptability of the WF method, we tested it across various models and architectures, employing the same weighted-flips-to-accuracy function, f, used in our primary experiments (Table 1). Testing encompassed dif- ferent ResNet variants, including models enhanced with noise augmentation techniques, such as ANT (Rusak et al., 2020), AugMix (Hendrycks et al., 2019), and DeepAugment (Hendrycks et al., 2021a). Additionally, we evaluated a ResNext-101 (Xie et al., 2017), ViTB-16 (Dosovitskiy et al., 2010), and MaxViT-T (Tu et al., 2022). The mean absolute errors between estimated and actual accuracies are reported in Table 2. Remarkably, 5 of the 8 models tested achieved a lower mean absolute error than the baseline model, RN-50, showing that f maintains its efficacy across different model architectures. When f is refitted on the architecture that WF is evaluated on, performance improves (see Appendix F). Robustness to Dataset Choice: In Table 1, we derived the weighted-flips-to-accuracy function f using IN-C holdout and ImageNet validation noises. We further validated the robustness of the WF method by fitting f using a subset of the 23 datasets and then assessing its performance on the remaining datasets. As an added challenge, we excluded datasets used in the original configuration: IN-C Holdout and ImageNet-Validation. For each subset size, we repeated the fitting and evaluation process 50 times. The results, plot- ted in Figure 7, illustrate that the WF method consistently outperforms COT across almost all subset sizes, reinforc- ing its resilience and reliability across a broad spectrum of datasets. 6The Entropy Enigma: Success and Failure of Entropy Minimization Table 1.Mean Absolute Error between estimated accuracy, and true accuracy on a ResNet-50 model, for 4 estimation methods (AC, DoC, ATC, COT) (Hendrycks & Gimpel, 2016; Guillory et al., 2021; Garg et al., 2022; Lu et al., 2023), and ours. Our method (WF) is consistently either best or second best, with the best average and worst-case performance across many different OOD datasets. Best results are in bold; second best are underlined, {.} indicates how many splits are in each dataset, when there are more than 1. Datasets AC DoC ATC COT WF (ours) Noises IN-C {75} (Hendrycks & Dietterich, 2019) 10.06 6.61 7.44 2.23 4.79 IN-C {50} (Mintun et al., 2021) 19.48 15.96 12.16 3.17 7.35 IN-3DCC {60} (Kar et al., 2022) 11.83 3.44 8.15 3.02 3.66 CCC {27} (Press et al., 2023) 15.51 11.95 6.05 2.04 2.80 Domain Shifts Stylized (Geirhos et al., 2019) 31.63 28.08 7.36 12.18 3.81 IN-V2 {3} (Recht et al., 2019) 5.58 2.41 0.45 2.68 4.70 IN-Sketch (Wang et al., 2019) 22.34 18.78 0.15 4.23 1.71 IN-R (Hendrycks et al., 2021a) 23.21 19.65 0.37 2.44 1.88 IN-D (Rusak et al., 2022a) Real 10.56 7.00 1.35 27.54 3.18 Painting 17.40 13.85 3.27 7.49 2.12 Clipart 21.27 17.72 1.62 4.52 3.37 Sketch 24.43 20.87 0.61 0.71 5.44 Infograph 54.12 50.57 36.26 3.44 3.63 Quickdraw 32.67 29.11 4.13 1.60 2.57 Cartoon & Drawing {2} (Salvador & Oberman, 2022) 15.69 12.13 4.42 1.62 13.25 Adversarial Noises BG Challenge {8} (Xiao et al., 2020) 10.54 7.37 4.88 19.68 6.92 IN-A (Hendrycks et al., 2021b) 45.12 41.57 20.51 30.38 21.61 IN-C Patch {75} (Gu et al., 2022) 4.37 0.16 4.42 2.57 1.60 IN-Hard (Taesiri et al., 2023) 29.71 26.15 6.73 15.33 3.64 Patch-IN {10} (Pintor et al., 2023) 8.06 5.11 5.11 10.13 8.87 IN-Obfuscations {3} (Stimberg et al., 2023) 99.90 96.34 99.90 0.12 4.58 OOD/Other ObjectNet (Barbu et al., 2019) 34.59 31.03 9.43 10.40 2.74 NINCO (Bitterwolf et al., 2023) 50.29 46.74 26.97 20.28 18.07 Average 26.02 22.29 11.81 8.17 5.75 Worst Case 99.90 96.34 99.90 30.38 21.61 Average (Worst Case Excluded) 22.66 18.92 7.81 7.16 5.03 5. Related Work To the best of our knowledge, the first time EM was shown to be useful for improving a classifier’s accuracy was in (Grandvalet & Bengio, 2004). They showed how EM can be applied to a logistic regressor, and found it to be ben- eficial in cases where the data was corrupted by outliers. Following this, Lee et al. (2013) proposed pseudo labeling as a means of improving classification accuracy on MNIST. Interestingly, t-SNE is used to show that pseudo labeling works partly by encouraging the model’s embeddings to be better clustered, and away from the decision boundaries of the model. Moreoever, it is stated that pseudo labeling is equivalent to entropy regularization (Grandvalet & Bengio, 2004). Although this might be true in the settings consid- ered then, pseudo labeling was shown to be less effective (and thus not equivalent) on larger-scale datasets, by Tent. Unlike previous work, we demonstrate that EM clusters by measuring the Silhouette score of the clusters themselves, allowing us to empirically evaluate ImageNet scale datasets. Additionally, we show what happens when EM fails, which is not discussed in prior work, with the exception of (Oliver et al., 2018), which shows how EM fails to adapt to a toy “two moons” dataset, because the model increases the mag- nitude of its output logits. This isn’t the case in most TTA settings, as the final layer of the model isn’t trained. Minimizing entropy at test time was popularized by Tent 7The Entropy Enigma: Success and Failure of Entropy Minimization Table 2.Mean Absolute Error between estimated accuracy and true accuracy, across different architectures. Using the same weighted-flips to-accuracy function, f, works across different architectures and models, without need for finetuning. For each model and dataset, the task is to estimate the accuracy of that model on the dataset. Best results are in bold; second best are underlined. AugMix: ♢ ANT: ‡ DeepAugment: ♠ (Hendrycks et al., 2019; Rusak et al., 2020; Hendrycks et al., 2021a) Datasets RN-50 RN-18 RN-34 RN-50 ‡ RN-50 ♢ RN-50 ♢♠ RNXt-101 RNXt-101 ♠ ViT-B/16 MaxViT-T IN-C 4.79 7.21 6.04 5.39 5.02 4.81 5.35 4.12 8.34 6.73 IN-C 7.35 7.90 6.77 6.84 6.60 6.48 5.60 5.63 6.59 4.97 IN-3DCC 3.66 3.58 3.89 3.20 3.07 2.98 7.23 4.37 7.19 6.79 IN-V2 4.70 4.11 3.37 3.67 5.06 5.00 6.47 5.54 4.44 6.08 IN-D Real 3.18 2.83 0.38 2.72 6.59 3.30 4.24 0.61 1.02 3.40 Painting 2.12 5.36 0.78 0.59 7.62 2.51 12.02 1.12 3.02 0.60 Clipart 3.37 1.59 4.42 0.32 6.19 2.19 7.24 0.53 12.82 4.52 Sketch 5.44 1.53 3.93 6.18 9.73 3.60 10.75 1.89 11.04 10.88 Infograph 3.63 1.76 3.67 3.74 6.78 0.28 6.37 2.34 9.27 9.13 Quickdraw 2.57 2.34 2.24 2.20 2.53 1.27 2.27 1.21 2.36 2.31 Average 4.08 3.82 3.55 3.49 5.92 3.10 6.76 2.74 6.61 5.54 Figure 7.WF outperforms other methods across almost all sub- set sizes. Mean Absolute Error of WF when using a weighted-flips- to-accuracy function f to fit on random subsets of the 23 datasets in Table 1. For each point on the x-axis, we sample 50 fitting datasets for WF, and plot the average and the standard deviation of the MAE. For the other methods, we plot average MAE across all datasets . (Wang et al., 2020), which demonstrated the effectiveness of EM on large-scale datasets, such as ImageNet-C. Entropy minimization is ideal for domain adaptation: it can be used on a trained model, without retraining, and doesn’t require balancing a proxy loss with a classification loss, as in (Gidaris et al., 2018; Sun et al., 2020; Gandelsman et al., 2022). Though many prior works use losses that are based on en- tropy (Wang et al., 2020; Rusak et al., 2022b; Goyal et al., 2022; Mummadi et al., 2021; Wang et al., 2022; Niu et al., 2022; Cho et al., 2023; Press et al., 2023; Niu et al., 2023; D¨obler et al., 2024; Marsden et al., 2024), little is known as to why it works. Additionally, entropy minimization, when used in TTA settings, is effective for only a limited num- ber of iterations, before the classifier degrades to chance accuracy, shown in (Press et al., 2023). Interestingly, this degradation of accuracy, named “collapse”, differs from classical definitions of catastrophic forgetting in continual learning (De Lange et al., 2021), in that the task itself does not change. A plethora of methods have been used for adapting a trained classifier to out-of-domain data: from using an auxiliary loss to help learn the test domain (Sun et al., 2019; 2020; Gandelsman et al., 2022) through simply re-estimating the mean and variance statistics (Schneider et al., 2020; Nado et al., 2020) to using image augmentations (Wang et al., 2022; Song et al., 2023; Chakrabarty et al., 2023). However, for their simplicity and success, entropy minimization-based methods are still the most widely used and successful in settings most relevant to this work. Works that follow Tent improve EM by modifying the loss to be more robust to label noise (Rusak et al., 2022b) or smoother (Mummadi et al., 2021), or by adjusting the tem- perature of the output distribution (Goyal et al., 2022). While testing on long sequences of images, both (Wang et al., 2022) and (Niu et al., 2022) show that Tent degrades in accuracy, the more iterations it does. (Press et al., 2023) show that this is in fact true for all TTA methods apart from EATA (Niu et al., 2022), which uses an L2 regularizer to con- strain the adapting model’s weights to be close to those of 8The Entropy Enigma: Success and Failure of Entropy Minimization the pretrained model. (Niu et al., 2023) study the effects of batch size, label shifts and other factors on adaptation; they propose a method to stabilize adaptation. Similarly, (D¨obler et al., 2024) also test entropy minimization-based methods in real-world conditions, and propose a new method based on a diversity and a weighted entropy loss. Entropy has also been used in semi-supervised settings: (Sohn et al., 2020) propose augmentation and an entropy loss to train a classifier when only a few labels are available. Analyzing which labels flip during training has been studied in (Toneva et al., 2018), which explored which samples are forgotten during training. Another work, (Deng et al., 2022) looked at how to reduce the amount of times a label flips dur- ing training. The agreement/disagreement between different models on ID data was shown to be linearly correlated to OOD accuracy and has been recently used to estimate accu- racy in (Miller et al., 2021; Jiang et al., 2021; Baek et al., 2022; Kim et al., 2023). These works are beyond the scope of this work, as they require access to multiple models and ID data, which is disallowed in most TTA settings (Wang et al., 2020; Niu et al., 2022; Yuan et al., 2023). 6. Conclusion While EM is a cornerstone in many TTA methods, the me- chanics of its success have remained enigmatic. This study sheds light on the transformative journey of input data em- beddings under the EM adaption. It reveals a biphasic clus- tering process, where alignment with the training data’s embedding clusters bolsters accuracy, followed by a subse- quent phase where excessive divergence diminishes it. Our work goes beyond deciphering the mystery behind en- tropy minimization; it also utilizes this knowledge to signif- icantly refine the precision of model accuracy predictions in TTA contexts. This dual achievement underscores the potential of deep analytical approaches in enhancing the efficacy and applicability of machine learning models. Acknowledgements We thank Ofir Press for helpful insights and feedback. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Ori Press. Matthias Bethge is a member of the Machine Learning Cluster of Excellence, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Founda- tion) under Germany’s Excellence Strategy – EXC number 2064/1 – Project number 390727645 and acknowledges support by the German Research Foundation (DFG): SFB 1233, Robust Vision: Inference Principles and Neural Mech- anisms, TP 4, Project No: 276693517. This work was supported by the T¨ubingen AI Center. The authors declare no conflicts of interests. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel needs to be highlighted here specifically. References Amini, M.-R. and Gallinari, P. Semi-supervised logistic regression. In ECAI, volume 2, pp. 11, 2002. Baek, C., Jiang, Y ., Raghunathan, A., and Kolter, J. Z. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift.Advances in Neu- ral Information Processing Systems , 35:19274–19289, 2022. Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut- freund, D., Tenenbaum, J., and Katz, B. Objectnet: A large-scale bias-controlled dataset for pushing the lim- its of object recognition models. Advances in neural information processing systems, 32, 2019. Ben-Shaul, I., Shwartz-Ziv, R., Galanti, T., Dekel, S., and LeCun, Y . Reverse engineering self-supervised learning. arXiv preprint arXiv:2305.15614, 2023. Bitterwolf, J., M¨uller, M., and Hein, M. In or out? fixing imagenet out-of-distribution detection evaluation. arXiv preprint arXiv:2306.00826, 2023. Chakrabarty, G., Sreenivas, M., and Biswas, S. Santa: Source anchoring network and target alignment for con- tinual test time adaptation. Transactions on Machine Learning Research, 2023. Cho, Y ., Kim, Y ., and Lee, D. Beyond entropy: Style transfer guided single image continual test-time adaptation. arXiv preprint arXiv:2311.18270, 2023. De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., and Tuytelaars, T. A continual learning survey: Defying forgetting in classifi- cation tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021. Dempster, A. P., Laird, N. M., and Rubin, D. B. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society: series B (method- ological), 39(1):1–22, 1977. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. 9The Entropy Enigma: Success and Failure of Entropy Minimization Deng, X., Xiao, Y ., Long, B., and Zhang, Z. Reducing flip- ping errors in deep neural networks. InProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 6506–6514, 2022. D¨obler, M., Marencke, F., Marsden, R. A., and Yang, B. Diversity-aware buffer for coping with temporally corre- lated data streams in online test-time adaptation. arXiv preprint arXiv:2401.00989, 2024. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arxiv 2020. arXiv preprint arXiv:2010.11929, 2010. Gandelsman, Y ., Sun, Y ., Chen, X., and Efros, A. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 35:29374–29385, 2022. Garg, S., Balakrishnan, S., Lipton, Z. C., Neyshabur, B., and Sedghi, H. Leveraging unlabeled data to pre- dict out-of-distribution performance. arXiv preprint arXiv:2201.04234, 2022. Geirhos, R., Temme, C. R., Rauber, J., Sch¨utt, H. H., Bethge, M., and Wichmann, F. A. Generalisation in humans and deep neural networks. Advances in neural information processing systems, 31, 2018. Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich- mann, F. A., and Brendel, W. Imagenet-trained CNNs are biased towards texture; increasing shape bias im- proves accuracy and robustness. In International Confer- ence on Learning Representations, 2019. URL https: //openreview.net/forum?id=Bygh9j09KX. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Goyal, S., Sun, M., Raghunathan, A., and Kolter, J. Z. Test time adaptation via conjugate pseudo-labels. Advances in Neural Information Processing Systems, 35:6204–6218, 2022. Grandvalet, Y . and Bengio, Y . Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. Gu, J., Tresp, V ., and Qin, Y . Evaluating model robustness to patch perturbations. In ICML 2022 Shift Happens Workshop, 2022. Guillory, D., Shankar, V ., Ebrahimi, S., Darrell, T., and Schmidt, L. Predicting with confidence on unseen distri- butions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1134–1144, 2021. Han, X., Papyan, V ., and Donoho, D. L. Neural collapse under mse loss: Proximity to and dynamics on the central path. arXiv preprint arXiv:2106.02073, 2021. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. Hendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Hendrycks, D., Mu, N., Cubuk, E. D., Zoph, B., Gilmer, J., and Lakshminarayanan, B. Augmix: A simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8340–8349, 2021a. Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples. CVPR, 2021b. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448– 456. pmlr, 2015. Jiang, Y ., Nagarajan, V ., Baek, C., and Kolter, J. Z. As- sessing generalization of sgd via disagreement. arXiv preprint arXiv:2106.13799, 2021. Kar, O. F., Yeo, T., Atanov, A., and Zamir, A. 3d common corruptions and data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18963–18974, 2022. Kim, E., Sun, M., Raghunathan, A., and Kolter, Z. Reliable test-time adaptation via agreement-on-the-line. arXiv preprint arXiv:2310.04941, 2023. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Kuhn, H. W. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83– 97, 1955. 10The Entropy Enigma: Success and Failure of Entropy Minimization Kullback, S. and Leibler, R. A. On information and suf- ficiency. The annals of mathematical statistics , 22(1): 79–86, 1951. Lee, D.-H. et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896. Atlanta, 2013. Lu, Y ., Wang, Z., Zhai, R., Kolouri, S., Campbell, J., and Sycara, K. Predicting out-of-distribution er- ror with confidence optimal transport. arXiv preprint arXiv:2302.05018, 2023. Marsden, R. A., D ¨obler, M., and Yang, B. Universal test- time adaptation through weight ensembling, diversity weighting, and prior correction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Com- puter Vision, pp. 2555–2565, 2024. Miller, J. P., Taori, R., Raghunathan, A., Sagawa, S., Koh, P. W., Shankar, V ., Liang, P., Carmon, Y ., and Schmidt, L. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In International Conference on Machine Learning, pp. 7721– 7735. PMLR, 2021. Mintun, E., Kirillov, A., and Xie, S. On interaction between augmentations and corruptions in natural corruption ro- bustness. Advances in Neural Information Processing Systems, 34:3571–3583, 2021. Mummadi, C. K., Hutmacher, R., Rambach, K., Levinkov, E., Brox, T., and Metzen, J. H. Test-time adaptation to distribution shift by confidence maximization and input transformation. arXiv preprint arXiv:2106.14999, 2021. Nado, Z., Padhy, S., Sculley, D., D’Amour, A., Lakshmi- narayanan, B., and Snoek, J. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Niu, S., Wu, J., Zhang, Y ., Chen, Y ., Zheng, S., Zhao, P., and Tan, M. Efficient test-time model adaptation with- out forgetting. In International conference on machine learning, pp. 16888–16905. PMLR, 2022. Niu, S., Wu, J., Zhang, Y ., Wen, Z., Chen, Y ., Zhao, P., and Tan, M. Towards stable test-time adaptation in dynamic wild world. arXiv preprint arXiv:2302.12400, 2023. Oliver, A., Odena, A., Raffel, C., Cubuk, E., and Goodfel- low, I. Realistic evaluation of semi-supervised learning algortihms. In International conference on learning rep- resentations, pp. 1–15, 2018. Papyan, V ., Han, X., and Donoho, D. L. Prevalence of neural collapse during the terminal phase of deep learn- ing training. Proceedings of the National Academy of Sciences, 117(40):24652–24663, 2020. Pintor, M., Angioni, D., Sotgiu, A., Demetrio, L., Demontis, A., Biggio, B., and Roli, F. Imagenet-patch: A dataset for benchmarking machine learning robustness against adversarial patches. Pattern Recognition, 134:109064, 2023. Poland, W. B. and Shachter, R. D. Mixtures of gaussians and minimum relative entropy techniques for modeling continuous uncertainties. In Uncertainty in Artificial Intelligence, pp. 183–190. Elsevier, 1993. Press, O., Schneider, S., K ¨ummerer, M., and Bethge, M. Rdumb: A simple approach that questions our progress in continual test-time adaptation. Advances in Neural Information Processing Systems, 36, 2023. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do imagenet classifiers generalize to imagenet? In Interna- tional conference on machine learning, pp. 5389–5400. PMLR, 2019. Rousseeuw, P. J. Silhouettes: a graphical aid to the inter- pretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:53–65, 1987. Rusak, E., Schott, L., Zimmermann, R. S., Bitterwolf, J., Bringmann, O., Bethge, M., and Brendel, W. A simple way to make neural networks robust against diverse im- age corruptions. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16, pp. 53–69. Springer, 2020. Rusak, E., Schneider, S., Gehler, P. V ., Bringmann, O., Bren- del, W., and Bethge, M. Imagenet-d: A new challenging robustness dataset inspired by domain adaptation. In ICML 2022 Shift Happens Workshop, 2022a. Rusak, E., Schneider, S., Pachitariu, G., Eck, L., Gehler, P. V ., Bringmann, O., Brendel, W., and Bethge, M. If your data distribution shifts, use self-learning. Transactions on Machine Learning Research, 2022b. Salvador, T. and Oberman, A. M. Imagenet-cartoon and imagenet-drawing: two domain shift datasets for ima- genet. In ICML 2022 Shift Happens Workshop, 2022. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Bren- del, W., and Bethge, M. Improving robustness against common corruptions by covariate shift adaptation. Ad- vances in neural information processing systems , 33: 11539–11551, 2020. 11The Entropy Enigma: Success and Failure of Entropy Minimization Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C. A., Cubuk, E. D., Kurakin, A., and Li, C.-L. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural informa- tion processing systems, 33:596–608, 2020. Song, J., Lee, J., Kweon, I. S., and Choi, S. Ecotta: Memory- efficient continual test-time adaptation via self-distilled regularization. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition , pp. 11920–11929, 2023. Stimberg, F., Chakrabarti, A., Lu, C.-T., Hazimeh, H., Stretcu, O., Qiao, W., Liu, Y ., Kaya, M., Rashtchian, C., Fuxman, A., et al. Benchmarking robustness to adversar- ial image obfuscations. arXiv preprint arXiv:2301.12993, 2023. Sun, Y ., Tzeng, E., Darrell, T., and Efros, A. A. Unsuper- vised domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825, 2019. Sun, Y ., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generaliza- tion under distribution shifts. In International conference on machine learning, pp. 9229–9248. PMLR, 2020. Taesiri, M. R., Nguyen, G., Habchi, S., Bezemer, C.-P., and Nguyen, A. Zoom is what you need: An empirical study of the power of zoom and spatial biases in image classification. arXiv preprint arXiv:2304.05538, 2023. Teney, D., Lin, Y ., Oh, S. J., and Abbasnejad, E. Id and ood performance are sometimes inversely correlated on real-world datasets. arXiv preprint arXiv:2209.00613, 2022. Toneva, M., Sordoni, A., Combes, R. T. d., Trischler, A., Bengio, Y ., and Gordon, G. J. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018. Tu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., and Li, Y . Maxvit: Multi-axis vision transformer. In European conference on computer vision, pp. 459–479. Springer, 2022. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. Tent: Fully test-time adaptation by entropy minimiza- tion. arXiv preprint arXiv:2006.10726, 2020. Wang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning ro- bust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pp. 10506–10518, 2019. Wang, Q., Fink, O., Van Gool, L., and Dai, D. Continual test- time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201–7211, 2022. Xiao, K., Engstrom, L., Ilyas, A., and Madry, A. Noise or signal: The role of image backgrounds in object recogni- tion. ArXiv preprint arXiv:2006.09994, 2020. Xie, S., Girshick, R., Doll´ar, P., Tu, Z., and He, K. Aggre- gated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vi- sion and pattern recognition, pp. 1492–1500, 2017. Yuan, L., Xie, B., and Li, S. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15922–15932, 2023. 12The Entropy Enigma: Success and Failure of Entropy Minimization A. The Relationship between Entropy Minimization and Clustering In this section, we explain the connection between entropy minimization and the Expectation-Maximization algorithm (Dempster et al., 1977) with a mixture of Gaussians and show how the iterative entropy minimization objective leads to a clustering process similar to the Expectation-Maximization algorithm. In the Expectation-Maximization algorithm for clustering, the latent variables represent the cluster assignments, and the algorithm alternates between estimating the cluster assignments (E-step) and updating the cluster parameters (M-step). The convergence of the EM algorithm in this setting has been formally established (Dempster et al., 1977). Poland & Shachter (1993) showed that for a random variableX with a given distribution and the mixture of random variables Y that derive from it, the objective of minimizing the “relative entropy” between X and Y generalizes the objective of the Expectation Maximization algorithm: to maximize the likelihood of the observations x drawn from Y ’s distribution. In our setting, the iterative entropy minimization process corresponds to the Expectation Maximization algorithm, as iterative entropy minimization can also be seen as a form of “self-training” with minimization of the relative entropy (the DKL (Kullback & Leibler, 1951)) of the pseudo-labels (the model’s predictions) (Grandvalet & Bengio, 2004). The forward pass of our training process serves two purposes: (1) it sets the “observations”, which are the model’s predictions, and (2) it acts as the E-step of the algorithm, estimating the distribution given the model parameters (the clustering assignment). The backpropagation step, which updates the model parameters (the cluster parameters), serves as the M-step and maximizes the likelihood under the current pseudo-label estimates (Amini & Gallinari, 2002). It is important to note that in our setting, the entropy minimization procedure involves changing both X and Y in each iteration, which may be different from the original Expectation Maximization algorithm. Using these insights, we can provide a better explanation for the two-phase clustering phenomenon observed in our experiments. In the initial “success” phase, where the change in the embeddings is relatively small during the process, the entropy minimization effectively performs Expectation Maximization unsupervised clustering in the model’s embedding space, guided by the smart initialization provided by the pre-trained model. The E-step estimates the pseudo-labels based on the current embedding structure, while the M-step updates the model to refine the embeddings and increase intra-cluster similarity. This process leads to the formation of well-separated clusters, as reflected by the increasing Silhouette score. However, as the Expectation Maximization algorithm continues over many iterations in the “failure” phase or if there is bad initialization, it starts to overfit the model to the specific characteristics of the ”new” test data. Unlike the regular Expectation Maximization algorithm, in our case, the data distribution (the observations) changes over time, which leads to a drift in the embeddings away from the initialized representations learned from the training data. This overfitting effect, which might even converge to a global minimum, is captured by the increasing Shift distance between the test data embeddings and the training class embeddings. To support this explanation, we also provide visualizations of the prediction space to illustrate the clustering process and the eventual drift from the training embeddings. We used a mixture of Gaussians, and trained a GMM with the Expectation Maximization algorithm using maximum likelihood, where the means are initialized based on random samples. The covariance is used as the identity matrix, with the input samples being trainable and optimizing their location. In Figure 8, each dot represents a sample colored by its original class, where the Xs are the centroids at each iteration. As we can see, with the “smart initialization” of the cluster centers, the points converge to the “right” clusters based on the original cluster centers. However, when we start the cluster centers with some shift, namely there is “wrong” initialization, the clusters start with good clustering but then converge to wrong solutions where they mix points with different classes. 13The Entropy Enigma: Success and Failure of Entropy Minimization Figure 8.Top: With the “smart initialization” of the cluster centers, the points converge to the “right” clusters based on the original cluster centers. Middle: When we start the cluster centers with some shift, namely there is “wrong” initialization, the clusters start with good clustering but then converge to wrong solutions where they mix points with different classes. Bottom: For reference, we also show the regular Expectation Maximization algorithm on the shifted dataset. The X’s represent cluster centroids at each iteration. 14The Entropy Enigma: Success and Failure of Entropy Minimization B. Different Parameterizations of f In this section, we test the different ways of parameterizing the weighted-flips-to-accuracy function, f. Firstly, we look at the effects of not weighing each flip, and then we look at linear and cubic interpolations between flips and accuracy (as opposed to quadratic interpolation, used in the rest of the paper). Our results in Table 3 show that the optimalf is a weighted and interpolated quadratically, with the other variations not far behind. Importantly, all variations off perform better than the second best performing method, COT (Lu et al., 2023). Table 3.Mean Absolute Error between estimated accuracy, and true accuracy on a ResNet-50 model, for weighted and unweighted flips-to-accuracy functions, that are either linear, quadratic, or cubic interpolations of points. Datasets Unweighted Linear Unweighted Quadratic Weighted Linear Weighted Quadratic Weighted Cubic Noises IN-C {75} (Hendrycks & Dietterich, 2019) 4.95 5.04 5.94 4.79 5.23 IN-C {50} (Mintun et al., 2021) 7.19 7.36 7.94 7.35 7.01 IN-3DCC {60} (Kar et al., 2022) 4.10 4.12 4.33 3.66 4.25 CCC {27} (Press et al., 2023) 2.97 3.22 4.8 2.80 4.34 Domain Shifts Stylized (Geirhos et al., 2019) 7.12 7.12 7.12 3.81 7.12 IN-V2 {3} (Recht et al., 2019) 3.55 3.71 5.42 4.70 4.03 IN-Sketch (Wang et al., 2019) 1.11 1.32 2.64 4.23 0.23 IN-R (Hendrycks et al., 2021a) 1.43 1.67 3.01 1.88 0.52 IN-D (Rusak et al., 2022a) Real 3.39 3.16 2.04 3.18 4.70 Painting 2.07 1.94 0.34 2.20 0.85 Clipart 2.78 3.08 5.12 3.37 2.44 Sketch 6.12 6.95 12.89 5.44 12.38 Infograph 7.28 8.76 10.35 3.63 10.35 Quickdraw 0.79 0.79 0.79 2.57 0.79 Cartoon & Drawing {2} (Salvador & Oberman, 2022) 13.60 13.76 14.34 13.25 12.96 Adversarial Noises BG Challenge {8} (Xiao et al., 2020) 7.19 7.36 7.33 6.92 8.50 IN-A (Hendrycks et al., 2021b) 23.70 23.53 20.39 21.61 22.91 IN-C Patch {75} (Gu et al., 2022) 1.95 2.00 2.42 1.60 1.48 IN-Hard (Taesiri et al., 2023) 5.27 4.92 0.72 3.64 3.49 Patch-IN {10} (Pintor et al., 2023) 7.42 7.55 9.02 8.87 7.98 IN-Obfuscations {3} (Stimberg et al., 2023) 0.20 0.10 0.10 4.58 0.10 OOD/Other ObjectNet (Barbu et al., 2019) 6.81 6.81 6.81 2.74 6.81 NINCO (Bitterwolf et al., 2023) 20.20 19.85 14.98 18.07 17.73 Average 6.14 6.27 6.47 5.75 6.36 Worst Case 23.70 23.53 20.39 21.61 22.91 Average (Worst Case Excluded) 5.34 5.48 5.84 5.03 5.60 15The Entropy Enigma: Success and Failure of Entropy Minimization C. WF with Limited Data To further test WF’s ability in a challenging setting, we look at how it performs under memory and data constraints. To this end, we test WF in the following scenarios: (1) WF is only allowed to store 100 samples for calculating flips, and (2) when whole dataset is limited to 100 samples for flip calculation and 1,000 samples for adaptation). We note that previous work assumes the existence of at least 2,000 test samples (Niu et al., 2022). In both cases, we use the original weighted-flips-to-accuracy function, f, by multiplying the the weighted flips calculated on 100 samples by 10, and plugging the output into f. Even with only using 100 samples, WF is able to best the original implementation by a bit. Surprisingly, even with limited data and memory, WF manages to remain competitive with unconstrained methods, and is significantly ahead of COT, when it is constrained in a similar manner. Table 4.WF is effective in memory constrained settings. Without finetuning or refitting f, WF beats the original implementation, when only using 100 samples to calculate weighted flips (WF limited mem). In the limited memory/data setting, WF gets access to only 1000 samples in total, 100 of which are used for flip calculations. In this setting, COT gets access to 1,000 input samples and 1,000 in distribution samples. Best results are in bold; second best are underlined, {.} indicates how many splits are in each dataset, when there are more than 1. Datasets COT original WF original WF limited mem COT limited mem/data WF limited mem/data Noises IN-C {75} (Hendrycks & Dietterich, 2019) 2.23 4.79 7.52 36.67 6.52 IN-C {50} (Mintun et al., 2021) 3.17 7.35 8.34 40.55 4.60 IN-3DCC {60} (Kar et al., 2022) 3.02 3.66 3.97 34.44 4.31 CCC {27} (Press et al., 2023) 2.04 2.80 3.71 26.67 4.92 Domain Shifts Stylized (Geirhos et al., 2019) 12.18 3.81 3.37 38.84 2.50 IN-V2 {3} (Recht et al., 2019) 2.68 4.70 4.00 43.96 3.80 IN-Sketch (Wang et al., 2019) 4.23 1.71 1.68 12.46 3.39 IN-R (Hendrycks et al., 2021a) 2.44 1.88 3.03 14.99 12.03 IN-D (Rusak et al., 2022a) Real 27.54 3.18 1.73 41.52 6.51 Painting 7.49 2.12 0.71 26.21 18.44 Clipart 4.52 3.37 5.91 15.98 8.10 Sketch 0.71 5.44 6.30 12.65 4.50 Infograph 3.44 3.63 1.24 4.57 2.51 Quickdraw 1.60 2.57 2.80 0.06 2.46 Cartoon & Drawing {2} (Salvador & Oberman, 2022) 1.62 13.25 16.48 33.25 13.44 Adversarial Noises BG Challenge {8} (Xiao et al., 2020) 19.68 6.92 5.84 32.84 10.15 IN-A (Hendrycks et al., 2021b) 30.38 21.61 16.75 15.30 29.15 IN-C Patch {75} (Gu et al., 2022) 2.57 1.60 1.98 47.03 1.92 IN-Hard (Taesiri et al., 2023) 15.33 3.64 0.65 5.83 14.73 Patch-IN {10} (Pintor et al., 2023) 10.13 8.87 9.09 49.68 9.81 IN-Obfuscations {3} (Stimberg et al., 2023) 0.12 4.58 4.67 0.09 8.93 OOD/Other ObjectNet (Barbu et al., 2019) 10.40 2.74 0.29 2.44 2.74 NINCO (Bitterwolf et al., 2023) 20.28 18.07 20.24 13.05 35.68 Average 8.17 5.75 5.67 23.87 9.40 Worst Case 30.38 21.61 20.24 49.68 35.68 Average (Worst Case Excluded) 7.16 5.03 5.00 22.70 8.21 16The Entropy Enigma: Success and Failure of Entropy Minimization D. Weighted Flips Ablations D.1. Stopping Iteration Ablations WF measures the amount of weighted flips from iteration 0 to iteration 1,000. This is done because RDumb resets the model to its pretrained state every 1,000 iterations, in order to avoid collapse (Press et al., 2023). Here, we look at how measuring weighted flips before iteration 1,000 affects the performance of WF. Interestingly, using 500 iterations increases performance by a relative 26.89% as opposed to the 1,000 iterations used in the rest of the paper. Stopping Iteration Datasets 1000 500 250 100 50 IN-C 4.79 4.88 4.99 5.48 5.67 IN-C 7.35 7.48 7.84 8.96 10.10 IN-3DCC 3.66 3.32 3.37 3.00 3.06 IN-V2 4.70 4.59 4.93 5.11 5.49 IN-D Real 3.18 0.36 0.96 2.82 5.01 Painting 2.12 1.21 4.02 11.28 14.83 Clipart 3.37 0.31 3.75 9.30 14.85 Sketch 5.44 2.49 0.33 5.61 9.26 Infograph 3.63 3.46 0.09 4.37 7.53 Quickdraw 2.57 1.73 7.55 17.54 25.35 Average 4.08 2.98 3.78 7.35 10.12 Figure 9. Left: Mean Absolute Error between estimated accuracy and true accuracy, when measuring weighted flips between iteration 0 and various stopping iterations. Right: For different stopping iterations, interpolating between the points in the holdout set yields different weighted-flips-to-accuracy functions. D.2. Holdout Set Size Ablations Holdout Set Size Datasets 1000 500 250 100 50 IN-C 4.79 4.77 5.52 6.07 7.62 IN-C 7.35 5.91 6.74 7.06 8.40 IN-3DCC 3.66 5.10 4.34 5.17 5.22 IN-V2 4.70 5.13 5.48 3.47 9.10 IN-D Real 3.18 3.50 1.78 0.57 4.21 Painting 2.12 5.38 1.92 6.93 8.57 Clipart 3.37 6.69 7.96 8.79 7.70 Sketch 5.44 10.23 7.39 4.87 4.10 Infograph 3.63 6.15 0.88 3.05 3.77 Quickdraw 2.57 0.71 9.87 38.86 31.66 Average 4.08 5.36 5.19 8.48 9.04 Figure 10.Left: Mean Absolute Error between estimated accuracy and true accuracy, when measuring weighted flips on sets of images of different sizes. Right: For different holdout set sizes, interpolating between the points in the holdout set yields different weighted-flips-to- accuracy functions. f can only output values that are between 0 and 100. 17The Entropy Enigma: Success and Failure of Entropy Minimization E. WF with other TTA Methods WF esimates the accuracy of a dataset as RDumb (Press et al., 2023) is used to adapt to it. In this section, we show that WF work with a variety of different EM methods. To further showcase the versatility of WF, we do not finetune any method, and use the original weighted-flips-to-accuracy function f, for all experiments in Table 5. Table 5. Mean Absolute Error between estimated accuracy and true accuracy, when adapting to data using a ResNet-50 backbone and different TTA methods: Tent (Wang et al., 2020), RPL (Rusak et al., 2022b), and CPL (Goyal et al., 2022). In all cases, the original weighted-flips-to-accuracy function f is used, highlighting the versatility of WF. Datasets RDumb Tent RPL CPL IN-C 4.79 6.75 6.85 5.14 IN-C 7.35 9.68 7.20 7.44 IN-3DCC 3.66 2.92 3.99 3.72 IN-V2 4.70 3.80 3.82 4.42 IN-D Real 3.18 5.15 5.15 0.31 Painting 2.12 7.59 7.59 0.03 Clipart 3.37 6.98 7.11 2.57 Sketch 5.44 3.30 3.52 3.86 Infograph 3.63 2.29 2.24 3.40 Quickdraw 2.57 2.24 2.24 2.37 Average 4.08 5.07 4.97 3.33 F. Additional Vision Transformer Experiments To further analyze WF and the second best method, COT, we add additionally analysis using a ViT-B/16 model. The task is to estimate the accuracy of a ViT-B/16 on a variety of datasets. We compare between using the original weighted-flips-accuracy function, f, which was interpolated using data from a ResNet-50, and interpolating the function using ViT-B/16 data points. In both cases, the datasets used to interpolate are the same. Additionally, we compare to COT on this task. Table 6. Mean Absolute Error between estimated accuracy and true accuracy, when estimating the accuracy of a ViT-B/16 on different datasets. Datasets WF WF (new f) COT IN-C 8.34 1.64 22.24 IN-C 6.59 1.48 25.37 IN-3DCC 7.19 1.87 18.43 IN-V2 4.44 3.63 21.29 IN-D Real 1.02 7.27 37.21 Painting 3.02 7.06 19.13 Clipart 12.82 0.25 13.58 Sketch 11.04 1.90 5.74 Infograph 9.27 3.34 1.16 Quickdraw 2.36 13.04 0.28 Average 6.61 4.15 16.44 18The Entropy Enigma: Success and Failure of Entropy Minimization G. Omitting Samples by Top-k Accuracy/Entropy Level In addition to removing samples by Top-k accuracy, we also analyze the effects of removing samples according to their initial entropy level. We find that both experiments exhibit similar behaviour: it is possible to remove many Top- k/low entropy samples, without significantly affecting the accuracy gain of Tent (on a holdout set of Gaussian Noise 3). Figure 11.Left: Average entropy across top-k samples for different values of k. The percentages shown are the fraction of images out of the whole dataset. The original dataset, Gaussian Noise 3, has an average entropy of 2.84. Right: The relative size of the datasets, when top-k samples are removed. Figure 12.Left: Accuracy gain per iteration on a holdout set, as Tent adapts to its inputs. Each line corresponds to a different experiment where we remove samples based on their initial entropy level. Similarly to Figure 2, it’s possible to remove low entropy samples while barely hurting performance. When entropy ≤ 0, no images are excluded. Right: The relative size of the datasets and their average entropy, when samples with a entropy level≤ k are removed. 19The Entropy Enigma: Success and Failure of Entropy Minimization H. Silhouette score, Shift distance, and Accuracy Throughout Entropy Minimization In Figure 4, we looked at the changes of Silhouette scores/Shift distances for each phase in EM. Here, we show how these scores, along with accuracy, change in every iteration of Tent. For each one of the datasets analyzed, we group noises based on severity level, and plot their averages and standard deviations, for every iteration. Figure 13.Changes in Silhouette scores, Shift distances, and Accuracies as Tent adapts to its inputs. We group together noises by severity level, and average the data for every iteration. 20The Entropy Enigma: Success and Failure of Entropy Minimization I. WF on CIFAR10/100 WF is not as effecitve on CIFAR10 (Krizhevsky et al., 2009) as it is on ImageNet (Deng et al., 2009) scale datasets. CIFAR10 is an outlier in entropy minimization: for example, Press et al. (2023) showed that Tent doesn’t degrade in accuracy, even after 100 million CIFAR10 images seen. We nonetheless run our method on CIFAR10. On average, we see only 0-5 label flips per dataset on C10-C. This is far from what we see ImageNet-scale datasets we tested. Like in the paper, we interpolate a weighted-flips-to-accuracy function f on the holdout set and get: f(x) = −249.36x2 − 87.39x + 77.01 which has a MAE of 16.64 on the C10 validation set. We repeat this for CIFAR100 and get: f(x) = 0.000322x2 − 0.287x + 99.54 which has a MAE of 9.10 on the C100 validation set. Apart from refitting f, we did not tune any other parameter in these two experiments. J. RDumb WF uses RDumb (Press et al., 2023) to estimate accuracy. We go over the implementation of the method in brief. RDumb is based on ETA (Niu et al., 2022), wherein the model is reset to its pretrained state every 1,000 iterations. Rdumb optimizes the BatchNorm (Ioffe & Szegedy, 2015) parameters, Θ of a given classifier f. The loss optimized is entropy, with two filtration steps: the first, in which samples with high entropy are filtered out, and the second, in which samples that produce logits similar to previous samples are filtered out. For a sample x, the first filtration is given by: Sent(x) = 1 exp[E(x; Θ)− E0] · IE(x;Θ)<E0 (x), with E0 = 0.4 × ln103. The second filtration is given by: Sdiv(x) = I{cos(fo(x),m−1)<ϵ}(x) where cos() is the cosine similarity, and mt is an exponential moving average of the logits of previously seen samples at iteration t: mt = ( y1, if t = 1 αyt + (1 − α)mt−1, if t >1 and yt is the average model prediction on a batch of inputs at step t, and α = 0.9. Put together with entropy minimization, the optimization formula becomes: min ˆΘ −Sent(x) · Sdiv(x) X y∈C fΘ(y|x) logfΘ(y|x) RDumb uses a SGD with a learning rate of 2.5 × 10−4, and a batch size of 64, and is reset to its pre-trained state every 1,000 iterations. 21The Entropy Enigma: Success and Failure of Entropy Minimization K. Software Licenses • ImageNet-C (Hendrycks & Dietterich, 2019)Apache License 2.0 https://github.com/hendrycks/robustness • ImageNet-R (Hendrycks et al., 2021a) MIT License https://github.com/hendrycks/imagenet-r • ImageNet-3D-CC (Kar et al., 2022): CC-BY-NC 4.0 License https://github.com/EPFL-VILAB/3DCommonCorruptions • ImageNet- C (Mintun et al., 2021): MIT License https://github.com/facebookresearch/augmentation-corruption • ImageNet-V2 (Recht et al., 2019): MIT License https://github.com/modestyachts/ImageNetV2 • Backgrounds Challenge (Xiao et al., 2020): https://github.com/MadryLab/backgrounds_challenge • CCC (Press et al., 2023): MIT License https://github.com/oripress/CCC • Stylized ImageNet (Geirhos et al., 2019): MIT License https://github.com/rgeirhos/Stylized-ImageNet • NINCO (Bitterwolf et al., 2023): MIT License https://github.com/j-cb/NINCO • ImageNet-D (Rusak et al., 2022a): Apache License 2.0 https://github.com/bethgelab/robustness • ObjectNet (Barbu et al., 2019): MIT License https://objectnet.dev/ • Shift Happens Benchmark: Apache License 2.0 https://github.com/shift-happens-benchmark/ icml-2022 22",
      "references": [
        "Semi-supervised logistic regression",
        "Agreement-on-the-line: Predicting the performance of neural networks under distribution shift",
        "Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models",
        "Reverse engineering self-supervised learning",
        "In or out? fixing imagenet out-of-distribution detection evaluation",
        "Santa: Source anchoring network and target alignment for continual test time adaptation",
        "Beyond entropy: Style transfer guided single image continual test-time adaptation",
        "A continual learning survey: Defying forgetting in classification tasks",
        "Maximum likelihood from incomplete data via the em algorithm",
        "Imagenet: A large-scale hierarchical image database",
        "Reducing flipping errors in deep neural networks",
        "Diversity-aware buffer for coping with temporally correlated data streams in online test-time adaptation",
        "An image is worth 16x16 words: Transformers for image recognition at scale",
        "Test-time training with masked autoencoders",
        "Leveraging unlabeled data to predict out-of-distribution performance",
        "Generalisation in humans and deep neural networks",
        "Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
        "Unsupervised representation learning by predicting image rotations",
        "Test time adaptation via conjugate pseudo-labels",
        "Semi-supervised learning by entropy minimization",
        "Evaluating model robustness to patch perturbations",
        "Predicting with confidence on unseen distributions",
        "Neural collapse under mse loss: Proximity to and dynamics on the central path",
        "Deep residual learning for image recognition",
        "Benchmarking neural network robustness to common corruptions and perturbations",
        "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
        "Augmix: A simple data processing method to improve robustness and uncertainty",
        "The many faces of robustness: A critical analysis of out-of-distribution generalization",
        "Natural adversarial examples",
        "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
        "Assessing generalization of sgd via disagreement",
        "3d common corruptions and data augmentation",
        "Reliable test-time adaptation via agreement-on-the-line",
        "Learning multiple layers of features from tiny images",
        "The hungarian method for the assignment problem",
        "On information and sufficiency",
        "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
        "Predicting out-of-distribution er- ror with confidence optimal transport",
        "Universal test-time adaptation through weight ensembling, diversity weighting, and prior correction",
        "Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization",
        "On interaction between augmentations and corruptions in natural corruption robustness",
        "Test-time adaptation to distribution shift by confidence maximization and input transformation",
        "Evaluating prediction-time batch normalization for robustness under covariate shift",
        "Efficient test-time model adaptation without forgetting",
        "Towards stable test-time adaptation in dynamic wild world",
        "Realistic evaluation of semi-supervised learning algortihms",
        "Prevalence of neural collapse during the terminal phase of deep learning training",
        "Imagenet-patch: A dataset for benchmarking machine learning robustness against adversarial patches",
        "Mixtures of gaussians and minimum relative entropy techniques for modeling continuous uncertainties",
        "Rdumb: A simple approach that questions our progress in continual test-time adaptation",
        "Do imagenet classifiers generalize to imagenet?",
        "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis",
        "A simple way to make neural networks robust against diverse image corruptions",
        "Imagenet-d: A new challenging robustness dataset inspired by domain adaptation",
        "If your data distribution shifts, use self-learning",
        "Imagenet-cartoon and imagenet-drawing: two domain shift datasets for imagenet",
        "Improving robustness against common corruptions by covariate shift adaptation",
        "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
        "Ecotta: Memory-efficient continual test-time adaptation via self-distilled regularization",
        "Benchmarking robustness to adversarial image obfuscations",
        "Unsupervised domain adaptation through self-supervision",
        "Test-time training with self-supervision for generalization under distribution shifts",
        "Zoom is what you need: An empirical study of the power of zoom and spatial biases in image classification",
        "Id and ood performance are sometimes inversely correlated on real-world datasets",
        "An empirical study of example forgetting during deep neural network learning",
        "Maxvit: Multi-axis vision transformer",
        "Tent: Fully test-time adaptation by entropy minimization",
        "Learning robust global representations by penalizing local predictive power",
        "Continual test-time domain adaptation",
        "Noise or signal: The role of image backgrounds in object recognition",
        "Aggregated residual transformations for deep neural networks",
        "Robust test-time adaptation in dynamic scenarios"
      ],
      "meta_data": {
        "arxiv_id": "2405.05012v2",
        "authors": [
          "Ori Press",
          "Ravid Shwartz-Ziv",
          "Yann LeCun",
          "Matthias Bethge"
        ],
        "published_date": "2024-05-08T12:26:15Z",
        "github_url": "https://github.com/hendrycks/robustness"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper analyzes why entropy minimization (EM) helps test-time adaptation initially but fails after many steps, and uses these insights to develop a practical accuracy-estimation method. It shows that EM first compresses embeddings into tighter clusters that align with training distributions, boosting accuracy, but over many iterations the embeddings drift away from training embeddings, causing collapse. It introduces Weighted Flips (WF), a label-flip based accuracy estimator that weights flips by initial confidence, and demonstrates that WF can predict a model’s accuracy on unseen datasets with high accuracy across 23 diverse, ImageNet-scale datasets, achieving a mean absolute error of 5.75% (SoTA) and outperforming prior methods by a substantial margin.",
        "methodology": "The study treats EM as an embedding-space clustering process related to Expectation-Maximization in a mixture-of-Gaussians view. It tracks two metrics during EM: (1) Silhouette score to quantify cluster quality, and (2) Shift distance (via Hungarian matching) to quantify alignment of test-data cluster centers with training-class embeddings. It reveals a two-phase behavior: phase 1 (success) increases clustering quality and keeps embeddings aligned with training data, improving accuracy; phase 2 (failure) increases cluster shift as embeddings diverge, reducing accuracy. It introduces Label Flips (predictions that change during adaptation) and Weighted Flips (flip count weighted by initial per-image confidence) and fits a nonlinear function f that maps WF to accuracy using holdout data (ImageNet validation, IN-C Holdout). It applies Rdumb and other TTA methods (Tent, RPL, CPL) and demonstrates the WF estimator generalizes across architectures (ResNet-50, ViT, MaxViT, etc.) with minimal overhead (about 20 extra forward passes per 1,000 Rdumb steps). It includes extensive ablations on stopping iteration, holdout size, and memory constraints.",
        "experimental_setup": "Experiments on 23 ImageNet-scale dataset splits from Shift Happens benchmark and robustness corpora (IN-C, IN-3DCC, CCC, Stylized ImageNet, IN-R, IN-V2, IN-Sketch, IN-D with Real/Painting/Clipart/Sketch/Infograph/Quickdraw/Cartoon, Patch-IN, BG Challenge, IN-Obfuscations, NINCO, ObjectNet, among others). Backbones include ResNet-50/18/34, RNXt-101, ViT-B/16, MaxViT-T, etc. Adaptation methods include Tent, Rdumb, RPL, CPL. The WF method uses a weighted-flips-to-accuracy function f fitted on IN-C Holdout and ImageNet Validation data, enabling accuracy estimation on unseen datasets. Evaluation uses 1,000 adaptation iterations with evaluations every 10 iterations; memory-limited and limited-data scenarios are explored. Baselines for accuracy estimation include AC, DoC, ATC, and COT. Table 1 reports MAEs across divers datasets; Table 2 shows architecture-generalization results; additional CIFAR-10/100 experiments illustrate limitations on smaller datasets. The method operates with minimal overhead (three parameters for f; ~20 extra forwards per 1,000 Rdumb steps) and generalizes across multiple TTA methods without re-fitting in many cases.",
        "limitations": "Limitations include: (1) Silhouette/Shift metrics require access to training embeddings (Shift distance) or dataset-specific clustering, which may not be available in many TTA scenarios; (2) WF relies on a learned mapping f that, while robust across many datasets, may require re-fitting for substantially different domains or drastically different data distributions (noted by CIFAR-10/100 results where WF underperforms without refitting); (3) EM collapse can occur with prolonged adaptation, and the two-phase dynamics suggest limits to EM-based adaptation; (4) results are primarily image-classification benchmarks; applicability to other modalities or real-time streaming may require adaptation; (5) additional computational overhead, albeit small, and the method’s dependence on an initial smart initialization.",
        "future_research_directions": "Investigate more robust, dataset-agnostic mappings f that adapt online to new distribution shifts; develop strategies to stabilize EM to prevent collapse while preserving early gains; extend WF to other self-supervised losses and modalities (text, audio); integrate WF with regularizers that mitigate drift; design adaptive stopping criteria for EM to maximize useful adaptation without collapse; study theoretical connections between EM dynamics, neural collapse, and clustering under distribution drift; explore joint training of f with models for improved generalization; apply WF to real-time streaming and continual learning contexts and to broader sets of TTA methods; refine clustering-based metrics to be more practical when training data embeddings are inaccessible.",
        "experimental_code": "Extracted code snippets related to the Weighted Flips (WF) concept and experimental evaluation from the Repository Content:\n\n1) WF-like flip and stability metrics used for accuracy estimation (ImageNet-P tests).\n\ndef dist(sigma, mode='top5'):\n    if mode == 'top5':\n        return np.sum(np.abs(cum_sum_top5[:5] - cum_sum_top5[sigma-1][:5]))\n    elif mode == 'zipf':\n        return np.sum(np.abs(recip - recip[sigma-1])*recip)\n\n\ndef ranking_dist(ranks, noise_perturbation=False, mode='top5'):\n    result = 0\n    step_size = 1 if noise_perturbation else args.difficulty\n\n    for vid_ranks in ranks:\n        result_for_vid = []\n\n        for i in range(step_size):\n            perm1 = vid_ranks[i]\n            perm1_inv = np.argsort(perm1)\n\n            for rank in vid_ranks[i::step_size][1:]:\n                perm2 = rank\n                result_for_vid.append(dist(perm2[perm1_inv], mode))\n                if not noise_perturbation:\n                    perm1 = perm2\n                    perm1_inv = np.argsort(perm1)\n\n        result += np.mean(result_for_vid) / len(ranks)\n\n    return result\n\n\ndef flip_prob(predictions, noise_perturbation=False):\n    result = 0\n    step_size = 1 if noise_perturbation else args.difficulty\n\n    for vid_preds in predictions:\n        result_for_vid = []\n\n        for i in range(step_size):\n            prev_pred = vid_preds[i]\n\n            for pred in vid_preds[i::step_size][1:]:\n                result_for_vid.append(int(prev_pred != pred))\n                if not noise_perturbation: prev_pred = pred\n\n        result += np.mean(result_for_vid) / len(predictions)\n\n    return result\n\n2) Experimental setup and evaluation hooks in ImageNet-P/test.py (extracted portions):\n- Model: resnext_50_32x4d, resnext_101_32x4d, resnext_101_64x4d; 3 seeds/models; .test inflated via test loaders.\n- Datasets: In-domain validation (test_loader_in) from ImageNet val with standard transforms; Out-of-distribution (test_loader_out) using ImageNet-22K class mappings.\n- Distortions: distortions list includes gaussian_noise, shot_noise, motion_blur, zoom_blur, spatter, brightness, translate, rotate, tilt, scale, as well as others in related scripts.\n- Metrics printed: Flipping Prob, Top5 Distance, Zipf Distance; final aggregate mCE-like metric computed as mean of error rates across datasets.\n- Outputs stored in state: test_in_loss, test_in_accuracy, test_out_accuracy.\n- Evaluation flow: compute predictions on in-distribution and out-of-distribution splits, derive stability metrics, print and save state.\n\n3) A minimal code excerpt that demonstrates WF-style evaluation within the repo:\n# Flipping/stability metrics for WF-like evaluation across model predictions under perturbations\n# flip_prob, ranking_dist, dist functions as above\n",
        "experimental_info": "Experimental settings inferred from the Repository Content:\n- Task focus: model stability and accuracy-estimation under distribution shifts and perturbations, implemented via flip-based metrics on image-classifiers.\n- Datasets used for evaluation: ImageNet-P (perturbations) and related sources; ImageNet-C also appears elsewhere in the repo as robustness benchmarks. The ImageNet-P/test.py module contains explicit evaluation code for in-distribution and out-of-distribution data, as well as perturbation perturbations.\n- Models used for evaluation: ResNet/ResNeXt family (e.g., resnext_50_32x4d, resnext_101_32x4d, resnext_101_64x4d) with PyTorch, including possible FineTune/transfer-presets in related code. The test scripts load pretrained weights from URLs or local checkpoints.\n- Evaluation metrics derived from code: flip probability (flip_prob), top-5 distance and Zipf-distance (ranking_dist), and a dist function for top-5 or Zipf modes. These metrics mirror aspects of stabilization/drift in the embedding space across perturbations or adaptation steps.\n- Experimental procedure characteristics visible in the code:\n  - Evaluation across a set of distortions/perturbations (ImageNet-P style): gaussian_noise, shot_noise, motion_blur, zoom_blur, spatter, brightness, translate, rotate, tilt, scale, etc.\n  - In-distribution vs out-of-distribution assessment via separate test_in and test_out data loaders.\n  - Computation of stabilization metrics over perturbations per batch and per perturbation severity, with results aggregated into a final stability metric (flip probability and ranking-based distances).\n  - The experiments are implemented in Python using PyTorch, with models evaluated on ImageNet-scale data, and outputs saved to a state dictionary for reporting (e.g., test_in_loss, test_in_accuracy, test_out_accuracy).\n- Relationship to WF method: The code in the repo reflects core WF-like ideas by tracking flips in predictions across adaptation/perturbation sequences, weighting flips implicitly by initial confidence via ranking-based measures, and mapping stability to accuracy estimates via the ranking-dist and flip_prob mechanisms, which aligns with the paper’s WF-based accuracy-estimation approach."
      }
    },
    {
      "title": "Progressive Transformation Learning for Leveraging Virtual Images in Training",
      "full_text": "Progressive Transformation Learning for Leveraging Virtual Images in Training Yi-Ting Shen⋆,1 Hyungtae Lee⋆,2 Heesung Kwon2 Shuvra S. Bhattacharyya1 ⋆ equal contribution 1 University of Maryland, College Park 2 DEVCOM Army Research Laboratory Code: https://gitlab.umiacs.umd.edu/dspcad/ptl-release Abstract To effectively interrogate UAV-based images for detect- ing objects of interest, such as humans, it is essential to acquire large-scale UAV-based datasets that include human instances with various poses captured from widely varying viewing angles. As a viable alternative to laborious and costly data curation, we introduce Progressive Transfor- mation Learning (PTL), which gradually augments a train- ing dataset by adding transformed virtual images with en- hanced realism. Generally, a virtual2real transformation generator in the conditional GAN framework suffers from quality degradation when a large domain gap exists be- tween real and virtual images. To deal with the domain gap, PTL takes a novel approach that progressively iterates the following three steps: 1) select a subset from a pool of vir- tual images according to the domain gap, 2) transform the selected virtual images to enhance realism, and 3) add the transformed virtual images to the training set while remov- ing them from the pool. In PTL, accurately quantifying the domain gap is critical. To do that, we theoretically demon- strate that the feature representation space of a given object detector can be modeled as a multivariate Gaussian dis- tribution from which the Mahalanobis distance between a virtual object and the Gaussian distribution of each object category in the representation space can be readily com- puted. Experiments show that PTL results in a substantial performance increase over the baseline, especially in the small data and the cross-domain regime. 1. Introduction Training an object detector usually requires a large-scale training image set so that the detector can acquire the abil- ity to detect objects’ diverse appearances. This desire for a large-scale training set is bound to be greater for object cat- egories with more diverse appearances, such as the human category whose appearances vary greatly depending on its pose or viewing angles. Moreover, a person’s appearance Transformation  Candidate Selection Virtual2Real Transformation Set Update Real images w/ conditional GAN Real images Virtual images - - + : add -: subtract +  + Virtual images select generator generator Figure 1. Overview of Progressive Transformation Learning. becomes more varied in images captured by an unmanned aerial vehicle (UA V), leading to a wide variety of camera viewing angles compared to ground-based cameras, mak- ing the desire for a large-scale training set even greater. In this paper, we aim to satisfy this desire, especially when the availability of UA V-based images to train a human detector is scarce, where this desire is more pressing. As an intuitive way to expand the training set, one might consider synthesizing virtual images to imitate real-world images by controlling the optical and physical conditions in a virtual environment. Virtual images are particularly use- ful for UA V-based object detection since abundant object instances can be rendered with varying UA V locations and camera viewing angles along with ground-truth informa- tion (e.g., bounding boxes, segmentation masks) that comes free of charge. Therefore, a large-scale virtual image set covering diverse appearances of human subjects that are rarely shown in existing UA V-based object detection bench- marks [1,4,55] can be conveniently acquired by controlling entities and parameters in a virtual environment, such as poses, camera viewing angles, and illumination conditions. To make virtual images usable for training real-world ob- ject detection models, recent works [20, 30, 39, 40] trans- form virtual images to look realistic. They commonly use the virtual2real generator [37] trained with the conditional GAN framework to transform images in the source domain to have the visual properties of images in the target do- main. Here, virtual and real images are treated as the source and target domains, respectively. However, the large dis- arXiv:2211.01778v2  [cs.CV]  27 Mar 2023crepancy in visual appearance between the two domains, referred to as the “domain gap”, result in the degraded transformation quality of the generator. In fact, the afore- mentioned works using virtual images validate their meth- ods where the domain gap is not large (e.g., digit detec- tion [20]) or when additional information is available (e.g., animal pose estimation with additional keypoint informa- tion [30, 39, 40]). In our case, real and virtual humans in UA V-based images inevitably have a large domain gap due to the wide variety of human appearances. To address the large domain gap, one critical question inherent in our task is how to measure accurately the do- main gap.Consequently, we estimate the domain gap in the representation space of a human detector trained on the real images. The representation space of the detector is learned such that test samples, which have significantly different properties than training samples from the perspective of the detector, are located away from the training samples. In this paper, we show that the feature distribution of object entities belonging to a certain category, such as the human category, in the representation space can be modeled with a multivariate Gaussian distribution if the following two con- ditions are met: i) the detector uses the sigmoid function to normalize the final output and ii) the representation space is constructed using the output of the penultimate layer of the detector. This idea is inspired by [29], which shows that softmax-based classifiers can be modeled as multivari- ate Gaussian distributions. In this paper, we show that the proposition is also applicable to sigmoid-based classifiers, which are widely used by object detectors. Based on this modeling, when the two aforementioned conditions are met, the human category in the representation space can be rep- resented by two parameters (i.e., mean and covariance) of a multivariate Gaussian distribution that can be computed on the training images. With the empirically calculated mean and covariance, the domain gap from a single virtual hu- man image to real human images (i.e., the training set) can be measured using the Mahalanobis distance [36]. To add virtual images to the training set to include more diverse appearances of objects while preventing the trans- formation quality degradation caused by large domain gaps, we introduce Progressive Transformation Learning (PTL) (Figure 1). PTL progressively expands the training set by adding virtual images through iterating the three steps: 1) transformation candidate selection, 2) virtual2real transfor- mation, and 3) set update. When selecting transformation candidates from a virtual image pool, we use weighted ran- dom sampling, which gives higher weights to images with smaller domain gaps. The weight takes an exponential term with one hyperparameter controlling the ratio between im- ages with smaller domain gaps and images with more di- verse appearances. Then, the virtual2real transformation generator is trained via the conditional GAN, taking the se- lected transformation candidates as the “source” and the im- ages in the training set as the “target”. After transforming the transformation candidates by applying the virtual2real transformation generator, the training set is expanded with the transformed candidates while the original candidates are excluded from the pool of virtual images. The main contribution of this paper is that we have val- idated the utility of virtual images in augmenting training data via PTL coupled with carefully designed comprehen- sive experiments. We first use the task of low-shot learn- ing, where adequately expanding datasets has notable ef- fects. Specifically, PTL provides better accuracy on three UA V-view human detection benchmarks than other previ- ous works that leverage virtual images in training, as well as methods that only use real images. Then, we validate PTL on the cross-domain detection task where training and test sets are from distinct domains and virtual images can serve as a bridge between these two sets. The experimen- tal results indicate that a high-performance human detection model can be effectively learned via PTL, even with the sig- nificant lack of real-world training data. 2. Related Works Leveraging virtual images in training.In this section, we have listed previous works that demonstrate how virtual im- ages can play a role in a variety of real-world applications when used for training. In fact, virtual images are desir- able for model training as large-scale labeled datasets can be built virtually almost free of charge. Unfortunately, when virtual images are used without proper care, it is shown that the performance improvement is limited due to the domain gap between the virtual images and the real test images. Generally, previous works leveraging virtual images during model training can be summarized into three approaches ac- cording to how they exploit the advantages of virtual images and address the challenges of using virtual images. The most intuitive and widely used approach of using virtual images is to pre-train a model on virtual images and fine-tune the pre-trained model on real images acquired in the same domain as the test images [3, 11, 13, 17, 18, 22, 27, 38, 39, 46]. This approach aims to avoid the domain gap by fine-tuning the model on real images acquired under the same conditions and environments of test images. While the first approach seeks to use the representative capabil- ity learned from large-scale virtual datasets, the second ap- proach seeks to exploit additional information that can be easily labeled on virtual images. For example, [34] anno- tates part segmentation maps when acquiring virtual vehicle images, and uses the part segmentation results from the pre- trained model on the virtual images during the fine-tuning process. Similarly, [52] uses depth and semantic informa- tion labeled when acquiring virtual images. The third approach directly builds training batches con-sisting of both virtual images and real images. [42] and [41] adopt the most naive approach to build a batch by randomly selecting a fixed number of images from each of the real and virtual image sets. However, even if the number of virtual images is several orders of magnitude greater than the number of real training images, this approach does not provide remarkably better accuracy and may even provide worse accuracy than its counterparts using only real images for training. In this case, the effect of using virtual images during model training does not appear as expected because the large domain gap between the real (test) images and the virtual images is not adequately addressed. In this paper, we also use virtual images directly for model training while considering appropriately reducing this domain gap. Progressive learning. Progressive learning is a machine learning strategy that continuously trains a model from easy to hard tasks, primarily for the purpose of training stabiliza- tion or fast optimization. One of the most common appli- cations for progressive learning is incrementally increasing the network capacity to improve network capability. The most intuitive approach in this category is to gradually in- crease the network size (e.g., depth or width) to ease the training difficulty of very deep networks [5,9,12,14,32,44, 50]. Conversely, [53] uses progressive learning in the direc- tion of reducing the network size for fast training. Progres- sive learning is also used in GAN frameworks to enhance the generator’s ability to transform input images of larger resolution [24]. Curriculum learning [6, 15, 26, 45], which continuously raises the level of training from easy to diffi- cult samples, also falls into this category. Progressive learning is also used to deal with the scala- bility of datasets that are incompletely labeled. In a semi- supervised learning task, [8,47,49] adopt progressive learn- ing by gradually increasing the number of unlabeled data used for training. Self-learning in [25, 28] uses progres- sive learning by repeating the two steps, assigning labels depending on the current detector and updating the current detector with these labels. Our method can be seen as similar to the second ap- proach in that it also intend to expand the training dataset. However, our method uses progressive learning to reliably add realistically transformed virtual images to the training dataset by avoiding quality degradation of the transforma- tion, which has never been attempted before using the pro- gressive learning strategy. 3. Method 3.1. Measuring the Domain Gap between Real and Virtual Images Modeling training set with multivariate Gaussian distri- bution. The purpose of adopting progressive transforma- tion learning, which progressively expands the training set with a subset of the realistically transformed virtual images instead of expanding it with the full set at once, is to avoid the significant domain gap between the real and virtual im- ages when training the transformation generator. Here, the domain gap is measured in the representation space of the detector, which is learned so that two samples with different properties from the perspective of the detector are separated far from each other. In general, the representation space of the detector refers to the space formed by the output of the penultimate layer of the detector since all layers of the detector except for the last layer can be transferred for different downstream tasks [10, 16, 19]. In [29], it is shown that for the softmax- based classifier, the distribution of each category in the rep- resentation space can be modeled as a multivariate Gaus- sian distribution. Object detector generally uses the sigmoid function (i.e., fsigmoid (x) = 1/(1+exp( −wT c x−bc) for the category c), which does not consider outputs for other cate- gories, instead of the softmax function (i.e., fsoftmax (x) = exp(wT c x + bc)/ P c′ exp(wT c′ x + bc′ ) for the category c) that competes for outputs for all categories to normalize the model output to [0 1]. This is because, unlike classifica- tion, the detection task must take into account that two or more co-located objects may be active on a single output. In the supplementary material, we show that even for the sigmoid-based detector, the distribution of each category in the representation space can also be modeled as a multivari- ate Gaussian distribution. Specifically, let x ∈ Xand y = {yc}c=1,···,C ∈ Y, yc ∈ {0, 1} be an input and its label, respectively. Then, the rep- resentation space of the sigmoid-based detector can be ex- pressed as follows: P(f(x)|yc = 1) ∼ N(f(x)|µc, Σc), (1) where f(·) denotes the output of the penultimate layer of the detector. µc and Σc are the mean and the covariance of the multivariate Gaussian distribution for the category c. µc and Σc can be calculated over the entire set of training images as follows: µc = 1 |Dc| X x∈Dc f(x), Σc = 1 |Dc| X x∈Dc (f(x) − µc) (f(x) − µc)⊤, (2) where Dc is the set of instances for the category c. Practi- cally, any detection whose IoU with the groundtruth of the category c is greater than 0.5 belongs to Dc. Measuring domain gap.After µc and Σc are empirically calculated to represent D c, the domain gap between a new image xnew and Dc can be measured using the Mahalanobis distance, as follows: d(xnew ) = (f(xnew ) − µc)⊤ Σ−1 c (f(xnew ) − µc) . (3)Detector  training fD t, (𝝻t, 𝞢t)  Weighted random  sampling Generator  training fG t Transformation Rt+1 Set union Set subtraction Target Source Rt∪ CR t Vt/ CV t Transformation  Candidate Selection Virtual2Real Transformation Set UpdateRt R0: Seed Vt+1CR tCR tCV tCV t Vt Figure 2. Progressive Transformation Learning (PTL) pipeline.The red arrow indicates the processing flow of the virtual images selected to be added to the training set. This measurement of the domain gap is highly dependent on the detector’s ability to detect objects in the image. It is commonly known that the detection capability of a detector is greatly affected by the image size as well as the object ap- pearance in the image. To mitigate the effect of image size on measuring the domain gap, the Mahalanobis distance for xnew is calculated at multiple image scales, and the mini- mum distance is used as the domain gap, as follows: d(xnew ) = min s∈S ({d(xs new )}), (4) where xs new is the resized image of xnew to be s×s. S is the set of resizing factors. In our experiments, we use S = {128, 256, 384, 512}. 3.2. Progressive Transformation Learning Our objective is to expand the training set consisting of real images by adding virtual images which are trans- formed to intimate real images. The virtual2real transfor- mation can be performed by a generator trained by treating virtual images and real images as “source” and “target”, re- spectively, in the conditional GAN framework. Inevitably, the transformation quality of the trained generator is de- graded when the domain gap between the source domain and the target domain is large. To prevent the degraded transformation quality due to the large domain gap, we in- troduce Progressive Transformation Learning (PTL), which progressively and iteratively expands the training set with a subset of virtual images carefully selected to avoid the large domain gap. PTL goes through three steps for each iter- ation (Fig 2): i) sampling a subset of virtual images from a virtual image pool by giving heavier weights to images close to the current training set (Transformation candidate selection), ii) transforming the selected images to be realis- tic (Virtual2real transformation), and iii) adding the trans- formed images to the training set while excluding the se- lected images from the virtual image pool (Set update). The details of each step are described next. Transformation candidate selection. When selecting transformation candidates, we must consider two conflict- ing claims simultaneously: i) to suit the purpose of PTL, virtual images with a small domain gap should be selected, but ii) to suit the purpose of expanding the training set, vir- tual images with appearances that rarely appear in the train- ing set, which usually implies a large domain gap, should also be selected. To jointly consider these two claims, we use weighted random sampling. The sampling weight takes the expo- nential term which gives higher weights to virtual images with smaller domain gaps, while introducing one hyper- parameter τ to control the amplitude of the weights, as fol- lows: w(x) = exp \u0012 −d(x) τ \u0013 , (5) where d(x) is the Mahalanobis distance, which is used to measure the domain gap of x from the current training set (eq 4). Intuitively, using a small τ allows a more frequent selection of images with smaller domain gaps by giving them larger weights than using a large τ. (We use τ=5.0 throughout all experiments.) In practice, transformation candidates are selected from virtual image pool through the following four steps: i) train- ing the human detector ft D on the current training set of real images Rt, ii) calculating µt and Σt on Rt as in eq. 2, iii) calculating weights for each image in the current set of vir- tual images Vt as in eq. 5, and iv) applying weighted ran- dom sampling to V t to select a pre-defined number n of transformation candidates. (We use n=100 throughout all experiments.) Virtual2Real transformation.In line with the goal of this paper to obtain a human detector that can identify humans with diverse appearances captured by a UA V , we design the virtual2real transformation to focus on the person region rather than the background of the selected virtual images. To do so, we crop the person region in the virtual image, ap- ply the transformation only to this region, and segment the transformed person back to the original background. Foraccurate segmentation, the pixel-wise segmentation mask is required. Obtaining such pixel-wise segmentation mask at no cost is another benefit of using virtual images. The conditional GAN framework [37], in which the gen- erator is trained to transform a given input image from source styles into target styles, is widely used to transform virtual images to look like real images [21, 30]. Among many variants of conditional GANs, we use CycleGAN [54] in which the generator is trained to minimize the reconstruc- tion error between the input image and the reconstructed image transformed back to the original style of the input image after the initial transformation to the target style. It is shown in [37] that the transformation with CycleGAN is likely to maintain the original object pose while changing detailed styles such as patterns (e.g., transforming a white horse into a zebra in the same pose). We intend to borrow this characteristic of CycleGAN to transform virtual images in the direction that makes the detailed styles realistic while maintaining the overall human appearances, which depend on various viewing angles or human poses. In practice, the virtual2real transformation generator ft G is trained using the CycleGAN framework by treating the selected transformation candidates C V t and the current set of real images R t as “source” and “target”, respectively. Then, CV t are transformed to realistic transformed images CR t by applying the virtual2real transformation generator. Set update.After the transformed images CR t are acquired from the selected transformation candidates C V t, PTL up- dates the current real image set R t and the current virtual image set Vt as follows: Rt+1 = Rt ∪ CR t Vt+1 = Vt/CV t (6) When this progressive learning is terminated, the final hu- man detection model can be acquired by training on the final set of real images. In practice, the first two steps of PTL are applied to the tightly cropped image region of human region, but in the ‘set update’ step, the entire image including the human re- gion and the background is added to the training set for training the human detector. More precisely, when train- ing the virtual2real transformation generator, the tightly cropped image region around each human from the train- ing images are used as the “target”. 4. Experiments Datasets and evaluation metrics.We perform experiments on three real UA V-based datasets, VisDrone [55], Okutama- Action [4], and ICG [1], and one virtual dataset,Archangel- Synthetic [43], all including human instances. Archangel- Synthetic consists of various virtual characters with differ- ent poses across a range of altitudes and circle radii with different camera pitch angles (i.e., 17.3K images with eight characters, three poses, ten altitudes, six circle radii, and twelve camera pitch angles). Each image in Archangel- Synthetic accompanies metadata about the above imaging conditions, allowing us to analyze how the feature distribu- tions of virtual characters evolve with respect to these imag- ing conditions when PTL progresses. We use AP@.5 and AP@[.5:.95] as evaluation metrics for all experiments. Detector. For the detector, we use RetinaNet [33] that uses the feature pyramid network (FPN) to provide a rich multi- scale feature pyramid and processes features at all scale lev- els with the same subnetwork responsible for the final clas- sification and bounding box regression. It is important to use the same subnetwork across all scale levels since the domain gap for each virtual image should be measured in a shared representation space regardless of the image size. Most other object detectors using FPN (e.g., SSD [35] and v4 or later versions of YOLO [7, 23, 31, 48]) use different subnetworks at each scale level. However, PTL is not struc- turally limited to RetinaNet as it can be used with any de- tector with minor modifications, such as adding one shared layer across all scale levels. 4.1. Properties of Progressive Learning Which virtual images are selected for each PTL itera- tion? The top row of Figure 3 shows the change in the accu- mulated distribution of virtual images added to the training set via each PTL iteration with respect to camera locations. By examining the distribution, we can identify which cam- era locations of the virtual images contribute more to the training set at each PTL iteration. It is observed that after the 1st PTL iteration, most vir- tual images included in the training set are taken from the camera locations close to the human subjects. As PTL pro- gresses, the camera locations of the virtual images included in the training set gradually spread across the UA V altitudes and rotation circle radii. Consequently, after the 5th PTL iteration, transformed virtual images with diverse appear- ances from much broader camera locations are included in the final training set. This demonstrates that the proposed transformation candidate selection process is adequately de- signed to consider the two conflicting claims together. How close does the domain gap get as PTL progresses? The bottom row of Figure 3 shows the domain gap between the virtual images and the training set at each PTL itera- tion. We can observe the domain gap distribution of virtual images to the training set gradually becomes narrower and smaller. Additionally, some virtual images which have not been included in the training set also appear in the long tail of the distribution. Accuracy variation as PTL evolves. Figure 4 shows how the accuracy changes as PTL progresses on the three datasets. Overall, for AP@.5, accuracy increases rapidly until the 3rd iteration and does not change significantlyiter. 1 iter. 2 iter. 3 iter. 4 iter. 5 Figure 3. Analysis of the use of virtual images when PTL progresses.The figures in the top row show the accumulated distribution of transformation candidates with respect to camera locations (i.e., altitude and rotation circle radius from the target human in x and y axes, respectively) for each PTL iteration. Darker bins indicate that more virtual images have been added to the training set. The figures in the bottom row (x axis: domain gap, y axis: the corresponding number of virtual images) show the domain gap distribution of virtual images measured by eq. 4. These figures are collected from the experimental setup of using 50 real images from the VisDrone dataset for training. AP@.5: VisDrone AP@[.5:.95]: VisDroneAP@.5: Okutama-ActionAP@[.5:.95]: Okutama-ActionAP@.5: ICG AP@[.5:.95]: ICG Figure 4. Learning curvesof the two metrics (AP@.5 and AP@[.5:.95]) on the three datasets thereafter. On the other hand, for AP@[.5:.95], accuracy continues to increase even after the 3rd iteration on the Okutama-Action and ICG datasets. This can be interpreted such that human bounding boxes are estimated more accu- rately as PTL progresses on these two datasets. 4.2. Results on Low-shot Learning Baselines. We compare PTL with the method utilizing only real images for training (i.e., ‘baseline’) and other three methods also leveraging virtual images in conjunc- tion with real images for training (i.e., ‘pretrain-finetune’, ‘naive merge’, and ‘naive merge w/ transform’) in terms of human detection accuracy. ‘Pretrain-finetune’ is the strategy to train a model by pre-training on virtual im- ages and then fine-tuning on real images, which is the most widely used approach leveraging virtual images in previous works [3, 11, 13, 17, 18, 22, 27, 38, 39, 46]. ‘Naive merge’ is the strategy that uses a training set naively merging from real and virtual images for model training, which has also been used in previous works [41, 42]. ‘Naive merge w/ transform’ naively adds transformed virtual images to the training set, where the transformation generator is trained with the CycleGAN framework by considering all virtual images and all real images as “source” and “target”. Main Results. In Table 1, we compare PTL to the base- lines in terms of human detection accuracy in two low-shot detection regimes (i.e., 20 and 50 real images are used for training) on the three real-world UA V-based datasets. Low- shot detection is a suitable task to validate the proper use of virtual images as notable effects can be expected from adequately expanded datasets. In all cases, the previous methods leveraging virtual im- ages do not present significantly better, or even worse, ac- curacy than their counterpart (i.e., ‘baseline’) using real images only for training. ‘Pretrain-finetune’ is the only method, except for PTL, that presents better accuracy than the baseline in some cases. This two-step method is effec- tive in avoiding adverse effects due to the large domain gap while indirectly taking advantage of the large-scale dataset. However, the increase in accuracy is marginal as the task- specific properties (i.e., human detection) of the large-scale virtual dataset used for pre-training are not fully exploited due to the catastrophic forgetting issue inherent in this in- direct method. In addition, ‘naive merge w/ transforma- tion’, which is the only previous method that uses trans- formed virtual images, presents significantly reduced ac- curacy, which can be regarded as the adverse effects (e.g., transformation quality degradation) when the large domain gap is not properly addressed. The final model obtained af- ter the 5th PTL iteration or the best model achieved when PTL progresses consistently presents significantly better performance than any compared method. This demonstrates the effectiveness of PTL such that accuracy is substantially improved by expanding the training set using virtual images while the large domain gap is appropriately addressed. Ablation: The effect ofτ. In Table 2, we compare five dif- ferent τ values to investigate the effect of τ used to control the sampling weights when selecting transformation candi- dates (eq. 5). It is found that as large τ values (≥ 100) are used, the accuracy on the VisDrone dataset decreasesmethod train set VisDrone Okutama-Action ICG 20 50 20 50 20 50 baseline R 3.74/ 1.09 6.42/ 1.86 41.61/ 11.23 49.84/ 13.76 49.35/ 14.69 66.75/ 23.91 pretrain-finetune R+V 4.99/ 1.46 6.25/ 1.99 44.57/ 12.78 49.06/ 15.08 66.92/ 26.67 68.41/ 29.73 naive merge R+V 3.41/ 1.02 5.18/ 1.65 34.26/ 9.21 48.33/ 14.61 55.95/ 20.76 65.68/ 26.73 w/ transform R+V 1.26/ 0.49 4.02/ 1.37 27.37/ 7.84 41.36/ 12.64 48.02/ 17.62 65.03/ 27.21 PTL(5th itr.) R+V 6.83/ 1.94 9.09/ 2.85 52.89/ 15.57 59.90/ 18.48 69.11/27.33 74.14 / 31.41 +3.09/+0.85 +2.67/+0.99 +11.28/ +4.34 +10.06/ +4.72 +19.76/+12.64 +7.39/ +7.50 PTL(best) R+V 7.52/ 2.13 9.33 / 2.94 53.82/ 15.59 60.65 / 18.48 70.23/ 27.33 74.14 / 31.41 +3.78/+1.04 +2.91/+1.08 +12.21/ +4.36 +10.81/ +4.72 +20.88/+12.64 +7.39/ +7.50 Table 1. Low-shot learning accuracywith 20 and 50 real images. AP@.5 and AP@[.5:.95] are reported in each bin. For PTL, the margin from the baseline accuracy is shown below the reported accuracy. The best accuracy for each setting is shown in bold. R and V denote the set of real images and the set of virtual images, respectively. τ VisDrone Okutama-Action ICG 1 9.48/ 3.01 39.37/ 10.45 27.87/ 7.75 5 9.09/ 2.85 42.39/ 11.41 29.26/ 7.27 10 9.68/ 2.87 37.48/ 9.51 33.06/ 7.66 100 8.97/ 2.54 38.25/ 10.18 33.78/ 9.29 1000 8.90/ 2.63 39.15/ 10.00 43.90/ 11.97 Table 2. Varyingτ in PTL (after 5th iteration). Models are trained on the VisDrone dataset with 50 shot learning setup. while the accuracy on the ICG dataset increases. The find- ing indicates that adding more virtual images with a large domain gap to the training set using a large τ has no sig- nificant effect or even adverse effect in terms of accuracy when the training set and the test set are in the same domain. However, a large τ shows a remarkable effect in the cross- domain setting, especially when the training set and the test set have very different characteristics, such as the VisDrone train set and the ICG test set. We use τ=5 throughout the experiment for the same-domain setup, but we can also con- sider using a large τ for the cross-domain setup. Ablation: The effect ofn. n, which is the number of virtual images added to the training set per PTL iteration, can affect not only the optimality of training but also the scalability of models trained by PTL toward other datasets different from the real dataset used for training. Accordingly, to find the optimal value of n by taking these effects into account, we carry out ablation experiments under the cross-domain setup as shown in Table 3. We compare three cases when n is 50, 100, and 200. When the training set and the test set comes from the same domain (i.e., training and testing on the VisDrone dataset), the human detection accuracy is not very sensitive to n. The best accuracy is obtained with the fewest virtual im- ages (i.e., n=50). The insensitivity to the hyperparameter n is also observed in the Okutama-Action dataset, having relatively similar properties to the VisDrone dataset under the cross-domain setup. However, for ICG, which is con- sidered to have noticeably different properties from the Vis- Drone dataset under the cross-domain setup, the accuracy is # img per itr VisDrone Okutama-Action ICG 50 9.59/ 2.90 41.62/ 11.19 25.94/ 6.04 100 9.33/2.94 42.39/ 11.46 30.01/ 7.36 200 9.04/ 2.91 41.29/ 11.18 35.50/ 10.20 Table 3. Varying # of virtual images added to the training set per PTL iteration. Models are trained on the VisDrone dataset with 50 shot learning setup. The reported accuracies are obtained by using the best PTL models. significantly improved as more virtual images (i.e., n=200) are used per PTL iteration. This implies that using more virtual images through more PTL iterations tends to further reduce the cross-domain gap when the discrepancy between the training set and the test set is considerably large, such as VisDrone vs. ICG, resulting in improved PTL scalability. Considering the trade-off between the training optimality and the PTL scalability, we use n=100 throughout all other experiments in this paper. Qualitative analysis of transformation.Figures 10 show several samples of transformed virtual images included in the training set using methods with virtual2real transfor- mation (i.e., PTL and ‘naive merge w/ transformation’) in the ‘VisDrone and 50-shot’ setup. First, the quality of the transformed image by ’naive merge w/transformation’ gen- erally deteriorates a lot, and there are samples, in some se- vere cases, in which it is difficult to recognize the human appearance anymore. On the other hand, in the case of the virtual image transformed by PTL considering the domain gap when training the transformation generator, it can be seen that only the pattern is changed to be similar to the im- age of the real dataset while the human pose is maintained well. This qualitative analysis supports the validity of our claim that the domain gap between virtual images and real images should be considered when training the virtual2real transformation generator. 4.3. Results on Cross-domain Detection In Table 4, we show the accuracy for the cross-domain setups on the three datasets. The “cross-domain detection”Naive merge w/ transform PTL Figure 5. Sample Virtual2Real transformation output (VisDrone, 50-shot).Each set consists of three images: original virtual image (left), transformed image (middle), and transformed image with background (right). method VisDrone Okutama-Action ICG 20 50 20 50 20 50 Vis→Vis Oku→Oku ICG→ICG baseline 3.74/ 1.09 6.42/ 1.86 41.61/ 11.23 49.84/ 13.76 49.35/ 14.69 66.75/ 23.91 PTL (5th itr.) 6.83/ 1.94 9.09/ 2.85 52.89/ 15.57 59.90/ 18.48 69.11/ 27.33 74.14/ 31.41 PTL (best) 7.52/ 2.13 9.33/ 2.94 53.82/ 15.59 60.65/ 18.48 70.23/ 27.33 74.14/ 31.41 Oku→Vis Vis→Oku Vis→ICG baseline 1.62/ 0.47 2.04/ 0.57 17.13/ 4.53 36.82/ 9.87 2.92/ 0.56 7.46/ 1.83 PTL (5th itr.) 2.72/ 0.94 3.05/ 1.07 30.72/ 7.45 42.39/ 11.41 26.86/ 7.22 29.26/ 7.27 PTL (best) 3.00/ 1.22 3.56/ 1.17 33.25/ 8.59 42.39/ 11.46 29.60/ 7.69 30.01/ 7.36 ICG→Vis ICG→Oku Oku→ICG baseline 0.54/ 0.13 0.99/ 0.26 3.56/ 0.75 10.27/ 2.49 5.37/ 1.25 5.23/ 1.20 PTL (5th itr.) 1.09/ 0.33 1.61/ 0.50 11.19/ 2.58 14.20/ 3.56 28.98/ 8.14 25.39/ 6.53 PTL (best) 1.58/ 1.02 1.70/ 0.63 12.82/ 2.96 14.20/ 3.71 28.98/ 8.14 26.62/ 6.53 Table 4. Cross-domain detection accuracy.The table shows experiments with 3×3 cross-domain setups. For each setup, datasets shown to the left and right of the arrow are the training and test sets, respectively. The accuracies of PTL and the baseline without using virtual images for training are shown. Setups on the top use training and test images from the same domain, which provides a baseline accuracy in the cross-domain setups. All setups in each column are tested on the same dataset and the same low-shot regime. setups are used to validate the impact of using virtual im- ages for training when training and test images have distinct characteristics, such as when the distributions of human ap- pearances with respect to human poses and viewing angles are different. First, it is observed that using PTL yields much better accuracies than the baseline when using the same real train- ing dataset. Despite the inherent difficulty of cross-domain learning, it is also observed that leveraging virtual images through PTL produces results not far behind the accuracy of the baseline (shown in the first row of Table 4) which uses the training images from the same domain as the test images. However, the improvement in the cross-domain de- tection accuracy when using the ICG dataset for training is relatively low compared to other cases. This is because the ICG dataset has very different characteristics from other datasets, and virtual images added to the training set do not reduce this difference. Note that if the real images in the initial training set have very distinct appearances, a very dif- ferent set of virtual images may be selected in the first PTL iteration. Thus, the disparity may not be overcome even if the PTL progresses further. Nevertheless, in general, we can confirm that human detectors trained using virtual images through PTL can improve substantially detection accuracy, regardless of which real dataset is used during training. 5. Discussion Our method has been proved effective in leveraging vir- tual images during training as it presents much better accu- racy than any other previous methods for low-shot learning task, where scaling up the training dataset can significantly impact. In addition, compared to the baseline which does not use virtual images, our method also presents remarkable accuracy on cross-domain detection, where the real training and test datasets are from two distinct domains with very different characteristics. Despite the merit of the proposed method, there is still room to improve PTL further. Specifically, the current ver- sion of PTL can only leverage a subset of the entire virtual images within a limited number of iterations, beyond which the accuracy may decrease. We hope that more advanced methods can be developed to address this issue so that PTL can progress for more iterations with a continuous accuracy increase until all virtual images are leveraged for training. Acknowledgements. This research was sponsored by the Defense Threat Reduction Agency (DTRA) Contract Num- ber: A2205097021023559.A. Gaussian Discriminant Analysis for Model- ing Representation Space of Detector In this section, we describe modeling the representation space of a general object detector by fitting a multivariate Gaussian distribution. We denote the random variable of the input and its label of a linear classifier as x ∈ Xand y = {yc}c=1,···,C ∈ Y, yc = {0, 1}, respectively. Then, the posterior distribution defined by the linear classifier whose output formed by the sigmoid function can be expressed as follows: P(yc = 1|x) = 1 1 + exp (−wcx−bc) = exp (wcx + bc) exp (wcx + bc) + 1, (7) where wc and bc are weights and bias of the linear classifier for a category c, respectively. Gaussian Discriminant Analysis (GDA) models the pos- terior distribution of the classifier by assuming that the class conditional distribution ( P(x|y)) and the class prior dis- tribution (P(y)) follow the multivariate Gaussian and the Bernoulli distributions, respectively, as follows: P(x|yc = 0) = N(µ0, Σ0), P(x|yc = 1) = N(µ1, Σ1), P(yc = 0) = β0/ (β0 + β1) , P(yc = 1) = β1/ (β0 + β1) , (8) where µ{0,1} and Σ{0,1} are the mean and covariance of the multivariate Gaussian distribution, and β{0,1} is the unnor- malized prior for the category c and the background. For the special case of GDA where all categories share the same covariance matrix (i.e., Σ0 = Σ1 = Σc), known as Linear Discriminant Analysis (LDA), the posterior distri- bution (P(yc|x)) can be expressed with P(x|yc) and P(yc) as follows: P(yc = 1|x) = P(yc = 1)P(x|yc = 1) P(yc = 0)P(x|yc = 0) + P(yc = 1)P(x|yc = 1) = exp \u0010 (µ1−µ0)⊤ Σ−1 c x . . . −1 2µ⊤ 1 Σ−1 c µ1 + 1 2µ⊤ 0 Σ−1 c µ0 + logβ1/β0 \u0011 exp \u0010 (µ1−µ0)⊤ Σ−1 c x . . . −1 2µ⊤ 1 Σ−1 c µ1 + 1 2µ⊤ 0 Σ−1 c µ0 + logβ1/β0 \u0011 + 1 . (9) Note that the quadratic term is canceled out since the shared covariance matrix is used. The posterior distribution de- rived by GDA in eq. 9 then becomes equivalent to the posterior distribution of the linear classifier with the sig- moid function in eq. 7 when wc = ( µ1−µ0)⊤ Σ−1 c and bc = −1 2 µ⊤ 1 Σ−1 c µ1 + 1 2 µ⊤ 0 Σ−1 c µ0 +log β1/β0. This implies that the representation space formed byx can be modeled by a multivariate Gaussian distribution. Based on the above derivation, if x is the output of the penultimate layer of an object detector for a region pro- posal, and a linear classifier defined by wc and bc is the last layer of the object detector, it can be said that the rep- resentation space of the object detector for a category c can be modeled with a multivariate Gaussian distribution. In other words, the representation space for a category c can be represented by two parametersµ1 (i.e., µc) and Σc of the multivariate Gaussian distribution. Discussion. The sigmoid function can be viewed as a spe- cial case of the softmax function defined for a single cate- gory as both functions take the form of an exponential term for the category-of-interest normalized by the sum of ex- ponential terms for all considered categories. Therefore, it is straightforward to derive the modeling for the sigmoid- based detector from the previous work by [29], who shows that the softmax-based classifier can be modeled with a mul- tivariate Gaussian distribution in the representation space. However, our derivation is still meaningful in that it ex- tends the applicability of an existing modeling limited to a certain type of classifier (i.e., based on softmax) to gen- eral object detectors (i.e., based on sigmoid). Most object detectors, especially one-stage detectors, generally use the sigmoid function, which does not consider other categories when calculating the model output for a certain category, since more than one category can be active on a single out- put. B. Implementation Details Multi-scale training. We apply the multi-scale training strategy when preparing input images, in addition to the 5- level multi-scaling property provided by the detector’s FPN module, to train the detector. The goal is to make the detec- tor more robust to the variations of human size in the im- ages. Since the real and virtual datasets have widely vary- ing human sizes in the images, we apply different scaling factors for each dataset to share similar human sizes after image rescaling. Specifically, for the real dataset, the input image is resized by one of the scaling factors randomly se- lected from {768, 800, 832, 864} for the short side, in which the long side is constrained not to be larger than 1440, for every training iteration. For the virtual dataset, the scaling factors are {128, 256, 384, 512} for the short side and 512 for the long side. Network architecture. To obtain a generator, we adopt CycleGAN, where two generators and two discrimina- tors are involved during model training. For each gen-(a) Detector training config baseline pretrain-finetunenaive merge PTLpretrain finetune optimizer SGD momentum 0.9 weight decay 0.0001 baselr 0.001 lrschedule multi-steplr gamma 0.1 warmup iter. 1000 total iter. 6000 6000 600 6000 6000 steps 5000 5000 500 5000 5000 batch size 16 filter empty annot. False box threshold 10 aspect ratio grouping False (b) Generator training config naive merge w/ transformPTL optimizer Adam momentum β1, β2 = 0.5, 0.999 lr 0.0002 total epochs 80 100 batch size 8 load size 256 preprocess None Table 5. Training settings. erator, we use a 24-layer UNet-like architecture (i.e., the resnet 9blocks generator in the official repository of CycleGAN [2]), which contains nine 2-layer residual mod- ules in the middle of the architecture where the encoder and decoder are connected. For each discriminator, a 5-layer fully convolutional network (i.e., the basic discriminator in the official repository of CycleGAN [2]) is used. For the detector, we adopt the official RetinaNet archi- tecture implemented in Detectron2 [51] with few modifica- tions. First of all, the feature dimension of RetinaNet’s clas- sification subnet and box regression subnet are decreased from 256 to 32 since we are dealing with single category (i.e., human) instead of multiple categories (e.g., 80 cate- gories for detectors trained on the MS COCO dataset). In addition, we switch the activation function used in these two subnets from ReLU to LeakyReLU to avoid the singular co- variance matrix problem, which may occur when calculat- ing the Mahalanobis distance, occasionally triggered by the dying ReLU problem. Finally, the kernel size of the last convolutional layer in the classification subnet, the layer just in front of the sigmoid layer, is reduced from 3 ×3 to 1×1 so that each output prediction is associated with only one feature vector in the feature representation space. Here, ResNet50 is used as the backbone. Training details.The settings used for training the human detector through our method and the other baseline meth- ods are listed in Table 5a. Without further specification, we follow all the settings and initialization strategies de- fined by the original RetinaNet training [33]. Unlike other model training, using fewer iterations for fine-tuning when adopting the pretrain-finetune method is common because training converges faster. Although fine-tuning usually uses a lower learning rate than pre-training, we use the same learning rate (i.e., 0.001) for both stages because we also fine-tune the ImageNet-pretrained backbone on the virtual images in the pre-training stage. The settings used for training the generator through the baseline method using transformation (i.e., ‘naive merge w/ transformation’) and our method are listed in Table 5b. These settings and initialization strategies are taken from the original CycleGAN training [54]. ‘Naive merge w/ transform’ used fewer training epochs than PTL (80 vs 100) because all images in the virtual set are used for training. In general, as the size of the dataset increases, the number of epochs required for training decreases. Datasets. In this section, we provide the details of the three real UA V-based datasets, VisDrone [55], Okutama- Action [4], and ICG [1]), and the virtual UA V-based dataset, Archangel-Synthetic, used in this paper. Sample images for each dataset are shown in Figure 6. VisDrone has four tracks (i.e., object detection (DET), video object detection (VID), single object tracking (SOT), and multi-object tracking (MOT)) with a separate dataset for each track. We use the dataset from the DET track and focus on detecting the person and the pedestrian categories among the ten object categories covered by the track. The VisDrone dataset in the DET track consists of 10,209 im- ages, where 6,471 images are used in the training set, 548 images are used in thevalidation set, 1,580 images are used in the test-challenge set, and 1,610 images are used in the test-dev set. We use the training set for model training and thetest-dev set for model testing. The maximal resolution of images in the VisDrone dataset is 2000×1500. Okutama-Action was originally created for human ac- tion detection from the aerial view. Although the main task associated with this dataset is human action detection, the dataset also includes a sub-task of pedestrian detection. To acquire images from various aerial views, a drone with an embedded camera flew freely at some altitudes between 10m and 45m, and the camera angle was set at 45 or 90 degrees. The Okutama-Action dataset contains 43 video se-VisDrone Okutama-Action ICG ·character: juliet·pose: standing·altitude: 5m·radius: 5m·camera angle: 90◦ ·sun angle: 1 ·character: scott·pose: standing·altitude: 5m·radius: 15m·camera angle: 240◦ ·sun angle: 2 ·character: juliet·pose: squatting·altitude: 5m·radius: 30m·camera angle: 150◦ ·sun angle: 3 ·character: scott·pose: squatting·altitude: 25m·radius: 5m·camera angle: 180◦ ·sun angle: 2 ·character: juliet·pose: standing·altitude: 25m·radius: 15m·camera angle: 210◦ ·sun angle: 3 ·character: scott·pose: standing·altitude: 25m·radius: 30m·camera angle: 330◦ ·sun angle: 4 ·character: scott·pose: squatting·altitude: 5m·radius: 5m·camera angle: 270◦ ·sun angle: 1 ·character: juliet·pose: squatting·altitude: 5m·radius: 15m·camera angle: 180◦ ·sun angle: 2 ·character: scott·pose: standing·altitude: 5m·radius: 30m·camera angle: 120◦ ·sun angle: 1 ·character: juliet·pose: standing·altitude: 25m·radius: 5m·camera angle: 60◦ ·sun angle: 2 ·character: scott·pose: squatting·altitude: 25m·radius: 15m·camera angle: 0◦ ·sun angle: 3 ·character: juliet·pose: squatting·altitude: 25m·radius: 30m·camera angle: 300◦ ·sun angle: 4 Archangel-Synthetic Figure 6. Sample images from each dataset.For the Archangel-Synthetic dataset, metadata for each image is shown to the right of the image. quences in 4K resolution (i.e., 3840×2160), where 33 video sequences are used for thetrain-val set and the remain- ing 10 video sequences are used for the test set. Since adjacent frames in the video sequence are very similar, we use every tenth frame in both the train-val set and the test set. Our model is trained on thetrain-val set and tested with the test set. ICG was collected for studying semantic understanding of urban scenes. Additionally, the ICG dataset also pro- vides information, such as ground-truth human bounding boxes, for the human detection task. Images in the ICG dataset were captured from a camera located at some alti- tudes between 5-50 m above the ground at a resolution of 6000×4000. The ICG dataset provides a training set of 400 images for the human detection task, of which we use the first 320 images for training and the remaining 80iter. 1 iter. 2 iter. 3 iter. 4 iter. 5 VisDrone, 20-img iter. 1 iter. 2 iter. 3 iter. 4 iter. 5 Okutama-Action, 20-img iter. 1 iter. 2 iter. 3 iter. 4 iter. 5 Okutama-Action, 50-img iter. 1 iter. 2 iter. 3 iter. 4 iter. 5 ICG, 20-img iter. 1 iter. 2 iter. 3 iter. 4 iter. 5 ICG, 50-img Figure 7. Accumulated distributions of transformation candidates with respect to camera locations in more setups.This figure shows the distributions in other five experimental setups except for the setup (i.e., VisDrone, 50-img) shown in Figure 3 of the main manuscript. The x and y axes indicate altitude and rotation circle radius from the target human. images for testing. Archangel-Synthetic is one of the three sub-datasets in- cluded in the Archangel dataset, along withArchangel-Real and Archangel-Mannequin. The Archangel dataset is a hy- brid UA V-based dataset captured with similar imaging con- ditions in real and synthetic domains. An important prop- erty of the Archangel dataset that sets it apart from other datasets is that it provides metadata about the camera posi- tions in terms of UA V altitudes and radii of the rotation cir- cles for each image. The Archangel-Synthetic dataset was generated by using the Unity game engine. The dataset in- cludes eight virtual characters in three different poses cap- tured with camera viewing angles ranging from 0 ◦ to 358◦ in increments of 2◦, UA V altitudes and rotation circle radii from 5m to 80m in increments of 5 m, and four different sun angles. The total number of images in the Archangel- Synthetic dataset is 4.4M. Considering the significant dif- ference in dataset size between the Archangel-Synthetic dataset with the other real datasets, a small subset of the en- tire dataset (17.6K) was used as the virtual image set in our experiments. The size of images in the Archangel-Synthetic dataset is 512×512. C. Additional Analyses Training time comparison.Training times for PTL and the baselines are shown in Tab 6. For PTL, it is observed that the detector training time (i.e., ‘Dtr train’ in the table) de- creases as training progresses because the number of virtual images (i.e., Archangel-Synthetic) with a usually smaller image size than real images (i.e., VisDrone) increases dur- ing training. Detector training in PTL uses the same num-iter. 1 iter. 2 iter. 3 iter. 4 iter. 5 VisDrone, 20-img iter. 1 iter. 2 iter. 3 iter. 4 iter. 5 Okutama-Action, 20-img iter. 1 iter. 2 iter. 3 iter. 4 iter. 5 Okutama-Action, 50-img iter. 1 iter. 2 iter. 3 iter. 4 iter. 5 ICG, 20-img iter. 1 iter. 2 iter. 3 iter. 4 iter. 5 ICG, 50-img Figure 8. Distributions of virtual images with respect to the domain gap in more setups.This figure shows the domain gap distributions of virtual images in other five experimental setups except for the setup (i.e., VisDrone, 50-img) shown in Figure 3 of the main manuscript. The x axis represents the domain gap and the y axis represents the corresponding number of virtual images. ber of iterations regardless of the PTL iteration. In contrast, the CycleGAN training time (i.e., ‘GAN train’ in the table) gradually increases due to the increased number of virtual images. PTL is slow due to CycleGAN training, but it is still much faster than ‘naive merge w/ transform’ (i.e., the base- line using transformation) because only a subset of virtual images is used instead of the full set for each PTL iteration. PTL can lead to scalabillity issues due to its training time when a large number of virtual images are used with longer iterations. To address this issue, we can reduce the Cycle- GAN training time for each PTL iteration by fine-tuning the model trained in the previous PTL iteration. Analysis of domain gap measurement.To investigate the effect of using the Mahalanobis distance to measure the do- main gap, we compare it to other metric available under the assumption that the representation for a certain category in method total detail baseline 40 pretrain-finetune 21 4/17 (pretrain / finetune) naive merge 17 w/ transform2,777 2,760/17 (GAN train / Dtr train) PTL 600 20/iter. (domain gap calc. on virtual set) 40/36/32/28/25/22 (Dtr train,∼6thiter.) 28/41/56/69/83 (GAN train,∼5thiter.) Table 6. Wall-clock training time inmins (VisDrone, 20-shot). GeForce RTX 2080 Ti GPUs are used for this comparison. a real dataset is modeled by a multivariate Gaussian distri- bution. Specifically, we use Euclidean distance, which de- pends only on the mean but not on the covariance of the dis- tribution. As shown in Table 7, using Mahalanobis distance consistently presents better accuracy in both the in-domain and cross-domain setups.metric Vis Oku ICG Euclidean 5.28 /1.52 28.80 /6.83 26.00 /7.06 Mahalanobis 6.83/1.94 30.72/7.45 26.86 /7.22 Table 7. Comparison of various distance metrics (VisDrone, 20-shot). test close 100 mid 100 far 100 our Vis 9.11/ 2.71 9.11/ 2.89 8.64/ 2.53 9.38/ 2.94 Oku 39.57/11.05 43.31/11.55 41.98/11.32 42.39/11.47 ICG 29.66/ 7.40 34.98/ 8.97 31.24/ 8.19 30.01/ 7.36 Table 8. Various transformation candidate selections (Vis- Drone, 50-shot). Analysis of transformation candidate selection.To in- vestigate the effect of using weighted random sampling to select transformation candidates, we compare it with a va- riety of other selection strategies. We carry out experiments selecting 100 virtual images with three different ranges of domain gaps ({close, mid, far }) instead of using weighted random sampling (‘our’ in the table) for each PTL iteration in Tab 8. We observe that our selection strategy is the best in the in-domain setup without sacrificing accuracy much in the cross-domain setup. Further analysis of the properties of progressive learn- ing. In this section, we show the distributions of transfor- mation candidates in relation to the camera position (in Fig- ure 7) and the domain gap (in Figure 8) for other five cases not shown in Figure 3 of the main manuscript. We intend to show that the analysis described in the main manuscript can be also applied to these cases. For the distributions of transformation candidates with respect to camera locations (Figure 7), the observation shown in the main manuscript is also applied to the other five cases. That is, as PTL progresses, the camera locations of virtual images included in the training set are gradually spread over the entire area. Therefore, the validity of the transformation candidate selection process of PTL extends to all the experimental setups considered in this paper. Note that the distributions of humans in the real train- ing set with respect to camera locations is likely to be sim- ilar to the distributions of transformation candidates with respect to camera locations at the first PTL iteration as a virtual image with a smaller domain gap is selected with a higher probability. Accordingly, we speculate that humans in the real training set were captured from various camera locations in the Okutama-action dataset. In contrast, in the other two datasets, most of them were taken at some similar ranges. In addition, in the ICG dataset, the camera loca- tions where most images were taken are in the close range, which might be the reason why diversifying camera loca- tions through PTL does not significantly improve accuracy in the cross-domain setup. For the distributions of virtual images with respect to the domain gap (Figure 8), the observation mentioned in the main manuscript that the distribution becomes narrower and smaller as PTL progresses is still perceived in these settings. However, the speed of this distribution change is particularly slow for the ICG dataset compared to the other datasets, as shown in Figure 8. This observation also im- plies that the ICG dataset has very different characteristics compared to the other two datasets. Qualitative analysis of transformation. Fig- ures 9, 10, 11, 12, 13, and 14 show several samples of transformed virtual images included in the training set using methods with virtual2real transformation (i.e., PTL and ‘naive merge w/ transformation’) in the six experi- mental setups. Since the trends seen in these examples are similar to Figure 5 in the main manuscript, a qualitative analysis of the superiority of PTL over ‘naive merge w/ transform’ and our claim that the domain gap between virtual images and real images should be considered when training the virtual2real transformation generator can also be applied to these experimental setups. References [1] Aerial semantic segmentation drone dataset. http:// dronedataset.icg.tugraz.at. 1, 5, 10 [2] Cyclegan and pix2pix in pytorch. https : //github.com/junyanz/pytorch- CycleGAN- and-pix2pix. 10 [3] Kyungjune Baek and Hyunjung Shim. Commonality in nat- ural images rescues GANs: Pretraining GANs with generic and privacy-free synthetic data. In Proc. CVPR, 2022. 2, 6 [4] Mohammadamin Barekatain, Miquel Mart ´ı, Hsueh-Fu Shih, Samuel Murray, Kotaro Nakayama, Yutaka Matsuo, and Hel- mut Prendinger. Okutama-action: An aerial view video dataset for concurrent human action detection. In Proc. CVPR Workshop, 2017. 1, 5, 10 [5] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Proc. NeurIPS, 2006. 3 [6] Yoshua Bengio, J ´erˇome Louradour, Ronan Collobert, and Ja- son Weston. Curriculum learning. In Proc. ICML, 2009. 3 [7] Alexey Bochkovskiy, Chien-Yao Wang, and Hong- Yuan Mark Liao. YOLOv4: Optimal speed and accuracy of object detection. arXiv:2004.10934, 2020. 5 [8] Tianyue Cao, Yongxin Wang, Yifan Xing, Tianjun Xiao, Tong He, Zheng Zhang, Hao Zhou, and Joseph Tighe. PSS: Progressive sample selection for open-world visual represen- tation learning. In Proc. ECCV, 2022. 3 [9] Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2Net: Accelerating learning via knowledge transfer. In Proc. ICLR, 2016. 3 [10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. In Proc. ICML, 2020. 3 [11] Matteo Fabbri, Fabio Lanzi, Simone Calderara, Andrea Palazzi, Roberto Vezzani, and Rita Cucchiara. Learning toNaive merge w/ transform PTL Figure 9. Sample Virtual2Real transformation output (VisDrone, 20-shot).Each set consists of three images: original virtual image (left), transformed image (middle), and transformed image with background (right). Naive merge w/ transform PTL Figure 10. Sample Virtual2Real transformation output (VisDrone, 50-shot).Each set consists of three images: original virtual image (left), transformed image (middle), and transformed image with background (right). detect and track visible and occluded body joints in a virtual world. In Proc. ECCV, 2018. 2, 6 [12] Scott E. Fahlman and Christian Lebiere. The cascade- correlation learning architecture. In Proc. NeurIPS, 1989. 3 [13] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multi-object tracking anal- ysis. In Proc. CVPR, 2016. 2, 6 [14] Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tie-Yan Liu. Efficient training of BERT by progressively stacking. In Proc. ICML, 2019. 3 [15] Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks. In Proc. ICML, 2017. 3 [16] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Do- ersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham- mad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R´emi Munos, and Michal Valko. Bootstrap your own latent - a new approach to self-supervised learning. In Proc. NeurIPS, 2020. 3 [17] Xi Guo, Wei Wu, Dongliang Wang, Jing Su, Haisheng Su,Naive merge w/ transform PTL Figure 11. Sample Virtual2Real transformation output (Okutama-Action, 20-shot).Each set consists of three images: original virtual image (left), transformed image (middle), and transformed image with background (right). Naive merge w/ transform PTL Figure 12. Sample Virtual2Real transformation output (Okutama-Action, 50-shot).Each set consists of three images: original virtual image (left), transformed image (middle), and transformed image with background (right). Weihao Gan, Jian Huang, and Qin Yang. Learning video rep- resentations of human motion from synthetic data. In Proc. CVPR, 2022. 2, 6 [18] Ankur Handa, Viorica P ˇatrˇaucean, Vijay Badrinarayanan, Si- mon Stent, and Roberto Cipolla. SceneNet: Understanding real world indoor scenes with synthetic data. InProc. CVPR, 2016. 2, 6 [19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In Proc. CVPR, 2020. 3 [20] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. CyCADA: Cycle-consistent adversarial domain adaptation. In Proc. ICML, 2018. 1, 2 [21] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adver- sarial networks. In Proc. CVPR, 2017. 5 [22] Zhao Jin, Yinjie Lei, Naveed Akhtar, Haifeng Li, and Mu- nawar Hayat. Deformation and correspondence aware un- supervised synthetic-to-real scene flow estimation for point clouds. In Proc. CVPR, 2022. 2, 6 [23] Glenn Jocher. ultralytics/yolov5: v3.1 - Bug Fixes andNaive merge w/ transform PTL Figure 13. Sample Virtual2Real transformation output (ICG, 20-shot).Each set consists of three images: original virtual image (left), transformed image (middle), and transformed image with background (right). Naive merge w/ transform PTL Figure 14. Sample Virtual2Real transformation output (ICG, 50-shot).Each set consists of three images: original virtual image (left), transformed image (middle), and transformed image with background (right). Performance Improvements. https://github.com/ ultralytics/yolov5, 2020. 5 [24] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In Proc. ICLR, 2018. 3 [25] Prannay Kaul, Weidi Xie, and Andrew Zisserman. Label, verify, correct: A simple few shot object detection method. In Proc. CVPR, 2022. 3 [26] Faisal Khan, Bilge Mutlu, and Jerry Zhu. How do humans teach: On curriculum learning and teaching dimension. In Proc. NeurIPS, 2011. 3 [27] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Kweon. Video panoptic segmentation. InProc. CVPR, 2016. 2, 6 [28] M. Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In Proc. NeurIPS, 2010. 3 [29] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Proc. NeurIPS, 2018. 2, 3, 9 [30] Chen Li and Gim Hee Lee. From synthetic to real: Unsu-pervised domain adaptation for animal pose estimation. In Proc. CVPR, 2021. 1, 2, 5 [31] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng, Weiqiang Nie, Yiduo Li, Bo Zhang, Yufei Liang, Linyuan Zhou, Xiaoming Xu, Xiangxiang Chu, Xiaoming Wei, and Xiaolin Wei. YOLOv6: A single-stage object detection framework for industrial applications. arXiv:2209.02976, 2022. 5 [32] Changlin Li, Bohan Zhuang, Guangrun Wang, Xiaodan Liang, Xiaojun Chang, and Yi Yang. Automated progres- sive learning for efficient training of vision transformers. In Proc. CVPR, 2022. 3 [33] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense object detection. In Proc. ICCV, 2017. 5, 10 [34] Qing Liu, Adam Kortylewski, Zhishuai Zhang, Zizhang Li, Mengqi Guo, Qihao Liu, Xiaoding Yuan, Jiteng Mu, We- ichao Qiu, and Alan Yuille. Learning part segmentation through unsupervised domain adaptation from synthetic ve- hicles. In Proc. CVPR, 2022. 2 [35] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C. Berg. SSD: Single shot multibox detector. In Proc. ECCV, 2016. 5 [36] Prasanta Chandra Mahalanobis. On the generalised distance in statistics. Proc. National Institute of Sciences of India., 1936. 2 [37] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv:1411.1784, 2014. 1, 5 [38] Samarth Mishra, Rameswar Panda, Cheng Perng Phoo, Chun-Fu (Richard) Chen, Leonid Karlinsky, Kate Saenko, Venkatesh Saligrama, and Rogerio S. Feris. Task2Sim: To- wards effective pre-training and transfer from synthetic data. In Proc. CVPR, 2022. 2, 6 [39] Jiteng Mu, Weichao Qiu, Gregory Hager, and Alan Yuille. Learning from synthetic animals. In Proc. CVPR, 2020. 1, 2, 6 [40] Haibo Qiu, Baosheng Yu, Dihong Gong, Zhifeng Li, Wei Liu, and Dacheng Tao. SynFace: Face recognition with syn- thetic data. In Proc. ICCV, 2021. 1, 2 [41] Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In Proc. ECCV, 2016. 3, 6 [42] German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M. Lopez. The SYNTHIA dataset: A large collection of synthetic images for semantic segmen- tation of urban scenes. In Proc. CVPR, 2016. 3, 6 [43] Yi-Ting Shen, Yaesop Lee, Heesung Kwon, Damon M. Conover, Shuvra S. Bhattacharyya, Nikolas Vale, Joshua D. Gray, G. Jeremy Leong, Kenneth Evensen, and Frank Skirlo. Archangel: A hybrid UA V-based human detection bench- mark with position and pose metadata. arXiv:2209.00128, 2022. 5 [44] Leslie N. Smith, Emily M. Hand, and Timothy Doster. Grad- ual DropIn of layers to train very deep neural networks. In Proc. CVPR, 2016. 3 [45] Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. Baby steps: How “less is more” in unsupervised dependency parsing. In Proc. NeurIPS Workshop, 2009. 3 [46] Gul Varol, Javier Romero, Xavier Martin, Naureen Mah- mood, Michael J. Black, Ivan Laptev, and Cordelia Schmid. Learning from synthetic humans. In Proc. CVPR, 2018. 2, 6 [47] Can Wang, Sheng Jin, Yingda Guan, Wentao Liu, Chen Qian, Ping Luo, and Wanli Ouyang. Pseudo-labeled auto- curriculum learning for semi-supervised keypoint localiza- tion. In Proc. ICLR, 2022. 3 [48] Chien-Yao Wang, Alexey Bochkovskiy, and Hong- Yuan Mark Liao. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proc. CVPR, 2023. 5 [49] Guangcong Wang, Xiaohua Xie, Jianhuang Lai, and Jiaxuan Zhuo. Deep growing learning. In Proc. ICCV, 2017. 3 [50] Tao Wei, Changhu Wang, Yong Rui YONGRUI, and Chang Wen Chen. Network morphism. InProc. ICML, 2016. 3 [51] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github. com/facebookresearch/detectron2, 2019. 10 [52] Qi Yan, Jianhao Zheng, Simon Reding, Shanci Li, and Iordan Doytchinov. CrossLoc: Scalable aerial localization assisted by multimodal synthetic data. In Proc. CVPR, 2022. 2 [53] Minjia Zhang and Yuxiong He. Accelerating training of transformer-based language models with progressive layer dropping. In Proc. NeurIPS, 2020. 3 [54] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle- consistent adversarial networks. In Proc. ICCV, 2017. 5, 10 [55] Pengfei Zhu, Longyin Wen, Dawei Du, Xiao Bian, Heng Fan, Qinghua Hu, and Haibin Ling. Detection and track- ing meet drones challenge. IEEE Trans. Pattern Anal. Mach. Intell., 44(11):7380–7399, Nov 2022. 1, 5, 10",
      "references": [
        "Aerial semantic segmentation drone dataset.",
        "Cyclegan and pix2pix in pytorch.",
        "Commonality in natural images rescues GANs: Pretraining GANs with generic and privacy-free synthetic data.",
        "Okutama-action: An aerial view video dataset for concurrent human action detection.",
        "Greedy layer-wise training of deep networks.",
        "Curriculum learning.",
        "YOLOv4: Optimal speed and accuracy of object detection.",
        "PSS: Progressive sample selection for open-world visual representation learning.",
        "Net2Net: Accelerating learning via knowledge transfer.",
        "A simple framework for contrastive learning of visual representations.",
        "Virtual worlds as proxy for multi-object tracking analysis.",
        "The cascade-correlation learning architecture.",
        "Efficient training of BERT by progressively stacking.",
        "Automated curriculum learning for neural networks.",
        "Bootstrap your own latent - a new approach to self-supervised learning.",
        "Learning video representations of human motion from synthetic data.",
        "SceneNet: Understanding real world indoor scenes with synthetic data.",
        "Momentum contrast for unsupervised visual representation learning.",
        "CyCADA: Cycle-consistent adversarial domain adaptation.",
        "Image-to-image translation with conditional adversarial networks.",
        "Deformation and correspondence aware unsupervised synthetic-to-real scene flow estimation for point clouds.",
        "ultralytics/yolov5: v3.1 - Bug Fixes and Performance Improvements.",
        "Progressive growing of GANs for improved quality, stability, and variation.",
        "Label, verify, correct: A simple few shot object detection method.",
        "How do humans teach: On curriculum learning and teaching dimension.",
        "Video panoptic segmentation.",
        "Self-paced learning for latent variable models.",
        "A simple unified framework for detecting out-of-distribution samples and adversarial attacks.",
        "From synthetic to real: Unsupervised domain adaptation for animal pose estimation.",
        "YOLOv6: A single-stage object detection framework for industrial applications.",
        "Automated progressive learning for efficient training of vision transformers.",
        "Focal loss for dense object detection.",
        "Learning part segmentation through unsupervised domain adaptation from synthetic vehicles.",
        "SSD: Single shot multibox detector.",
        "On the generalised distance in statistics.",
        "Conditional generative adversarial nets.",
        "Task2Sim: Towards effective pre-training and transfer from synthetic data.",
        "Learning from synthetic humans.",
        "SynFace: Face recognition with synthetic data.",
        "Playing for data: Ground truth from computer games.",
        "The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes.",
        "Archangel: A hybrid UAV-based human detection benchmark with position and pose metadata.",
        "Gradual DropIn of layers to train very deep neural networks.",
        "Baby steps: How “less is more” in unsupervised dependency parsing.",
        "Pseudo-labeled open-world curriculum learning for semi-supervised keypoint localization.",
        "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.",
        "Deep growing learning.",
        "Network morphism.",
        "Detectron2.",
        "CrossLoc: Scalable aerial localization assisted by multimodal synthetic data.",
        "Accelerating training of transformer-based language models with progressive layer dropping.",
        "Unpaired image-to-image translation using cycle-consistent adversarial networks.",
        "Detection and tracking meet drones challenge."
      ],
      "meta_data": {
        "arxiv_id": "2211.01778v2",
        "authors": [
          "Yi-Ting Shen",
          "Hyungtae Lee",
          "Heesung Kwon",
          "Shuvra Shikhar Bhattacharyya"
        ],
        "published_date": "2022-11-03T13:04:15Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes Progressive Transformation Learning (PTL) to bridge the domain gap between virtual UAV images and real UAV images for human detection by progressively selecting and transforming virtual images into realistic variants and adding them to the training set. The approach models the detector's representation space as a per-class multivariate Gaussian and uses Mahalanobis distance to quantify domain gap. PTL iterates: (1) select transformation candidates from a virtual image pool with weights favoring smaller domain gaps, (2) transform the selected images with a CycleGAN-based virtual2real generator trained against real images (focusing on the cropped human region), and (3) update the training set by adding transformed images and removing used virtual candidates. Empirically, PTL yields substantial gains in low-shot detection and cross-domain transfer across VisDrone, Okutama-Action, ICG, and Archangel-Synthetic datasets, with ablations on hyperparameters and demonstrations of improved realism while preserving pose.",
        "methodology": "Key techniques include: modeling detector feature space as Gaussian per class using penultimate-layer features with sigmoid outputs; estimating mu_c and Sigma_c from training; computing domain gap via Mahalanobis distance and evaluating at multiple scales; progressive candidate selection with exponential weights w(x)=exp(-d(x)/tau) and tau hyperparameter; training a CycleGAN-based virtual2real generator on selected candidates with Rt as target and cropping around the human region; updating Rt and Vt accordingly; using RetinaNet with FPN as detector, with network adjustments (shared subnetwork across scales, reduced channel dims, LeakyReLU, 1x1 final conv). Multi-scale training strategies are used for real and virtual data; experiments conducted under 20- and 50-shot regimes; comparing PTL to baseline and other virtual-image methods.",
        "experimental_setup": "Datasets: VisDrone (det track; train-dev real images; test-dev), Okutama-Action (aerial pedestrian detection), ICG (aerial urban scenes with 320 training real images and 80 test), Archangel-Synthetic (4.4M synthetic images; 17.6K subset used as virtual). Virtual data Archangel-Synthetic used as pool; PTL iterations (n=100 per iter, tau=5). Evaluation metrics AP@0.5 and AP@[0.5:0.95]. Training details: RetinaNet with FPN, backbone ResNet50; modifications to support single-class detector; multi-scale training; CycleGAN-based generator (24-layer UNet-like) and 5-layer discriminators; training durations; ablation results showing impact of tau and n; cross-domain experiments with VisDrone, Oku-Action, ICG in 3x3 cross-domain setups; qualitative visuals showing transformed images preserving pose.",
        "limitations": "Assumptions: Gaussian modeling of detector representation space per class; domain gap measured in penultimate-layer features with sigmoid outputs; transformation quality depends on GAN training and data diversity; the PTL loop may saturate or degrade if too many virtual images are iterated; computationally expensive due to CycleGAN training; the approach focuses on single-class (human) detection with adjusted subnetwork; cross-domain gains vary by dataset (e.g., ICG less improved due to large domain differences); selection hyperparameters tau and n require tuning.",
        "future_research_directions": "Investigate more flexible domain-gap metrics beyond Gaussian assumption; extend PTL to multi-class detectors; automate adaptive control of tau and n and number of iterations to maximize transfer while avoiding diminishing returns; accelerate GAN training via pretraining or incrementalUpdate; combine PTL with semi-supervised or active learning, or use additional synthetic assets (depth, segmentation maps) for better realism; evaluate on broader UAV tasks (vehicle, object, action) and different domains; integrate with curriculum learning for scheduling transformations.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Modeling Adversarial Noise for Adversarial Training",
      "full_text": "MODELING ADVERSARIAL NOISE FOR ADVERSARIAL TRAINING Dawei Zhou∗1, Nannan Wang∗1, Bo Han2, Tongliang Liu†3 1Xidian University; 2Hong Kong Baptist University; 3The University of Sydney ABSTRACT Deep neural networks have been demonstrated to be vulnerable to adversarial noise, promoting the development of defense against adversarial attacks. Motivated by the fact that adversarial noise contains well-generalizing features and that the relationship between adversarial data and natural data can help infer natural data and make reliable predictions, in this paper, we study to model adversarial noise by learning the transition relationship between adversarial labels (i.e. the ﬂipped labels used to generate adversarial data) and natural labels (i.e. the ground truth labels of the natural data). Speciﬁcally, we introduce an instance-dependent transition matrix to relate adversarial labels and natural labels, which can be seamlessly embedded with the target model (enabling us to model stronger adaptive adversarial noise). Empirical evaluations demonstrate that our method could effectively improve adversarial accuracy. 1 I NTRODUCTION Deep neural networks have been demonstrated to be vulnerable to adversarial noise Goodfellow et al. (2015); Szegedy et al. (2014); Jin et al. (2019); Liao et al. (2018); Ma et al. (2018); Wu et al. (2020a). The vulnerability of deep neural networks seriously threatens many decision-critical deep learning applications LeCun et al. (1998); He et al. (2016); Zagoruyko & Komodakis (2016); Simonyan & Zisserman (2015); Kaiming et al. (2017); Ma et al. (2021). To alleviate the negative affects caused by adversarial noise, many adversarial defense methods have been proposed. A major class of adversarial defense methods focus on exploiting adversarial instances to help train the target model Madry et al. (2018); Ding et al. (2019); Zhang et al. (2019); Wang et al. (2019), which achieve the state-of-the-art performance. However, these methods do not explicitly model the adversarial noise. The relationship between the adversarial data and natural data has not been well studied yet. Studying (or modeling) the relationship between adversarial data and natural data is considered to be beneﬁcial. If we can model the relationship, we can infer natural data information by exploiting the adversarial data and the relationship. Previous researches have made some explorations on this idea. Some data pre-processing based methods Jin et al. (2019); Liao et al. (2018); Naseer et al. (2020); Zhou et al. (2021a;b), which try to recover the natural data by removing the adversarial noise, share the same philosophy. However, those methods suffer from the high dimensionality problem because both the adversarial data and natural data are high dimensional. The recovered data is likely to have human-observable loss (i.e., obvious inconsistency between processed instances and natural instances) Xu et al. (2017) or contain residual adversarial noise Liao et al. (2018). To avoid the above problems, in this paper, we propose to model adversarial noise in the low- dimensional label space. Speciﬁcally, instead of directly modeling the relationship between the adversarial data and the natural data, we model the adversarial noise by learning the label transition from the adversarial labels (i.e., the ﬂipped labels used to generate adversarial data) to the natural labels (i.e., the ground truth labels of the natural data). Note that the adversarial labels have not been well exploited in the community of adversarial learning, which guide the generation of adversarial ∗Equal contribution. †Corresponding author. 1 arXiv:2109.09901v5  [cs.LG]  17 Jul 2022noise and thus contain valuable information for modeling the well-generalizing features of the adversarial noise Ilyas et al. (2019). It is well-known that the adversarial noise depends on many factors, e.g., the data distribution, the adversarial attack strategy, and the target model. The proposed adversarial noise modeling method is capable to take into account of the factors because that the label transition is dependent of the adversarial instance (enabling us to model how the patterns of data distribution and adversarial noise affect label ﬂipping) and that the transition can be seamlessly embedded with the target model (enabling us to model stronger adaptive adversarial noise). Speciﬁcally, we employ a deep neural network to learn the complex instance-dependent label transition. By using the transition relationship, we propose a defense method based on Modeling Adversarial Noise (called MAN). Speciﬁcally, we embed a labeltransition network (i.e., a matrix-valued transition function parameterized by a deep neural network) into the target model, which denotes the transition relationship from mixture labels (mixture of adversarial labels and natural labels) to natural labels (i.e., the ground truth labels of natural instances and adversarial instances). The transition matrix explicitly models adversarial noise and help us to infer natural labels. Considering that adversarial data can be adaptively generated, we conduct joint adversarial training on the target model and the transition network to achieve an optimal adversarial accuracy. We empirically show that the proposed MAN-based defense method could provide signiﬁcant gains in the classiﬁcation accuracy against adversarial attacks in comparison to state-of-the-art defense methods. The rest of this paper is organized as follows. In Section 2, we introduce some preliminary information and brieﬂy review related work on attacks and defenses. In Section 3, we discuss how to design an adversarial defense by modeling adversarial noise. Experimental results are provided in Section 4. Finally, we conclude this paper in Section 5. 2 P RELIMINARIES In this section, we ﬁrst introduce some preliminary about notation and the problem setting. We then review the most relevant literature on adversarial attacks and adversarial defenses. Notation. We use capital letters such as X and Y to represent random variables, and lower-case letters such as xand y to represent realizations of random variables X and Y, respectively. For norms, we denote by ∥x∥a generic norm. Speciﬁc examples of norms include ∥x∥∞, the L∞-norm of x, and ∥x∥2, the L2-norm of x. Let B(x,ϵ) represent the neighborhood of x: {˜x: ∥˜x−x∥≤ ϵ}, where ϵis the perturbation budget. We deﬁne the classiﬁcation function as f : X→{ 1,2,...,C }, where Xis the feature space of X. It can be parametrized, e.g., by deep neural networks. Problem setting. This paper focuses on a classiﬁcation task under the adversarial environment, where adversarial attacks are utilized to craft adversarial noise to mislead the prediction of the target classiﬁcation model. Let X and Y be the variables for natural instances and natural labels (i.e., the ground truth labels of natural instances) respectively. We sample natural data{(xi,yi)}n i=1 according to the distribution of the variables(X,Y ), where (X,Y ) ∈X×{ 1,2,...,C }. Given a deep learning model based classiﬁer f and a pair of natural data (x,y), the adversarial instance ˜xsatisﬁes the following constraint: f(˜x) ̸= y s.t. ∥x−˜x∥≤ ϵ. (1) The generation of the adversarial instance is guided by the adversarial label ˜ywhich is different with the natural label y. Adversarial labels can be set by the attacker in target attacks, or be randomly initialized and then found by non-target attacks. Let ˜Xand ˜Y be the variables for adversarial instances and adversarial labels respectively. We denote by{(˜xi,˜yi)}n i=1 the adversarial data drawn according to a distribution of the variables ( ˜X, ˜Y). Our aim is to design an adversarial defense to correct the adversarial label ˜yinto natural label yby modeling the relationship between adversarial data (i.e., ˜x or ˜y) and natural data (i.e., xor y). Adversarial attacks. Adversarial instances are inputs maliciously designed by adversarial attacks to mislead deep neural networks Szegedy et al. (2014). They are generated by adding imperceptible but adversarial noise to natural instances. Adversarial noise can be crafted by multi-step attacks following the direction of adversarial gradients, such as PGD Madry et al. (2018) and AA Croce & Hein (2020). The adversarial noise is bounded by a small norm-ball ∥·∥ p ≤ϵ, so that their 2adversarial instances can be perceptually similar to natural instances. Optimization-based attacks such as CW Carlini & Wagner (2017b) and DDN Rony et al. (2019) jointly minimize the perturbation L2 and a differentiable loss based on the logit output of a classiﬁer. Some attacks such as FW A Wu et al. (2020b) and STA Xiao et al. (2018) focus on mimicking non-suspicious vandalism by exploiting the geometry and spatial information. Adversarial defenses. The issue of adversarial instances promotes the development of adversarial defenses. A major class of adversarial defense methods is devoted to enhance the adversarial robustness in an adversarial training manner Madry et al. (2018); Ding et al. (2019); Zhang et al. (2019); Wang et al. (2019). They augment training data with adversarial instances and use a min-max formulation to train the target model Madry et al. (2018). However, these methods do not explicitly model the adversarial noise. The relationship between the adversarial data and natural data has not been well studied yet. In addition, some data pre-processing based methods try to remove adversarial noise by learning a denoising function or a feature-squeezing function. For example, denoising based defenses Liao et al. (2018); Naseer et al. (2020) transfer adversarial instances into clean instances, and feature squeezing based defenses Guo et al. (2018) aim to reduce redundant but adversarial information. However, these methods suffer from the high dimensionality problem because both the adversarial data and natural data are high dimensional. For examples, the recovered instances are likely to have signiﬁcant inconsistency between the natural instances Xu et al. (2017). Besides, the recovered instances may contain residual adversarial noise Liao et al. (2018), which would be ampliﬁed in the high-level layers of the target model and mislead the ﬁnal predictions. To avoid the above problems, we propose to model adversarial noise in the low-dimensional label space. 3 M ODELING ADVERSARIAL NOISE BASED DEFENSE In this section, we presen the Modeling Adversarial Noise (MAN) based defense method, to improve the adversarial accuracy against adversarial attacks. We ﬁrst illustrate the motivation of the proposed defense method (Section 3.1). Next, we introduce how to model adversarial noise (Section 3.2). Finally, we present the training process of the proposed adversarial defense (Section 3.3). The code is available at https://github.com/dwDavidxd/MAN. 3.1 M OTIVATION Studying the relationship between adversarial data and natural data is considered to be beneﬁcial for adversarial defense. If we can model the relationship, we can infer clean data information by exploiting the adversarial data and the relationship. Processing the input data to remove the adversarial noise is a representative strategy to estimate the relationship. However, data pre-processing based methods may suffer from high dimensionality problems. To avoid the problem, we would like to model adversarial noise in the low-dimensional label space. Adversarial noise can be modeled because it have imperceptible but well-generalizing features. Speciﬁcally, classiﬁers are usually trained to solely maximize accuracy for natural data. They tend to use any available signal including those that are well-generalizing, yet brittle. Adversarial noise can arise as a result of perturbing these features Ilyas et al. (2019), and it controls the ﬂip from natural labels to adversarial labels. Thus, adversarial noise contains well-generalizing features which can be modeled by learning the label transition from adversarial labels to natural labels. Note that the adversarial noise depends on many factors, such as data distribution, the adversarial attack strategy, and the target model. Modeling adversarial noise in label space is capable to take into account of these factors. Speciﬁcally, since that the label transition is dependent of the adversarial instance, we can model how the patterns of data distribution and adversarial noise affect label ﬂipping, and exploit the modeling to correct the ﬂipping of the adversarial label. In addition, we can model adversarial noise crafted by the stronger white-box adaptive attack, because the label transition can be seamlessly embedded with the target model. By jointly training the transition matrix with the target model, it can also be adaptive to adversarial attacks. We employ a deep neural network to learn the complex label transition from the well-generalizing and misleadingly predictive features of instance-dependent adversarial noise. 3.02 .85 .03 .03 .07 .01 .03 .02 .93 .01 × = Transition matrix Adversarial label Natural label (a) Transition network 𝒈𝒈𝝎𝝎 Transition matrix �𝑻𝑻 Input instance 𝒙𝒙′  (b) Figure 1: (a). Infer the natural label by utilizing the transition matrix and the adversarial label. (b). The transition network gω parameterized by ωtakes the instance x′as input, and output the label transition matrix ˆT(x′; ω) =gω(x′). Mixture  instance 𝑥𝑥′ Transition matrix �𝑻𝑻(𝑥𝑥′; 𝜔𝜔) Transition network 𝒈𝒈𝝎𝝎 Target model 𝒉𝒉𝜽𝜽 Transition  network 𝒈𝒈𝝎𝝎 Target model 𝒇𝒇𝜽𝜽 𝓛𝓛𝑻𝑻/𝓛𝓛𝒕𝒕𝒕𝒕𝒕𝒕 Adversarial  noise Maximize the cross- entropy loss Iterative adversarial training Natural  instance 𝑥𝑥 + Softmax 𝜹𝜹(�) �𝒚𝒚 𝒚𝒚 �𝒚𝒚′ × Figure 2: An overview of the training procedure for the proposed defense method. ˆy′and ˆy denote the probability of the estimated mixture label ˆy′and the probability of the inferred natural label ˆy respectively, i.e., ˆy′= δ(hθ(x′)) and ˆy = ˆy′·ˆT(x′; ω). y is yin the form of a vector. Motivated by the value of modeling adversarial noise for adversarial defense, we propose a defense method based on Modeling Adversarial Noise (MAN) by exploiting label transition relationship. 3.2 M ODELING ADVERSARIAL NOISE We exploit a transition network to learn the label transition for modeling adversarial noise. The transition network can be regarded as a matrix-valued transition function parameterized by a deep neural network. The transition matrix, estimated by the label transition network, explicitly models adversarial noise and help us to infer natural labels from adversarial labels. The details are discussed below. Transition matrix. To model the adversarial noise, we need to relate adversarial labels and natural labels in a explicit form (e.g., a matrix). Inspired by recent researches in label-noise learning Xia et al. (2020); Liu & Tao (2015); Xia et al. (2019); Yang et al. (2021); Wu et al. (2021); Xia et al. (2021), we design a label transition matrix, which can encode the probabilities that adversarial labels ﬂip into natural labels. We then can infer natural labels by utilizing the transition matrix and adversarial data (see Fig. 1(a)). Note that the transition matrix used in our work is different from that used in label-noise learning. The detailed illustration can be found in Appendix A. Considering that the adversarial noise is instance-dependent, the transition matrix should be designed to depend on input instances, that is, we should ﬁnd a transition matrix speciﬁc to each input instance. 4In addition, we not only need to focus on the accuracy of adversarial instances, but also the accuracy of natural instances. An adversarially robust target model is often trained with both natural data and adversarial data. We therefore exploit the mixture of natural and adversarial instances (called mixture instances) as input instances, and design the transition matrix to model the relationship between the mixture of natural and adversarial labels (called mixture labels) to the ground-truth labels of mixture instances (called natural labels). Speciﬁcally, let X′denote the variable for the mixture instances, Y′denote the variable for the mixture labels, and Y denote the variable for the natural labels. We combine the natural data and adversarial data as the mixture data, i.e., {(x′ i,y′ i)}2n i=1 = {(xi,yi)}n i=1 ∪{( ˜xi, ˜yi)}n i=1, where {(x′ i,y′ i)}2n i=1 is the data drawn according to a distribution of the random variables (X′,Y ′). Since we have the ground-truth labels of mixture instances (i.e., natural labels), we can extend the mixture data {(x′ i,y′ i)}2n i=1 into a triplet form {(x′ i,y′ i,yi)}2n i=1. We utilize the transition matrixT ∈[0,1]C×C to model the relationship between mixture labels Y′and natural labels Y. T is dependent of the mixture instances X′. It is deﬁned as: Ti,j(X′= x′) =P(Y = j |Y′= i,X′= x′) , (2) where Ti,j denotes the (i,j) −thelement of the matrix T(X′= x′). It indicates the probability of the mixture label iﬂipped to the natural label jfor the input x′. The basic idea is that given the mixture class posterior probability (i.e., the class posterior probability of the mixture labels Y′) P(Y ′|X′= x′) = [P(Y′= 1|X′= x′),...,P (Y′= C |X′= x′)]⊤, the natural class posterior probability (i.e., the class posterior probability of natural labels Y) P(Y |X′= x′) could be inferred by exploiting P(Y ′|X′= x′) and T(X′= x′): P(Y |X′= x′) =T(X′= x′)⊤P(Y ′|X′= x′). (3) We then can obtain the natural labels by choose the class labels that maximize the robust class posterior probabilities. Note that the mixture class posterior probability P(Y ′|X′= x′) can be estimated by exploiting the mixture data. Transition network. we employ a deep neural network (called transition network) to estimate the label transition matrix by exploiting the mixture data {(x′ i,y′ i,yi)}2n i=1. Speciﬁcally, as shown in Fig. 1(b), the transition network gω(·) parameterized by ωtakes the mixture instance x′as input, and output the label transition matrix ˆT(x′; ω) =gω(x′). We then can infer the natural class posterior probability P(Y |X′= x′) according to Eq. 3, and thus infer the natural label. To optimize the parameter ωof the transition network, we minimize the difference between the inferred natural labels and the ground-truth natural labels. The loss function of the transition network is deﬁne as: LT(ω) =−1 2n 2n∑ i=1 ℓ(y′ i ·ˆT(x′ i; ω),yi), (4) where y′ i, yi are y′ i, yi in the form of vectors. ℓ(·) is the cross-entropy loss between the inferred natural labels and the ground-truth natural labels, i.e., ℓ(y′ i ·ˆT(x′ i; ω),yi) =yi ·log(y′ i ·ˆT(x′ i; ω)). 3.3 T RAINING Considering that adversarial data can be adaptively generated, and the adversarial labels are also depended on the target model, we conduct joint adversarial training on the target model and the transition network to achieve the optimal adversarial accuracy. We provide the overview of the training procedure in Fig. 2. For the transition network, we optimize its model parameter ωaccording to Eq. 4. For the target model, considering that the ﬁnal inferred natural class posterior probability is inﬂuenced by the target model, we also use the cross-entropy loss between the inferred natural labels and the ground-truth natural labels to optimize the parameter θ. The loss function for the target model hθ is deﬁned as: Ltar(θ) =−1 2n 2n∑ i=1 [yi ·log(ˆy′ i ·ˆT(x′ i; ω))], ˆy′ i = δ(hθ(x′ i)), (5) 5Algorithm 1 Training the defense model based on Modeling Adversarial Noise (MAN). Input: Target model hθ(·) parameterized by θ, transition network gω(·) parameterized by ω, batch size m, and the perturbation budget ϵ; 1: repeat 2: Read mini-batch B= {xi}m i=1 from training set; 3: Craft adversarial instance {˜xi}m i=1 at the given perturbation budget ϵfor each instance xi in B; 4: Obtain the mixture mini-batch B′= {x′ i}2m i=1 with mixture labels {y′ i}2m i=1; 5: for i= 1to 2m(in parallel) do 6: Forward-pass x′ i through hθ(·) and obtain ˆy′ i; 7: Forward-pass x′ i through gω(·) to estimate ˆT(x′ i; ω); 8: Infer the ﬁnal prediction label by exploiting ˆy′ i and ˆT(x′ i; ω); 9: end for 10: Calculate LT(ω) and Ltar(θ) using Eq. 4 and Eq. 5; 11: Back-pass and update ωand θ; 12: until training converged. where δ(·) denote the softmax function. ˆy′ i denote the mixture label in the form of a vector predicted by the target model. The details of the overall procedure are presented in Alg. 1. Speciﬁcally, for each mini-batch B= {xi}m i=1 sampled from natural training set, we ﬁrst generate adversarial instances ˜B= {˜xi}m i=1 via a strong adversarial attack algorithm, and obtain the mixture mini-batch B′ = {x′ i}2m i=1 (i.e., mixture of Band ˜B) with mixture labels {y′ i}2m i=1 (i.e., mixture of adversarial labels {˜yi}m i=1 and natural labels {yi}m i=1). Then, we input the mixture instances {x′ i}2m i=1 into the target model hθ(·) and output {ˆy′ i}2m i=1. We also input the mixture instances {x′ i}2m i=1 into the transition network gω(·) to estimate the label transition matrices {ˆT(x′ i; ω)}2m i=1. We next infer the ﬁnal prediction labels by exploiting {ˆy′ i}2m i=1 and {ˆT(x′ i; ω)}2m i=1. Finally, we compute the loss functions LT(ω) and Ltar(θ), and update the parameters ωand θ. By iteratively conducting the procedures of adversarial instance generation and defense training, ωand θare expected to be adversarially optimized. To demonstrate the effectiveness of joint adversarial training, we conduct an ablation study by independently training the transition network with a ﬁxed pre-trained target model. In addition, to prove that the improvement of our method is not mainly due to the introduction of more model parameters, we conduct an additional experiment by using an adversarially trained target model fused by two backbone networks as the baseline. The details could be found in Section 4.3. Table 1: Adversarial accuracy (percentage) of defense methods against white-box adaptive attacks on CIFAR-10 and Tiny-ImageNet. The target model is ResNet-18. We show the most successful defense with bold. Dataset Defense None PGD-40 AA FW A-40 CW 2 DDN CIFAR-10 AT 83.39 42.38 39.01 15.44 0.00 0.09 MAN 82.72 44.83 39.43 29.53 43.17 10.63 TRADES 80.70 46.29 42.71 20.54 0.00 0.06 MAN TRADES 80.34 48.65 44.40 29.13 1.46 0.31 MART 78.21 50.23 43.96 25.56 0.02 0.07 MAN MART 77.83 50.95 44.42 31.23 1.53 0.47 Tiny-ImageNet AT 48.40 17.35 11.27 10.29 0.00 0.29 MAN 48.29 18.15 12.45 13.17 16.27 4.01 TRADES 48.25 19.17 12.63 10.67 0.00 0.05 MAN TRADES 48.19 20.12 12.86 14.91 0.67 1.10 MART 47.83 20.90 15.57 12.95 0.00 0.06 MAN MART 47.79 21.22 15.84 15.10 0.89 1.23 64 E XPERIMENTS In this section, we ﬁrst introduce the experiment setup in Section 4.1. Then, we evaluate the effectiveness of our defense against representative and commonly used L∞norm and L2-norm adversarial attacks inSection 4.2. In addition, we conduct ablation studies in Section 4.3. Finally, we present that MAN is also suitable for detecting adversarial samples Section 4.4 4.1 E XPERIMENT SETUP Datasets. We verify the effective of our defense method on two popular benchmark datasets, i.e., CIFAR-10 Krizhevsky et al. (2009) and Tiny-ImageNet Wu et al. (2017). CIFAR-10 has 10 classes of images including 50,000 training images and 10,000 test images. Tiny-ImageNet has 200 classes of images including 100,000 training images, 10,000 validation images and 10,000 test images. Images in the two datasets are all regarded as natural instances. All images are normalized into [0,1], and are performed simple data augmentations in the training process, including random crop and random horizontal ﬂip. For the target model, we mainly use ResNet-18 He et al. (2016) for both CIFAR-10 and Tiny-ImageNet. Attack settings. Adversarial samples for evaluating defense models are crafted by applying state- of-the-art attacks. These attacks include L∞-norm PGD Madry et al. (2018), L∞-norm AA Croce & Hein (2020), L∞-norm FWA Wu et al. (2020b), L2-norm CW Carlini & Wagner (2017b) and L2-norm DDN Rony et al. (2019). Among them, the AA attack algorithm integrates three non-target attacks and a target attack. Other attack algorithms belong to non-target attacks. The iteration number of PGD and FW A is set to 40 with step size 0.007. The iteration numbers of CW2 and DDN are set to 200 and 40 respectively with step size 0.01. For CIFAR=10 and Tiny-ImageNet, the perturbation budgets for L2-norm attacks and L∞-norm attacks are ϵ= 0.5 and 8/255 respectively. Defense settings. We use three representative defense methods as the baselines: standard adversarial training method AT Madry et al. (2018), optimized adversarial training methods TRADES Zhang et al. (2019) and MART Wang et al. (2019). For all baselines and our defense method, we use the L∞-norm non-target PGD-10 (i.e., PGD with iteration number of 10) with random start and step size ϵ/4 to craft adversarial training data. The perturbation budget ϵis set to 8/255 for both CIFAR-10 and Tiny-ImageNet. All the defense models are trained using SGD with momentum 0.9 and an initial learning rate of 0.1. For our defense method, we exploit the ResNet-18 as the transition network for both CIFAR-10 and Tiny-ImageNet. Other detailed settings can be found in Appendix B. 4.2 D EFENSE EFFECTIVENESS Defending against adaptive attacks. A powerful adaptive attack strategy has been proposed to break defense methods Athalye et al. (2018); Carlini & Wagner (2017a). In this case, the attacker can access the architecture and model parameters of both the target model and the defense model, and then can design speciﬁc attack algorithms. We study the following three adaptive attack scenarios for evaluating our defense method. Scenario (i): disturb the ﬁnal output. Considering that the models in baselines (i.e., the target models) are completely leaked to the attacker in the white-box setting, for fair comparison, we utilize white-box adversarial attacks against the combination of the target model and the transition matrix. Similar to attacks against baselines, the goal of the non-target attack in this scenario is to maximize the distances between ﬁnal predictions of our defense and the ground-truth natural labels. The adversarial instance ˜xis crafted by solving the following optimization problem: max ˜x L(˜y ·ˆT(˜x; ω),y), subject to: ∥x−˜x∥≤ ϵ, (6) where ˜y = δ(hθ(˜x)) and L(·) denote the speciﬁc loss function used by each attack. Similarly, we can generate adversarial instances via the target attack. The details of the target attack are presented in Appendix C.1. We combine this attack strategy with ﬁve representative adversarial attacks introduced in Section Attack settings to evaluate defenses. The average natural accuracy (i.e., the results in the third column) and the average adversarial accuracy of defenses are shown in Tab. 1. 7The results show that our defense (i.e., MAN) achieves superior adversarial accuracy compared with AT. This presents that our defense is effective. Although our method has a slight drop in the natural accuracy (0.80%), it provides more gains for adversarial robustness (e.g., 5.78% against PGD-40 and 1.08% against stronger AA). In addition, our method achieves signiﬁcant improvements against some more destructive attacks (e.g., the adversarial accuracy is increased from 15.44% to 29.53% against FW A-40 and from 0.09% to 10.63% against DDN). The standard deviation is shown in Appendix C.1. Besides, we evaluate the effectiveness of our defense method on other model architecture by using the VggNet-19 as the target model and the transition network. In addtion, we evaluate the robustness performance of our defense method at a small batch-size (e.g., 128). These detailed results are also shown in Appendix C.1. Note that the training mechanism in our method can be regarded as the standard adversarial training on the combination of the target network and the transition matrix, our method thus is applicable to different adversarial training methods. To avoid the bias caused by different adversarial training methods, we apply the optimized adversarial training methods TRADES and MART to our method respectively (i.e., MAN TRADES and MAN MART). As shown in Tab. 1, the results show that our method can improve the adversarial accuracy. Although our method has a slight drop in the natural accuracy (e.g., 0.49% on CIFAR-10 and 0.08% on Tiny-ImageNet for MART), it provides more gains for adversarial robustness (e.g., 1.43% and 1.53% against PGD-40 on CIFAR-10 and Tiny-ImageNet respectively). Besides, we ﬁnd that using TRADES and MART affects the improvement of defense effectiveness against L2-norm based attacks (e.g., CW2 and DDN). We will further study and address this issue in the future work. Scenario (ii): attack the transition matrix. In this scenario, we design an adversarial attack to destroy the crucial transition matrix of our defense method. If the transition matrix is destroyed, the defense would immediately become ineffective. Since the ground-truth transition matrix is not given, we use the target attack strategy to craft adversarial samples. We choose an anti-diagonal identity matrix (see Fig. 5 in Appendix C.2) as an example of the target transition matrix in the target attack. The optimization goal is designed as: max ˜x −Lmse( ˆT(˜x; ω),T∗), subject to: ∥x−˜x∥≤ ϵ, (7) where Lmse denote the mean square error loss. We use L∞-norm PGD-40 with this target attack strategy to evaluate the MAN-based defense trained in scenario (i). The adversarial accuracy is 70.18% on CIFAR-10, which represents that our defense is effective against such type of adaptive target attack. This may be because that the attack in scenario (i) also tries to craft adversarial data by destroying the transition matrix to reduce the adversarial accuracy. The transition network adversarially trained with such adversarial data thus have good robustness against the attack designed for the transition matrix. Scenario (iii): dual attack. We explore another adaptive attack scenario. In this scenario, the attack not only disturbs the ﬁnal prediction labels, but also disturbs the output of the target model. We call such attack dual adaptive attack. The optimization goal of the non-target dual attack can be designed as: max ˜x [L(˜y ·T(˜x; ω),y) +Lce(˜y,y)], subject to: ∥x−˜x∥≤ ϵ, (8) where ˜y = δ(hθ(˜x)), L(·) denote the speciﬁc loss function used by each adversarial attack andLce(·) is the cross-entropy loss of the target model, i.e. Lce(˜y,y) =−1 n ∑n i=1 yi ·log(˜y). Similarly, we can generate adversarial instances via the target attack. The details of the target attack are presented in Appendix C.3. We combine this dual attack strategy with ﬁve attacks to evaluate our defense model trained in Scenario (i). As shown in Tab. 2, the results in the second row are the adversarial accuracy of our MAN-based defense. The results in the third row (ModelT) are the accuracy of the target model for adversarial instances. The results demonstrate that our defense method can help infer natural labels by using the transition matrix and adversarial labels, and can provide effective protection against multiple attacks in this scenario. 8Table 2: Adversarial accuracy (percentage) of our MAN-based defense against dual white-box adaptive attacks on CIFAR-10. The target model is ResNet-18. Defense None PGD-40 AA FW A-40 CW DDN MAN 82.72 70.63 45.13 64.09 45.21 38.16 ModelT 8.67 2.36 1.43 1.65 2.10 0.73 Table 3: Adversarial accuracy (percentage) of our MAN-based defense against general attacks on CIFAR-10. The target model is ResNet-18. Defense None PGD-40 AA FW A-40 CW DDN MAN 89.01 81.07 79.90 80.02 77.89 77.82 ModelT 88.98 0.00 0.00 0.00 0.00 0.00 We note that the adversarial accuracy of MAN in Tab. 2 is higher than that in Tab. 1. This may be because, for some input instances, the dual attack mainly uses the gradient information of the attack loss against the target model, to generate adversarial noise for breaking the target model. Our MAN-based defense can model the adversarial noise (i.e., learn the transition relationship for the adversarial label predicted by the target model) and infer the ﬁnal natural label. The ﬁnal adversarial accuracy thus can be improved. We just showed three examples for non-targeted and targeted attacks designed to our defense model in the above three scenarios. Note that more different attacks can be designed, which however is beyond the scope of our work in this paper. Defending against general attacks. We also evaluate the effectiveness of the proposed defense method against general adversarial attacks. The general adversarial attacks usually only focus on disrupting the performance of the target model. We utilize ﬁve adversarial attacks to evaluate the proposed MAN-based defense method. To train the defense model, we use the adversarial instances crafted by non-target L∞-norm PGD-10 attack against the target model as the adversarial training data. The adversarial accuracy of our MAN-based defense and the adversarial accuracy of the target model (ModelT) are shown in Tab. 3. It can be seen that our defense method achieve a great defense effect against general adversarial attacks. Note that we only train the defense model in the proposed method (i.e., the transition network) and ﬁx the model parameters of the target model in this experiment. In addition, to illustrate that our method does not utilize gradient masking, we conduct additional experiments listed in Athalye et al. (2018). Detailed results can be found in the Appendix D Defense transferability. The work in Ilyas et al. (2019) shows that any two target models are likely to learn similar mixture pattern. Therefore, modeling the adversarial noise which manipulate such features would apply to both target models for improving adversarial accuracy. We apply the MAN-based defense which is trained for the ResNet-18 (ResNet), to other naturally pre-trained target models, such as VGG-19 (VGG) Simonyan & Zisserman (2015) and Wide-ResNet 28×10 (WRN) Zagoruyko & Komodakis (2016) for evaluating the transferability of our defense method. Against general adversarial attacks, we deploy the defense model trained in SectionDefending against general attacks on the VGG and WRN target models. In addition, against adaptive adversarial attacks, we deploy our defense model trained in Section Scenario (i) on VGG. The performances on CIFAR-10 are reported in Tab. 4. It can be seen that our defense method has a certain degree of transferability for providing cross-model protections against adversarial attacks. Moreover, for black-box target models whose model parameters and training manners cannot be accessed (e.g., the VGG target model), we can ﬁne-tune the transition network in our defense method to further improve the defense effect against adaptive adversarial attacks. As shown by MAN ⋆ in Tab. 4, the ﬁne-tuned defense model achieves a higher adversarial accuracy. 4.3 A BLATION STUDY To demonstrate that compared with only training the transition network, the joint adversarial training on the target model and the transition network could achieve better defense effectiveness, we conduct an ablation study. Speciﬁcally, we use a naturally pre-trained ResNet-18 target model, and train the transition network independently (called MAN-). The model parameters of the target model are 9Table 4: Adversarial accuracy (percentage) of our defense method for different target models on CIFAR-10. MAN⋆ denote the ﬁne-tuned defense model on VGG. Defense PGD-40 AA FW A-40 CW 2 DDN General adverarial attacks ResNet-MAN 81.07 79.90 80.02 77.89 77.82 VGG-MAN 71.93 68.62 68.19 70.91 70.76 WRN-MAN 72.08 69.13 68.83 71.12 70.93 Adaptive adverarial attacks ResNet-MAN 44.83 39.43 29.53 43.17 10.63 VGG-MAN 11.71 7.36 6.98 6.31 2.29 VGG-MAN⋆ 33.26 30.30 13.42 10.07 3.83 0 10 20 30 40 50 60 70 80 90 None PGD-40 AA FWA-40 CW DDN Adversarial accuracy (%) MAN MAN- 0 10 20 30 40 50 60 70 80 90 None PGD-40 AA FWA-40 CW DDN Adversarial accuracy (%) MAN MAN' Figure 3: Ablation study by independently training the transition network. ﬁxed in the training procedure. We use the adversarial instances crafted by the adaptive L∞-norm PGD-10 as adversarial training data. Compared with jointly trained MAN, the results shown in Fig. 4.3 demonstrated the joint adversarial training manner can provide positive gains. In addition, to demonstrate that the improvement of our method has little relationship with the fact that the transition network introduces more model parameters, we conduct an experiment by using two parallel ResNet-18 as the target model of the baseline. We use the AT method to train the new target model. The result demonstrates that the gain of our method is not due to the increase of model parameters. The details and results can be found in Appendix E. 4.4 D ETECTING ADVERSARIAL SAMPLES Besides improving adversarial robustness, we ﬁnd that the proposed MAN can be utilized to detect adversarial samples. By observing the distribution of element values on the diagonal of the generated transition matrix, we can discriminate whether the input is natural or adversarial. Speciﬁcally, we use the label predicted by the target model as the index to obtain the element valuep from the diagonal of the transition matrix. If the value of the obtained element is larger than the values of other elements on the diagonal, the input sample is a natural sample, otherwise, it is an adversarial sample. Therefore, we take 1 −p as the probability that the input is adversarial. We use 10,000 test samples onCIFAR-10 to compute the AUROC against L∞-norm PGD-40 and L2-norm DDN (see Fig. 4). These results further validate the roles of the transition network and the transition matrix in defending against adversarial attacks. (a) PGD  (b) DDN Figure 4: AUROC of detecting adversarial samples. 105 C ONCLUSION Traditional adversarial defense methods typically focus on directly exploiting adversarial instances to remove adversarial noise or train an adversarially robust model. In this paper, motivated by that the relationship between adversarial data and natural data can help infer clean data from adversarial data, we study to model adversarial noise by learning the label transition relationship for improving adversarial accuracy. We propose a defense method based on Modeling Adversarial Noise (called MAN). Speciﬁcally, we embed a label transition matrix into the target model, which denote the transition relationship from adversarial labels to natural labels. The transition matrix explicitly models adversarial noise and help us infer natural labels. We design a transition network to generate the instance-independent transition matrix. Considering that adversarial data can be adaptively generated, we conduct joint adversarial training on the target model and the transition network to achieve an optimal adversarial accuracy. The empirical results demonstrate that our defense method can provide effective protection against white-box general attacks and adaptive attacks. Our work provides a new adversarial defense strategy for the community of adversarial learning. In future, we will further optimize the MAN-based defense method to improve its transferability and its performance when applied to other adversarial training methods. For datasets with more classes, how to effectively learn the transition matrix is also our future focus. 6 A CKNOWLEDGEMENTS This work was supported in part by the National Key Research and Development Program of China under Grant 2018AAA0103202, in part by the National Natural Science Foundation of China under Grant 61922066, 61876142, 62036007 and 62006202, in part by the Technology Innovation Leading Program of Shaanxi under Grant 2022QFY01-15, in part by Open Research Projects of Zhejiang Lab under Grant 2021KG0AB01, in part by the RGC Early Career Scheme No. 22200720, in part by Guangdong Basic and Applied Basic Research Foundation No. 2022A1515011652, in part by Australian Research Council Projects DE-190101473, IC-190100031, and DP-220102121, in part by the Fundamental Research Funds for the Central Universities, and in part by the Innovation Fund of Xidian University. The authors thank the reviewers and the meta-reviewer for their helpful and constructive comments on this work. Thanks to Chaojian Yu for his important advice on Section attack the transition matrix. REFERENCES Athalye, A., Carlini, N., and Wagner, D. A. Obfuscated gradients give a false sense of security: Cir- cumventing defenses to adversarial examples. In Proceedings of the 35th International Conference on Machine Learning, 2018. Carlini, N. and Wagner, D. Magnet and” efﬁcient defenses against adversarial attacks” are not robust to adversarial examples. arXiv preprint arXiv:1711.08478, 2017a. Carlini, N. and Wagner, D. Towards evaluating the robustness of neural networks. In 2017 Ieee Symposium on Security and Privacy (sp), pp. 39–57. IEEE, 2017b. Carmon, Y ., Raghunathan, A., Schmidt, L., Duchi, J. C., and Liang, P. Unlabeled data improves adversarial robustness. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d’Alch´e-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11190–11201, 2019. Croce, F. and Hein, M. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In Proceedings of the 37th International Conference on Machine Learning, 2020. Ding, G. W., Lui, K. Y . C., Jin, X., Wang, L., and Huang, R. On the sensitivity of adversarial robustness to input data distributions. In ICLR (Poster), 2019. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015. 11Guo, C., Rana, M., Ciss ´e, M., and van der Maaten, L. Countering adversarial images using input transformations. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016. Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., and Madry, A. Adversarial examples are not bugs, they are features. arXiv preprint arXiv:1905.02175, 2019. Jin, G., Shen, S., Zhang, D., Dai, F., and Zhang, Y . APE-GAN: adversarial perturbation elimination with GAN. In International Conference on Acoustics, Speech and Signal Processing, pp. 3842– 3846, 2019. Kaiming, H., Georgia, G., Piotr, D., and Ross, G. Mask r-cnn. IEEE Transactions on Pattern Analysis & Machine Intelligence, PP:1–1, 2017. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. LeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Liao, F., Liang, M., Dong, Y ., Pang, T., Hu, X., and Zhu, J. Defense against adversarial attacks using high-level representation guided denoiser. In Conference on Computer Vision and Pattern Recognition, pp. 1778–1787, 2018. Liu, T. and Tao, D. Classiﬁcation with noisy labels by importance reweighting. IEEE Transactions on pattern analysis and machine intelligence, 38(3):447–461, 2015. Ma, X., Li, B., Wang, Y ., Erfani, S. M., Wijewickrema, S. N. R., Schoenebeck, G., Song, D., Houle, M. E., and Bailey, J. Characterizing adversarial subspaces using local intrinsic dimensionality. In International Conference on Learning Representations, 2018. Ma, X., Niu, Y ., Gu, L., Wang, Y ., Zhao, Y ., Bailey, J., and Lu, F. Understanding adversarial attacks on deep learning based medical image analysis systems. Pattern Recognition, 110:107332, 2021. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations , 2018. Naseer, M., Khan, S., Hayat, M., Khan, F. S., and Porikli, F. A self-supervised approach for adversarial robustness. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 262–271, 2020. Rony, J., Hafemann, L. G., Oliveira, L. S., Ayed, I. B., Sabourin, R., and Granger, E. Decoupling direction and norm for efﬁcient gradient-based L2 adversarial attacks and defenses. In Conference on Computer Vision and Pattern Recognition, pp. 4322–4330, 2019. Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. In Bengio, Y . and LeCun, Y . (eds.),3rd International Conference on Learning Representations, 2015. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., and Fergus, R. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014. Wang, Y ., Zou, D., Yi, J., Bailey, J., Ma, X., and Gu, Q. Improving adversarial robustness requires revisiting misclassiﬁed examples. In International Conference on Learning Representations, 2019. Wu, D., Xia, S.-T., and Wang, Y . Adversarial weight perturbation helps robust generalization. Advances in Neural Information Processing Systems, 33, 2020a. Wu, J., Zhang, Q., and Xu, G. Tiny imagenet challenge. Technical Report, 2017. 12Wu, K., Wang, A. H., and Yu, Y . Stronger and faster wasserstein adversarial attacks. InProceedings of the 37th International Conference on Machine Learning, volume 119, pp. 10377–10387, 2020b. Wu, S., Xia, X., Liu, T., Han, B., Gong, M., Wang, N., Liu, H., and Niu, G. Class2simi: A noise reduction perspective on learning with noisy labels. In International Conference on Machine Learning, pp. 11285–11295. PMLR, 2021. Xia, X., Liu, T., Wang, N., Han, B., Gong, C., Niu, G., and Sugiyama, M. Are anchor points really indispensable in label-noise learning? arXiv preprint arXiv:1906.00189, 2019. Xia, X., Liu, T., Han, B., Wang, N., Gong, M., Liu, H., Niu, G., Tao, D., and Sugiyama, M. Part- dependent label noise: Towards instance-dependent label noise. Advances in Neural Information Processing Systems, 33, 2020. Xia, X., Liu, T., Han, B., Gong, C., Wang, N., Ge, Z., and Chang, Y . Robust early-learning: Hindering the memorization of noisy labels. In International Conference on Learning Representations, 2021. Xiao, C., Zhu, J., Li, B., He, W., Liu, M., and Song, D. Spatially transformed adversarial examples. In 6th International Conference on Learning Representations, 2018. Xu, W., Evans, D., and Qi, Y . Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155, 2017. Yang, S., Yang, E., Han, B., Liu, Y ., Xu, M., Niu, G., and Liu, T. Estimating instance-dependent label-noise transition matrix using dnns. arXiv preprint arXiv:2105.13001, 2021. Zagoruyko, S. and Komodakis, N. Wide residual networks. In Wilson, R. C., Hancock, E. R., and Smith, W. A. P. (eds.),Proceedings of the British Machine Vision Conference 2016, 2016. Zhang, H., Yu, Y ., Jiao, J., Xing, E., El Ghaoui, L., and Jordan, M. Theoretically principled trade- off between robustness and accuracy. In International Conference on Machine Learning , pp. 7472–7482. PMLR, 2019. Zhou, D., Liu, T., Han, B., Wang, N., Peng, C., and Gao, X. Towards defending against adversarial examples via attack-invariant features. In Proceedings of the 38th International Conference on Machine Learning, pp. 12835–12845, 2021a. Zhou, D., Wang, N., Peng, C., Gao, X., Wang, X., Yu, J., and Liu, T. Removing adversarial noise in class activation feature space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7878–7887, 2021b. 13Appendices A T HE DIFFERENCE OF THE TRANSITION MATRIX BETWEEN OUR METHOD AND LABEL -NOISE LEARNING In label-noise learning, the transition matrix is used to infer clean labels from given noisy labels. The transition matrix denotes the probabilities that clean labels ﬂip into noisy labels Xia et al. (2020); Liu & Tao (2015); Yang et al. (2021). The label-noise learning methods utilize the transition matrix to correct the training loss on noisy data (i.e., the clean instance with the noisy label). Given the noisy class posterior probability, the clean class posterior probability can be obtained, i.e., p(y|x) = (T(x)⊤)−1p(y∗|x), where y∗denotes the noisy label in the form of a vector. Differently, in our method, the transition matrix is used to infer natural labels from observed adversarial labels. The transition matrix denotes the probabilities that adversarial labels ﬂip into natural labels. We utilize the transition matrix to help compute the training loss on the adversarial instance with the natural label. For the observed adversarial class posterior probability, the natural class posterior probability can be obtained, i.e., p(y|˜x) =T(˜x)⊤p(˜y|˜x). In addition, to the best of our knowledge, our method for the ﬁrst time utilizes the transition matrix to explicitly model adversarial noise for improving adversarial robustness. B T RAINING SETTINGS For all baselines and our defense method, we use the L∞-norm non-target PGD-10 (i.e., PGD with iteration number of 10) with random start and step size ϵ/4 to craft adversarial training data. The perturbation budget ϵis set to 8/255 for both CIFAR-10 and Tiny-ImageNet. All the defense models are trained using SGD with momentum 0.9 and an initial learning rate of 0.1. The weight decay is 2 ×10−4 for CIFAR-10, and is 5 ×10−4 for Tiny-ImageNet. The batch-size is set as 1024 to reduce time cost. For a fair comparison, we adjust the hyperparameter settings of the defense methods so that the natural accuracy is not severely compromised and then compare the robustness. The epoch number is set to 100. The learning rate is divided by 10 at the 75-th and 90-th epoch. We report the evaluation results of the last checkpoint rather than those of the best checkpoint. C D EFENDING AGAINST ADAPTIVE ATTACKS C.1 S CENARIO (I): DISTURB THE FINAL OUTPUT Table 5: Adversarial accuracy (percentage) of defense methods against white-box adaptive attacks on CIFAR-10. The target model is ResNet-18. We show the most successful defense with bold. Defense None PGD-40 AA FW A-40 CW 2 DDN AT 83.39±0.95 42.38±0.56 39.01 ±0.51 15.44 ±0.32 0.00 ±0.00 0.09 ±0.03 MAN 82.72 ±0.53 44.83±0.47 39.43 ±0.73 29.53 ±0.47 43.17 ±0.68 10.63 ±0.49 TRADES 80.70±0.63 46.29±0.59 42.71 ±0.49 20.54 ±0.47 0.00 ±0.00 0.06 ±0.01 MAN TRADES 80.34 ±0.61 48.65±0.41 44.40 ±0.56 29.13 ±0.60 1.46 ±0.21 0.31 ±0.05 MART 78.21±0.65 50.23±0.70 43.96 ±0.67 25.56 ±0.61 0.02 ±0.00 0.07 ±0.01 MAN MART 77.83 ±0.67 50.95±0.61 44.42 ±0.69 31.23 ±0.58 1.53 ±0.27 0.47 ±0.07 In scenario (i), the target attack solve the following optimization problem: max ˜x −L(˜y ·ˆT(˜x; ω),y∗), subject to: ∥x−˜x∥≤ ϵ, (9) where y∗is the target label in the form of a vector set by the attacker. The adversarial accuracy of defense methods against white-box adaptive attacks on CIFAR-10 and Tiny-ImageNet are shown in Tab. 5 and Tab. 6 respectively. 14Table 6: Adversarial accuracy (percentage) of defense methods against white-box adaptive attacks on Tiny-ImageNet. The target model is ResNet-18. We show the most successful defense with bold. Defense None PGD-40 AA FW A-40 CW 2 DDN AT 48.40±0.68 17.35±0.59 11.27 ±0.53 10.29 ±0.47 0.00 ±0.00 0.29 ±0.03 MAN 48.29 ±0.57 18.15±0.51 12.45 ±0.67 13.17 ±0.69 16.27 ±0.71 4.01 ±0.19 TRADES 48.25±0.71 19.17±0.58 12.63 ±0.51 10.67 ±0.68 0.00 ±0.00 0.05 ±0.01 MAN TRADES 48.19 ±0.62 20.12±0.49 12.86 ±0.59 14.91 ±0.47 0.67 ±0.12 1.10 ±0.15 MART 47.83±0.65 20.90±0.59 15.57 ±0.52 12.95 ±0.49 0.00 ±0.00 0.06 ±0.01 MAN MART 47.79 ±0.59 21.22±0.60 15.84 ±0.47 15.10 ±0.39 0.89 ±0.16 1.23 ±0.21 In addition, we evaluate the adversarial accuracy of defense methods against white-box adaptive attacks on CIFAR-10 by using the VggNet-19 as the target model and the transition network. As shown in Tab. 7, our method still achieves better performance. Moreover, we evaluate the robustness performance of our defense method at a small batch-size, such as 128. The initial learning rate is still 0.1. The results are shown in Tab. 8. Table 7: Adversarial accuracy (percentage) of defense methods against white-box adaptive attacks on CIFAR-10. The target model is VggNet-19. Defense None PGD-40 AA FW A-40 CW 2 DDN AT 80.91±0.61 29.83±0.43 26.00 ±0.31 7.55 ±0.19 0.10 ±0.01 0.15 ±0.02 MAN 80.25 ±0.50 37.13±0.61 33.19 ±0.43 17.88 ±0.44 38.13 ±0.37 6.10 ±0.09 Table 8: Adversarial accuracy (percentage) of defense methods against white-box adaptive attacks on CIFAR-10. The target model is ResNet-18. The batch-size is 128. Defense None PGD-40 AA AT 84.92±0.49 46.47±0.47 43.55 ±0.43 MAN 84.70 ±0.41 48.06±0.50 44.67 ±0.51 C.2 S CENARIO (II): ATTACK THE TRANSITION MATRIX In scenario (ii), we design an adversarial attack to destroy the crucial transition matrix of our defense. Since the ground-truth transition matrix is not given, we use the target attack strategy to craft adversarial examples. We choose an anti-diagonal identity matrix as an example of the target transition matrix in the target attack. The target transition matrix T∗is shown in Fig. 5. Note that there may be other attacks that can be designed, but this is beyond the scope of our work, and we would not explore further in this paper. C.3 S CENARIO (III ): DUAL ATTACK In scenario (iii), the optimization goal of the target attack can be designed as: max ˜x [−L(˜y ·T(˜x; ω),y∗) +Lce(˜y,y)], subject to: ∥x−˜x∥≤ ϵ, (10) where y∗is the target label in the form of a vector set by the attacker. Note that we report the ∗last∗adversarial accuracy (i.e., the adversarial accuracy of the last epoch models obtained during training) following the discussion in Carmon et al. (2019), instead of the ∗best∗results. 15Target transition matrix 0 0 ⋯ 0 0 ⋱ ⋮ 0 0 1 0 1 0 0 1 0 0 1 0 1 0 ⋮ 0 1 0 1 0 0 1 0 0 1 0 1 0 ⋯ 0 ⋱ 0 0 0 0 Figure 5: Target transition matrix T∗for the target adversarial attack. D T HE POSSIBILITY OF GRADIENT MASKING We consider ﬁve behaviors listed in Athalye et al. (2018) to identify the gradient masking. (i) One- step attacks do not perform better than iterative attacks. The accuracy against PGD-1 is 76.81% (vs 44.83% against PGD-40). (ii) Black-box attacks are not better (on attack success rate) than white-box attacks. We use ResNet-18 with standard AT as the target model to craft adversarial data. The accuracy against PGD-40/AA is 70.13%/67.30% (vs 44.83%/39.43% in white-box setting). (iii) Unbounded attacks reach 100% success. The accuracy against PGD-40 with ϵ= 255/255 is 0%. (iv) Random sampling does not ﬁnd adversarial examples. For samples that is not successfully attacked by PGD, we randomly sample 105 points within the ϵ-ball, and do not ﬁnd adversarial data. (v) Increasing distortion bound increases success. The accuracy against PGD-40 with increasing ϵ(8/255, 16/255, 32/255 and 64/255) is 44.83%, 26.59%, 14.66% and 8.12%. These results show that our method does not use gradient masking. E T HE INFLUENCE OF MORE MODEL PARAMETERS We think that the improvement of the adversarial robustness has little relationship with the fact that the transition network introduces more model parameters. The transition network is only used to learn the transition matrix, and it does not directly learn the logit output for the instance to predict the class label. The main reason why our method can improve the robustness is that we explicitly model adversarial noise in the form of the transition matrix. We can use this transition matrix to infer the natural label. In addition, considering that the model parameters of our method are indeed larger than those of AT, we conduct the following experiment to compare AT and our method in the case of similar number of model parameters. Considering that both the target model and the transition network in our method are mainly based on ResNet-18 on CIFAR-10, we use two parallel ResNet-18 as the target model to classify the input. We use the average of their logit outputs as the ﬁnal logit output. We use the training mechanism of AT to train the new target model. The target model for our MAN-based defense method is still ResNet-18. In this way, The number of model parameters of our method is similar to that of AT. As shown in Tab. 9, the results show that our method still has higher adversarial accuracy. This demonstrates that the improvement of the adversarial robustness is not due to the increase of model parameters. Table 9: Adversarial accuracy (percentage) of defense methods against white-box adaptive attacks on CIFAR-10. The defense models have a similar number of model parameters. Defense None PGD-40 AA FW A-40 CW 2 DDN AT 83.53±0.61 42.40±0.59 39.09 ±0.60 15.46 ±0.47 0.00 ±0.00 0.08 ±0.01 MAN 82.72 ±0.53 44.83±0.47 39.43 ±0.73 29.53 ±0.47 43.17 ±0.68 10.63 ±0.49 16",
      "references": [
        "Obfuscated gradients give a false sense of security: Cir- cumventing defenses to adversarial examples.",
        "Magnet and” efﬁcient defenses against adversarial attacks” are not robust to adversarial examples.",
        "Towards evaluating the robustness of neural networks.",
        "Unlabeled data improves adversarial robustness.",
        "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.",
        "On the sensitivity of adversarial robustness to input data distributions.",
        "Explaining and harnessing adversarial examples.",
        "Countering adversarial images using input transformations.",
        "Deep residual learning for image recognition.",
        "Adversarial examples are not bugs, they are features.",
        "APE-GAN: adversarial perturbation elimination with GAN.",
        "Mask r-cnn.",
        "Learning multiple layers of features from tiny images.",
        "Gradient-based learning applied to document recognition.",
        "Defense against adversarial attacks using high-level representation guided denoiser.",
        "Classification with noisy labels by importance reweighting.",
        "Characterizing adversarial subspaces using local intrinsic dimensionality.",
        "Understanding adversarial attacks on deep learning based medical image analysis systems.",
        "Towards deep learning models resistant to adversarial attacks.",
        "A self-supervised approach for adversarial robustness.",
        "Decoupling direction and norm for efﬁcient gradient-based L2 adversarial attacks and defenses.",
        "Very deep convolutional networks for large-scale image recognition.",
        "Intriguing properties of neural networks.",
        "Improving adversarial robustness requires revisiting misclassified examples.",
        "Adversarial weight perturbation helps robust generalization.",
        "Tiny imagenet challenge.",
        "Stronger and faster wasserstein adversarial attacks.",
        "Class2simi: A noise reduction perspective on learning with noisy labels.",
        "Are anchor points really indispensable in label-noise learning?",
        "Part- dependent label noise: Towards instance-dependent label noise.",
        "Robust early-learning: Hindering the memorization of noisy labels.",
        "Spatially transformed adversarial examples.",
        "Feature squeezing: Detecting adversarial examples in deep neural networks.",
        "Estimating instance-dependent label-noise transition matrix using dnns.",
        "Wide residual networks.",
        "Theoretically principled trade- off between robustness and accuracy.",
        "Towards defending against adversarial examples via attack-invariant features.",
        "Removing adversarial noise in class activation feature space."
      ],
      "meta_data": {
        "arxiv_id": "2109.09901v5",
        "authors": [
          "Dawei Zhou",
          "Nannan Wang",
          "Bo Han",
          "Tongliang Liu"
        ],
        "published_date": "2021-09-21T01:13:26Z",
        "github_url": "https://github.com/dwDavidxd/MAN"
      },
      "llm_extracted_info": {
        "main_contributions": "Problem: adversarial robustness requires modeling adversarial noise; contribution: propose MAN - Modeling Adversarial Noise by learning an instance-dependent label transition from adversarial labels to natural labels; embed a transition matrix via a transition network into the target model; jointly train the target model and transition network under adversarial training to improve adversarial accuracy; show empirical gains against white-box and adaptive attacks and potential for detecting adversarial samples.",
        "methodology": "Introduce a label transition matrix T(X') with elements Ti,j(X') = P(Y=j | Y'=i, X'=x'); transition network g_ω(x') outputs ˆT(x'; ω). Mix natural and adversarial data into mixture data (x', y') with corresponding mixture labels, along with ground-truth natural labels y. Infer natural label via P(Y|X') = T^⊤ P(Y'|X'). Train ˆT by minimizing cross-entropy between inferred natural labels and ground-truth, LT(ω). Train target model h_θ with cross-entropy using inferred natural labels, Ltar(θ); joint optimization of θ and ω during adversarial training (algorithm 1).",
        "experimental_setup": "Datasets: CIFAR-10 and Tiny-ImageNet; target model: ResNet-18; attacks for evaluation: PGD, AA, FW-A, CW2, DDN; perturbation budgets: ε=8/255 for L∞ and ε=0.5 (L2) per dataset; baselines: AT, TRADES, MART; evaluation includes white-box adaptive attacks, scenario (i) distorting final output, scenario (ii) attacking the transition matrix, scenario (iii) dual attacks, plus general attacks; ablation studies, transferability tests, and detection experiments using transition matrix diagonals.",
        "limitations": "Adversarial accuracy gains sometimes accompany slight natural accuracy reductions; relies on effective learning of instance-dependent transition matrices; additional transition network increases complexity and training time; evaluated mainly on CIFAR-10 and Tiny-ImageNet with specific architectures (ResNet-18, VGG-19, WRN) and may not generalize identically to other domains or very large number of classes; potential vulnerabilities under unforeseen adaptive attacks; some results depend on attack settings and budgets.",
        "future_research_directions": "Extend to more classes and larger datasets; improve transferability across models; explore alternative architectures for the transition network; integrate MAN with other adversarial training paradigms; reduce computational overhead; study theoretical guarantees and robustness bounds; explore instance-dependent transition modeling beyond label-space (e.g., feature-space transitions); apply MAN to adversarial detection and certification; investigate unsupervised or semi-supervised estimation of transition matrices.",
        "experimental_code": "<CONTENT_NOT_SHOWN>",
        "experimental_info": "<CONTENT_NOT_SHOWN>"
      }
    },
    {
      "title": "Graph Contrastive Learning with Augmentations",
      "full_text": "Graph Contrastive Learning with Augmentations Yuning You1*, Tianlong Chen2*, Yongduo Sui3, Ting Chen4, Zhangyang Wang2, Yang Shen1 1Texas A&M University,2University of Texas at Austin, 3University of Science and Technology of China, 4Google Research, Brain Team {yuning.you,yshen}@tamu.edu, {tianlong.chen,atlaswang}@utexas.edu syd2019@mail.ustc.edu.cn, iamtingchen@google.com Abstract Generalizable, transferrable, and robust representation learning on graph-structured data remains a challenge for current graph neural networks (GNNs). Unlike what has been developed for convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training are less explored for GNNs. In this paper, we propose a graph contrastive learning (GraphCL) framework for learning unsupervised representations of graph data. We ﬁrst design four types of graph augmentations to incorporate various priors. We then systematically study the impact of various combinations of graph augmentations on multiple datasets, in four different settings: semi-supervised, unsupervised, and transfer learning as well as adversarial attacks. The results show that, even without tuning augmentation extents nor using sophisticated GNN architectures, our GraphCL framework can produce graph representations of similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods. We also investigate the impact of parameterized graph augmentation extents and patterns, and observe further performance gains in preliminary experiments. Our codes are available at: https://github.com/Shen-Lab/GraphCL. 1 Introduction Graph neural networks (GNNs) [1, 2, 3], following a neighborhood aggregation scheme, are increas- ingly popular for graph-structured data. Numerous variants of GNNs have been proposed to achieve state-of-the-art performances in graph-based tasks, such as node or link classiﬁcation [1, 2, 4, 5, 6], link prediction [ 7] and graph classiﬁcation [ 8, 3]. Intriguingly, in most scenarios of graph-level tasks, GNNs are trained end-to-end under supervision. For GNNs, there is little exploration (except [9]) of (self-supervised) pre-training, a technique commonly used as a regularizer in training deep architectures that suffer from gradient vanishing/explosion [10, 11]. The reasons behind the intriguing phenomena could be that most studied graph datasets, as shown in [12], are often limited in size and GNNs often have shallow architectures to avoid over-smoothing [13] or “information loss” [14]. We however argue for the necessity of exploring GNN pre-training schemes. Task-speciﬁc labels can be extremely scarce for graph datasets (e.g. in biology and chemistry labeling through wet-lab experiments is often resource- and time-intensive) [ 15, 9], and pre-training can be a promising technique to mitigate the issue, as it does in convolutional neural networks (CNNs) [16, 17, 18]. As to the conjectured reasons for the lack of GNN pre-training: ﬁrst, real-world graph data can be huge and even benchmark datasets are recently getting larger [12, 19]; second, even for shallow models, pre- training could initialize parameters in a “better\" attraction basin around a local minimum associated with better generalization [11]. Therefore, we emphasize the signiﬁcance of GNN pre-training. Compared to CNNs for images, there are unique challenges of designing GNN pre-training schemes for graph-structured data. Unlike geometric information in images, rich structured information of *Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2010.13902v3  [cs.LG]  3 Apr 2021various contexts exist in graph data [20, 21] as graphs are abstracted representations of raw data with diverse nature (e.g. molecules made of chemically-bonded atoms and networks of socially-interacting people). It is thus difﬁcult to design a GNN pre-training scheme generically beneﬁcial to down-stream tasks. A naïve GNN pre-training scheme for graph-level tasks is to reconstruct the vertex adjacency information (e.g. GAE [22] and GraphSAGE [23] in network embedding). This scheme can be very limited (as seen in [ 20] and our Sec. 5) because it over-emphasizes proximity that is not always beneﬁcial [20], and could hurt structural information [24]. Therefore, a well designed pre-training framework is needed to capture highly heterogeneous information in graph-structured data. Recently, in visual representation learning, contrastive learning has renewed a surge of interest [25, 26, 27, 18, 28]. Self-supervision with handcrafted pretext tasks [29, 30, 31, 32] relies on heuristics to design, and thus could limit the generality of the learned representations. In comparison, contrastive learning aims to learn representations by maximizing feature consistency under differently augmented views, that exploit data- or task-speciﬁc augmentations [33], to inject the desired feature invariance. If extended to pre-training GCNs, this framework can potentially overcome the aforementioned limitations of proximity-based pre-training methods [ 22, 23, 34, 35, 36, 37, 38, 39]. However, it is not straightforward to be directly applied outside visual representation learning and demands signiﬁcant extensions to graph representation learning, leading to our innovations below. Contributions. In this paper, we have developed contrastive learning with augmentations for GNN pre-training to address the challenge of data heterogeneity in graphs. (i) Since data augmentations are the prerequisite for constrastive learning but are under-explored in graph-data [ 40], we ﬁrst design four types of graph data augmentations, each of which imposes certain prior over graph data and parameterized for the extent and pattern. (ii) Utilizing them to obtain correlated views, we propose a novel graph contrastive learning framework (GraphCL) for GNN pre-training, so that representations invariant to specialized perturbations can be learned for diverse graph-structured data. Moreover, we show that GraphCL actually performs mutual information maximization, and the connection is drawn between GraphCL and recently proposed contrastive learning methods that we demonstrate that GraphCL can be rewritten as a general frameworkunifying a broad family of contrastive learning methods on graph-structured data. (iii) Systematic study is performed to assess the performance of contrasting different augmentations on various types of datasets, revealing the rationale of the performances and providing the guidance to adopt the framework for speciﬁc datasets. (iv) Experiments show that GraphCL achieves state-of-the-art performance in the settings of semi-supervised learning, unsupervised representation learning and transfer learning. It additionally boosts robustness against common adversarial attacks. 2 Related Work Graph neural networks. In recent years, graph neural networks (GNNs) [1, 2, 3] have emerged as a promising approach for analyzing graph-structured data. They follow an iterative neighborhood aggregation (or message passing) scheme to capture the structural information within nodes’ neigh- borhood. Let G= {V,E}denote an undirected graph, with X ∈R|V|×N as the feature matrix where xn = X[n,:]T is the N-dimensional attribute vector of the node vn ∈V. Considering a K-layer GNN f(·), the propagation of the kth layer is represented as: a(k) n = AGGREGATION(k)({h(k−1) n′ : n′∈N(n)}),h(k) n = COMBINE(k)(h(k−1) n ,a(k) n ), (1) where h(k) n is the embedding of the vertex vn at the kth layer with h(0) n = xn, N(n) is a set of vertices adjacent to vn, and AGGREGATION(k)(·) and COMBINE(k)(·) are component functions of the GNN layer. After the K-layer propagation, the output embedding for Gis summarized on layer embeddings through the READOUT function. Then a multi-layer perceptron (MLP) is adopted for the graph-level downstream task (classiﬁcation or regression): f(G) = READOUT({h(k) n : vn ∈V,k ∈K}), zG= MLP(f(G)). (2) Various GNNs have been proposed [1, 2, 3], achieving state-of-the-art performance in graph tasks. Graph data augmentation. Augmentation for graph-structured data still remains under-explored, with some work along these lines but requiring prohibitive additional computation cost [ 40]. Tra- ditional self-training methods [ 40, 13] utilize the trained model to annotate unlabelled data; [ 41] 2proposes to train a generator-classiﬁer network in the adversarial learning setting to generate fake nodes; and [42, 43] generate adversarial perturbations to node feature over the graph structure. Pre-training GNNs. Although (self-supervised) pre-training is a common and effective scheme for convolutional neural networks (CNNs) [16, 17, 18], it is rarely explored for GNNs. One exception [9] is restricted to studying pre-training strategies in the transfer learning setting, We argue that a pre-trained GNN is not easy to transfer, due to the diverse ﬁelds that graph-structured data source from. During transfer, substantial domain knowledge is required for both pre-training and downstream tasks, otherwise it might lead to negative transfer [9, 44]. Contrastive learning. The main idea of contrastive learning is to make representations agree with each other under proper transformations, raising a recent surge of interest in visual representation learning [45, 25, 26, 27, 18]. On a parallel note, for graph data, traditional methods trying to reconstruct the adjacency information of vertices [22, 23] can be treated as a kind of “local contrast”, while over-emphasizing the proximity information at the expense of the structural information [24]. Motivated by [46, 47], [24, 21, 48] propose to perform contrastive learning between local and global representations to better capture structure information. However, graph contrastive learning has not been explored from the perspective of enforcing perturbation invariance as [27, 18] have done. 3 Methodology 3.1 Data Augmentation for Graphs Data augmentation aims at creating novel and realistically rational data through applying certain transformation without affecting the semantics label. It still remains under-explored for graphs except some with expensive computation cost (see Sec. 2). We focus on graph-level augmentations. Given a graph G∈{G m : m ∈M}in the dataset of M graphs, we formulate the augmented graph ˆG satisfying: ˆG∼ q( ˆG|G), where q(·|G) is the augmentation distribution conditioned on the original graph, which is pre-deﬁned, representing the human prior for data distribution. For instance for image classiﬁcation, the applications of rotation and cropping encode the prior that people will acquire the same classiﬁcation-based semantic knowledge from the rotated image or its local patches [49, 50]. When it comes to graphs, the same spirit could be followed. However, one challenge as stated in Sec. 1 is that graph datasets are abstracted from diverse ﬁelds and therefore there may not be universally appropriate data augmentation as those for images. In other words, for different categories of graph datasets some data augmentations might be more desired than others. We mainly focus on three categories: biochemical molecules (e.g. chemical compounds, proteins) [ 9], social networks [1] and image super-pixel graphs [12]. Next, we propose four general data augmentations for graph-structured data and discuss the intuitive priors that they introduce. Table 1: Overview of data augmentations for graphs. Data augmentation Type Underlying Prior Node dropping Nodes, edges Vertex missing does not alter semantics. Edge perturbation Edges Semantic robustness against connectivity variations. Attribute masking Nodes Semantic robustness against losing partial attributes. Subgraph Nodes, edges Local structure can hint the full semantics. Node dropping. Given the graph G, node dropping will randomly discard certain portion of vertices along with their connections. The underlying prior enforced by it is that missing part of vertices does not affect the semantic meaning of G. Each node’s dropping probability follows a default i.i.d. uniform distribution (or any other distribution). Edge perturbation. It will perturb the connectivities in Gthrough randomly adding or dropping certain ratio of edges. It implies that the semantic meaning of Ghas certain robustness to the edge connectivity pattern variances. We also follow an i.i.d. uniform distribution to add/drop each edge. Attribute masking. Attribute masking prompts models to recover masked vertex attributes using their context information, i.e., the remaining attributes. The underlying assumption is that missing partial vertex attributes does not affect the model predictions much. Subgraph. This one samples a subgraph from Gusing random walk (the algorithm is summarized in Appendix A). It assumes that the semantics of Gcan be much preserved in its (partial) local structure. 3......  ......  ......  ......  ......  ......  Input Graph Augmentations Shared GNN-based Encoder Embeddings ......  ......  ......  ......  ......  ......  Node Dropping ......  ......  ......  ......  ......  ......  Edge Perturbation ......  ......  Projection Head Projection Head Maximize Agreement Drop Node & Edge Add & Delete Edge Figure 1: A framework of graph contrastive learning. Two graph augmentationsqi(·|G) and qj (·|G) are sampled from an augmentation pool Tand applied to input graph G. A shared GNN-based encoder f(·) and a projection head g(·) are trained to maximize the agreement between representations zi and zj via a contrastive loss. The default augmentation (dropping, perturbation, masking and subgraph) ratio is set at 0.2. 3.2 Graph Contrastive Learning Motivated by recent contrastive learning developments in visual representation learning (see Sec. 2), we propose a graph contrastive learning framework (GraphCL) for (self-supervised) pre-training of GNNs. In graph contrastive learning, pre-training is performed through maximizing the agreement between two augmented views of the same graph via a contrastive loss in the latent space as shown in Fig. 1. The framework consists of the following four major components: (1) Graph data augmentation. The given graph Gundergoes graph data augmentations to obtain two correlated views ˆGi, ˆGj, as a positive pair, where ˆGi ∼qi(·|G), ˆGj ∼qj(·|G) respectively. For different domains of graph datasets, how to strategically select data augmentations matters (Sec. 4). (2) GNN-based encoder. A GNN-based encoder f(·) (deﬁned in (2)) extracts graph-level represen- tation vectors hi,hj for augmented graphs ˆGi, ˆGj. Graph contrastive learning does not apply any constraint on the GNN architecture. (3) Projection head. A non-linear transformation g(·) named projection head maps augmented representations to another latent space where the contrastive loss is calculated, as advocated in [18]. In graph contrastive learning, a two-layer perceptron (MLP) is applied to obtain zi,zj. (4) Contrastive loss function. A contrastive loss function L(·) is deﬁned to enforce maximizing the consistency between positive pairs zi,zj compared with negative pairs. Here we utilize the normalized temperature-scaled cross entropy loss (NT-Xent) [51, 25, 52]. During GNN pre-training, a minibatch of N graphs are randomly sampled and processed through contrastive learning, resulting in2Naugmented graphs and corresponding contrastive loss to optimize, where we re-annotate zi,zj as zn,i,zn,j for the nth graph in the minibatch. Negative pairs are not explicitly sampled but generated from the other N −1 augmented graphs within the same minibatch as in [53, 18]. Denoting the cosine similarity function as sim(zn,i,zn,j) =zT n,izn,j/∥zn,i∥∥zn,j∥, NT-Xent for the nth graph is deﬁned as: ℓn = −log exp(sim(zn,i,zn,j)/τ)∑N n′=1,n′̸=nexp(sim(zn,i,zn′,j)/τ) , (3) where τ denotes the temperature parameter. The ﬁnal loss is computed across all positive pairs in the minibatch. The proposed graph contrastive learning is summarized in Appendix A. Discussion. We ﬁrst show that GraphCL can be viewed as one way of mutual information maximiza- tion between the latent representations of two kinds of augmented graphs. The full derivation is in Appendix F, with the loss form rewritten as below: ℓ= EP ˆGi {−EP( ˆGj | ˆGi) T(f1( ˆGi),f2( ˆGj)) + log(EP ˆGj eT(f1( ˆGi),f2( ˆGj )))}. (4) The above loss essentially maximizes a lower bound of the mutual information between hi = f1( ˆGi),hj = f2( ˆGj) that the compositions of (f1, ˆGi),(f2, ˆGj) determine our desired views of 4NodeDropSubgraphEdgePertAttrMaskIdentical Identical AttrMask EdgePert Subgraph NodeDrop 0.42 1.25 -2.42 -0.17 -1.44 0.03 1.20 -0.62 -1.05 -1.14 -1.26 1.95 -3.07 -1.18 -2.44 1.63 1.17 2.10 1.90 1.62 0.85 1.57 -0.86 -0.59 -0.17 NCI1 NodeDropSubgraphEdgePertAttrMaskIdentical 2.47 2.27 1.01 1.07 -0.74 2.43 1.89 0.85 1.15 1.51 1.28 2.97 0.71 1.37 0.96 2.54 2.30 2.20 2.67 3.15 2.00 2.27 1.62 1.31 1.30 PROTEINS NodeDropSubgraphEdgePertAttrMaskIdentical 4.85 6.17 3.10 5.12 -2.64 6.02 6.54 4.05 5.26 4.61 6.62 6.43 0.61 5.28 3.53 6.16 6.56 7.11 7.03 6.27 6.49 6.10 5.71 4.37 5.64 COLLAB NodeDropSubgraphEdgePertAttrMaskIdentical 1.66 1.39 0.85 0.17 -0.26 1.37 1.53 0.47 -0.36 0.25 1.74 1.52 0.97 0.34 0.71 1.13 1.50 1.25 1.06 1.39 1.85 1.45 1.66 1.53 1.31 RDT-B Low High Figure 2: Semi-supervised learning accuracy gain (%) when contrasting different augmentation pairs, compared to training from scratch, under four datasets: NCI1, PROTEINS, COLLAB, and RDT-B. Pairing “Identical\" stands for a no-augmentation baseline for contrastive learning, where the positive pair diminishes and the negative pair consists of two non-augmented graphs. Warmer colors indicate better performance gains. The baseline training-from-scratch accuracies are 60.72%, 70.40%, 57.46%, 86.63% for the four datasets respectively. graphs. Furthermore, we draw the connection between GraphCL and recently proposed contrastive learning methods that we demonstrate that GraphCL can be rewrited as a general framework unifying a broad family of contrastive learning methods on graph-structured data, through reinterpreting(4). In our implementation, we choose f1 = f2 and generate ˆGi, ˆGj through data augmentation, while with various choices of the compositions result in (4) instantiating as other speciﬁc contrastive learning algorithms including [54, 55, 56, 21, 57, 58, 59] also shown in in Appendix F. 4 The Role of Data Augmentation in Graph Contrastive Learning Table 2: Datasets statistics. Datasets Category Graph Num. Avg. Node Avg. Degree NCI1 Biochemical Molecules 4110 29.87 1.08PROTEINS Biochemical Molecules 1113 39.06 1.86 COLLAB Social Networks 5000 74.49 32.99RDT-B Social Networks 2000 429.63 1.15 In this section, we assess and rationalize the role of data augmentation for graph- structured data in our GraphCL frame- work. Various pairs of augmentation types are applied, as illustrated in Fig. 2, to three categories of graph datasets (Table 2, and we leave the discussion on superpixel graphs in Appendix C). Experiments are performed in the semi-supervised setting, follow- ing the pre-training & ﬁnetuning approach [18]. Detailed settings are in Appendix B. 4.1 Data Augmentations are Crucial. Composing Augmentations Beneﬁts. We ﬁrst examine whether and when applying (different) data augmentations helps graph contrastive learning in general. We summarize the results in Fig. 2 using the accuracy gain compared to training from scratch (no pre-training). And we list the following Observations. Obs. 1. Data augmentations are crucial in graph contrastive learning. Without any data aug- mentation graph contrastive learning is not helpful and often worse compared with training from scratch, judging from the accuracy losses in the upper right corners of Fig. 2. In contrast, composing an original graph and its appropriate augmentation can beneﬁt the downstream performance. Judging from the top rows or the right-most columns in Fig. 2, graph contrastive learning with single best aug- mentations achieved considerable improvement without exhaustive hyper-parameter tuning: 1.62% for NCI1, 3.15% for PROTEINS, 6.27% for COLLAB, and 1.66% for RDT-B. The observation meets our intuition. Without augmentation, graphCL simply compares two original samples as a negative pair (with the positive pair loss becoming zero), leading to homogeneously pushes all graph representations away from each other, which is non-intuitive to justify. Importantly, when appropriate augmentations are applied, the corresponding priors on the data distribution are instilled, enforcing the model to learn representations invariant to the desired perturbations through maximizing the agreement between a graph and its augmentation. Obs. 2. Composing different augmentations beneﬁts more. Composing augmentation pairs of a graph rather than the graph and its augmentation further improves the performance: the maximum 50 20 40 60 80 100 Epochs 2.9 3.0 3.1 3.2 3.3 3.4Contrastive Loss Compose augmentations  with AttrMask on NCI1 AttrMask+NodeDrop AttrMask+EdgePert AttrMask+AttrMask 0 20 40 60 80 100 Epochs 2.9 3.0 3.1 3.2 3.3 3.4Contrastive Loss Compose augmentations  with AttrMask on PROTEINS AttrMask+NodeDrop AttrMask+EdgePert AttrMask+AttrMask 0 20 40 60 80 100 Epochs 2.9 3.0 3.1 3.2 3.3Contrastive Loss Compose augmentations  with EdgePert on NCI1 EdgePert+NodeDrop EdgePert+AttrMask EdgePert+EdgePert 0 20 40 60 80 100 Epochs 2.9 3.0 3.1 3.2 3.3 3.4Contrastive Loss Compose augmentations  with EdgePert on PROTEINS EdgePert+NodeDrop EdgePert+AttrMask EdgePert+EdgePert Figure 3: Contrastive loss curves for different augmentation pairs. In the two ﬁgures of the left attribute masking is contrasted with other augmentations and that of the right for edge perturbation, where contrasting the same augmentations always leads to the fastest loss descent. accuracy gain was 2.10% for NCI1, 3.15% for PROTEINS, 7.11% for COLLAB, and 1.85% for RDT-B. Interestingly, applying augmentation pairs of the same type (see the diagonals of Fig. 2) does not usually lead to the best performance (except for node dropping), compared with augmentation pairs of different types (off-diagonals). Similar observations were made in visual representation learning [18]. As conjectured in [18], composing different augmentations avoids the learned features trivially overﬁtting low-level “shortcuts”, making features more generalizable. Here we make a similar conjecture that contrasting isogenous graph pairs augmented in different types presents a harder albeit more useful task for graph representation learning. We thus plot the contrastive loss curves composing various augmentations (except subgraph) together with attribute masking or edge perturbation for NCI1 and PROTEINS. Fig. 3 shows that, with augmentation pairs of different types, the contrastive loss always descents slower than it does with pairs of the same type, when the optimization procedure remains the same. This result indicates that composing augmentation pairs of different types does correspond to a “harder\" contrastive prediction task. We will explore in Sec. 4.3 how to quantify a “harder\" task in some cases and whether it always helps. 4.2 The Types, the Extent, and the Patterns of Effective Graph Augmentations We then note that the (most) beneﬁcial combinations of augmentation types can be dataset-speciﬁc, which matches our intuition as graph-structured data are of highly heterogeneous nature (see Sec. 1). We summarize our observations and derive insights below. And we further analyze the impact of the extent and/or the pattern of given types of graph augmentations. Obs. 3. Edge perturbation beneﬁts social networks but hurts some biochemical molecules. Edge perturbation as one of the paired augmentations improves the performances for social-network data COLLAB and ROT-B as well as biomolecule data PROTEINS, but hurts the other biomolecule data NCI1. We hypothesize that, compared to the case of social networks, the “semantemes” of some biomolecule data are more sensitive to individual edges. Speciﬁcally, a single-edge change in NCI1 corresponds to a removal or addition of a covalent bond, which can drastically change the identity and even the validity of a compound, let alone its property for the down-stream semantemes. In contrast the semantemes of social networks are more tolerant to individual edge perturbation [60, 61]. Therefore, for chemical compounds, edge perturbation demonstrates a prior that is conceptually incompatible with the domain knowledge and empirically unhelpful for down-stream performance. We further examine whether the extent or strength of edge perturbation can affect the conclusion above. We evaluate the downstream performances on representative examples NCI1 and COLLAB. And we use the combination of the original graph (“identical”) and edge perturbation of various ratios in our GraphCL framework. Fig. 4A shows that edge perturbation worsens the NCI1 performances regardless of augmentation strength, conﬁrming that our earlier conclusion was insensitive to the extent of edge perturbation. Fig. 4B suggests that edge perturbation could improve the COLLAB performances more with increasing augmentation strength. Obs. 4. Applying attribute masking achieves better performance in denser graphs. For the social network datasets, composing the identical graph and attribute masking achieves 5.12% improve- ment for COLLAB (with higher average degree) while only 0.17% for RDT-B. Similar observations are made for the denser PROTEINS versus NCI1. To assess the impact of augmentation strength on this observation, we perform similar experiments on RDT-B and COLLAB, by composing the 6identical graph and its attributes masked to various extents. Fig. 4C and D show that, masking less for the very sparse RDT-B does not help, although masking more for the very dense COLLAB does. 0 5 10 15 20 Perturbation Ratio (%) 56 57 58 59 60 61 62Accuracy (%) Performance vs. edge  perturbation ratio on NCI1 EdgePert Train from scratch 0 5 10 15 20 Perturbation Ratio (%) 52 54 56 58 60 62Accuracy (%) Performance vs. edge  perturbation ratio on COLLAB EdgePert Train from scratch 0 5 10 15 20 Masking Ratio (%) 85 86 87 88 89 90Accuracy (%) Performance vs. attribute  masking ratio on RDT-B AttrMask Train from scratch 0 5 10 15 20 Masking Ratio (%) 52 54 56 58 60 62 64Accuracy (%) Performance vs. attribute  masking ratio on COLLAB AttrMask Train from scratch Figure 4: Performance versus augmentation strength. Left two ﬁgures implemented edge perturbation with different ratios. The right two ﬁgures apply attribute masking with different masking ratios. We further hypothesize that masking patterns also matter, and masking more hub nodes with high degrees beneﬁt denser graphs, because GNNs cannot reconstruct the missing information of isolated nodes, according to the message passing mechanism [ 62]. To test the hypothesis, we perform an experiment to mask nodes with more connections with higher probability on denser graphs PROTEINS and COLLAB. Speciﬁcally, we adopt a masking distributiondegα n rather than the uniform distribution, where degn is the degree of vertexvn and αis the control factor. A positiveαindicates more masking for high-degree nodes. Fig. 5C and D showing that, for very dense COLLAB, there is an apparent upward tendency on performance if masking nodes with more connections. Obs. 5. Node dropping and subgraph are generally beneﬁcial across datasets. Node dropping and subgraph, especially the latter, seem to be generally beneﬁcial in our studied datasets. For node dropping, the prior that missing certain vertices (e.g. some hydrogen atoms in chemical compounds or edge users for social networks) does not alter the semantic information is emphasized, intuitively ﬁtting for our cognition. For subgraph, previous works [ 20, 21] show that enforcing local (the subgraphs we extract) and global information consistency is helpful for representation learning, which explains the observation. Even for chemical compounds in NCI1, subgraphs can represent structural and functional “motifs” important for the down-stream semantemes. We similarly examined the impact of node dropping patterns by adopting the non-uniform distribution as mentioned in changing attribute-masking patterns. Fig. 5B shows that, for the dense social-network COLLAB graphs, more GraphCL improvements were observed while dropping hub nodes more in the range considered. Fig. 5A shows that, for the not-so-dense PROTEINS graphs, changing the node-dropping distribution away from uniform does not necessarily help. 2  1  0 1 2 Control Factor 69 70 71 72 73 74Accuracy (%) Performance vs. dropping  control factor on PROTEINS NodeDrop Train from scratch 2  1  0 1 2 Control Factor 58 60 62 64Accuracy (%) Performance vs. dropping  control factor on COLLAB NodeDrop Train from scratch 2  1  0 1 2 Control Factor 69 70 71 72 73 74Accuracy (%) Performance vs. masking  control factor on PROTEINS AttrMask Train from scratch 2  1  0 1 2 Control Factor 58 60 62 64 66Accuracy (%) Performance vs. masking  control factor on COLLAB AttrMask Train from scratch Figure 5: Performance versus augmentation patterns. Node dropping and attribute masking are performed with various control factors (negative to positive: dropping/masking more low-degree vertices to high-degree ones). 4.3 Unlike “Harder” Ones, Overly Simple Contrastive Tasks Do Not Help. As discussed in Obs. 2, “harder” contrastive learning might beneﬁt more, where the “harder” task is achieved by composing augmentations of different types. In this section we further explore quantiﬁable difﬁculty in relationship to parameterized augmentation strengths/patterns and assess the impact of the difﬁculty on performance improvement. Intuitively, larger dropping/masking ratios or control factor αleads to harder contrastive tasks, which did result in better COLLAB performances (Fig. 4 and 5) in the range considered. Very small ratios or negative α, corresponding to overly simple tasks, We also design subgraph variants of increasing difﬁculty levels and reach similar conclusions. More details are in Appendix D. 7Summary. In total, we decide the augmentation pools for Section 5 as: node dropping and subgraph for biochemical molecules; all for dense social networks; and all except attribute masking for sparse social networks. Strengths or patterns are default even though varying them could help more. 5 Comparison with the State-of-the-art Methods In this section, we compare our proposed (self-supervised) pre-training framework, GraphCL, with state-of-the-art methods (SOTAs) in the settings of semi-supervised, unsupervised [21] and transfer learning [9] on graph classiﬁcation (for node classiﬁcation experiments please refer to Appendix G). Dataset statistics and training details for the speciﬁc settings are in Appendix E. Semi-supervised learning. We ﬁrst evaluate our proposed framework in the semi-supervised learn- ing setting on graph classiﬁcation [63, 3] on the benchmark TUDataset [64]. Since pre-training & ﬁnetuning in semi-supervised learning for the graph-level task is unexplored before, we take two conventional network embedding methods as pre-training tasks for comparison: adjacency informa- tion reconstruction (we refer to GAE [ 22] for implementation) and local & global representation consistency enforcement (refer to Infomax [21] for implementation). Besides, the performance of training from scratch and that with augmentation (without contrasting) is also reported. We adopt graph convolutional network (GCN) with the default setting in [63] as the GNN-based encoder which achieves comparable SOTA performance in the fully-supervised setting. Table 3 shows that GraphCL outperforms traditional pre-training schemes. Table 3: Semi-supervised learning with pre-training & ﬁnetuning. Red numbers indicate the best performance and the number that overlap with the standard deviation of the best performance (comparable ones). 1% or 10% is label rate; baseline and Aug. represents training from scratch without and with augmentations, respectively. Dataset NCI1 PROTEINS DD COLLAB RDT-B RDT-M5K GITHUB MNIST CIFAR10 1% baseline 60.72 ±0.45 - - 57.46 ±0.25 - - 54.25 ±0.22 60.39 ±1.95 27.36 ±0.75 1% Aug. 60.49 ±0.46 - - 58.40 ±0.97 - - 56.36 ±0.42 67.43 ±0.36 27.39 ±0.44 1% GAE 61.63 ±0.84 - - 63.20 ±0.67 - - 59.44 ±0.44 57.58 ±2.07 21.09 ±0.53 1% Infomax 62.72 ±0.65 - - 61.70 ±0.77 - - 58.99 ±0.50 63.24 ±0.78 27.86 ±0.43 1% GraphCL 62.55 ±0.86 - - 64.57 ±1.15 - - 58.56 ±0.59 83.41 ±0.33 30.01 ±0.84 10% baseline 73.72 ±0.24 70.40 ±1.54 73.56 ±0.41 73.71 ±0.27 86.63 ±0.27 51.33 ±0.44 60.87 ±0.17 79.71 ±0.65 35.78 ±0.81 10% Aug. 73.59 ±0.32 70.29 ±0.64 74.30 ±0.81 74.19 ±0.13 87.74 ±0.39 52.01 ±0.20 60.91 ±0.32 83.99 ±2.19 34.24 ±2.62 10% GAE 74.36 ±0.24 70.51 ±0.17 74.54 ±0.68 75.09 ±0.19 87.69 ±0.40 53.58 ±0.13 63.89 ±0.52 86.67 ±0.93 36.35 ±1.04 10% Infomax 74.86 ±0.26 72.27 ±0.40 75.78 ±0.34 73.76 ±0.29 88.66 ±0.95 53.61 ±0.31 65.21 ±0.88 83.34 ±0.24 41.07 ±0.48 10% GraphCL 74.63±0.25 74.17 ±0.34 76.17 ±1.37 74.23 ±0.21 89.11 ±0.19 52.55 ±0.45 65.81 ±0.79 93.11 ±0.17 43.87 ±0.77 Unsupervised representation learning. Furthermore, GraphCL is evaluated in the unsupervised representation learning following [65, 21], where unsupervised methods generate graph embeddings that are fed into a down-stream SVM classiﬁer [21]. Aside from SOTA graph kernel methods that graphlet kernel (GL), Weisfeiler-Lehman sub-tree kernel (WL) and deep graph kernel (DGK), we also compare with four unsupervised graph-level representation learning methods as node2vec [66], sub2vec [67], graph2vec [65] and InfoGraph [21]. We adopt graph isomorphism network (GIN) with the default setting in [21] as the GNN-based encoder which is SOTA in representation learning. Table 4 shows GraphCL outperforms in most cases except on datasets with small graph size (e.g. MUTAG and IMDB-B consists of graphs with average node number less than 20). Table 4: Comparing classiﬁcation accuracy on top of graph representations learned from graph kernels, SOTA representation learning methods, and GIN pre-trained with GraphCL. The compared numbers are from the corresponding papers under the same experiment setting. Dataset NCI1 PROTEINS DD MUTAG COLLAB RDT-B RDT-M5K IMDB-B GL - - - 81.66 ±2.11 - 77.34 ±0.18 41.01 ±0.17 65.87 ±0.98 WL 80.01 ±0.50 72.92 ±0.56 - 80.72 ±3.00 - 68.82 ±0.41 46.06 ±0.21 72.30 ±3.44 DGK 80.31 ±0.46 73.30 ±0.82 - 87.44 ±2.72 - 78.04 ±0.39 41.27 ±0.18 66.96 ±0.56 node2vec 54.89 ±1.61 57.49 ±3.57 - 72.63 ±10.20 - - - - sub2vec 52.84 ±1.47 53.03 ±5.55 - 61.05 ±15.80 - 71.48 ±0.41 36.68 ±0.42 55.26 ±1.54 graph2vec 73.22 ±1.81 73.30 ±2.05 - 83.15 ±9.25 - 75.78 ±1.03 47.86 ±0.26 71.10 ±0.54 InfoGraph 76.20 ±1.06 74.44 ±0.31 72.85 ±1.78 89.01 ±1.13 70.65 ±1.13 82.50 ±1.42 53.46 ±1.03 73.03 ±0.87 GraphCL 77.87 ±0.41 74.39 ±0.45 78.62 ±0.40 86.80 ±1.34 71.36 ±1.15 89.53 ±0.84 55.99 ±0.28 71.14 ±0.44 Transfer learning. Lastly, experiments are performed on transfer learning on molecular property prediction in chemistry and protein function prediction in biology following [9], which pre-trains and ﬁnetunes the model in different datasets to evaluate the transferability of the pre-training scheme. We adopt GIN with the default setting in [9] as the GNN-based encoder which is SOTA in transfer 8learning. Experiments are performed for 10 times with mean and standard deviation of ROC-AUC scores (%) reported as [9]. Although there is no universally beneﬁcial pre-training scheme especially for the out-of-distribution scenario in transfer learning (Sec. 1), Table 5 shows that GraphCL still achieves SOTA performance on 5 of 9 datasets compared to the previous best schemes. Table 5: Transfer learning comparison with different manually designed pre-training schemes, where the compared numbers are from [9]. Dataset BBBP Tox21 ToxCast SIDER ClinTox MUV HIV BACE PPI No Pre-Train 65.8 ±4.5 74.0 ±0.8 63.4 ±0.6 57.3 ±1.6 58.0 ±4.4 71.8 ±2.5 75.3 ±1.9 70.1 ±5.4 64.8 ±1.0 Infomax 68.8 ±0.8 75.3 ±0.5 62.7 ±0.4 58.4 ±0.8 69.9 ±3.0 75.3 ±2.5 76.0 ±0.7 75.9 ±1.6 64.1 ±1.5 EdgePred 67.3 ±2.4 76.0 ±0.6 64.1 ±0.6 60.4 ±0.7 64.1 ±3.7 74.1 ±2.1 76.3 ±1.0 79.9 ±0.9 65.7 ±1.3 AttrMasking 64.3 ±2.8 76.7 ±0.4 64.2 ±0.5 61.0 ±0.7 71.8 ±4.1 74.7 ±1.4 77.2 ±1.1 79.3 ±1.6 65.2 ±1.6 ContextPred 68.0 ±2.0 75.7 ±0.7 63.9 ±0.6 60.9 ±0.6 65.9 ±3.8 75.8 ±1.7 77.3 ±1.0 79.6 ±1.2 64.4 ±1.3 GraphCL 69.68 ±0.67 73.87 ±0.66 62.40 ±0.57 60.53 ±0.88 75.99 ±2.65 69.80 ±2.66 78.47 ±1.22 75.38 ±1.44 67.88 ±0.85 Adversarial robustness. In addition to generalizability, we claim that GNNs also gain robustness using GraphCL. The experiments are performed on synthetic data to classify the component number in graphs, facing the RandSampling, GradArgmax and RL-S2V attacks following the default setting in [60]. Structure2vec [ 68] is adopted as the GNN-based encoder as in [ 60]. Table 6 shows that GraphCL boosts GNN robustness compared with training from scratch, under three evasion attacks. Table 6: Adversarial performance under three adversarial attacks for GNN with different depth (following the protocol in [60]). Red numbers indicate the best performance. Two-Layer Three-Layer Four-Layer Methods No Pre-Train GraphCL No Pre-Train GraphCL No Pre-Train GraphCL Unattack 93.20 94.73 98.20 98.33 98.87 99.00 RandSampling 78.73 80.68 92.27 92.60 95.13 97.40 GradArgmax 69.47 69.26 64.60 89.33 95.80 97.00 RL-S2V 42.93 42.20 41.93 61.66 70.20 84.86 6 Conclusion In this paper, we perform explicit study to explore contrastive learning for GNN pre-training, facing the unique challenges in graph-structured data. Firstly, several graph data augmentations are proposed with the discussion of each of which on introducing certain human prior of data distribution. Along with new augmentations, we propose a novel graph contrastive learning framework (GraphCL) for GNN pre-training to facilitate invariant representation learning along with rigorous theoretical analysis. We systematically assess and analyze the inﬂuence of data augmentations in our proposed framework, revealing the rationale and guiding the choice of augmentations. Experiment results verify the state-of-the-art performance of our proposed framework in both generalizability and robustness. Broader Impact Empowering deep learning for reasoning and predicting over graph-structured data is of broad interests and wide applications, such as recommendation systems, neural architecture search, and drug discovery. The proposed graph contrastive learning framework with augmentations contributes a general framework that can potentially beneﬁt the effectiveness and efﬁciency of graph neural networks through model pre-training. The numerical results and analyses would also inspire the design of proper augmentations toward positive knowledge transfer on downstream tasks. References [1] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [2] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [3] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018. 9[4] Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. L2-gcn: Layer-wise and learned efﬁcient training of graph convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2127–2135, 2020. [5] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 338–348, 2020. [6] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11249–11259, 2019. [7] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Advances in Neural Information Processing Systems, pages 5165–5175, 2018. [8] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. In Advances in Neural Information Processing Systems, pages 4800–4810, 2018. [9] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019. [10] Dumitru Erhan, Pierre-Antoine Manzagol, Yoshua Bengio, Samy Bengio, and Pascal Vincent. The difﬁculty of training deep architectures and the effect of unsupervised pre-training. In Artiﬁcial Intelligence and Statistics, pages 153–160, 2009. [11] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, pages 249–256, 2010. [12] Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Bench- marking graph neural networks. arXiv preprint arXiv:2003.00982, 2020. [13] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [14] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classiﬁcation. arXiv preprint cs.LG/1905.10947, 2019. [15] Marinka Zitnik, Jure Leskovec, et al. Prioritizing network communities. Nature communications, 9(1):1–9, 2018. [16] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised visual representation learning. arXiv preprint arXiv:1905.01235, 2019. [17] Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representation learning. arXiv preprint arXiv:1901.09005, 2019. [18] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020. [19] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [20] Petar Veliˇckovi´c, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018. [21] Fan-Yun Sun, Jordan Hoffmann, and Jian Tang. Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. arXiv preprint arXiv:1908.01000, 2019. [22] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016. [23] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in neural information processing systems, pages 1024–1034, 2017. [24] Leonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. struc2vec: Learning node represen- tations from structural identity. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 385–394, 2017. [25] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non- parametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3733–3742, 2018. [26] Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embedding learning via invariant and spreading instance feature. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6210–6219, 2019. 10[27] Xu Ji, João F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised image classiﬁcation and segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pages 9865–9874, 2019. [28] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729–9738, 2020. [29] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision, pages 69–84. Springer, 2016. [30] Fabio M Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2229–2238, 2019. [31] Trieu H Trinh, Minh-Thang Luong, and Quoc V Le. Selﬁe: Self-supervised pretraining for image embedding. arXiv preprint arXiv:1906.02940, 2019. [32] Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial robustness: From self-supervised pre-training to ﬁne-tuning. arXiv preprint arXiv:2003.12862, 2020. [33] Roei Herzig, Amir Bar, Huijuan Xu, Gal Chechik, Trevor Darrell, and Amir Globerson. Learning canonical representations for scene graph to image generation. arXiv preprint arXiv:1912.07414, 2019. [34] Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. When does self-supervision help graph convolutional networks? arXiv preprint arXiv:2006.09136, 2020. [35] Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, and Jiliang Tang. Self-supervised learning on graphs: Deep insights and new direction. arXiv preprint arXiv:2006.10141, 2020. [36] Qikui Zhu, Bo Du, and Pingkun Yan. Self-supervised training of graph convolutional networks. arXiv preprint arXiv:2006.02380, 2020. [37] Jiawei Zhang, Haopeng Zhang, Li Sun, and Congying Xia. Graph-bert: Only attention is needed for learning graph representations. arXiv preprint arXiv:2001.05140, 2020. [38] Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. Gpt-gnn: Generative pre- training of graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1857–1867, 2020. [39] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, and Jie Tang. Self-supervised learning: Generative or contrastive. arXiv preprint arXiv:2006.08218, 2020. [40] Vikas Verma, Meng Qu, Alex Lamb, Yoshua Bengio, Juho Kannala, and Jian Tang. Graphmix: Regularized training of graph neural networks for semi-supervised learning. arXiv preprint arXiv:1909.11715, 2019. [41] Ming Ding, Jie Tang, and Jie Zhang. Semi-supervised learning on graphs with generative adversarial nets. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 913–922, 2018. [42] Zhijie Deng, Yinpeng Dong, and Jun Zhu. Batch virtual adversarial training for graph convolutional networks. arXiv preprint arXiv:1902.09192, 2019. [43] Fuli Feng, Xiangnan He, Jie Tang, and Tat-Seng Chua. Graph adversarial training: Dynamically regulariz- ing based on graph structure. IEEE Transactions on Knowledge and Data Engineering, 2019. [44] Michael T Rosenstein, Zvika Marx, Leslie Pack Kaelbling, and Thomas G Dietterich. To transfer or not to transfer. In NIPS 2005 workshop on transfer learning, volume 898, pages 1–4, 2005. [45] Suzanna Becker and Geoffrey E Hinton. Self-organizing neural network that discovers surfaces in random- dot stereograms. Nature, 355(6356):161–163, 1992. [46] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018. [47] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018. [48] Zhen Peng, Yixiang Dong, Minnan Luo, Xiao-Ming Wu, and Qinghua Zheng. Self-supervised graph representation learning via global context prediction. arXiv preprint arXiv:2003.01604, 2020. [49] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmenta- tion. arXiv preprint arXiv:1904.12848, 2019. [50] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. InAdvances in Neural Information Processing Systems, pages 5050–5060, 2019. 11[51] Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In Advances in neural information processing systems, pages 1857–1865, 2016. [52] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [53] Ting Chen, Yizhou Sun, Yue Shi, and Liangjie Hong. On sampling strategies for neural network-based collaborative ﬁltering. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 767–776, 2017. [54] Petar Velickovic, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. In ICLR (Poster), 2019. [55] Yuxiang Ren, Bo Liu, Chao Huang, Peng Dai, Liefeng Bo, and Jiawei Zhang. Heterogeneous deep graph infomax. arXiv preprint arXiv:1911.08538, 2019. [56] Chanyoung Park, Donghyun Kim, Jiawei Han, and Hwanjo Yu. Unsupervised attributed multiplex network embedding. In AAAI, pages 5371–5378, 2020. [57] Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and Junzhou Huang. Graph representation learning via graphical mutual information maximization. In Proceedings of The Web Conference 2020, pages 259–270, 2020. [58] Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. arXiv preprint arXiv:2006.05582, 2020. [59] Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1150–1160, 2020. [60] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on graph structured data. arXiv preprint arXiv:1806.02371, 2018. [61] Daniel Zügner, Amir Akbarnejad, and Stephan Günnemann. Adversarial attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2847–2856, 2018. [62] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural mes- sage passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning-V olume70, pages 1263–1272. JMLR. org, 2017. [63] Ting Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? a dissection on graph classiﬁcation. arXiv preprint arXiv:1905.04579, 2019. [64] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020), 2020. [65] Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, and Shan- tanu Jaiswal. graph2vec: Learning distributed representations of graphs. arXiv preprint arXiv:1707.05005, 2017. [66] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages 855–864, 2016. [67] Bijaya Adhikari, Yao Zhang, Naren Ramakrishnan, and B Aditya Prakash. Sub2vec: Feature learning for subgraphs. In Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining, pages 170–182. Springer, 2018. [68] Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702–2711, 2016. 12",
      "references": [
        "Semi-supervised classiﬁcation with graph convolutional networks",
        "Graph attention networks",
        "How powerful are graph neural networks?",
        "L2-gcn: Layer-wise and learned efﬁcient training of graph convolutional networks",
        "Towards deeper graph neural networks",
        "Layer-dependent importance sampling for training deep and large graph convolutional networks",
        "Link prediction based on graph neural networks",
        "Hierarchical graph representation learning with differentiable pooling",
        "Pre-training graph neural networks",
        "The difficulty of training deep architectures and the effect of unsupervised pre-training",
        "Understanding the difficulty of training deep feedforward neural networks",
        "Benchmarking graph neural networks",
        "Deeper insights into graph convolutional networks for semi-supervised learning",
        "Graph neural networks exponentially lose expressive power for node classification",
        "Prioritizing network communities",
        "Scaling and benchmarking self-supervised visual representation learning",
        "Revisiting self-supervised visual representation learning",
        "A simple framework for contrastive learning of visual representations",
        "Open graph benchmark: Datasets for machine learning on graphs",
        "Deep graph Infomax",
        "Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization",
        "Variational graph auto-encoders",
        "Inductive representation learning on large graphs",
        "struc2vec: Learning node representations from structural identity",
        "Unsupervised feature learning via non-parametric instance discrimination",
        "Unsupervised embedding learning via invariant and spreading instance feature",
        "Invariant information clustering for unsupervised image classification and segmentation",
        "Momentum contrast for unsupervised visual representation learning",
        "Unsupervised learning of visual representations by solving jigsaw puzzles",
        "Domain generalization by solving jigsaw puzzles",
        "Self-supervised pretraining for image embedding",
        "Adversarial robustness: From self-supervised pre-training to fine-tuning",
        "Learning canonical representations for scene graph to image generation",
        "When does self-supervision help graph convolutional networks?",
        "Self-supervised learning on graphs: Deep insights and new direction",
        "Self-supervised training of graph convolutional networks",
        "Graph-bert: Only attention is needed for learning graph representations",
        "Gpt-gnn: Generative pre-training of graph neural networks",
        "Self-supervised learning: Generative or contrastive",
        "Graphmix: Regularized training of graph neural networks for semi-supervised learning",
        "Semi-supervised learning on graphs with generative adversarial nets",
        "Batch virtual adversarial training for graph convolutional networks",
        "Graph adversarial training: Dynamically regularizing based on graph structure",
        "To transfer or not to transfer",
        "Self-organizing neural network that discovers surfaces in random-dot stereograms",
        "MINE: mutual information neural estimation",
        "Learning deep representations by mutual information estimation and maximization",
        "Self-supervised graph representation learning via global context prediction",
        "Unsupervised data augmentation",
        "Mixmatch: A holistic approach to semi-supervised learning",
        "Improved deep metric learning with multi-class n-pair loss objective",
        "Representation learning with contrastive predictive coding",
        "On sampling strategies for neural network-based collaborative filtering",
        "Heterogeneous deep graph infomax",
        "Unsupervised attributed multiplex network embedding",
        "Graph representation learning via graphical mutual information maximization",
        "Contrastive multi-view representation learning on graphs",
        "GCC: Graph contrastive coding for graph neural network pre-training",
        "Adversarial attack on graph structured data",
        "Adversarial attacks on neural networks for graph data",
        "Neural message passing for quantum chemistry",
        "Are powerful graph neural nets necessary? a dissection on graph classification",
        "Tudataset: A collection of benchmark datasets for learning with graphs",
        "graph2vec: Learning distributed representations of graphs",
        "node2vec: Scalable feature learning for networks",
        "Sub2vec: Feature learning for subgraphs",
        "Discriminative embeddings of latent variable models for structured data"
      ],
      "meta_data": {
        "arxiv_id": "2010.13902v3",
        "authors": [
          "Yuning You",
          "Tianlong Chen",
          "Yongduo Sui",
          "Ting Chen",
          "Zhangyang Wang",
          "Yang Shen"
        ],
        "published_date": "2020-10-22T20:13:43Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "GraphCL addresses the challenge of learning generalizable, transferrable, and robust graph representations in a self-supervised setting. It introduces a graph contrastive learning (GraphCL) framework that pre-trains GNNs by creating two correlated augmented views of each graph and maximizing their agreement. The core contributions are: (1) design of four graph augmentations (node dropping, edge perturbation, attribute masking, subgraph sampling) with tunable extents and patterns to inject domain priors; (2) a generic contrastive pre-training framework that unifies and extends prior graph self-supervised methods by interpreting the objective as mutual information maximization between views; (3) a systematic, dataset-driven analysis of augmentation combinations across semi-supervised, unsupervised, transfer, and adversarial-attack settings, yielding practical guidance on augmentation choices; (4) state-of-the-art results on graph classification benchmarks in semi-supervised and unsupervised settings, improved transfer performance on molecular/protein tasks, and enhanced robustness to adversarial attacks; (5) public release of code and a discussion of augmentation-induced generalization benefits.",
        "methodology": "GraphCL constructs two augmented views of a graph G using a pool of graph data augmentations qi(·|G) and qj(·|G). A shared GNN encoder f(·) produces graph representations for each view, which are then mapped by a projection head g(·) to a latent space (z_i, z_j). The model is trained with a contrastive loss (NT-Xent): for each graph in a mini-batch, the loss pushes the augmented views of the same graph (positive pair) closer while treating other graphs in the batch as negatives. The approach does not constrain the GNN architecture and can be instantiated with various encoder choices (e.g., GCN, GIN). The authors show that GraphCL implements a form of mutual information maximization between the latent representations of the two views and can recover a broad family of graph contrastive methods by varying the compositions of f1, f2 and the augmentations. Four augmentations are proposed, each encoding a prior about graph data: Node dropping, Edge perturbation, Attribute masking, and Subgraph sampling; their extents and patterns are tunable. ",
        "experimental_setup": "Datasets: studies include NCI1, PROTEINS, COLLAB, and RDT-B for graph classification in semi-supervised settings; transfer learning experiments are conducted on molecular/biological benchmarks (BBBP, Tox21, ToxCast, SIDER, ClinTox, MUV, HIV, BACE, PPI); unsupervised representation learning uses graph-level embeddings evaluated via downstream classifiers; baselines include GAE, Infomax, graph kernels (GL, WL, DGK), and various graph-level representation methods (node2vec, sub2vec, graph2vec, InfoGraph); adversarial robustness experiments use synthetic data with RandSampling, GradArgmax, and RL-S2V attacks on graphs with 2–4 layers. Validation: semi-supervised experiments train a GNN encoder (GCN) with pre-training followed by fine-tuning; unsupervised representations are fed to SVM/linear classifiers; transfer results report ROC-AUC across datasets; multiple runs with standard deviations reported. ",
        "limitations": "Limitations include dependence on the choice of augmentations, which are not universally optimal across domains (e.g., edge perturbation degrades some biomolecule datasets like NCI1; masking patterns and densities influence gains); the framework requires careful tuning of augmentation extents and patterns for different graphs; additional computational cost arises from augmentations and multiple views; theoretical guarantees are limited to a lower bound on mutual information, and generalization bounds are not provided; transfer learning remains challenging in out-of-distribution scenarios, and there is potential for negative transfer if augmentations misalign with downstream tasks.",
        "future_research_directions": "Future directions include: (1) learning or searching for optimal augmentation policies automatically (autoML-style augmentation selection and extents); (2) extending GraphCL to node-level tasks and more diverse GNN architectures; (3) integrating GraphCL with stronger or alternative self-supervised objectives (e.g., alternative MI estimators, contrastive losses, or multi-view objectives); (4) scaling to very large graphs and dynamic/temporal graphs; (5) domain-aware augmentation design or meta-learning of augmentations to maximize transferability; (6) deeper theoretical analysis of mutual information bounds and their relation to generalization, as well as systematic study of augmentation hardness vs. performance.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning",
      "full_text": "Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning Berken Utku Demirel Department of Computer Science ETH Zürich, Switzerland berken.demirel@inf.ethz.ch Christian Holz Department of Computer Science ETH Zürich, Switzerland christian.holz@inf.ethz.ch Abstract The success of contrastive learning is well known to be dependent on data aug- mentation. Although the degree of data augmentations has been well controlled by utilizing pre-defined techniques in some domains like vision, time-series data augmentation is less explored and remains a challenging problem due to the com- plexity of the data generation mechanism, such as the intricate mechanism in- volved in the cardiovascular system. Moreover, there is no widely recognized and general time-series augmentation method that can be applied across different tasks. In this paper, we propose a novel data augmentation method for quasi- periodic time-series tasks that aims to connect intra-class samples together, and thereby find order in the latent space. Our method builds upon the well-known mixup technique by incorporating a novel approach that accounts for the peri- odic nature of non-stationary time-series. Also, by controlling the degree of chaos created by data augmentation, our method leads to improved feature rep- resentations and performance on downstream tasks. We evaluate our proposed method on three time-series tasks, including heart rate estimation, human ac- tivity recognition, and cardiovascular disease detection. Extensive experiments against state-of-the-art methods show that the proposed approach outperforms prior works on optimal data generation and known data augmentation techniques in the three tasks, reflecting the effectiveness of the presented method. Source code: https://github.com/eth-siplab/Finding_Order_in_Chaos. 1 Introduction Self-supervised learning methods have gained significant attention as they enable the discovery of meaningful representations from raw data without explicit annotations. These self-supervised methods learn representations without labels by designing pretext tasks that transform the unsupervised representation learning problem into a supervised one such as predicting the rotation of images [1], or contexts [2, 3]. Among these methods, contrastive learning (CL), which learns to distinguish semantically similar examples over dissimilar ones, stands out as a powerful approach in self- supervised learning across various domains including computer vision [4–6], speech recognition [7– 10], and natural language processing [11–14]. The success of contrastive learning relies on the creation of similar and dissimilar examples, which is typically achieved through the use of data augmentations [ 15, 16]. Recently, it was shown that data augmentations have a role to create a “ chaos” between different intra-class samples such that they become more alike. For example, two different cars become very similar when they are both cropped to the wheels. [ 17]. However, in time-series data, creating similar samples with augmentation techniques is more challenging due to the complexity of the dynamical data generation mechanisms [18]. Moreover, research on contrastive learning for time series has demonstrated the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2309.13439v2  [cs.LG]  21 Dec 2023absence of a unique data augmentation technique that consistently performs better than others in different tasks [19, 20]. Instead, the choice of augmentation depends on the contextual characteristics of the signal, such as perturbing the high-frequency content of a signal that carries characteristic information in low frequencies does not generate useful data samples that are helpful for contrastive learning to learn class invariant features [21]. Considering these limitations, in this work, we first propose a novel data augmentation method for time series data by performing a tailored mixup while considering the phase and amplitude information as two separate features. Then, we perform specific operations for both features to generate positive samples by controlling the mixup coefficients for each feature to prevent aggressive augmentation. Specifically, our method employs a technique that controls the mixup ratio for each randomly chosen pair based on their distance in the latent space which is acquired through the use of a variational autoencoder (V AE), whose objective is to learn disentangled representations of data without labels. To this end, subjecting to the distance constraint in the latent space, the mixup tries to connect semantically closer samples together more aggressively while preventing the excessive interpolation of dissimilar samples that are likely to belong to different classes. Therefore, the purpose of our proposed method for quasi-periodic time-series data augmentation is to find an order in “chaos” between different samples such that they become more alike by interpolating them in a novel manner to prevent the loss of information. We summarize our contributions as follows: • We propose a novel mixup method for non-stationary quasi-periodic time-series data by considering phase and magnitude as two separate features to generate samples that enhance intra-class similarity and help contrastive learning to learn class-separated representations. • We present a novel approach for sampling mixup coefficients for each pair based on their similarity in the latent space, which is constructed without supervision while learning disentangled representations, to prevent aggressive augmentation between samples. • We show that the tailored mixup with coefficient sampling consistently improves the perfor- mance of contrastive learning in three time-series tasks compared to prior mixup techniques and proposed augmentation methods that generate optimal/hard positives or negatives. 2 Preliminaries 2.1 Notation We use lowercase symbols (x) to denote scalar quantities, bold lowercase symbols (x) for vector values, and capital letters (X) for random variables. Functions with a parametric family of mappings are represented as fθ(.) where θ is the parameters. The discrete Fourier transformation of a real- valued time series sample is denoted as F(.), yielding a complex variable as Xk where X ∈ C and k ∈ [0, fs/2] is the frequency with the maximum value of Nyquist rate. The amplitude and phase of the F(x) are represented as A(x) and P(x). The real and imaginary parts of a complex variable are shown as Re(.) and Im(.). The detailed calculations for operations are given in the Appendix A.1. 2.2 Setup We follow the common CL setup as follows. Given a datasetD = {(xi)}K i=1 where each xi consists of real-valued sequences with length L andC channels. The objective is to train a learnerfθ which seeks to learn an invariant representation such that when it is fine-tuned on a datasetDl = {(xi, yi)}M i=1 with M ≪ K and yi ∈ {1, . . . , N}, it can separate samples from different classes. 2.3 Motivation As stated by prior works, mixup-based methods have poor performance in domains where data has a non-fixed topology, such as trees, graphs, and languages [22, 23]. Here, we demonstrate how we derived our proposed method by revealing the limitations of mixup for time series theoretically while considering the temporal dependencies and non-stationarities. Assumption 2.1 (SNR Matters). There exist one or multiple bandlimited frequency ranges of interest f∗, where the information that average raw time-series data conveys about the labels (i.e.,I(y; x)) is directly proportional to normalized signal power in that frequency range as in Equation 1. 2I(y; x) ∝ Z f∗ Sx(f) / Z ∞ −∞ Sx(f) where Sx(f) = lim N→∞ 1 2N \f\f\f\f\f NX n=−N xne−j2πfn \f\f\f\f\f 2 (1) Assumption 2.1 states that the information from a time series depends on its signal-to-noise ratio (SNR). Prior works showed that specific frequency bands hold inherent information about the characteristics of time series, which helps classification [21, 24]. Assumption 2.2. The true underlying generative process f(.), for a given data distribution D = {xk}K k=1, is quasiperiodic, i.e., f(x + τ) = g(x, f(x)), where τ can be either fixed or varied. Assumption 2.2 posits that the observed data samples from the distribution D are generated by a quasiperiodic function. This is a minimal assumption since the quasiperiodicity is the relaxed version of the periodic functions. In simpler terms, quasiperiodicity can be described as the observed signals exhibiting periodicity on a small scale, while being unpredictable on a larger scale. And, several prior works showed that the data generation mechanism of time-series data for several applications in the real world are quasiperiodic [25–30]. Therefore, Assumption 2.2 is realistic. Proposition 2.3 (Destructive Mixup). If Assumptions 2.1 and 2.2 hold, there exist λ ∼ Beta(α, α) or λ ∼ U(β, 1.0) with high values of β such that when linear mixup techniques are utilized, the lower bound of the mutual information for the augmented sample distribution decreases to zero. 0 ≤ I(y; x+) < I(y; x∗), where x∗is the optimal sample, x+ = λx + (1 − λ)˜ xand Z f∗ Sx∗ (f) = Z ∞ −∞ Sx∗ (f) (2) Proofs can be found in Appendix A. This proposition indicates that although the augmented samples are primarily derived from anchor samples (x) with high ratios, the resulting instances may not contain any task-relevant information. In other words, the augmentation process can potentially discard the whole task-specific information. This destructive behavior of mixup for quasi-periodic data can be attributed to the interference phenomenon in which two waves interact to form the resultant wave of the lower or higher amplitude according to the phase difference as shown in Proposition 2.3. 3 Method We introduce a novel approach to overcome the limitations of mixup by treating the magnitude and phase of sinusoidal signals as two distinct features with separate behaviors. Subsequently, we apply tailored mixup operations to each feature, considering their specific characteristics and effects. We perform the linear mixup for the magnitude of each sinusoidal. However, for the phase, we take a different approach and bring the phase components of the two coherent signals together by adding a small value to the anchor’s phase in the direction of the other sample. The mixup operation performs the linear interpolation of features [31], however, interpolation of two complex variables can result in a complex variable whose phase and magnitude are completely different/far away from those two, i.e., mixup can be destructive extrapolation rather than the interpolation of features. Therefore, we mix the phase of two sinusoidal as follows. We start by calculating the shortest phase difference between the two samples, denoted as ∆Θ, as described in Equation 31. θ ≡ [P(x) − P(˜ x)] (mod 2π) ∆Θ = \u001aθ − 2π, if θ > π θ, otherwise (3) The sign of the calculated phase difference provides information about the relative phase location of the other sample, in either a clockwise or counterclockwise direction in the phasor diagram. And, the absolute value of it represents the shortest angular difference between two samples in radians. Based 1We use phase in radians throughout the paper in the range (−π, π] 3on the calculated phase difference between two samples, we perform mixup operation to generate diverse positive samples as in Equation 4 such that phase and magnitude of augmented instances are interpolated properly according to the anchor sample x, without causing any destructive interference. x+ = F−1(A(x+)∠P(x+)) where A(x+) = λAA(x) + (1− λA)A(˜ x) and P(x+) = \u001aP(x) − |∆Θ| ∗(1 − λP ), if ∆Θ > 0 P(x) + |∆Θ| ∗(1 − λP ), otherwise (4) The proposed method which mixes the magnitude and phase of each frequency component with tailored operations, not only prevents destructive interference between time series, resulting in an increase in the lower bound of mutual information (as shown in Theorem 3.1), but also generates diverse augmented instances with the same two samples by using two different mixing coefficients. a) b) Figure 1: The phasor representation of linear mixupa), and proposed mixup b). The anchor, randomly chosen sample, and generated instances are represented as x, ˜ x, and x+, respectively. Theorem 3.1 (Guarantees for Mixing). Under assumptions 2.1 and 2.2, given any λ ∈ (0, 1], the mutual information for the augmented instance lower bounded by the sampled λ and anchor x. λI(y; x) ≤ I(y; x+) < I(y; x∗) where x+ = F−1(A(x+)∠P(x+)) (5) We provide an intuitive demonstration in Figure 1, along with a detailed mathematical proof presented in Appendix A. Our approach also offers increased flexibility in selecting the mixing coefficients of phase and magnitude, based on their sensitivities to the mixing process as well as the augmentation degree for each randomly chosen pair. Since the degree of augmentations has crucial importance for contrastive learning, there can be cases where augmentations are either too weak (intra-class features cannot be clustered together) or too strong (inter-class features can also collapse to the same point) and lead to sub-optimal results [17]. To mitigate this issue and find an order for augmentation degree, we search pairs of samples that are semantically closer, meaning they are more likely to belong to the same class. We then perform the proposed mixup more aggressively on these pairs, creating more closer and diverse samples while decreasing the augmentation strength for less similar pairs. To find similar samples without labels, we train a completely unsupervised β-V AE [32] that maps data points to a latent space such that two random samples are semantically similar if they are close in the latent as shown in Proposition 3.2. Proposition 3.2 (Consistency in Latent Space [33]). Given a well-trained unconditional VAE with the encoder E(.) that produces distribution pE(z|x), the decoder D(.) that produces distribution qD(x|z) while the prior for z is p(z), let z1 and z2 be two latent vectors of two different real samples x1 and x2, i.e., E(x1) = z1 and E(x2) = z2. if the distance d(z1, z2) ≤ δ, then D(z1) and D(z2) will have a similar semantic label as in Equation 6. |I(D(z1); y) − I(D(z2); y)| ≤ϵ, (6) 4where ϵ stands for tolerable semantic difference, δ is the maximum distance to maintain semantic consistency, and d(.) is a distance measure such as cosine similarity between two vectors. The above proposition with Theorem 3.1 motivates us to perform augmentation aggressively if two randomly chosen samples are semantically closer. Therefore, we sample the mixup coefficient for both phase and magnitude from a uniform distribution λA, λP ∼ U(β, 1.0) with low values of β if the distance between the latent vectors is below a threshold, otherwise, they are drawn from a truncated normal distribution λA, λP ∼ Nt(µ, σ,1.0) with a high mean and low standard deviation. 4 Experiments We conduct experiments on the proposed approach and compare it with other mixup methods or optimal/hard positive sample generation in the contrastive learning setup. During our experiments, we use SimCLR [15] framework without specialized architectures or a memory bank for all baselines to have a fair comparison. Results with other CL frameworks can be found in Appendix E. Complete training details and hyper-parameter settings for datasets and baselines are provided in Appendix D. 4.1 Datasets We performed extensive experiments on eight datasets from three tasks that include activity recogni- tion from inertial measurements (IMUs), heart rate prediction from photoplethysmography (PPG), and cardiovascular disease classification from electrocardiogram (ECG). We provided short descriptions of each dataset below, and further detailed information with metrics can be found in Appendix B. Activity recognition We used UCIHAR [34], HHAR [35], and USC [36] for activity recognition. During the evaluation, we assess the cross-person generalization capability of the contrastive models, i.e., the model is evaluated on a previously unseen target domain. We follow the settings in GILE [37] to treat each person as a single domain while the fine-tuning dataset is much smaller than the unsupervised one. Heart rate prediction We used the IEEE Signal Processing Cup in 2015 (IEEE SPC) [ 38], and DaLia [39] for PPG-based heart rate prediction. The SPC provides two datasets, one smaller with lesser artifacts (referred to as SPC12) [38] and a bigger dataset with more participants including heavy motions (referred to as SPC22). In line with previous studies, we adopted the leave-one-session-out (LOSO) cross-validation, which involves evaluating methods on subjects or sessions that were not used for pre-training and fine-tuning. Cardiovascular disease (CVD) classification We conducted our experiments on China Physiologi- cal Signal Challenge 2018 (CPSC2018) [40] and Chapman University, Shaoxing People’s Hospital (Chapman) ECG dataset [41]. We selected the same four specific leads as in [42] while treating each dataset as a single domain with a small portion of the remainder dataset used for fine-tuning the pre-trained model. We split the dataset for fine-tuning and testing based on patients (each patient’s recordings appear in only one set). 4.2 Baselines Comparison with prior mixup techniques We evaluate the effectiveness of our proposed mixup by comparing it with other commonly used mixup methods, including Linear-Mixup [31], Binary- Mixup [43], Geometric-Mixup [22], Cut-Mix [44], Amplitude-Mix [45] and Spec-Mix [46]. When we compare the performance of mixup techniques, we follow the same framework with [47] where the samples of mixture operation only happen in current batch samples. And, the mixup samples are paired with anchors, i.e., without applying mixup second times, for contrastive pre-training. Comparison with methods for optimal sample generation We evaluate the performance of our proposed method by comparing it with other data generation methods and baselines in contrastive learning setup while considering previously known augmentation techniques. Traditional data augmentations for time-series [19], such as resampling, flipping, adding noise, etc. InfoMin which leverages an adversarial training strategy to decrease the mutual information between samples 5while maximizing the NCE loss [ 48]. NNCLR [ 49], which uses nearest neighbors in the learned representation space as the positive samples. Positive feature extrapolation [ 50], which creates hard positives through feature extrapolation. GenRep which uses the latent space of a generative model to generate “views” of the same semantic content by sampling nearby latent vectors [51]. Aug. Bank [21], which proposes an augmentation bank that manipulates frequency components of a sample with a limited budget. STAug [52], which combines spectral and time augmentation for generating samples using the empirical mode decomposition and linear mixup.. DACL [ 22], which creates positive samples by mixing hidden representations. IDAA [53], which is an adversarial method by modifying the data to be hard positives without distorting the key information about their original identities using a V AE. More implementation details for each baseline are given in Appendix C. 4.3 Implementation We use a combination of convolutional with LSTM-based network, which shows superior performance in many time-series tasks [19, 54, 55], as backbones for the encoder fθ(.) where the projector is two fully-connected layers. We use InfoNCE as the loss, which is optimized using Adam with a learning rate of 0.003. We train with a batch size of 256 for 120 epochs and decay the learning rate using the cosine decay schedule. After pre-training, we train a single linear layer classifier on features extracted from the frozen pre-trained network, i.e., linear evaluation, with the same hyperparameters. Reported results are mean and standard deviation values across 3 independent runs with different seeds on the same split. More details about the implementation, architectures, and hyperparameters with the trained V AEs are given in Appendix D. 5 Results and Discussion Tables 1, 2, and 3 present the results of our proposed approach compared to state-of-the-art methods for optimal/hard positive sample generation in contrastive learning setup across the three tasks in eight datasets. Additionally, Figure 2 compares our approach with prior mixup methods (e.g., linear mixup, cutmix) without applying any other additional augmentation techniques. Overall, our proposed method has demonstrated superior performance compared to other methods in seven datasets, with the second-best performance in the remaining dataset, and a minor performance gap. Table 1: Performance Comparison of ours with prior works in Activity Recognition datasets Method UCIHAR HHAR USC ACC↑ MF1↑ ACC↑ MF1↑ ACC↑ MF1↑ Supervised DCL [37] 77.63 – 51.27 – 60.35 – CoDATS [56] 68.22 – 45.69 – – – GILE [37] 88.17 – 55.61 – – – Self-Supervised Traditional Augs. 87.05±1.07 86.13 ±0.96 85.48 ±1.16 84.31 ±1.31 53.47 ±1.10 52.09 ±0.95 NNCLR [49] 85.31 ±0.91 83.56 ±1.25 83.16 ±1.32 82.15 ±1.25 55.41 ±1.43 52.64 ±1.37 InfoMin [48] 38.07 ±8.15 30.66 ±9.15 31.58 ±10.2 29.72 ±11.1 35.89 ±14.3 37.77 ±9.12 IDAA [53] 82.23 ±0.69 79.84 ±0.89 88.98±0.62 89.01±0.55 59.23 ±1.10 56.11 ±1.54 PosET [50] 88.13 ±0.91 87.35 ±0.96 85.77 ±1.11 85.90 ±1.20 41.37 ±5.63 39.43 ±5.72 STAug [52] 89.83 ±0.71 88.91 ±0.62 87.69 ±1.03 87.73 ±0.93 55.61 ±1.08 56.74 ±1.21 Aug. Bank [21] 65.27 ±1.12 71.16 ±1.24 67.95 ±1.45 75.13 ±1.32 43.28 ±4.37 47.31 ±4.68 GenRep [51] 87.22 ±1.05 86.48 ±0.95 87.05 ±0.95 86.45 ±0.90 50.13 ±2.85 49.50 ±2.73 DACL [22] 73.12 ±1.23 66.28 ±1.11 80.89 ±0.91 81.31 ±0.78 53.61 ±2.60 51.76 ±2.21 Ours 91.60±0.65 90.46±0.53 88.05 ±1.05 87.95 ±1.10 60.13±0.75 59.13±0.69 From these tables, we can see that our proposed method significantly outperforms DACL, which sug- gests creating a positive sample by mixing fixed hidden representations in an intermediate layer [22], by a large margin (up to 20.8% with a 10.1% on average in activity recognition). This suggests that when the representations are not yet linearly separable at the beginning of the contrastive training process, the interpolated representations using mixup may be dissimilar to the actual interpolated samples and may not capture their underlying features. One interesting result from our experiments is that IDAA [53] exhibits comparable performance to our method in some datasets, and even slightly outperforms our approach in the HHAR dataset for activity recognition. Despite using distinct methods to generate positive instances, i.e., adversarial and mixup, our approach and IDAA algorithm 6Table 2: Performance comparison of ours with prior works in Heart Rate Prediction datasets Method IEEE SPC12 IEEE SPC 22 DaLia MAE↓ RMSE↓ MAE↓ RMSE↓ MAE↓ RMSE↓ SupervisedDCL 22.02 28.44 28.10 32.45 6.58 11.30CNN Ensemble∗ [39] 3.89 – 8.74 – 8.58 – Self-SupervisedTraditional Augs. 20.67 ±1.13 26.35 ±0.98 16.84 ±1.10 22.23 ±0.72 12.01 ±0.65 21.09 ±0.86NNCLR [49] 20.28 ±2.21 28.23 ±1.63 23.49 ±1.54 28.75 ±3.66 11.56 ±0.63 19.95 ±0.89InfoMin [48] 36.84 ±5.11 29.78 ±7.31 31.58 ±4.72 29.72 ±4.83 45.89 ±8.71 50.77 ±9.72IDAA [53] 19.02 ±0.96 27.42 ±1.11 15.37 ±1.21 22.41 ±1.42 11.12 ±0.64 20.45 ±0.69PosET [50] 25.60 ±1.93 33.80 ±2.71 23.42 ±1.50 31.51 ±3.71 35.99 ±3.95 39.92 ±3.12STAug [52] 27.44 ±1.93 35.63 ±3.10 19.86 ±2.11 30.70 ±3.54 18.70 ±4.06 30.81 ±3.61Aug. Bank [21] 27.31 ±2.17 37.93 ±2.96 27.84 ±2.03 36.41 ±3.98 35.87 ±4.18 40.61 ±3.74GenRep [51] 21.02 ±1.41 28.42 ±1.65 15.67 ±1.23 22.33 ±1.43 25.41 ±1.62 36.83 ±1.87DACL [22] 21.85 ±1.63 28.17 ±1.75 14.67 ±1.10 20.06 ±1.21 18.44 ±1.32 25.61 ±1.45Ours 16.26±0.72 22.48±0.95 12.25±0.47 18.20±0.61 10.57±0.55 20.37±0.73 * The entire dataset, excluding the test, is utilized with labels, while in DCL, the labeled data size matches that of the CL share similarities in approaches for positive instance generation in CL setup. The IDAA algorithm aims to create hard positive samples that lie near class boundaries without changing the identity of the sample, while our method interpolates two samples to produce a positive instance that is similar to the original sample while adding noise to the phase and amplitude in the direction of a randomly chosen sample. In other words, both approaches try to keep the sample identity intact by taking special precautions while generating new positive instances, which may explain their similar performance in our experiments. In contrast, approaches that do not prioritize preserving sample identity while generating samples or hidden representations often demonstrate suboptimal performance on average while exhibiting increased variability across tasks. Table 3: Performance comparison between ours and prior work in CVD. Method CPSC 2018 Chapman AUC↑ AUC↑ Supervised CNN [57] — 95.80 Casual CNN [58] — 97.70 Self-Supervised Traditional Augs. 67.86±3.41 74.69 ±2.04 NNCLR [49] 70.06 ±2.05 77.19 ±2.41 InfoMin [48] 64.48 ±6.15 56.34 ±9.12 IDAA [53] 80.90 ±0.73 93.63 ±0.91 PosET [50] 72.58 ±2.12 78.27 ±2.34 STAug [52] 74.15 ±1.15 93.88 ±0.87 Aug. Bank [21] 81.78 ±1.24 94.75 ±0.90 GenRep [51] 52.49 ±3.43 86.72 ±1.13 DACL [22] 82.38 ±0.84 92.28 ±0.97 Ours 85.30±0.45 95.90±0.82 Examples of such methods include PosET [ 50], which generates hard positive samples to im- prove contrastive learning by extrapolating features, STAug [52], which uses empirical mode decompo- sition with linear mixup technique together, and InfoMin [48], which tries to minimize mutual in- formation between two instances in an adversar- ial manner. The performance comparison of prior mixup techniques and our proposed one is shown in Figure 2. On average, our proposed method outper- forms all other mixup techniques while reducing the variance across tasks. What is interesting about this figure is that while the linear [ 31] and ampli- tude mixup [45] reach our method in some datasets for activity recognition, the performance of the lin- ear mixup decreases heavily for the other two tasks whereas the amplitude mixup gives reasonable per- formance. This empirical outcome supports our initial theorem about the destructive effect of mixup, which suggests linear mixup or other derivatives can discard the whole task-specific information in the generated positive sample for quasi-periodic signals even though the mixing coefficient is sampled from a distribution such that the generated samples are much closer to the anchor. 5.1 Ablation Studies Here, we present a comprehensive examination of our proposed method and the effect of its com- ponents on the performance. Mainly, we investigate the effect of the proposed mixup by applying the instance selection algorithm to the linear mixup (w/o Prop. Mixup). Then, we perform our proposed mixup with the constant λA and λP coefficients without investigating latent space distances between pairs (w/o Aug. Degree). Tables 4, 5 and 6 summarize the results. The second row in the tables shows the performance when the proposed mixup method is not applied while choosing mixup coefficients according to the distances in the latent space for the linear mixup. The last row illustrates 710 15 20 25 30 35 40 IEEE SPC12 IEEE SPC22 DaLia Ours Linear Binary Cut Spec Amp 15 35 55 75 95 UCIHAR HHAR USC Ours Linear Binary Cut Spec Amp Geo 65 70 75 80 85 90 95 100 CPSCP 2018 Chapman Ours Linear Binary Cut Spec Amp a) b) c) Accuracy in  % Higher is better. Mean Absolute Error Lower is better. AUC in % Higher is better. Figure 2: The comparison of mixup methods where the error bars represent the deviation across random seeds (explicit numbers are given in Appendix E). a) shows the performance in activity recognition, b) is for heart rate prediction, and finally c) shows the CVD classification. For the last two tasks, we excluded Geomix as its performance is extremely poor and distorts the y-axis scale. the performance change resulting from randomly sampling mixup coefficients without considering any relationship between the selected pair while applying tailored mixup for phase and magnitude. Table 4: Ablation on proposed mixup with coefficient selection in Activity Recognition datasets Method UCIHAR HHAR USC ACC↑ MF1↑ ACC↑ MF1↑ ACC↑ MF1↑ Ours 91.60±0.65 90.46±0.53 88.05±1.05 87.95±1.10 60.13±0.75 59.13±0.69 w/o Prop. Mixup83.09 (-8.51) 81.65 (-8.81) 85.89 (-2.16) 86.01 (-1.94) 45.10 (-15.03) 43.64 (-15.49) w/o Aug. Degree80.86 (-10.74) 80.18 (-10.26) 87.53 (-0.95) 87.75 (-0.20) 57.00 (-3.13) 54.75 (-4.38) The results obtained from the ablation study support the previous claims and outcomes. For example, when the linear mixup is applied instead of the proposed mixup technique for heart rate prediction (Table 5, w/o Prop. Mixup), the performance decrease is significant compared to the case when coefficients are sampled without considering the distance in the latent space (Table 5, w/o Aug. Degree). This observation indicates that as the periodicity in data increases, linear mixup can lead to significant destructive interferences, whereas our method effectively prevents such issues. Table 5: Ablation on proposed mixup with coefficient selection in Heart Rate Prediction datasets. Method IEEE SPC12 IEEE SPC 22 DaLia MAE↓ RMSE↓ MAE↓ RMSE↓ MAE↓ RMSE↓ Ours 16.26±0.72 22.48±0.95 12.25±0.47 18.20±0.61 10.57±0.55 20.37±0.73w/o Prop. Mixup20.45 (+4.19) 28.51 (+6.03) 15.29 (+3.04) 24.08 (+5.88) 24.11 (+13.54) 35.45 (+15.18)w/o Aug. Degree19.30 (+3.04) 24.84 (+2.36) 16.01 (+3.76) 21.21 (+3.19) 11.10 (+0.53) 20.13 (-0.24) Table 6: Ablation on proposed mixup with coeffi- cient selection in CVD. Method CPSC 2018 Chapman AUC↑ AUC↑ Ours 85.30 ± 0.45 95.90 ± 0.82 w/o Prop. Mixup 81.20 (-4.10) 86.30 (-9.60) w/o Aug. Degree 80.67 (-4.63) 95.98 (+0.08) While our mixup technique consistently en- hances performance across datasets, we ob- serve a decline when the mixing coefficients are sampled based on the distance in the latent space for two datasets. Also, the performance increase gained by sampling coefficients based on distance is relatively low compared to the proposed mixup. Several factors can explain this observation. First, the V AE might not be well trained due to the limited size of data in each class, i.e., the assumption in Proposition 3.2 does not hold. This can lead to inconsistencies in the semantic similarity of the latent space such that two close samples in the latent space might have different labels. Second, if the number of classes increases for a downstream task, the probability of sampling intra-class samples in a batch will decrease, leading to a lack of performance improvement. Therefore, in future investigations, it might be beneficial to use a different distance metric for quasi-periodic time-series data such that it can scale with the number of classes while considering the lack of big datasets. More ablation studies about the sensitivity of mixing coefficients and performance in different self-supervised learning frameworks, like BYOL [59] can be found in Appendix E.1 and E.2. And, investigations regarding whether we still need known data augmentations are given in Appendix E.3. Examples that visually demonstrate the negative effects of linear mixup and our proposed mixup 8technique to prevent this problem can be found in Appendix F. Comparative results regarding the performance of the tailored mixup in the supervised learning paradigm are given in Appendix G. 6 Related Work The goal of contrastive learning is to contrast positive with negative pairs [60]. In other words, the embedding space is governed by two forces, the attraction of positive pairs and the repellence of negatives, actualized through the contrastive loss [61]. Since label information is unavailable during the training, positive pairs are generated using augmentation techniques on a single sample, while negative pairs are randomly sampled from the entire dataset. Therefore, the choice or generation of positive and negative samples plays a pivotal role in the success of contrastive learning [62–65] and both approaches, generation/selection of positive/negative pairs, have been investigated thoroughly in the literature [ 66–71], we limit our discussion about prior works related to data augmentation techniques that create optimal or hard samples without labels. Adversarial based approaches A growing body of literature has investigated generating samples by using adversarial training for both positives and negatives [48, 72, 73]. A seminal work about the importance of augmentations in CL, InfoMin, presented an adversarial training strategy where players try to minimize and maximize the mutual information using the NCE loss [48]. CLAE, one of the first works that leveraged the adversarial approach, shows that adversarial training generates challenging positive and hard negative pairs [ 72]. Another recent study proposed an adversarial approach that generates hard samples while retaining the original sample identity by leveraging the identity-disentangled feature of V AEs [53]. However, adversarial augmentations may change the original sample identity due to excessive perturbations and it is infeasible to tune the attack strength for every sample to preserve the identity. In other words, these approaches do not consider the sample-specific features and use a constant perturbation coefficient for all samples whereas our proposed method considers the similarity between pairs and tunes the mixing coefficients accordingly. Mixup based approaches Mixup-based methods have been recently explored in contrastive learn- ing [22, 71, 47, 74, 75]. According to a recent theoretical work [22], mixup has implicit data-adaptive regularization effects that promote generalization better than adding Gaussian noise, which is a commonly used augmentation strategy in both time-series and vision data [76–78]. Although, mixup- based approaches have shown success in different problems [ 79, 80], such as domain adaptation, creating samples using mixup in the input space is infeasible in domains where data has a non-fixed topology, such as sequences, trees, and graphs [22]. Therefore, recent works suggest mixing hidden representations of samples, similar to Manifold Mixup [81]. However, this method claims that mixing fixed-length hidden representation via an intermediate layer \"z = αz + (1 − α)˜ z\" can be interpreted as adding noise to a given sample in the direction of another. However, it is an overly optimistic claim because early during training, where in most cases there is usually no linear separability among the representations, this synthesis may result in hidden representations that are completely different and far away from the samples [71, 82]. Therefore, in this work, we take a different approach and modify the mixup method considering its limitations for quasi-periodic non-stationary time-series data. Also, unlike most existing methods that aim to generate hard samples—samples that are close to class boundaries—using adversarial approaches [53, 48, 72] or feature extrapolation [50, 71], our method seeks to connect semantically closer samples together using interpolation in a tailored way. 7 Conclusion In this paper, we first demonstrate the destructive effect of linear mixup for quasi-periodic time- series data, then introduce a novel tailored mixup method to generate positive samples for the contrastive learning formulation while preventing this destructive effect and interpolating the samples appropriately. Theoretically, we show that our proposed method guarantees the interpolation of pairs without causing any loss of information while generating a diverse set of samples. Empirically, our method outperforms the prior approaches in three real-world tasks. By conducting experiments on contrastive and supervised learning settings, we show that our approach is agnostic to the choice of learning paradigm. Thus, it holds the potential for utilization in generating augmented data for different learning paradigms as well. We believe that the method proposed in this paper has the potential to significantly improve learning solutions for a diverse range of time series tasks. 9References [1] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [2] Carl Doersch, Abhinav Kumar Gupta, and Alexei A. Efros. Unsupervised visual representation learning by context prediction. 2015 IEEE International Conference on Computer Vision (ICCV), pages 1422–1430, 2015. [3] Deepak Pathak, Philipp Krähenbühl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. Context encoders: Feature learning by inpainting. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016, pages 2536–2544. IEEE Computer Society, 2016. [4] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), 2:1735–1742, 2006. [5] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discrimi- native unsupervised feature learning with convolutional neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors,Advances in Neural Infor- mation Processing Systems, volume 27. Curran Associates, Inc., 2014. [6] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. [7] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. Wav2vec 2.0: A framework for self-supervised learning of speech representations. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. [8] shuang ma, Zhaoyang Zeng, Daniel McDuff, and Yale Song. Contrastive learning of global and local video representations. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,Advances in Neural Information Processing Systems, volume 34, pages 7025–7040. Curran Associates, Inc., 2021. [9] Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, and Xuedong Huang. Unispeech: Unified speech representation learning with labeled and unlabeled data. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 10937–10947. PMLR, 18–24 Jul 2021. [10] Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. w2v-bert: Combining contrastive learning and masked language modeling for self- supervised speech pre-training. 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 244–250, 2021. [11] Hao Fu, Shaojun Zhou, Qihong Yang, Junjie Tang, Guiquan Liu, Kaikui Liu, and Xiaolong Li. Lrc-bert: Latent-representation contrastive knowledge distillation for natural language understanding. Proceedings of the AAAI Conference on Artificial Intelligence, 35(14):12830– 12838, May 2021. [12] Qianglong Chen, Feng Ji, Xiangji Zeng, Feng-Lin Li, Ji Zhang, Haiqing Chen, and Yin Zhang. Kace: Generating knowledge aware contrastive explanations for natural language inference. In Annual Meeting of the Association for Computational Linguistics, 2021. [13] X. Y . Shen, Ying Sun, Yao zhong Zhang, and Mani Najmabadi. Semi-supervised intent discovery with contrastive learning. Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI, 2021. 10[14] Yang Liu and Maosong Sun. Contrastive unsupervised word alignment with non-local features. Proceedings of the AAAI Conference on Artificial Intelligence, 29(1), Feb. 2015. [15] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML’20. JMLR.org, 2020. [16] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, C. Kwoh, Xiaoli Li, and Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In International Joint Conference on Artificial Intelligence, 2021. [17] Yifei Wang, Qi Zhang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Chaos is a ladder: A new theoretical understanding of contrastive learning via augmentation overlap. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. [18] Mike West, Raquel Prado, and Andrew D. Krystal. Evaluation and comparison of eeg traces: Latent structure in nonstationary time series. Journal of the American Statistical Association, 94(446):375–387, 1999. [19] Hangwei Qian, Tian Tian, and Chunyan Miao. What makes good contrastive learning on small-scale wearable-based tasks? In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , KDD ’22, page 3761–3771, New York, NY , USA, 2022. Association for Computing Machinery. [20] Qingsong Wen, Liang Sun, Fan Yang, Xiaomin Song, Jingkun Gao, Xue Wang, and Huan Xu. Time series data augmentation for deep learning: A survey. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4653–4660. International Joint Conferences on Artificial Intelligence Organization, 8 2021. Survey Track. [21] Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised contrastive pre-training for time series via time-frequency consistency. InProceedings of Neural Information Processing Systems, NeurIPS, 2022. [22] Vikas Verma, Thang Luong, Kenji Kawaguchi, Hieu Pham, and Quoc Le. Towards domain- agnostic contrastive learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 10530–10541. PMLR, 18–24 Jul 2021. [23] Soyoung Yoon, Gyuwan Kim, and Kyumin Park. Ssmix: Saliency-based span mixup for text classification. ArXiv, abs/2106.08062, 2021. [24] Martin Ullrich, Arne Küderle, Julius Hannink, Silvia Del Din, Heiko Gaßner, Franz Marxreiter, Jochen Klucken, Bjoern M. Eskofier, and Felix Kluge. Detection of gait from continuous inertial sensor data using harmonic frequencies. IEEE Journal of Biomedical and Health Informatics, 24(7):1869–1878, 2020. [25] Hong Luo, Deye Yang, Andrew Barszczyk, Naresh Vempala, Jing Wei, Si Jia Wu, Paul Pu Zheng, Genyue Fu, Kang Lee, and Zhong-Ping Feng. Smartphone-based blood pressure measurement using transdermal optical imaging technology. Circulation: Cardiovascular Imaging, 12(8):e008857, 2019. [26] Bryan P. Yan, William H. S. Lai, Christy K. Y . Chan, Stephen Chun-Hin Chan, Lok-Hei Chan, Ka-Ming Lam, Ho-Wang Lau, Chak-Ming Ng, Lok-Yin Tai, Kin-Wai Yip, Olivia T. L. To, Ben Freedman, Yukkee C. Poh, and Ming-Zher Poh. Contact-free screening of atrial fibrillation by a smartphone using facial pulsatile photoplethysmographic signals. Journal of the American Heart Association, 7(8):e008585, 2018. [27] Saman Noorzadeh, Mohammad Niknazar, Bertrand Rivet, Julie Fontecave-Jallon, Pierre-Yves Guméry, and Christian Jutten. Modeling quasi-periodic signals by a non-parametric model: Application on fetal ecg extraction. In 2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, pages 1889–1892, 2014. 11[28] A Koulali and P J Clarke. Modelling quasi-periodic signals in geodetic time-series using Gaussian processes. Geophysical Journal International, 226(3):1705–1714, 04 2021. [29] M. Lemay, L. Dang, and J.M. Vesin. Quasi-periodic atrial activity components in the ecg used to discriminate between paroxysmal and chronic atrial fibrillation. In 2008 Computers in Cardiology, pages 821–824, 2008. [30] A Garfinkel, P S Chen, D O Walter, H S Karagueuzian, B Kogan, S J Evans, M Karpoukhin, C Hwang, T Uchida, M Gotoh, O Nwasokwa, P Sager, and J N Weiss. Quasiperiodicity and chaos in cardiac fibrillation. The Journal of Clinical Investigation, 99(2):305–314, 1 1997. [31] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, 2018. [32] Irina Higgins, Loïc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual con- cepts with a constrained variational framework. In International Conference on Learning Representations, 2016. [33] Yinqi Li, Hong Chang, Bingpeng MA, Shiguang Shan, and Xilin Chen. Optimal positive gener- ation via latent transformation for contrastive learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 18327–18342. Curran Associates, Inc., 2022. [34] D. Anguita, Alessandro Ghio, L. Oneto, Xavier Parra, and Jorge Luis Reyes-Ortiz. Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine. In International Workshop on Ambient Assisted Living and Home Care, 2012. [35] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kjær- gaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen. Smart devices are different: Assessing and mitigatingmobile sensing heterogeneities for activity recognition. SenSys ’15, page 127–140, New York, NY , USA, 2015. Association for Computing Machinery. [36] Mi Zhang and Alexander A. Sawchuk. Usc-had: A daily activity dataset for ubiquitous activity recognition using wearable sensors. In Proceedings of the 2012 ACM Conference on Ubiquitous Computing, UbiComp ’12, page 1036–1043, New York, NY , USA, 2012. Association for Computing Machinery. [37] Hangwei Qian, Sinno Jialin Pan, and Chunyan Miao. Latent independent excitation for general- izable sensor-based cross-person activity recognition. Proceedings of the AAAI Conference on Artificial Intelligence, 35(13):11921–11929, May 2021. [38] Zhilin Zhang, Zhouyue Pi, and Benyuan Liu. Troika: A general framework for heart rate monitoring using wrist-type photoplethysmographic signals during intensive physical exercise. IEEE Transactions on Biomedical Engineering, 62(2):522–531, 2015. [39] Attila Reiss, Ina Indlekofer, Philip Schmidt, and Kristof Van Laerhoven. Deep ppg: Large-scale heart rate estimation with convolutional neural networks. Sensors, 19(14), 2019. [40] Eddie Y . K. Ng, Feifei Liu, Chengyu Liu, Lina Zhao, X. Zhang, Xiaoling Wu, Xiaoyan Xu, Yulin Liu, Caiyun Ma, Shoushui Wei, Zhiqiang He, and Jianqing Li. An open access database for evaluating the algorithms of electrocardiogram rhythm and morphology abnormality detection. Journal of Medical Imaging and Health Informatics, 2018. [41] Jianwei Zheng, Jianming Zhang, Sidy Danioko, Hai Yao, Hangyuan Guo, and Cyril Rakovski. A 12-lead electrocardiogram database for arrhythmia research covering more than 10,000 patients. Scientific Data, 7(1):48, February 2020. [42] Erick A Perez Alday, Annie Gu, Amit J Shah, Chad Robichaux, An-Kwok Ian Wong, Chengyu Liu, Feifei Liu, Ali Bahrami Rad, Andoni Elola, Salman Seyedi, Qiao Li, Ashish Sharma, Gari D Clifford, and Matthew A Reyna. Classification of 12-lead ecgs: the physionet/computing in cardiology challenge 2020. Physiological Measurement, 41(12):124003, dec 2020. 12[43] Christopher Beckham, Sina Honari, Vikas Verma, Alex M Lamb, Farnoosh Ghadiri, R Devon Hjelm, Yoshua Bengio, and Chris Pal. On adversarial mixup resynthesis. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [44] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Young Joon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 6022–6031, 2019. [45] Q. Xu, R. Zhang, Y . Zhang, Y . Wang, and Q. Tian. A fourier-based framework for domain generalization. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14378–14387, Los Alamitos, CA, USA, jun 2021. IEEE Computer Society. [46] Gwantae Kim, David K. Han, and Hanseok Ko. Specmix : A mixed sample data augmentation method for training withtime-frequency domain features. In Interspeech, 2021. [47] Zhiqiang Shen, Zechun Liu, Zhuang Liu, Marios Savvides, Trevor Darrell, and Eric Xing. Un-mix: Rethinking image mixtures for unsupervised visual representation learning. 2022. [48] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. [49] D. Dwibedi, Y . Aytar, J. Tompson, P. Sermanet, and A. Zisserman. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9568–9577, Los Alamitos, CA, USA, oct 2021. IEEE Computer Society. [50] Rui Zhu, Bingchen Zhao, Jingen Liu, Zhenglong Sun, and Chang Wen Chen. Improving contrastive learning by visualizing feature transformation. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 10286–10295, 2021. [51] Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola. Generative models as a data source for multiview representation learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [52] Xiyuan Zhang, Ranak Roy Chowdhury, Jingbo Shang, Rajesh Gupta, and Dezhi Hong. Towards diverse and coherent augmentation for time-series forecasting. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5, 2023. [53] Kaiwen Yang, Tianyi Zhou, Xinmei Tian, and Dacheng Tao. Identity-disentangled adversarial augmentation for self-supervised learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 25364–25381. PMLR, 17–23 Jul 2022. [54] Dwaipayan Biswas, Luke Everson, Muqing Liu, Madhuri Panwar, Bram-Ernst Verhoef, Shrishail Patki, Chris H. Kim, Amit Acharyya, Chris Van Hoof, Mario Konijnenburg, and Nick Van Helleputte. Cornet: Deep learning framework for ppg-based heart rate estimation and biometric identification in ambulant environment. IEEE Transactions on Biomedical Circuits and Systems, 13(2):282–291, 2019. [55] Akara Supratak, Hao Dong, Chao Wu, and Yike Guo. Deepsleepnet: a model for automatic sleep stage scoring based on raw single-channel eeg. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 25(11):1998–2008, Nov 2017. [56] Garrett Wilson, Janardhan Rao Doppa, and Diane J. Cook. Multi-source deep domain adaptation with weak supervision for time-series sensor data. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &; Data Mining, KDD ’20, page 1768–1778, New York, NY , USA, 2020. Association for Computing Machinery. 13[57] Dani Kiyasseh, Tingting Zhu, and David A. Clifton. Clocs: Contrastive learning of cardiac signals across space, time, and patients. In International Conference on Machine Learning, 2020. [58] Crystal T. Wei, Ming-En Hsieh, Chien-Liang Liu, and Vincent S. Tseng. Contrastive heartbeats: Contrastive learning for self-supervised ecg representation and phenotyping. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1126–1130, 2022. [59] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent a new approach to self-supervised learning. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. [60] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [61] Tri Huynh, Simon Kornblith, Matthew R. Walter, Michael Maire, and Maryam Khademi. Boosting contrastive self-supervised learning with false negative cancellation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) , pages 2785–2795, January 2022. [62] Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saun- shi. A theoretical analysis of contrastive unsupervised representation learning. In International Conference on Machine Learning, 2019. [63] Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic, and Jiri Matas. Working hard to know your neighbor's margins: Local descriptor learning loss. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [64] Hong Xuan, Abby Stylianou, Xiaotong Liu, and Robert Pless. Hard negative examples are hard, but useful. In Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV, page 126–142, Berlin, Heidelberg, 2020. Springer-Verlag. [65] Songwei Ge, Shlok Mishra, Chun-Liang Li, Haohan Wang, and David Jacobs. Robust contrastive learning using negative samples with diminished semantics. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,Advances in Neural Information Processing Systems, volume 34, pages 27356–27368. Curran Associates, Inc., 2021. [66] Yue Cao, Zhenda Xie, Bin Liu, Yutong Lin, Zheng Zhang, and Han Hu. Parametric instance classification for unsupervised visual feature learning. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. [67] Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. Debiased contrastive learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 8765– 8775. Curran Associates, Inc., 2020. [68] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ond ˇrej Chum. Mining on manifolds: Metric learning without labels. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7642–7651, 2018. [69] Mike Wu, Chengxu Zhuang, Milan Mosse, Daniel Yamins, and Noah Goodman. On mutual information in contrastive learning for visual representations, 2020. [70] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 815–823, 2015. 14[71] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 21798–21809. Curran Associates, Inc., 2020. [72] Chih-Hui Ho and Nuno Nvasconcelos. Contrastive learning with adversarial examples. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 17081–17093. Curran Associates, Inc., 2020. [73] Xiao Wang, Yuhang Huang, Dan Zeng, and Guo-Jun Qi. Caco: Both positive and negative sam- ples are directly learnable via cooperative-adversarial contrastive learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1–12, 2023. [74] Sungnyun Kim, Gihun Lee, Sangmin Bae, and Se-Young Yun. Mixco: Mix-up contrastive learning for visual representation. arXiv preprint arXiv:2010.06300, 2020. [75] Kibok Lee, Yian Zhu, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin, and Honglak Lee. i-mix: A domain-agnostic strategy for contrastive representation learning. In ICLR, 2021. [76] Yuzhe Yang, Xin Liu, Jiang Wu, Silviu Borac, Dina Katabi, Ming-Zher Poh, and Daniel McDuff. Simper: Simple self-supervised learning of periodic targets. In The Eleventh International Conference on Learning Representations, 2023. [77] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 2352–2359, 2021. [78] Pengxiang Shi, Wenwen Ye, and Zheng Qin. Self-supervised pre-training for time series classification. In 2021 International Joint Conference on Neural Networks (IJCNN), 2021. [79] Ronghang Zhu, Ronghang Zhu, Xiang Yu, and Sheng Li. Progressive mix-up for few-shot supervised multi-source domain transfer. In ICLR, 2023. [80] JangHyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint mixup with supermodular diversity. In International Conference on Learning Representa- tions, 2021. [81] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez- Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 ofProceedings of Machine Learning Research, pages 6438–6447, Long Beach, California, USA, 09–15 Jun 2019. PMLR. [82] Tsz-Him Cheung and Dit-Yan Yeung. {MODALS}: Modality-agnostic automated data aug- mentation in the latent space. In International Conference on Learning Representations, 2021. [83] Leandro Giacomini Rocha, Dwaipayan Biswas, Bram-Ernst Verhoef, Sergio Bampi, Chris Van Hoof, Mario Konijnenburg, Marian Verhelst, and Nick Van Helleputte. Binary cornet: Accelerator for hr estimation from wrist-ppg. IEEE Transactions on Biomedical Circuits and Systems, 14(4):715–726, 2020. [84] Sayeed Shafayet Chowdhury, Rakib Hyder, Md. Samzid Bin Hafiz, and Mohammad Ariful Haque. Real-time robust heart rate estimation from wrist-type ppg signals using multiple reference adaptive noise cancellation. IEEE Journal of Biomedical and Health Informatics , 22(2):450–459, 2018. [85] Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. In H. Wal- lach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [86] Ricky T. Q. Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentanglement in variational autoencoders. In S. Bengio, H. Wallach, H. Larochelle, K. Grau- man, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. 15Appendix A Proof In this section, we present complete proofs of our theoretical study, starting with notations. A.1 Notations Fourier transform of a real-valued sample with a finite duration is obtained as in Equation 7. Xk = F(x) = ∞X n=−∞ xne−j2πkn (7) The amplitude and phase for each frequency are calculated from the Fourier transform as follows. A(x) = p Re(Xk)2 + Im(Xk)2 P(x) = arctan2(Im(Xk), Re(Xk)), (8) where arctan is a 2-argument arctangent which is the angle measure in radians. The phasor, as in Figure 1, of a sample is represented as in Equation 9. Xk = F(x) = A(x)ejP (x) (9) A.2 Proof for Proposition 2.3 Proposition A.1 (Destructive Mixup). If Assumptions 2.1 and 2.2 hold, there exist λ ∼ Beta(α, α) or λ ∼ U(β, 1.0) with high values of β such that when linear mixup techniques are utilized, the lower bound of the mutual information for the augmented sample distribution decreases to zero. 0 ≤ I(y; x+) < I(y; x∗) where x+ = λx + (1 − λ)˜ xand Z f∗ Sx∗ (f) = Z ∞ −∞ Sx∗ (f) (10) Proof. x+ = λx + (1 − λ)˜ x (11) From the linearity of Fourier transformation and ignoring k in Xk for the sake of easiness. X+ = λX + (1 − λ) ˜X (12) X+ = ˜X + λ(X − ˜X) (13) Let ˜X = e−jωϕk Xωk, where ϕk and ωk are random phase and frequency modulators for each frequency, sampled from distributions ϕk ∼ Φ, ωk ∼ Ω. X+ = e−jωϕk Xωk + λ(X − e−jωϕk Xωk) (14) X+ = X \u0002 λ + e−jωϕk ωk − λe−jωϕk ωk \u0003 (15) X+ = X \u0002 λ + (1 − λ)e−jωϕk ωk \u0003 (16) X+ = X [λ + (1 − λ)ωk(cos (ωϕk) − j sin (ωϕk))] (17) 16From the quasi-periodicity, assume that the frequency ranges of interest (f∗, i.e., k∗) are overlapped for both samples while the sampled random modulators have the following relationship. ωk∗ ≈ λ 1 − λ and θ ≡ [ωϕk∗ ] (mod 2π), (18) where θ is an odd multiple of π. Equation 17 can be simplified as follows. X+ k∗ = Xk∗ [λ + λ cos (ωϕk∗ )] (19) X+ k∗ ≈ 0 (20) X+ k∗ = ∞X n=−∞ xne−j2πk∗n −→ ∞X n=−∞ xne−j2πk∗n ≈ 0 (21) Sx+(f∗) = lim N→∞ 1 2N \f\f\f\f\f NX n=−N xne−j2πf∗n \f\f\f\f\f 2 (22) From Assumption 2.1, I(y; x+) ∝ Z f∗ Sx+(f) / Z ∞ −∞ Sx+(f) (23) 0 ≤ I(y; x+) < I(y; x∗) (24) We use Euler’s formula to expand Equation 16 to 17. While we use the frequency bins ( k) and frequency values in Hz (f) interchangeably for Equations 21 and 22. Although the above proof is to show the resulting instances may not contain any task-relevant information, it can also be demonstrated that the augmentation process can potentially discard the partial task-specific information (not whole) if ϕk and ωk are close to indicated relationships. 17A.3 Proof for Theorem 3.1 Theorem A.2 (Guarantees for Mixing). Under assumptions 2.1 and 2.2, given any λ ∈ (0, 1], the mutual information for the augmented instance lower bounded by the sampled λ and anchor x. λI(y; x) ≤ I(y; x+) < I(y; x∗) where x+ = F−1(A(x+)∠P(x+)) (25) Proof. x+ = F−1(A(x+)∠P(x+)) where A(x+) = λAA(x) + (1− λA)A(˜ x) and P(x+) = \u001aP(x) − |∆Θ| ∗(1 − λP ), if ∆Θ > 0 P(x) + |∆Θ| ∗(1 − λP ), otherwise (26) X+ = A(x+)ejP (x+) (27) \f\fX+\f\f = \f\f\fA(x+)ejP (x+) \f\f\f (28) \f\fX+\f\f = A(x+) where \f\fX+ k \f\f = \f\f\f\f\f ∞X n=−∞ x+ n e−j2πkn \f\f\f\f\f (29) \f\fX+\f\f = λA(x) + (1− λ)A(˜ x) (30) \f\fX+\f\f = λ \f\f\f\f\f ∞X n=−∞ xne−j2πkn \f\f\f\f\f + (1 − λ) \f\f\f\f\f ∞X n=−∞ ˜ xne−j2πkn \f\f\f\f\f (31) λ \f\f\f\f\f ∞X n=−∞ xne−j2πkn \f\f\f\f\f + (1 − λ) \f\f\f\f\f ∞X n=−∞ ˜ xne−j2πkn \f\f\f\f\f ≥ λ \f\f\f\f\f ∞X n=−∞ xne−j2πkn \f\f\f\f\f (32) Z f∗ Sx+(f) ≥ λ Z f∗ Sx(f) (33) Using the R∞ −∞ Sx+(f) = R∞ ∞ S˜x(f) (i.e., both samples are normalized to have the same power) and Assumption 2.1, I(y; x+) ∝ Z f∗ Sx+(f) / Z ∞ −∞ Sx+(f) (34) λI(y; x) ≤ I(y; x+) < I(y; x∗) (35) Proof is completed with Equation 35 by combining equations 33 and 34. Although this proof ignores the effect of phase mixing on the mutual information with the assump- tion 2.1, it is known that phase components carry semantically important features [45]. Therefore, it is necessary to note that the objective of this proof is to demonstrate that by applying mixup separately to the phase and amplitude components, we can avoid destructive interference. 18B Datasets In this section, we give details about the datasets that are used during our experiments. B.1 Human Activity Recognition UCIHAR Human activity recognition using smartphones dataset (UCIHAR) [34] is collected by 30 subjects within an age range of 16 to 48 performing six daily living activities with a waist-mounted smartphone. Six activities include walking, sitting, lying, standing, walking upstairs, and walking downstairs. Data is captured by 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50 Hz. We used the pre-processing technique the same as in [ 37, 19] such that the input contains nine channels with 128 features (it is sampled in sliding window of 2.56 seconds and 50% overlap, resulting in 128 features for each window). Windows are normalized to zero mean and unit standard deviation before feeding to models. Also, we follow the same experimental setup with prior works as follows. The experiments are conducted with a leave-one-domain-out strategy, where one of the domains is chosen to be the unseen target [19]. The contrastive pre-training is conducted with all subjects without any label information except the target one. Training of the linear layer, which is added to the frozen trained encoder, is only performed with the first five subjects of UCIHAR after excluding the target subject. In other words, if the target subject is 0, the subjects from 1 to 29 are used to train the encoder without any label information. Then, subjects from 1 to 4 are used to train the linear layer. And, evaluation is performed for subject 0. This is performed for the first five subjects with three random seeds and the mean value is reported. HHAR Heterogeneity Dataset for Human Activity Recognition (HHAR) is collected by nine subjects within an age range of 25 to 30 performing six daily living activities with eight differ- ent smartphones—Although HHAR includes data from smartwatches as well, we use data from smartphones—that were kept in a tight pouch and carried by the users around their waists [ 35]. Subjects then perform 6 activities: ‘bike’, ‘sit’, ‘stairs down’, ‘stairs up’, ‘stand’, and ‘walk’. Due to variant sampling frequencies of smart devices used in HHAR dataset, we downsample the readings to 50 Hz and apply 100 (two seconds) and 50 as sliding window length with step size, the windows are normalized to zero mean with unit standard deviation. We used the first four subjects (i.e., a, b, c, d) as source domains. USC USC human activity dataset (USC-HAD) is composed of 14 subjects (7 male, 7 female, aged 21 to 49 with a mean of 30.1) executing 12 activities with a sensor on the front right hip. The data dimension is six (3-axis accelerometer, 3-axis gyroscope) and the sample rate is 100 Hz. 12 activities include walking forward, walking left, walking right, walking upstairs, walking downstairs, running forward, jumping up, sitting, standing, sleeping, elevator up, and elevator down. We used the pre-processing technique with a smaller window size such that the input contains six channels with 100 features (it is sampled in a sliding window of 1 second and 50% overlap, resulting in 100 features for each window). The same normalization is also applied to windows before feeding to models. We used the same setup with UCIHAR while source subjects are chosen as the last four this time. B.2 Heart Rate Prediction IEEE SPC This competition provided a training dataset of 12 subjects (SPC12) and a test dataset of 10 subjects [39]. The IEEE SPC dataset overall has 22 recordings of 22 subjects, ages ranging from 18 to 58 performing three different activities [83]. Each recording has sampled data from three accelerometer signals and two PPG signals along with the sampled ECG data and the sampling frequency is 125 Hz. All these recordings were recorded from the wearable device placed on the wrist of each individual. All recordings were captured with a 2-channel pulse oximeter with green LEDs, a tri-axial accelerometer, and a chest ECG for the ground-truth HR estimation. During our experiments, we used PPG channels. We choose the first five subjects of SPC12 as source domains similar to activity recognition setup while the last six subjects of SPC22 are used for source domains to prevent overlapping subjects with SPC12. Dalia PPG dataset for motion compensation and heart rate estimation in Daily Life Activities (DaLia) was recorded from 15 subjects (8 females, 7 males, mean age of 30.6), where each recording was approximately two hours long. PPG signals were recorded while subjects went through different 19daily life activities, for instance sitting, walking, driving, cycling, working, and so on. PPG signals were recorded at a sampling rate of 64 Hz. The first five subjects are used as source domains. All PPG datasets are standardized as follows. Initially, a fourth-order Butterworth bandpass filter with a frequency range of 0.5–4 Hz is applied to PPG signals. Subsequently, a sliding window of 8 seconds with 2-second shifts is employed for segmentation, followed by z-score normalization of each segment. Lastly, the signal is resampled to a frequency of 25 Hz for each segment. B.3 Cardiovascular disease (CVD) classification CPSC China Physiological Signal Challenge 2018 (CPSC2018), held during the 7th International Conference on Biomedical Engineering and Biotechnology in Nanjing, China. This dataset consists of 6,877 (male: 3,699; female: 3,178) and 12 lead ECG recordings lasting from 6 seconds to 60 seconds with 500 Hz. We use the original labelling [40] with one normal and eight abnormal types as follows: atrial fibrillation, first-degree atrioventricular block, left bundle branch block, right bundle branch block, premature atrial contraction, premature ventricular contraction, ST-segment depression, ST-segment elevated. We resampled recordings to 100 Hz and excluded recordings of less than 10 seconds. Chapman Chapman University, Shaoxing People’s Hospital (Chapman) ECG dataset which pro- vides 12-lead ECG with 10 seconds of a sampling rate of 500 Hz. The recordings are downsampled to 100 Hz, resulting in each ECG frame consisting of 1000 samples. The labeling setup follows the same approach as in [41] with four classes: atrial fibrillation, GSVT, sudden bradycardia, and sinus rhythm. The ECG frames are normalized to have a mean of 0 and scaled to have a standard deviation of 1. We split the dataset to 80–20% for training and testing as suggested in [41]. We choose leads I, II, III, and V2 during our experiments for both ECG datasets. We followed a similar setup with prior works [57] and considered each dataset as a single domain different from previous tasks. The fine-tuning of the linear layer, which is added to the frozen pre-trained encoder, is performed with 80% of the same domain. B.4 Metrics We used the common evaluation metrics in the literature for each task. Specifically, we used accuracy (Acc) and F1 score for activity recognition [19], mean absolute error (MAE), and root mean square error (RMSE) for heart rate prediction [39, 84], and the area under the ROC curve (AUC) for cardiovascular disease classification [57]. In this section, we explain how to calculate each metric for different time-series tasks. For activity recognition, the accuracy metric is computed by dividing the sum of true positives and true negatives by the total number of samples where a window has a single label. The MF1 score is calculated as a harmonic mean of the precision and recall where metrics are obtained globally by counting the total true positives, false negatives, and false positives similar to [19]. For heart rate prediction, the Mean Absolute Error (MAE) and Root-Mean-Square Error (RMSE) are calculated using the following equation: MAE = 1 K KX k=1 |HRmodel(k) − HRref(k)| (36) RMSE = sPK k=1(HRmodel(k) − HRref(k))2 K , (37) where K represents the total number of segments. The variables HRmodel(k) and HRref(k) denote the output of the model and reference heart rate values in beats-per-minute for the kth segment, respectively. This performance metric is commonly used in PPG-based heart rate estimation studies [39]. The estimated heart rate values (HRmodel(k)) are obtained using our model, while the reference heart rate values (HRref(k)) are directly taken from datasets. 20The AUC score for CVD classification is calculated using the one-vs-one scheme where the average AUC is computed for all possible pairwise combinations of classes for both datasets. C Baselines C.1 Prior Mixup Techniques In this section, we give a detailed explanation of each mixup technique we compare our proposed method. LinearMix We apply linear mixup as in Equation 38 to generate positive samples, ifx has more than one channel, mixup is applied independently for each of them. x+ = λx + (1 − λ)˜ x (38) BinaryMix We implement the binary mixup [43] by swapping the elements of x with the elements of another randomly chosen sample ˜ xas shown below. x+ = m ⊙ x + (1 − m) ⊙ ˜ x, (39) where m is a binary mask sampled from a Bernouilli(ρ) with high values, and ⊙ stands for Hadamard product. GeometricMix In Geometric Mixup, we create a positive sample corresponding to a sample x by taking its weighted-geometric mean with another randomly chosen sample ˜ xsame as [22] as shown below. x+ = xλ + ˜ x(1−λ) (40) CutMix Cutmix is implemented similarly to Binarymix. However, instead of changing each sample point with a probability, we cut a continuous portion using a rectangle mask M from a signal x and replace it with the same portion of another randomly chosen one ˜ x. The starting point of the mask is uniformly sampled while its length is sampled from lower values such that the augmented sample is more similar to the anchor. If the signal has multiple channels, this process is applied to all channels in the same section. x+ = M ⊙ x + (1 − M) ⊙ ˜ x, and M = rect \u0012b a \u0013 , (41) where b and a are the starting point and length of the rectangle wave, respectively. AmplitudeMix AmplitudeMix is introduced for domain adaptation problems by mixing the ampli- tude information of images without mixing the phase of two samples [45]. In our setup, we perform amplitude mixing on the time series data across all channels while keeping the phase component unchanged. In other words, we perform the following operations. x+ = F−1(A(x+)∠P(x+)) where A(x+) = λAA(x) + (1− λA)A(˜ x) and P(x+) = P(x) (42) SpecMix We implement the SpecMix by applying CutMix to the spectrogram of time-series where the spectrogram is calculated using the short-time Fourier transform as follows. X+ k = ∞X n=−∞ xng[n − mR]e−j2πkn, (43) 21where g[n − mR] is an analysis window of length M with hop length of R over the signal and calculating the discrete Fourier transform (DFT) of each segment of windowed data. The length of the Fourier transform is set to the sample size of the input time series while the hop and window parameters are set to the quarter of the length. C.2 Prior Methods for Sample Generation In this section, we give a detailed explanation of prior methods for data generation methods. Traditional Augmentations We apply two separate data augmentation to the anchor for creating two instances, and the encoders are trained to maximize agreement using the contrastive loss in [15]. We search mainly for augmentations that are known in state-of-the-art works [ 19]. The detailed augmentations are given in Table 22. InfoMin We train a model gθ(.), which is restricted to sample-wise 1 × 1 convolutions and ReLU activations same as in [48], to decrease the mutual information between two instances. In the original paper, the input sample is split into two instances ( X1 and X2:3) and then adversarial training is performed. As we do not have RGB channels for time-series data, we added Gaussian noise to the signal for creating other instances and then perform adversarial training. NNCLR We follow a similar setup to SimCLR by applying two separate data augmentations, then we use nearest neighbors in the learned representation space as the positive in contrastive losses [49]. PosET We perform the dimension level mixing with extrapolation of positive features as follows: z+ = λ ⊙ z + (1 − λ) ⊙ ˜ z, (44) where ⊙ is Hadamard product, and λ ∼ Beta(α, α). We add 1 to sampled λ for extrapolation as in [50]. GenRep In the original implementation of GenRep, the authors use implicit generative models (IGMs) such as BigBiGAN [85] that are trained with millions of images to create the anchor and positive instance by sampling nearby latent vectors. However, as the number of samples for training is limited in time series and there is a well-trained generator for different time-series tasks, we use our trained V AE for sampling nearby latent vectors as positives. Mainly, we sample an anchor from real data, feed it to the encoder, add a Gaussian noise sampled from a truncated normal distribution, and use the output of the decoder for the positive sample with the anchor. STAug The Spectral and Time Augmentation (STAug) method is specifically proposed for the time-series forecasting task, where the authors apply the empirical mode decomposition to decompose time series into multiple subcomponents, then reassemble these subcomponents with random weights to generate new synthetic series. Finally, in the time domain, the method uses the linear mixup to generate samples from the reassembled components. Although, the mixing coefficient sampled from a beta distribution in the original implementation, we observe significant performance decreases when the same distribution with parameters is used in our experiments, possibly due to the generated samples being far away from the anchor. We, therefore, investigate the case when the mixing coefficient is sampled from uniform distribution with high values, e.g., same as our method. Since there is no Augmentation Bank The augmentation bank that perturbs frequency components of a time-series signal is proposed in [ 21] where the authors use it for unsupervised domain adaptation with a different framework than SimCLR, namely time-frequency consistency (TF-C). As it is a novel data augmentation technique, we have implemented the frequency augmentation bank as a baseline while using the SimCLR framework for a fair comparison with other methods. The authors also employed a collection of time-based augmentations for the time-domain contrastive encoder. Nonetheless, since these augmentations have already been studied in previous CL setups, we chose to exclusively utilize the frequency augmentation bank. In the paper, the authors mentioned using a small budget with low-frequency perturbations results in a performance increase, thus we chose the budget with a single frequency while choosing the α = 0.5 with the same settings in the paper. 22DACL We perform the mixup for hidden representations, i.e., before applying projection-head, as follows. v+ = λv + (1 − λ)˜ v, (45) where v is the fixed-length hidden representations of samples while λ is sampled from uniform distribution with high values. IDAA We follow the original implementation of authors with their proposed V AE architecture while optimizing the adversarial strength for each time-series task. We apply the FGSM adversarial attack the same as in the original implementation [53] by perturbing the encoded representation of a sample while adding noises along the gradient sign’s direction of the loss. One setup difference between this section and the previous mixup methods is that when we compare our work with PosET, GenRep, DACL, and IDAA, we apply the best traditional data augmentation techniques, which are used for SimCLR implementation, to the specific positive data generation mechanisms. The reason for this approach is that the original implementations of certain works indicate that the proposed methods achieve optimal results when used in conjunction with known augmentations, where our observations align with these findings. The detailed hyperparameters for each baseline with the corresponding time series tasks are given in the following section. D Implementation Details D.1 Parameters for mixing In this section, we provide the parameters that are used during our experiments. To determine the optimal parameters of the baselines for each task, we conduct a grid search. This search is performed on a small validation set taken from the largest dataset of the respective tasks, which are USC, Dalia and Chapman. We believe that this approach ensures fairness and produces more realistic results, as dataset-specific optimizations can lead to overfitting of parameters, particularly in smaller and less diverse datasets. Table 7: Parameters for baselines Method Activity Recognition Heart rate Prediction CVD Classification Linear Mixup λ∼U(0.9,1) λ∼U(0.9,1) λ∼U(0.85,1) Binary Mixup m∼U(0.8,1) m∼U(0.9,1) m∼U(0.9,1) Geometric Mixupλ∼U(0.9,1) λ∼U(0.9,1) λ∼U(0.9,1) CutMix b∼U(0,1) b∼U(0,1) b∼U(0,1) a∼U(0.1,0.4) a∼U(0.1,0.3) a∼U(0.1,0.3) AmplitudeMixλA∼U(0.9,1) λA∼U(0.9,1) λA∼U(0.8,1) SpecMix b∼U(0,1) b∼U(0,1) b∼U(0,1) a∼U(0.1,0.4) a∼U(0.1,0.3) a∼U(0.1,0.3) PosET λ∼Beta(2,2) λ∼Beta(2,2) λ∼Beta(2,2) GenRep λ∼ Nt(0,0.2,1.0) λ∼ Nt(0,0.25,1.0) λ∼ Nt(0,0.2,1.0) DACL λ∼U(0.9,1) λ∼U(0.9,1) λ∼U(0.85,1) IDAA δ= 0.1 δ= 0.15 δ= 0.2 Ours λA∼U(0.7,1),λP ∼U(0.9,1) λA∼U(0.7,1),λP ∼U(0.9,1) λA∼U(0.7,1),λP ∼U(0.9,1) ϵ= 0.7,λA, λP ∼ Nt(0.9,0.1,0.9) ϵ= 0.8,λA, λP ∼ Nt(1,0.1,0.9) ϵ= 0.7,λA, λP ∼ Nt(1,0.1,0.9) D.2 Baseline Encoder Architecture For the baseline encoder model, we adopt the DeepConvLSTM as in [19] where the architecture has 4 convolutional layers with 5 × 1 size of 64 kernels while ReLU is followed each convolution. After the convolutions, the tensor is passed through a dropout layer with a dropout rate of 0.5 to prevent overfitting. Then, the output of dropout is fed into the 2-layer LSTM with 128 units. After training 23the baseline encoder, we attach a linear layer and freeze the previous layers for fine-tuning. This architecture is widely used for the datasets we used during our experiments [37, 83, 19], we therefore adopt the same network across tasks. D.3 V AE Models We use the total correlation variational autoencoder (β-TCV AE) [86] to calculate the distance between two encoded samples in the latent space. We train the model for 100 epochs with a learning rate of 1e − 3 while setting the batch size to 2048. The latent dimensions and the β values are set to 10 and 5, respectively. Below, we present the tables providing detailed information about the architectures of the encoder and decoder for datasets. The output of convolutional layers is fed to the batch normalization before the activation layer is applied. For tasks Heart rate Prediction and CVD Classification, we use task-specific encoder and decoder as the number of channels and input size for datasets in each task are the same. However, two different networks, one for UCIHAR and one for others, are designed for the Activity Recognition due to different number of input channels. Table 8: Encoder Network for UCIHAR in Activity Recognition Encoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x128x9 Convolution Nx32x60x7 32 9x3 2x1 ReLU Convolution Nx32x27x5 32 7x3 2x1 ReLU Convolution Nx64x8x3 64 5x3 3x1 ReLU Convolution Nx128x2x1 128 5x3 2x1 ReLU Convolution Nx512x1x1 512 2x1 1x1 ReLU Convolution Nx20x1x1 10 1x1 1x1 Table 9: Decoder Network for UCIHAR in Activity Recognition Decoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x10x1 Transposed Convolution Nx512x2x9 512 2x9 1x1 ReLU Transposed Convolution Nx128x8x9 128 4x1 6x1 ReLU Transposed Convolution Nx64x16x9 64 4x1 2x1 ReLU Transposed Convolution Nx32x32x9 32 4x1 2x1 ReLU Transposed Convolution Nx32x64x9 32 4x1 2x1 ReLU Transposed Convolution Nx1x128x9 1 4x1 2x1 Table 10: Encoder Network for USC and HHAR in Activity Recognition Encoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x100x6 Convolution Nx32x46x5 32 9x2 2x1 ReLU Convolution Nx32x20x4 32 9x2 2x1 ReLU Convolution Nx64x8x3 64 5x2 2x1 ReLU Convolution Nx128x2x2 128 5x2 2x1 ReLU Convolution Nx512x1x1 512 2x2 1x1 ReLU Convolution Nx20x1x1 10 1x1 1x1 24Table 11: Decoder Network for USC and HHAR in Activity Recognition Decoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x10x1 Transposed Convolution Nx512x2x6 512 2x6 1x1 ReLU Transposed Convolution Nx128x6x6 128 6x1 2x1 ReLU Transposed Convolution Nx64x12x6 64 4x1 2x1 ReLU Transposed Convolution Nx32x25x6 32 5x1 2x1 ReLU Transposed Convolution Nx32x50x6 32 4x1 2x1 ReLU Transposed Convolution Nx1x100x6 1 4x1 2x1 Table 12: Encoder Network for Heart rate Prediction Encoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x200x1 Convolution Nx32x94x1 32 13x1 2x1 ReLU Convolution Nx32x43x1 32 9x1 2x1 ReLU Convolution Nx64x18x1 64 9x1 2x1 ReLU Convolution Nx128x6x1 128 7x1 2x1 ReLU Convolution Nx512x1x1 512 5x1 2x1 ReLU Convolution Nx20x1x1 20 2x1 1x1 Table 13: Decoder Network for Heart rate Prediction Decoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x10x1 Transposed Convolution Nx512x6x1 512 6x1 1x1 ReLU Transposed Convolution Nx128x12x1 128 4x1 2x1 ReLU Transposed Convolution Nx64x25x1 64 5x1 2x1 ReLU Transposed Convolution Nx32x50x1 32 4x1 2x1 ReLU Transposed Convolution Nx32x100x1 32 4x1 2x1 ReLU Transposed Convolution Nx1x200x1 1 4x1 2x1 Table 14: Encoder Network for CVD Classification Encoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x1000x4 Convolution Nx32x330x3 32 12x2 3x1 ReLU Convolution Nx32x107x2 32 10x2 3x1 ReLU Convolution Nx64x34x1 64 8x2 3x1 ReLU Convolution Nx128x9x1 128 8x1 3x1 ReLU Convolution Nx512x1x1 512 7x1 3x1 ReLU Convolution Nx20x1x1 20 1x1 1x1 Table 15: Decoder Network for CVD Classification Decoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x10x1 Transposed Convolution Nx512x4x4 512 6x1 1x1 ReLU Transposed Convolution Nx128x12x4 128 4x1 2x1 ReLU Transposed Convolution Nx64x36x4 64 5x1 2x1 ReLU Transposed Convolution Nx32x109x4 32 4x1 2x1 ReLU Transposed Convolution Nx32x331x4 32 4x1 2x1 ReLU Transposed Convolution Nx1x1000x4 1 4x1 2x1 2545 50 55 60 65 70 75 80 85 90 95 -0.2 -0.1 0 Accuracy ΔλP UCIHAR HHAR USC 70 75 80 85 90 95 100 -0.2 -0.1 0 AUC ΔλP CPSC Chapmana) b) c) 10 12 14 16 18 20 22 24 26 -0.2 -0.1 0 Mean Absolute Error ΔλP IEEE SPC12 IEEE SPC22 Dalia Figure 3: The experiment regarding the effect of phase mixup coefficients in eight datasets. a) shows the performance in activity recognition, b) is for heart rate prediction using PPG, and finally c) shows the cardiovascular disease classification 50 60 70 80 90 100 -0.2 -0.1 0 Accuracy ΔλA UCIHAR HHAR USC 75 80 85 90 95 100 -0.2 -0.1 0 AUC ΔλA CPSC Chapmana) b) c) 10 12 14 16 18 20 22 24 26 -0.2 -0.1 0 Mean Absolute Error ΔλA IEEE SPC12 IEEE SPC22 Dalia Figure 4: The experiment regarding the effect of amplitude mixup coefficients in eight datasets. a) shows the performance in activity recognition, b) is for heart rate prediction using PPG, and finally c) shows the cardiovascular disease classification E Additonal Results E.1 The effect and robustness of mixing coefficients In this section, our experiments focus on observing the impact of a diverse range of mixing coefficients for both phase and amplitude components. We decrease the lower threshold of distributions for sampling the mixing coefficient by 0.1 and 0.2. For example, normally the phase mixup coefficient for Activity Recognition is sampled from truncated normal λP ∼ Nt(1, 0.1, 0.9) and uniform λP ∼ U(0.9, 1). We decrease the low threshold value from 0.9 to 0.8 and 0.7 and report the results for both phase and amplitude. The results are reported in Figures 3 and 4 for eight datasets. From Figures 3 and 4, it can be inferred that the phase component is more sensitive to the changes. In other words, a significant decrease in performance is observed when the mixing coefficients for the phase are sampled from lower values whereas this effect is not as much as severe for the amplitude coefficient, indicating that the amplitude of frequencies is more robust to changes compared to phase. E.2 The performance in other frameworks In this section, we investigate the effect of data augmentations in three different unsupervised learning frameworks which are SimCLR [15], BYOL [59] and TS-TCC [16]. For BYOL, the hidden size of the projector is set to 128, the exponential moving average parameter is set to 0.996. For TS-TCC, the λ1 and λ2 coefficients of temporal and contextual contrasting losses are set to 1, the same as in the original implementation. In TS-TCC, the authors proposed to use a weak (jitter and scale) and strong (permutation and jitter) augmentation together, which is shown as TS-TCC + Traditional Augs in the tables. During our experiments, we followed the original implementation of TS-TCC and applied additional augmentations after the strong one without changing the original contrastive learning framework. We set the scaling ratio to 2 and 10 for permutation (splitting the signal into a random number of segments with a maximum of 10 and randomly shuffling them). These parameters for augmentation strengths are set to the same values as in the original implementation. 26Table 16: Performance comparison of our method in different CL frameworks forActivity Recognition Method UCIHAR HHAR USC ACC↑ MF1↑ ACC↑ MF1↑ ACC↑ MF1↑ SimCLR + Traditional Augs. 87.05±1.07 86.13±0.96 85.48±1.16 84.31±1.31 53.47±1.10 52.09±0.95SimCLR + Aug. Bank 65.27 ±1.12 71.16±1.24 67.95±1.45 75.13±1.32 43.28±4.37 47.31±4.68SimCLR + DACL 73.12 ±1.23 66.28±1.11 80.89±0.91 81.31±0.78 53.61±2.60 51.76±2.21SimCLR + Ours 91.60 ±0.65 90.46±0.53 88.05±1.05 87.95±1.10 60.13±0.75 59.13±0.69BYOL + Traditional Augs. 83.41±0.95 82.13±1.12 86.41±0.97 86.31±1.10 58.34±1.15 55.04±1.15BYOL + Aug. Bank 73.71 ±0.74 69.80±1.10 84.60±0.93 84.65±1.03 52.00±1.21 49.14±1.18BYOL + DACL 73.86 ±1.12 70.46±1.24 82.76±1.04 84.89±0.93 47.14±2.08 45.34±2.98BYOL + Ours 87.01 ±1.10 84.92±1.13 90.31±1.16 90.45±1.31 56.87±0.91 55.01±0.95TS-TCC + Traditional Augs. 90.95±0.87 90.30±0.64 35.57±1.43 40.13±1.67 39.76±1.61 43.12±1.10TS-TCC + Aug. Bank 76.78 ±0.95 76.52±0.97 20.25±1.54 19.25±1.32 21.37±1.78 20.15±1.48TS-TCC + DACL 73.86 ±1.12 70.46±1.24 33.89±1.87 37.41±1.39 36.74±1.36 40.18±1.45TS-TCC + Ours 91.86±0.97 91.92±1.02 38.45±1.12 43.52±1.33 42.61±1.92 45.06±1.11 Table 17: Performance comparison of our method in different CL frameworks for Heart Rate Prediction Method IEEE SPC12 IEEE SPC22 DaLia MAE↓ RMSE↓ MAE↓ RMSE↓ MAE↓ RMSE↓ SimCLR + Traditional Augs. 20.67±1.13 26.35±0.98 16.84±1.10 22.23±0.72 12.01±0.65 21.09±0.86SimCLR + Aug. Bank 27.31 ±2.17 37.93±2.96 27.84±2.03 36.41±3.98 35.87±4.18 40.61±3.74SimCLR + DACL 21.85 ±1.63 28.17±1.75 14.67±1.10 20.06±1.21 18.44±1.32 25.61±1.45SimCLR + Ours 16.26 ±0.72 22.48±0.95 12.25±0.47 18.20±0.61 10.57±0.55 20.37±0.73BYOL + Traditional Augs. 20.68±0.98 27.11±0.85 21.16±1.10 26.83±1.05 12.03±0.75 20.77±0.83BYOL + Aug. Bank 26.08 ±1.05 32.62±0.93 21.87±1.03 29.13±1.03 18.63±0.91 28.30±0.87BYOL + DACL 26.45 ±1.23 33.50±1.32 21.29±1.13 27.34±1.33 15.11±0.93 23.21±0.83BYOL + Ours 19.85 ±0.88 26.10±0.94 22.08±1.24 28.20±1.13 11.45±0.63 20.38±0.80TS-TCC + Traditional Augs. 11.08±1.03 16.97±0.92 16.10±1.23 26.11±1.11 16.18±1.03 24.27±0.95TS-TCC + Aug. Bank 11.44 ±1.01 17.06±0.94 13.79±1.21 22.41±1.08 17.28±1.12 25.41±0.98TS-TCC + DACL 11.60 ±1.16 18.26±1.20 15.25±1.26 24.40±1.10 16.27±1.16 24.28±0.97TS-TCC + Ours 10.82±0.65 16.93±0.73 13.63±1.02 21.80±1.11 15.90±0.57 23.81±0.89 Tables 16 17 and 18 compares the performance of three data augmentation techniques, traditional time-series augmentations, DACL and our proposed method, in contrastive learning frameworks of BYOL, SimCLR, and TS-TCC. Table 18: Performance comparison of our method in different CL frameworks for CVD classification Method CPSC 2018 Chapman AUC↑ AUC↑ SimCLR + Traditional Augs. 67.86±3.41 74.69 ±2.04 SimCLR + Aug. Bank 81.78 ±1.24 94.75 ±0.90 SimCLR + DACL 82.38 ±0.84 92.28 ±0.97 SimCLR + Ours 85.30 ±0.45 95.90±0.82 BYOL + Traditional Augs 75.41 ±1.34 85.63 ±1.43 BYOL + Aug. Bank 83.51 ±1.12 91.03 ±1.18 BYOL + DACL 77.61 ±1.16 81.62 ±1.24 BYOL + Ours 83.25 ±1.03 91.23 ±1.15 TS-TCC + Traditional Augs 87.07 ±1.10 92.03 ±1.17 TS-TCC + Aug. Bank 86.67 ±1.04 92.15 ±1.02 TS-TCC + DACL 87.63 ±0.83 92.21 ±0.86 TS-TCC + Ours 88.05±0.37 92.11 ±0.75 The results show that the BYOL is more robust to the choice of augmentations than SimCLR, which is also indicated in the original paper [59]. Also, another important outcome of this ablation experiment is that when the TS-TCC framework is used for datasets HHAR and USC, the performance decreases compared to other datasets. A possible explanation for this decrease in the TS-TCC might be the hyper-parameters of the augmentations that are used in the paper. The authors change the strength of the permutation window from dataset to dataset. In our experiments, we used the same hyperparameter for all activity recognition datasets, which can explain the outcome. This ablation experiment also shows that the degree of traditional augmentations is important for contrastive learning to learn class invariant representations. 27E.3 Do we still need data augmentations? In this section, we conduct experiments to observe the performance of methods without additional augmentations. During our experiments, we searched for the best traditional augmentation technique for each method in a given task. We searched over common time series augmentation methods in literature (Table 22), and applied them with baselines. Specifically, we apply Resample for Activity Recognition, Permutation with Noise for Heart rate Prediction and Noise with Scaling for CVD Classification. We have observed that these augmentations yield the best results for all baselines when applied prior to the proposed techniques. However, for GenRep, we found that applying the augmentations after generating instances results in better performance, similar to the original work [51]. We, therefore, apply these specified augmentations for each baseline and report the corresponding results. Different from other baselines, we observed performance increases for a few datasets when GenRep is applied without any augmentations. This phenomenon can be attributed to the generation of low-quality and less realistic positive samples, where additional augmentations lead to alterations in semantic information, due to less number of samples during training V AE models. However, in the end, we observe that applying additional augmentations always increases the performance on average for all baselines in each task. Table 19: Performance comparison of methods without Augs. in Activity Recognition datasets Method UCIHAR HHAR USC ACC↑ MF1↑ ACC↑ MF1↑ ACC↑ MF1↑ IDAA [53] 82.23 ±0.69 79.84 ±0.89 88.98±0.62 89.01±0.55 59.23 ±1.10 56.11 ±1.54 w/o Aug. 64.42 (-17.81) 65.17 (-14.67) 86.44 (-2.54) 86.31 (-2.70) 35.22 (-24.01) 33.62 (-22.59) GenRep [51] 87.22±1.05 86.48 ±0.95 87.05 ±0.95 86.45 ±0.90 50.13 ±2.85 49.50 ±2.73 w/o Aug. 88.01 (+0.79) 88.12 (+1.64) 86.51 (-0.54) 86.33 (-0.22) 48.31 (-1.82) 47.33 (-2.17) DACL [22] 73.12 ±1.23 66.28 ±1.11 80.89 ±0.91 81.31 ±0.78 53.61 ±2.60 51.76 ±2.21 w/o Aug. 45.17 (-27.95) 44.84 (-21.44) 56.70 (-24.19) 56.55 (-25.76) 27.12 (-26.49) 26.99 (-24.77) Ours 91.60±0.65 90.46±0.53 88.05 ±1.05 87.95 ±1.10 60.13±0.75 59.13±0.69 w/o Aug. 84.04 (-5.56) 83.34 (-7.12) 86.70 (-1.35) 86.72 (-1.23) 45.55 (-14.58) 44.94 (-14.19) Table 20: Performance comparison of methods without Augs. in Heart Rate Prediction datasets Method IEEE SPC12 IEEE SPC22 DaLia MAE↓ RMSE↓ MAE↓ RMSE↓ MAE↓ RMSE↓ IDAA [53] 19.02 ±0.96 27.42 ±1.11 15.37 ±1.21 22.41 ±1.42 11.12 ±0.64 20.45 ±0.69 w/o Aug. 20.19 (+1.17) 28.51 (+1.09) 16.34 (+0.97) 25.75 (+3.34) 16.01 (+4.89) 25.62 (+5.17) GenRep [51] 21.02±1.41 28.42 ±1.65 15.67 ±1.23 22.33 ±1.43 25.41 ±1.62 36.83 ±1.87 w/o Aug. 20.51 (-0.51) 28.35 (-0.07) 23.07 (+7.40) 33.20 (+10.87) 20.03 (-5.38) 31.01 (-5.82) DACL [22] 21.85 ±1.63 28.17 ±1.75 14.67 ±1.10 20.06 ±1.21 18.44 ±1.32 25.61 ±1.45 w/o Aug. 22.75 (+0.90) 29.90 (+1.73) 20.88 (+6.21) 29.51 (+2.70) 28.24 (+9.45) 37.33 (+11.72) Ours 16.26±0.72 22.48±0.95 12.25±0.47 18.20±0.61 10.57±0.55 20.37±0.73 w/o Aug. 19.41 (+3.15) 26.23 (+3.75) 16.41 (+4.16) 25.71 (+7.51) 16.73 (+6.16) 27.43 (+7.06) 28Table 21: Performance comparison of methods without Augs. in CVD classification datasets Method CPSC 2018 Chapman AUC↑ AUC↑ IDAA [53] 80.90 ± 0.73 93.63 ± 0.91 w/o Aug. 79.00 (-1.90) 92.37 (-1.26) GenRep [51] 52.49 ± 3.43 86.72 ± 1.13 w/o Aug. 45.17 (-7.32) 84.51 (-2.21) DACL [22] 82.38 ± 0.84 92.28 ± 0.97 w/o Aug. 73.00 (-9.38) 75.10 (-17.18) Ours 85.30 ± 0.45 95.90 ± 0.82 w/o Aug. 79.67 (-5.63) 93.48 (-2.42) Table 22: Common time series augmentations [19] Domain Augmentation Details Time Noise Add Gaussian noise sampled from normal distribution,N(0,0.4) Scale Amplify channels by a random distortion sampled from normal distributionN(2,1.1) Shuffle Randomly permute the channels of the sample. (Not available forHeart rate Prediction) Negate Multiply the value of the signal by a factor of -1 Permute Split signals into no more than 5 segments, then permute the segments and combine them into the original shape Resample Interpolate the time-series to 3 times its original sampling rate and randomly down-sample to its initial dimensions Rotation Rotate the 3-axial (x, y, and z) readings of each IMU sensor by a random degree, which follows a uniform around a random axis in the 3D space. (Only applied forActivity Recognition) Time Flip Flip the time series in time for all channels, i.e.,xAug[n] =x[−n] Random Zero Out Randomly chose a section to zero out Permutation + Noise Combination of Permutation and Noise Noise + Scale Combination of Noise and Scaling Frequency Highpass Apply a highpass filter in the frequency domain to reserve high-frequency components Lowpass Apply a lowpass filter in the frequency domain to reserve low-frequency components Phase shift Shift the phase of time-series data with a randomly generalized number Noise in Frequency Add Gaussian noise, sampled from normal distributionN(0,0.5), to the frequency spectrum 29E.4 The effect of interpolating phase components Here, we investigate the effect of phase interpolation of two samples on the CL performance. In our proposed method, we bring the phase components of the two coherent signals together by adding a small value to the anchor’s phase in the direction of the other sample. In this section, we apply the opposite case of our proposed method and increase the gap of phase difference between the anchor and randomly chosen sample. However, we mix their amplitudes according to our proposed method to only observe the phase effect. In other words, we perform the mixup as in Equation 46. Note that the phase mixing in Equation 46 differs from the proposed method only by the sign change. x+ = F−1(A(x+)∠P(x+)) where A(x+) = λAA(x) + (1− λA)A(˜ x) and P(x+) = P(x) + ∆Θ∗ (1 − λP ) (46) Also, It is important to note that we sample the mixing coefficients for both amplitude and phase from the same distributions in the proposed method to have a fair comparison. Tables 23 24 25. Table 23: Performance comparison of our method and its ablation regarding the phase interpolation in SimCLR and BYOL frameworks for Activity Recognition Method UCIHAR HHAR USC ACC↑ MF1↑ ACC↑ MF1↑ ACC↑ MF1↑ SimCLR + Traditional Augs. 87.05±1.07 86.13±0.96 85.48±1.16 84.31±1.31 53.47±1.10 52.09±0.95SimCLR + Phase Gap 79.62 ±1.10 80.57±1.03 86.55±0.83 86.68±0.71 53.61±2.60 51.76±2.21SimCLR + Ours 91.60±0.65 90.46±0.53 88.05±1.05 87.95±1.10 60.13±0.75 59.13±0.69BYOL + Traditional Augs. 83.41±0.95 82.13±1.12 86.41±0.97 86.31±1.10 58.34±1.15 55.04±1.15BYOL + Phase Gap 78.66 ±0.63 75.45±1.02 85.82±0.91 85.16±0.92 56.14±0.67 56.20±0.75BYOL + Ours 87.01 ±1.10 84.92±1.13 90.31±1.16 90.45±1.31 56.87±0.91 55.01±0.95 Table 24: Performance comparison of our method and its ablation regarding the phase interpolation in SimCLR and BYOL frameworks for Heart Rate Prediction Method IEEE SPC12 IEEE SPC22 DaLia MAE↓ RMSE↓ MAE↓ RMSE↓ MAE↓ RMSE↓ SimCLR + Traditional Augs. 20.67±1.13 26.35±0.98 16.84±1.10 22.23±0.72 12.01±0.65 21.09±0.86SimCLR + Phase Gap 18.90 ±1.43 25.29±1.56 14.60±1.03 19.84±1.15 17.57±1.13 27.72±1.35SimCLR + Ours 16.26±0.72 22.48±0.95 12.25±0.47 18.20±0.61 10.57±0.55 20.37±0.73BYOL + Traditional Augs. 20.68±0.98 27.11±0.85 21.16±1.10 26.83±1.05 12.03±0.75 20.77±0.83BYOL + Phase Gap 25.93 ±0.96 32.68±0.90 21.87±1.03 29.13±1.03 17.46±0.83 27.24±0.83BYOL + Ours 19.85 ±0.88 26.10±0.94 22.08±1.24 28.20±1.13 11.45±0.63 20.38±0.80 Table 25: Performance comparison of our method and its ablation regarding the phase interpolation in SimCLR and BYOL frameworks for CVD classification Method CPSC 2018 Chapman AUC↑ AUC↑ SimCLR + Traditional Augs. 67.86±3.41 74.69 ±2.04 SimCLR + Phase Gap 77.45 ±1.10 91.95 ±0.91 SimCLR + Ours 85.30±0.45 95.90±0.82 BYOL + Traditional Augs 75.41 ±1.34 85.63 ±1.43 BYOL + Phase Gap 83.11 ±1.03 91.02 ±1.11 BYOL + Ours 83.25 ±1.03 91.23 ±1.15 30E.5 The comparison of Mixup methods In this section, we give a detailed comparison of prior mixup methods with ours below tables, which are the explicit numbers for Figure 2. Our method demonstrates superior performance compared to previous mixup techniques in 11 out of 14 metrics, indicating its effectiveness. Additionally, the Amplitude Mixup technique, which yields comparable results in two datasets, further supports our claim regarding the destructive effect of simultaneously mixing phase and magnitude for time series. The relatively lower performance of Amplitude Mixup for some datasets can be explained by its limited diversity in generating positive samples since this technique has no solution for mixing the phase of samples in randomly chosen pairs. In other words, as the phase of the augmented instance is the same as the anchor in Amplitude Mix, the diversity of generated positive samples is less compared to other techniques. Table 26: Performance comparison of ours with prior mixups in Activity Recognition datasets Method UCIHAR HHAR USC ACC↑ MF1↑ ACC↑ MF1↑ ACC↑ MF1↑ Geo 36.31 ±10.15 33.21 ±12.25 33.16 ±8.32 31.15 ±9.25 24.85 ±9.43 21.64 ±8.94 Amp 81.76 ±0.89 80.78 ±0.78 87.85±0.83 85.53±1.10 41.29 ±0.56 39.77 ±1.03 Spec 40.14 ±2.05 38.34 ±1.95 56.73 ±2.01 53.54 ±1.98 23.45 ±2.55 21.30 ±2.41 Cut 50.21 ±1.34 48.23 ±1.23 57.71 ±1.12 53.87 ±1.09 25.63 ±2.95 23.41 ±3.11 Binary 74.13 ±1.12 71.31 ±1.10 77.12 ±0.75 75.23 ±0.95 42.21 ±0.97 41.53 ±1.10 Linear 82.23 ±2.10 80.25 ±1.93 80.11 ±2.05 81.31 ±1.73 40.15 ±1.43 39.71 ±1.14 Ours 84.30±0.73 83.23±0.58 84.51 ±1.10 83.98 ±1.03 45.36±0.97 43.14±0.81 Table 27: Performance comparison of ours with prior mixups in Heart Rate Prediction datasets Method IEEE SPC12 IEEE SPC 22 DaLia MAE↓ RMSE↓ MAE↓ RMSE↓ MAE↓ RMSE↓ Geo 32.65 ±7.25 48.90 ±9.87 37.15 ±6.74 36.32 ±6.21 38.45 ±7.31 41.32 ±6.21 Amp 23.01 ±0.95 30.10 ±1.04 18.07 ±1.13 23.13 ±1.43 19.05 ±1.63 30.41 ±1.65 Spec 24.09 ±4.10 38.41 ±3.98 24.41 ±4.10 29.93 ±4.10 26.71 ±4.34 35.31 ±3.93 Cut 24.98 ±3.93 35.67 ±4.15 21.77 ±4.45 28.43 ±3.97 31.75 ±4.10 43.56 ±3.88 Binary 32.23 ±1.67 40.21 ±1.98 22.55 ±1.87 28.78 ±2.10 19.71 ±2.15 28.83 ±2.45 Linear 24.31 ±1.54 31.29 ±1.75 18.52 ±1.43 22.54 ±1.49 24.16 ±1.89 32.46 ±1.97 Ours 21.13±0.89 28.21±1.15 16.17±0.85 21.13±1.05 16.64±1.20 28.43±1.43 Table 28: Performance comparison of ours with prior mixups in CVD classification datasets Method CPSC 2018 Chapman AUC↑ AUC↑ Geo 45.65 ± 6.43 61.32 ± 5.79 Amp 84.10 ± 1.05 89.83 ± 1.12 Spec 69.26 ± 3.10 70.48 ± 3.05 Cut 72.20 ± 2.98 79.23 ± 2.75 Binary 80.53 ± 1.62 82.56 ± 1.45 Linear 78.02 ± 1.43 90.21 ± 1.15 Ours 83.79 ± 1.10 93.85 ± 1.05 31F Illustrative Examples In this section, we show examples of the destructive behavior of linear mixup and how our proposed mixup technique solves this problem. In Figure 5 a), we show two PPG waveforms that are obtained from IEEE SPC15 with the same label i.e., the same heart rate value. Also, we give the corresponding frequency domain transformations of these two waveforms in Figure 5 b) where the frequency axis is converted to heart rate in beats-per-minute i.e., 1 Hz corresponds to 60 bpm. 40 60 80 100 120 140 160 180 200 0 0.1 0.2 0.3 0.4 0.5 Anchor  Sample Normalized Power (dB) a) b) 1 2 3 4 5 6 7 8 -2 -1 0 1 2 Anchor  Sample Normalized Magnitude (V) Time (s) Heart rate (bpm) Two samples with the same label where the anchor has a severe motion artifact at 2-3 Hz. 0 Figure 5: a) The waveforms of anchor and random sample, b) The frequency domain ( A(x)) representation of two samples. When the linear mixup is applied as in Equation 47 with a λ of 0.9, the resulting waveform is anticipated to contain heart rate information to an extent similar to both the anchor and the sample. x+ = λx + (1 − λ)˜ x (47) However, when there is a phase difference greater than π/2 between these two samples in the frequencies where the task-specific information is carried, the linear mixup destroys the information. 40 60 80 100 120 140 160 180 2000 0.1 0.2 0.3 0.4 0.5 Anchor  Sample  Linear mixup The linear mixup method  generates an augmented sample where the information in the critical frequency band decreased signiﬁcantly compared to both samples. Normalized Power (dB) Heart rate (bpm) a) b) 1 2 3 4 5 6 7 8 -2 -1 0 1 2 Anchor  Linear mixup Normalized Magnitude (V) Time (s) 0 Figure 6: a) The waveform of anchor and augmented sample with linear mixup, b) The frequency domain (A(x)) representations of samples where the augmented waveform has lost all the information in the critical frequency band, i.e., the task-specific information is lost. 32Figures 5 and 6 demonstrate the destructive behavior of linear mixup instead of feature interpolation. The linear mixup technique destroys the task-specific information even though the two samples have the same labels and the mixup ratio is relatively high. As our proposed mixup prevents this problem and interpolates between features of two samples, the information is not lost but rather enhanced as both samples have the same label, shown in Figure 7. 1 2 3 4 5 6 7 8 -2 -1 0 1 2 0 40 60 80 100 120 140 160 180 2000 0.1 0.2 0.3 0.4 0.5Normalized Power (dB) Heart rate (bpm) Normalized Magnitude (V) Time (s) Anchor  Sample  Proposed mixup Anchor  Proposed mixup a) b) The proposed mixup technique interpolates the features without causing any information loss in the critical frequency band. Figure 7: a) The waveform of anchor and augmented sample with proposed mixup technique, b) The frequency domain (A(x)) representations of samples where the augmented waveform carries the information in the critical frequency band as an interpolation of two samples. As can be seen From figures 6 and 7, our proposed mixup technique not only prevents information loss due to linear mixup but also generates an interpolated sample. 33G Performance in Supervised Learning Paradigm We also conduct experiments in the supervised learning paradigm with our proposed mixup method to see its effectiveness in different learning paradigms. We compare the performance of our method with prior mixup techniques. During the experiments, we follow the original implementation where the mixup is applied to the same minibatch after random shuffling. In the seminal work of mixup [31], the authors stated that interpolating only between inputs with equal labels does not lead to performance gains. Therefore, we only perform the tailored mixup without implementing any V AEs to check the similarity of the randomly chosen samples. We implement the tailored mixup for the supervised learning paradigm as follows. x+ = F−1(A(x+)∠P(x+)) where A(x+) = λAA(x) + (1− λA)A(˜ x) and P(x+) =    P(x) − |∆Θ| ∗(1 − λP ), if ∆Θ > 0 and λA ≥ 0.5 P(x) + |∆Θ| ∗(1 − λP ), if ∆Θ ≤ 0 and λA ≥ 0.5 P(˜ x) − |∆Θ| ∗(1 − λP ), if ∆Θ > 0 and λA < 0.5 P(˜ x) + |∆Θ| ∗(1 − λP ), if ∆Θ ≤ 0 and λA < 0.5 y+ = λAyx + (1 − λA)y˜ x, (48) where the coefficient for the λA is chosen from a beta distribution with α ∈ [0.1, 0.4] within the same range of the original implementation [ 31]. The mixing for the phase is constrained to our original implementation with a uniform λP ∼ U(0.9, 1). We searched for the best α value for each time-series task and augmentation method. Unlike linear mixup and our mixup approach, for cutmix, we followed the recommendation from the original paper and searched the α value close to 1. Table 29: Performance comparison in Activity Recognition within supervised learning scheme Method UCIHAR HHAR USC ACC↑ MF1↑ ACC↑ MF1↑ ACC↑ MF1↑ W/o Augs. 65.66 ±0.23 61.21 ±0.15 91.58 ±0.07 91.64 ±0.11 71.93 ±0.54 68.43 ±0.78 Linear Mix 77.06 ±0.18 73.21 ±0.17 93.64 ±0.17 93.67 ±0.08 74.45 ±0.28 71.93 ±0.43 Amp Mix 70.96 ±0.19 67.14 ±0.33 92.50 ±0.15 92.54 ±0.10 74.02 ±0.19 71.90 ±0.26 Binary Mix 69.01 ±0.36 71.63 ±0.11 92.36 ±0.19 92.42 ±0.10 72.81 ±0.15 70.98 ±0.35 CutMix 67.14 ±0.54 63.31 ±0.48 90.37 ±0.43 90.36 ±0.76 57.89 ±0.34 61.45 ±0.57 Ours 81.60±0.15 79.35±0.13 94.02±0.05 94.00±0.06 74.85±0.19 72.45±0.34 Table 30: Performance comparison in Heart Rate Prediction within supervised learning scheme Method IEEE SPC12 IEEE SPC 22 DaLia MAE↓ RMSE↓ MAE↓ RMSE↓ MAE↓ RMSE↓ W/o Augs. 20.01 ±0.03 27.16 ±0.05 20.29 ±0.87 26.60 ±1.13 6.58 ±0.10 11.30 ±0.58 Linear Mix 20.07 ±0.09 26.93 ±0.10 19.98 ±0.12 24.90 ±0.51 6.97 ±0.14 12.07 ±0.51 Amp Mix 20.14 ±0.07 26.98 ±0.07 19.61 ±0.07 24.11±0.21 11.20 ±0.17 16.07 ±0.43 Binary Mix 21.05 ±0.13 27.02 ±0.08 19.62 ±0.10 25.23 ±0.13 7.35 ±0.16 12.17 ±0.53 CutMix 20.12 ±0.06 26.89±0.11 19.64 ±0.13 24.18 ±0.20 10.78 ±1.23 14.40 ±1.43 Ours 19.97±0.05 26.98 ±0.10 19.45±0.12 24.35 ±0.18 6.49±0.08 11.69±0.10 Table 31: Performance comparison in CVD classification within supervised learning scheme Method CPSC 2018 Chapman AUC↑ AUC↑ W/o Augs. 82.01 ± 0.51 92.27 ± 0.35 Linear Mix 80.29 ± 0.93 93.02 ± 0.33 Amp Mix 80.01 ± 0.36 89.11 ± 0.27 Binary Mix 78.10 ± 0.98 80.31 ± 0.36 CutMix 80.75 ± 0.78 89.17 ± 0.58 Ours 83.75 ± 0.32 95.26 ± 0.24 34",
      "references": [
        "Unsupervised representation learning by predicting image rotations.",
        "Unsupervised visual representation learning by context prediction.",
        "Context encoders: Feature learning by inpainting.",
        "Dimensionality reduction by learning an invariant mapping.",
        "Discriminative unsupervised feature learning with convolutional neural networks.",
        "Big self-supervised models are strong semi-supervised learners.",
        "Wav2vec 2.0: A framework for self-supervised learning of speech representations.",
        "Contrastive learning of global and local video representations.",
        "Unispeech: Unified speech representation learning with labeled and unlabeled data.",
        "w2v-bert: Combining contrastive learning and masked language modeling for self- supervised speech pre-training.",
        "Lrc-bert: Latent-representation contrastive knowledge distillation for natural language understanding.",
        "Kace: Generating knowledge aware contrastive explanations for natural language inference.",
        "Semi-supervised intent discovery with contrastive learning.",
        "Contrastive unsupervised word alignment with non-local features.",
        "A simple framework for contrastive learning of visual representations.",
        "Time-series representation learning via temporal and contextual contrasting.",
        "Chaos is a ladder: A new theoretical understanding of contrastive learning via augmentation overlap.",
        "Evaluation and comparison of eeg traces: Latent structure in nonstationary time series.",
        "What makes good contrastive learning on small-scale wearable-based tasks?",
        "Time series data augmentation for deep learning: A survey.",
        "Self-supervised contrastive pre-training for time series via time-frequency consistency.",
        "Towards domain- agnostic contrastive learning.",
        "Ssmix: Saliency-based span mixup for text classification.",
        "Detection of gait from continuous inertial sensor data using harmonic frequencies.",
        "Smartphone-based blood pressure measurement using transdermal optical imaging technology.",
        "Contact-free screening of atrial fibrillation by a smartphone using facial pulsatile photoplethysmographic signals.",
        "Modeling quasi-periodic signals by a non-parametric model: Application on fetal ecg extraction.",
        "Modelling quasi-periodic signals in geodetic time-series using Gaussian processes.",
        "Quasi-periodic atrial activity components in the ecg used to discriminate between paroxysmal and chronic atrial fibrillation.",
        "Quasiperiodicity and chaos in cardiac fibrillation.",
        "mixup: Beyond empirical risk minimization.",
        "beta-vae: Learning basic visual con- cepts with a constrained variational framework.",
        "Optimal positive gener- ation via latent transformation for contrastive learning.",
        "Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine.",
        "Smart devices are different: Assessing and mitigatingmobile sensing heterogeneities for activity recognition.",
        "Usc-had: A daily activity dataset for ubiquitous activity recognition using wearable sensors.",
        "Latent independent excitation for general- izable sensor-based cross-person activity recognition.",
        "Troika: A general framework for heart rate monitoring using wrist-type photoplethysmographic signals during intensive physical exercise.",
        "Deep ppg: Large-scale heart rate estimation with convolutional neural networks.",
        "An open access database for evaluating the algorithms of electrocardiogram rhythm and morphology abnormality detection.",
        "A 12-lead electrocardiogram database for arrhythmia research covering more than 10,000 patients.",
        "Classification of 12-lead ecgs: the physionet/computing in cardiology challenge 2020.",
        "On adversarial mixup resynthesis.",
        "Cutmix: Regularization strategy to train strong classifiers with localizable features.",
        "A fourier-based framework for domain generalization.",
        "Specmix : A mixed sample data augmentation method for training withtime-frequency domain features.",
        "Un-mix: Rethinking image mixtures for unsupervised visual representation learning.",
        "What makes for good views for contrastive learning?",
        "With a little help from my friends: Nearest-neighbor contrastive learning of visual representations.",
        "Improving contrastive learning by visualizing feature transformation.",
        "Generative models as a data source for multiview representation learning.",
        "Towards diverse and coherent augmentation for time-series forecasting.",
        "Identity-disentangled adversarial augmentation for self-supervised learning.",
        "Cornet: Deep learning framework for ppg-based heart rate estimation and biometric identification in ambulant environment.",
        "Deepsleepnet: a model for automatic sleep stage scoring based on raw single-channel eeg.",
        "Multi-source deep domain adaptation with weak supervision for time-series sensor data.",
        "Clocs: Contrastive learning of cardiac signals across space, time, and patients.",
        "Contrastive heartbeats: Contrastive learning for self-supervised ecg representation and phenotyping.",
        "Bootstrap your own latent a new approach to self-supervised learning.",
        "Momentum contrast for unsupervised visual representation learning.",
        "Boosting contrastive self-supervised learning with false negative cancellation.",
        "A theoretical analysis of contrastive unsupervised representation learning.",
        "Working hard to know your neighbor's margins: Local descriptor learning loss.",
        "Hard negative examples are hard, but useful.",
        "Robust contrastive learning using negative samples with diminished semantics.",
        "Parametric instance classification for unsupervised visual feature learning.",
        "Debiased contrastive learning.",
        "Mining on manifolds: Metric learning without labels.",
        "On mutual information in contrastive learning for visual representations.",
        "FaceNet: A unified embedding for face recognition and clustering.",
        "Hard negative mixing for contrastive learning.",
        "Contrastive learning with adversarial examples.",
        "Caco: Both positive and negative samples are directly learnable via cooperative-adversarial contrastive learning.",
        "Mix-up contrastive learning for visual representation.",
        "i-mix: A domain-agnostic strategy for contrastive representation learning.",
        "Simper: Simple self-supervised learning of periodic targets.",
        "Self-supervised pre-training for time series classification.",
        "Progressive mix-up for few-shot supervised multi-source domain transfer.",
        "Co-mixup: Saliency guided joint mixup with supermodular diversity.",
        "Manifold mixup: Better representations by interpolating hidden states.",
        "{MODALS}: Modality-agnostic automated data aug- mentation in the latent space.",
        "Binary cornet: Accelerator for hr estimation from wrist-ppg.",
        "Real-time robust heart rate estimation from wrist-type ppg signals using multiple reference adaptive noise cancellation.",
        "Large scale adversarial representation learning.",
        "Isolating sources of disentanglement in variational autoencoders."
      ],
      "meta_data": {
        "arxiv_id": "2309.13439v2",
        "authors": [
          "Berken Utku Demirel",
          "Christian Holz"
        ],
        "published_date": "2023-09-23T17:42:13Z",
        "github_url": "https://github.com/eth-siplab/Finding_Order_in_Chaos"
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses data augmentation for time series in contrastive learning by introducing a phase-aware, two-component mixup for quasi-periodic non-stationary signals. It treats magnitude and phase separately, uses latent-space similarity (via a β-VAE) to guide mixup coefficients, and aggressively interpolates semantically similar pairs while avoiding destructive augmentation. Empirically outperforms state-of-the-art augmentation and mixup baselines across three time-series tasks (activity recognition, heart-rate estimation, cardiovascular disease classification).",
        "methodology": "Proposes a phase-aware mixup that disentangles magnitude and phase in the frequency domain. Magnitude is interpolated linearly; phase is adjusted by adding a phase-difference-based offset to align phase directions, avoiding destructive interference. Mixing coefficients for magnitude (λA) and phase (λP) are sampled based on latent-space proximity obtained from a β-VAE trained in an unsupervised manner to capture semantic similarity without labels; aggressive mixing for closer samples, milder for distant ones. Theoretical guarantees (lower bound on mutual information) are provided; the latent space guides interpolation strength. The method is integrated into a SimCLR-like contrastive framework with a Conv+LSTM encoder and InfoNCE loss; pretraining is unsupervised, with linear evaluation on labeled data. The approach is complemented by ablations and extensive baselines.",
        "experimental_setup": "Eight datasets across three time-series tasks were used: Activity recognition (UCIHAR, HHAR, USC), Heart rate prediction (IEEE SPC12, SPC22, DaLia), and Cardiovascular disease classification (CPSC2018, Chapman). Evaluation protocols include cross-person (leave-one-domain-out) for activity, leave-one-session-out (LOSO) for heart rate, and domain-specific fine-tuning for CVD. Baselines include Linear/Binary/Geometric/CutMix, AmplitudeMix, SpecMix, and advanced sample-generation methods (InfoMin, NNCLR, GenRep, DACL, IDAA, STAug, Aug Bank). Training details: SimCLR framework without memory bank, batch size 256, 120 epochs, Adam optimizer, linear evaluation after pretraining; metrics include ACC, MF1, MAE, RMSE, and AUC. Datasets and preprocessing details align with the paper’s Appendix; 3 random seeds for results. ",
        "limitations": "Relies on quality of unsupervised β-VAE latent space to reflect true semantic similarity; in some datasets latent-distance-based coefficient sampling may not help or may hurt; computational overhead due to training VAEs and per-frequency phase adjustments; assumptions of quasi-periodicity and band-limited informative frequencies may not hold for all time-series domains; performance can be sensitive to hyperparameters and data distribution (number of classes, sample size).",
        "future_research_directions": "Explore alternative latent-space similarity measures and distance metrics that scale to many classes or small datasets; extend the method to non-quasi-periodic time series and other modalities; integrate with other SSL frameworks (BYOL, TS-TCC) and online/streaming settings; investigate different phase representations or more sophisticated phase interpolation; broaden to more domains and larger datasets, analyze theoretical guarantees under varied data regimes, and refine augmentation coefficient sampling strategies for dynamic task characteristics.",
        "experimental_code": "# Implementation sections directly related to the method (phase-aware, two-component mixup guided by latent space)\n\n# 1) Magnitude/Phase mixing in frequency domain with latent-guided coefficients\n\ndef gen_new_aug_2_sup(sample, args, inds, out, DEVICE, similarities):\n    fftsamples = torch.fft.rfft(sample, dim=1, norm='ortho')\n    inds = torch.randperm(sample.size(0))\n    mixing_coeff = mixing_coefficient_set_for_each(similarities, inds, args)\n    coeffs = mixing_coeff.squeeze()\n\n    abs_fft = torch.abs(fftsamples)\n    phase_fft = torch.angle(fftsamples)\n    mixed_abs = abs_fft * coeffs[:, None, None] + (1 - coeffs[:, None, None]) * abs_fft[inds]\n    mixed_phase = phase_mix(phase_fft, inds, similarities)\n    z = torch.polar(mixed_abs, mixed_phase)\n    mixed_samples_time = torch.fft.irfft(z, dim=1, norm='ortho')\n    return mixed_samples_time\n\n\ndef gen_new_aug_3_ablation_sup(sample, args, DEVICE, similarities):  # Ablation: random coeffs, latent similarities not used\n    fftsamples = torch.fft.rfft(sample, dim=1, norm='ortho')\n    inds = torch.randperm(sample.size(0))\n    coeffs = torch.ones(sample.shape[0])\n    coeffs = torch.nn.init.trunc_normal_(coeffs, 1, 0.1, 0.9, 1)\n\n    abs_fft = torch.abs(fftsamples)\n    phase_fft = torch.angle(fftsamples)\n    mixed_abs = abs_fft * coeffs[:, None, None] + (1 - coeffs[:, None, None]) * abs_fft[inds]\n    dtheta, sign = phase_mix_2(phase_fft, inds)\n    mixed_phase = phase_fft + (1 - coeffs[:, None, None]) * torch.abs(dtheta) * sign\n    z = torch.polar(mixed_abs, mixed_phase)\n    mixed_samples_time = torch.fft.irfft(z, dim=1, norm='ortho')\n    return mixed_samples_time\n\n\ndef gen_new_aug(sample, args, DEVICE):\n    fftsamples = torch.fft.rfft(sample, dim=1, norm='ortho')\n    index = torch.randperm(sample.size(0))\n    mixing_coeff = (0.9 - 1) * torch.rand(1) + 1\n    abs_fft = torch.abs(fftsamples)\n    phase_fft = torch.angle(fftsamples)\n    mixed_abs = abs_fft * mixing_coeff + (1 - mixing_coeff) * abs_fft[index]\n    z = torch.polar(mixed_abs, phase_fft)  # Go back to fft\n    mixed_samples_time = torch.fft.irfft(z, dim=1, norm='ortho')\n    return mixed_samples_time\n\n\ndef phase_mix(phase_fft, inds, similarities):\n    phase_difference = phase_fft - phase_fft[inds]\n    dtheta = phase_difference % (2 * torch.pi)\n    dtheta[dtheta > torch.pi] -= 2 * torch.pi\n    clockwise = dtheta > 0\n    sign = torch.where(clockwise, -1, 1)\n    coeffs = torch.squeeze(mixing_coefficient_set_for_each_phase(similarities, inds))\n    mixed_phase = phase_fft\n    mixed_phase = phase_fft + (1 - coeffs[:, None, None]) * torch.abs(dtheta) * sign\n    return mixed_phase\n\n\ndef phase_mix_2(phase_fft, inds):\n    phase_difference = phase_fft - phase_fft[inds]\n    dtheta = phase_difference % (2 * torch.pi)\n    dtheta[dtheta > torch.pi] -= 2 * torch.pi\n    clockwise = dtheta > 0\n    locs = torch.where(torch.abs(phase_difference) > torch.pi, -1, 1)\n    sign = torch.where(clockwise, -1, 1)\n    return dtheta, sign\n\n\ndef mixing_coefficient_set_for_each(similarities, inds, args):\n    threshold = 0.8\n    mixing_coefficient = torch.ones(similarities.shape)\n    similarities = similarities.cpu()\n    distances = torch.gather(similarities, 0, inds.unsqueeze(1)).cpu().numpy()\n    mixing_coefficient = torch.ones(similarities.shape)\n    distances[distances > threshold] = (0.7 - 1) * torch.rand(1) + 1\n    mixing_coefficient = torch.ones(distances.shape)\n    mixing_coefficient = torch.nn.init.trunc_normal_(mixing_coefficient, args.mean, args.std, args.low_limit, args.high_limit)\n    distances[distances <= threshold] = mixing_coefficient[distances <= threshold]\n    distances = torch.from_numpy(distances)\n    return distances\n\n\ndef mixing_coefficient_set_for_each_phase(similarities, inds):\n    threshold = 0.8\n    mixing_coefficient = torch.ones(similarities.shape)\n    similarities = similarities.cpu()\n    distances = torch.gather(similarities, 0, inds.unsqueeze(1)).cpu().numpy()\n    mixing_coefficient = torch.ones(similarities.shape)\n    distances[distances > threshold] = (0.9 - 1) * torch.rand(1) + 1\n    mixing_coefficient = torch.ones(distances.shape)\n    mixing_coefficient = torch.nn.init.trunc_normal_(mixing_coefficient, 1, 0.1, 0.9, 1)\n    distances[distances <= threshold] = mixing_coefficient[distances <= threshold]\n    distances = torch.from_numpy(distances)\n    return distances\n\n\ndef calculate_similarity_latents(args, sample, DEVICE):\n    vae = load_vae(args, DEVICE)\n    qz_params = vae.encoder.forward(sample.to(DEVICE).float()).view(sample.size(0), args.latent_dim, vae.q_dist.nparams).data\n    latent_values = vae.q_dist.sample(params=qz_params)\n    a_norm = latent_values / latent_values.norm(dim=1)[:, None]\n    b_norm = latent_values / latent_values.norm(dim=1)[:, None]\n    res = torch.mm(a_norm, b_norm.transpose(0, 1))\n    res = res.fill_diagonal_(0)  # Make diagonals to 0\n    return res\n\n# 2) Latent-based coefficient extraction used to guide augmentation\n# (Note: In the repository, this function is used by calculate_similarity_latents above)\n\n# 3) Integration into training loop (example form in trainer.calculate_model_loss)\n\n# The following illustrative function shows the integration point where the augmented\n# samples are created for different augmentation strategies. The actual integration\n# occurs in trainer.calculate_model_loss which dispatches to these helpers depending on flags.\n\n# experimental_info\n# The implementation directly corresponds to a phase-aware mixup pipeline:\n# - Magnitude interpolation is done in the frequency domain by mixing the magnitudes |FFT|\n#   and keeping the phase, and then converting back to time domain via inverse FFT.\n# - Phase is re-aligned by computing a phase-difference-based offset (phase_mix/phase_mix_2),\n#   which aligns directions between paired samples to avoid destructive interference.\n# - The interpolation coefficients (λ_A for magnitude, λ_P for phase) are computed by sampling\n#   a latent-space proximity measure derived from a β-VAE: calculate_similarity_latents(...).\n# - An aggressive mixing strategy is used for near neighbors (high similarity) and milder for distant ones.\n# - The approach is designed to be plugged into a SimCLR-like contrastive learning setup as\n#   part of the augmentation pipeline (see trainer.calculate_model_loss and calculate_similarity_latents).\n\n# Experimental settings visible in code (highlights):\n# - β-VAE latent distribution is used to compute a similarity matrix over samples; this drives\n#   the sampling of mixing coefficients in mixing_coefficient_set_for_each.\n# - The code supports both a full latent-guided mix (gen_new_aug_2_sup) and a simpler ablation variant\n#   that uses random coefficients (gen_new_aug_3_ablation_sup).\n# - The mixing is performed in the frequency domain by FFT, and the transformed samples are reconstructed\n#   via inverse FFT (irfft).\n# - This module is used within a broader SSL training loop (e.g., SimCLR-like framework) where\n#   augmented views are created, encoded, and contrasted.\n",
        "experimental_info": "Summary of experimental settings implied by the repository for the phase-aware latent-guided mixup method: \n- The method introduces a phase-aware mixup that operates in the frequency domain by disentangling magnitude and phase of FFT components. Magnitude is interpolated with a linear mix; phase is adjusted using a phase difference based offset to align directions, avoiding destructive interference. \n- The mixing coefficients for magnitude and phase (λ_A and λ_P) are not fixed; they are sampled based latent-space proximity computed from a β-VAE. The function calculate_similarity_latents(args, sample, DEVICE) derives a similarity matrix in latent space from a β-VAE encoder, which is then used to guide the interpolation strengths via functions like mixing_coefficient_set_for_each and mixing_coefficient_set_for_each_phase. \n- Aggressive mixing is applied for samples with high latent-space similarity, while milder mixing is used for more distant samples. There is also an ablation variant gen_new_aug_3_ablation_sup that uses random coefficients instead of latent similarity to study the contribution of latent guidance. \n- The augmented samples are integrated into a SimCLR-like training loop via trainer.calculate_model_loss and the augmentations provided by new_augmentations.py. The code path typically includes selecting augmentation combos (args.aug1 and args.aug2), generating augmented samples (gen_aug, gen_new_aug_2_sup, or gen_new_aug_3_ablation_sup), and computing the contrastive loss with the augmented views. \n- The experiments imply unsupervised pretraining with a β-VAE latent space, followed by linear evaluation on labeled data, consistent with a typical SSL workflow. \n- Reported datasets and metrics in the paper are reflected by the codebase using a variety of time-series benchmarks (e.g., UCI HAR, HHAR, SHAR, ECG, etc.), with batch sizes and epochs configured in the training scripts; the exact seeds and splits align with the paper's described experimental protocol (3 random seeds noted in the paper)."
      }
    },
    {
      "title": "Parametric Augmentation for Time Series Contrastive Learning",
      "full_text": "Published as a conference paper at ICLR 2024 PARAMETRIC AUGMENTATION FOR TIME SERIES CON- TRASTIVE LEARNING Xu Zheng1, Tianchun Wang2, Wei Cheng3, Aitian Ma1, Haifeng Chen3, Mo Sha1, Dongsheng Luo1\u0000 1School of Computing and Information Sciences, Florida International University, US 2College Information Sciences and Technology, The Pennsylvania State University, US 3NEC Laboratories America, US {xzhen019,aima,msha,dluo}@fiu.edu tkw5356@psu.edu, {weicheng,haifeng}@nec-labs.com ABSTRACT Modern techniques like contrastive learning have been effectively used in many areas, including computer vision, natural language processing, and graph-structured data. Creating positive examples that assist the model in learning robust and discriminative representations is a crucial stage in contrastive learning approaches. Usually, preset human intuition directs the selection of relevant data augmentations. Due to patterns that are easily recognized by humans, this rule of thumb works well in the vision and language domains. However, it is impractical to visually inspect the temporal structures in time series. The diversity of time series augmentations at both the dataset and instance levels makes it difficult to choose meaningful augmentations on the fly. In this study, we address this gap by analyzing time series data augmentation using information theory and summarizing the most commonly adopted augmentations in a unified format. We then propose a contrastive learning framework with parametric augmentation, AutoTCL, which can be adaptively employed to support time series representation learning. The proposed approach is encoder-agnostic, allowing it to be seamlessly integrated with different backbone encoders. Experiments on univariate forecasting tasks demonstrate the highly competitive results of our method, with an average 6.5% reduction in MSE and 4.7% in MAE over the leading baselines. In classification tasks, AutoTCL achieves a 1.2% increase in average accuracy. 1 I NTRODUCTION Time series data is complex and high-dimensional, making it more difficult to gather the label than images or languages. This property hinders the deployment of powerful deep learning methods, which typically require a large amount of labeled data for training(Eldele et al., 2021). Self-supervised learning is a promising solution due to its capacity to learn from unlabelled data. Self-supervised learning methods learn a fixed-dimension embedding of the time series data that preserves its inherent features with better transferability and generalization capacity. A representative framework, contrastive learning, has been successful in representation learning for various types of data including vision, language, and graphs(Chen et al., 2020; Xie et al., 2020; You et al., 2020). These methods train an encoder to map instances to an embedding space where similar instances (positive pairs) are easily distinguished from dissimilar ones (negative pairs). As a result, model predictions are unaffected by minor noise introduced into the inputs or hidden states. As a key component, data augmentation such as jittering, scaling, permutation, and subsequence extraction (Fan et al., 2020; Wen et al., 2021), is usually adopted to produce positive pairs. Recently, some efforts have been made to develop contrastive learning methods for time series data (Eldele et al., 2021; Franceschi et al., 2019; Fan et al., 2020; Tonekaboni et al., 2021). However, due to the diversity and variability of real-world time series data, it is challenging to apply a general \u0000 Corresponding author. 1 arXiv:2402.10434v1  [cs.LG]  16 Feb 2024Published as a conference paper at ICLR 2024 augmentation technique to all datasets. As a result, current approaches to contrastive learning for time series data frequently need particular data augmentation techniques that are guided by domain knowledge and necessitate trial and error to identify the most appropriate augmentations. Attempts have recently been made to study the theory behind adaptive augmentation selection for contrastive learning (Tian et al., 2020; Suresh et al., 2021; Xu et al., 2021). Good augmentations, according to InfoMin (Tian et al., 2020), produce label-preserving views with less shared information. They discover the best perspectives through adversarial training in unsupervised or self-supervised environments by adding an invertible flow-based generative model. The InfoMin principle performs well in the vision area and has been successfully applied to graph-structured data (Xu et al., 2021; Suresh et al., 2021; Yin et al., 2022; You et al., 2022). However, in a self-supervised environment, most existing studies soften the label-preserving property and place a more significant emphasis on enhancing diversity by reducing the exchange of information between different views. They frequently use stronger transformers as augmentations and undermine the semantics, which is inapplicable to time series data. To accommodate various augmentation tactics for time series contrastive learning, we investigate the data augmentation for time series from the information theory perspective and provide a theoretically sound definition of good augmentations based on input factorization. We further present a contrastive learning framework with parametric augmentation, AutoTCL, to adaptively augment data for con- trastive time series learning based on the proposed factorization technique, which can prevent ad-hoc decisions or laborious trial-and-error tuning. Specifically, we utilize a parametric neural network to learn to factorize an instance into two parts: the informative part and the task-irrelevant part. The informative component is then applied to a lossless transform function which keeps the instance’s semantics. The adaptive transformation produces a prior mask for the input instance to generate workable positive samples. We demonstrate how the most effective time series data augmentation methods can be viewed as specialized forms of the suggested mask-based transformation. By includ- ing another random variable with adequate variance, the diversity of the augmented view is further increased. In order to learn representations through contrast, augmented pairs are then fed into a time series encoder along with randomly chosen negative pairings. Parameters in the factorization and transform functions are optimized in tandem with contrastive learning loss. Our main contributions are summarized as follows. • We introduce a novel factorization-based framework to guide data augmentations for contrastive self-supervised learning without prefabricated knowledge. • To automatically learn workable augmentations for time series data, we provide a straightforward yet effective instantiation that can handle a variety of frequently applied augmentations. • With comprehensive experimental studies, we empirically verify the advantage of the proposed method on benchmark time series forecasting datasets. We achieve highly competitive performances with a 6.5% reduction in MSE, 4.7% in MAE on univariate forecasting, a 2.9% reduction in MSE, and 1.2% in MAE on multivariate forecasting. In classification tasks, our method achieves a 1.2% increase in average accuracy. 2 R ELATED WORK Contrastive learning for time series. Contrastive learning has been widely used in representation learning, achieving superior results across various domains (Chen et al., 2020; Xie et al., 2020; You et al., 2020). Recently, there have been efforts to apply contrastive learning to the time series domain (Khaertdinov et al., 2021; Oord et al., 2018; Franceschi et al., 2019; Fan et al., 2020; Eldele et al., 2021; Tonekaboni et al., 2021; Yue et al., 2022; Yang & Hong, 2022). In (Franceschi et al., 2019), Franceschi et al. utilize subsequences to generate positive and negative pairs. TNC uses a debiased contrastive objective to make sure that in the representation space, signals from the local neighborhood are distinct from those that are not neighbors (Tonekaboni et al., 2021). TS2Vec uses hierarchical contrastive learning to acquire a representation for each time stamp (Yue et al., 2022). TF-C utilizes the distance between time and frequency components as the self-supervised signal for representation learning. Each component is independently optimized by contrastive estimation (Zhang et al., 2022). In (Nonnenmacher et al., 2022), the authors introduce an approach that incorporates expert knowledge into time-series representation learning using expert features, surpassing existing methods in unsupervised and semi-supervised learning on real-world datasets. CLUDA (Ozyurt et al., 2Published as a conference paper at ICLR 2024 2023), a novel framework for unsupervised domain adaptation of time series data, utilizes contrastive learning to learn contextual representations that preserve label information, achieving state-of-the-art performance in time series unsupervised domain adaptation. Adaptive data augmentation. Data augmentation is a crucial aspect of contrastive learning. Previous studies have shown that the choice of optimal augmentation methods depends on the specific task and dataset being used (Chen et al., 2020; Fan et al., 2020). Some studies have explored the adaptive selection of augmentation methods in the visual domain Tamkin et al. (2021); Cubuk et al. (2019); Hataya et al. (2020); Li et al. (2020); Tian et al. (2020); Rommel et al. (2022); Aboussalah et al. (2023); Ho et al. (2019). For example, AutoAugment (Cubuk et al., 2019) uses a reinforcement learning method to search for the best combination of policies. Later, CADDA investigates a gradient- based class-wise method to support larger search spaces for EGG signals (Rommel et al., 2022). DACL adopts a domain-agnostic approach that does not rely on domain-specific data augmentation techniques (Verma et al., 2021). MetAug (Li et al., 2022) and Hallucinator (Wu et al., 2023) aim to generate augmentations in the latent space. In the contrastive learning frameworks, the InfoMin theory is applied to guide the selection of good views for contrastive learning in the vision domain (Tian et al., 2020), it further proposes a flow-based generative model to transfer images from natural color spaces into novel color spaces for data augmentation. However, given the complexity of time series data, directly applying the InfoMin framework may not be suitable. Different from previous works, our focus is on the time series domain and we propose an end-to-end differentiable method to automatically learn the optimal augmentations for each time series instance. Time series forecasting. Forecasting is an essential component of time series analysis, and various deep learning architectures have been employed in the literature, such as MLP (Ekambaram et al., 2023), Recurrent Neural Networks (RNNs) (Salinas et al., 2020; Oreshkin et al., 2020), Convolutional Neural Networks (CNNs) (Bai et al., 2018; Zhang et al., 2023b), Transformers (Li et al., 2019; Zhou et al., 2021; Lin et al., 2023; Yu et al., 2023; Nie et al., 2023; Zhang et al., 2023a), and Graph Neural Networks (GNNs) for this task (Cao et al., 2020). In contrast to these works, the aim of our research is to learn general representations for time series data that can be applied not only to forecasting but also to other tasks, such as classification. Additionally, the proposed framework is designed to be compatible with multiple types of architectures as encoders. 3 M ETHODOLOGY In this section, we first describe the notations used in this paper. Then, we try to answer the following research questions. (1) What are the good views for contrastive learning in the self-supervised setting? (2) How to obtain good views for each time series instance for contrastive learning? 3.1 N OTATIONS We use a T ×F matrix to represent a time series instance x, where T is the length of its sequence and F is the dimension of features. With F >1, x is a multivariate time series instance. Otherwise, with F = 1, x is a single variate instance. Self-supervised contrastive learning aims to learn an encoder f that maps x from RT×F to a vector space RD, where D is the dimension of embedding vectors. In the paper, to distinguish random variables and instances, we use the Sans-serif style lowercase letters, such as x, to represent random time series variables, and italic lowercase letters, such as x, for real instances. Important notations are summarized in Appendix. 3.2 W HAT MAKES GOOD VIEWS FOR CONTRASTIVE SELF -SUPERVISED LEARNING ? In the literature, a well-accepted intuitive principle for view generation is that good views preserve the semantics and provide sufficient variances (Tian et al., 2020; Yin et al., 2022; Suresh et al., 2021). In the supervised setting, where training labels are available, the semantics of an instance is usually approximated with the label. On the other hand, semantics-preserving is much less explored in the more popular self-supervised learning. Moreover, while the semantics of images and natural language sentences can be manually verified, the underlying semantics of time series data are not 3Published as a conference paper at ICLR 2024 easily recognizable to humans. This makes it challenging, if not impossible, to apply strong yet faithful data augmentations to such data. To avoid the degenerate solutions caused by dismissing the semantics-preserving, InfoMin utilizes an invertible flow-based function, denoted by g, to generate a view v for an input x (Tian et al., 2020). Such that x can be restored by x = g−1(v). However, from the information theory perspective, invertible functions fail to include extra variance to the original variable. Formally, we have the following property. Property 1. If view v is generated fromx with an invertible function v = g(x). Then H(v) = H(x) = MI (x; v), where H(x), H(v) are entropy of variables x and v, respectively; MI (v; x) is mutual information between v and x. The detailed proof can be found in the Appendix. This property shows that the entropy of the augmented view, H(v), is no larger than that of original data, H(x), indicating that the existing data augmentation methods don’t bring new information for input instances, which limits their expressive power for time series contrastive learning. To address the challenge and facilitate powerful self- supervised learning in the time series domain, we propose a novel factorized augmentation technique. Specifically, given an instance x, we assume that x can be factorized into two parts, informative x∗ and task-irreverent part ∆x. Formally, x = x∗ + ∆x. (1) As the informative part, x∗ encodes the semantics of the original x. Motivated by the intuitive principle, we formally define good views for contrastive learning as follows. Definition 1 (Good View). Given a random variable x with its semantics x∗, a good view v for contrastive learning can be achieved by v = η(g(x∗), ∆v), where g is an inverible function, ∆v is a task-irrelevant noise, satisfying H(∆v) ≥ H(∆x), and η is an augmentation function that satisfies that g(x∗) → v is a one-to-many mapping. Intuitively, a good view, based on our definition, maintains the useful information in the original variable and at the same time, includes a larger variance to boost the robustness of encoder training. We theoretically show that the defined good view has the following properties. Property 2 (Task Agnostic Label Preserving). If a variable v is a good view of x, and the downstream task label y (although not visible to training) is independent to noise in x, the mutual information between v and y is equivalent to that between raw input x and y, i.e., MI(v; y) = MI(x; y). Property 3 (Containing More Information). A good view v contains more information comparing to the raw input x, i.e., H(v) ≥ H(x). Detailed proofs are given in the appendix. These properties show that in the self-supervised setting, adopting a good view for contrastive learning theoretically guarantees that we will not decrease the fidelity, regardless of the downstream tasks. Simultaneously, the good view is flexible to the choice of ∆v, meaning that strong augmentations may be utilized to incorporate enough diversity for training. 3.3 H OW TO ACHIEVE GOOD VIEWS ? The theoretical analysis suggests a factorized augmentation to preserve task-agnostic labels and improve the diversity of views. In this part, we introduce a practical instantiation to obtain good views based on parametric augmentations as demonstrated in Fig. 1. First, a factorization function h : RT×F → {0, 1}T×1 is introduced to discover where are the informative parts in input. Formally1, h = h(x), x ∗ = h ⊙ x, ∆x = x − x∗, (2) where h is the factorization mask, x∗ is the informative component, and ∆x is the noise component. ⊙ is a generalized Hadamard product operation performed between two vectors(matrices). When the inputs both are vectors, denoted as v and m, the expression v ⊙ m refers to the element-wise product. In the case where the first operand is a vector v ∈ RN and the second operand is a matrix M ∈ RN×M , the procedure involves duplicating v to form a matrix V ∈ RN×M and then applying the standard Hadamard product to V and M. If both inputs are matrices of the same 1A more formal notation is h(x) as h is dependent on x. In this section, for ease of notation, we use h to denote h(x) in contexts where x is fixed or evident from the surrounding discussion. 4Published as a conference paper at ICLR 2024 Figure 1: The framework of our AutoTCL. The augmentation network extracts the informative part from the original instance and losslessly transforms it to v∗. The encoder network is optimized with the contrastive objective. shape, the symbol ⊙ signifies the standard Hadamard product. For the invertible transformation function applied on x∗, we present a mask-based instantiation. More sophisticated mechanisms, such as normalizing flows (Kobyzev et al., 2020; Wang et al., 2023), can also be used as plug-and-play components. Specifically, we introduce a non-zero mask g ∈ RT ̸=0 to form the transformation, such that v∗ = g ⊙ x∗ . It is easy to show that such a non-zero mask transformation is lossless as the original x∗ can be restored by x∗ = 1 g ⊙ v∗. Due to the varied nature of time series instances, which might differ even within a dataset, it is impractical to have a universal mask that applies to all instances. For instance, the cutout and jitter transform might not work well for the same time series data. To ensure each instance has a suitable transform adaptively, a parametric mask generator, denoted by g : RT×F → RT ̸=0, is proposed to generate the non-zero mask by learning for lossless transformation through a data-driven approach. Formally, g = g(x) . Then a good view v for contrastive learning can be represented as follows by integrating the factorization function, the mask generator, and introducing random noise for perturbation (∆v). v = η(v∗, ∆v) = η(g ⊙ x∗, ∆v) = η(g(x) ⊙ h(x) ⊙ x, ∆v). (3) Relationship to existing augmentations for time series. Various types of data augmentation techniques have been applied to enhance the performance of deep learning on time series data (Wen et al., 2021; Yue et al., 2022; Fan et al., 2020), including time domain, frequency domain, and their hybrids. Our instantiation can be considered a general augmentation framework in the time domain. Most existing augmentation operations in this category, such as cropping, flipping, scaling, and jittering can be unified in our framework. For example, by setting η(v∗, ∆v) = v∗, cropping, which deletes a subsequence, can be achieved by letting g = 1 and the cropping time steps in h be 0; scaling, which multiplies the input time series by a scaling factor, either a constant or being sampled from a Gaussian distribution, can also be obtained by setting h = 1 and g being the scaling factor. Practical instantiation with augmentation neural network. According to the Universal Ap- proximation Theorem (Chapter 6 in (Goodfellow et al., 2016)), we implement g(x) and h(x) with neural networks, respectively. We first utilize the same input layer and a stacked dilated CNN module (Franceschi et al., 2019; Yue et al., 2022) for both g(x) and h(x), respectively. Then, we include two projector heads, a factorization head for h(x), and an transformation head for g(x). The architecture of the overall augmentation network is shown in Fig. 1. To ensure the binary output of the factorization function h(x), we introduce a stochastic mechanism following the factorization head. Specifically, we assume that each element in the output h, denoted by hi, is drawn from a Bernoulli distribution parameterized by πi, which is calculated by the factorization head. To enable efficient optimization with gradient-based methods, we approximate the discrete Bernoulli processes with hard binary concrete distributions (Louizos et al., 2018). Specifically, we first draw ˜hi from a binary concrete distribution with πi indicating the location (Maddison et al., 2017; Jang et al., 5Published as a conference paper at ICLR 2024 2017). Formally, ˜hi = σ((log ϵ − log(1 − ϵ) + log πi 1 − πi )/τ), (4) where ϵ ∼ Uniform(0, 1) is an independent variable, σ(·) is the sigmoid function, and τ is the temperature. The output of the binary concrete distribution is in the range of (0,1). To further facilitate the binary selection, we stretch ˜hi the range to (γ, ζ), with γ <0 and ζ >1. Then, the final masking element hi is obtained by clipping values to the range [0, 1]. Formally, hi = min(1, max(˜hi(ζ − γ) + γ, 0)). (5) η(v∗, ∆v) as random timestamp masking. To increase the variance of augmented views, inspired by Dropout (Srivastava et al., 2014) and TS2Vec (Yue et al., 2022), we implement the function η(v∗, ∆v) by randomly masking the hidden representation. Specifically, given a latent vector of a view v, after the first hidden layer, we randomly mask it along the time dimension with a binary vector. Each element is sampled independently from a Bernoulli distribution, Bern(0.5). 3.4 T RAINING ALGORITHM There are two parametric neural networks to be optimized in the proposed framework, i.e., the encoder and the augmentation networks. The augmentation network aims to generate good views with high diversities, and the encoder is trained with a contrastive learning objective. Our method is used as plug and play component and can be used in a wide range of contrastive learning frameworks, such as TS2Vec (Yue et al., 2022) and CoST (Woo et al., 2022). In this section, we first introduce a new objective to train the augmentation network followed by the alternating training of encoder and augmentation networks. Training the augmentation network with the principle of relevant information. Existing opti- mization techniques for the learnable augmentation and the encoder generally follow the principle of Information Bottleneck (IB) (Tishby et al., 2000), which aims to achieve sufficient and minimal representations (Tian et al., 2020; Yin et al., 2022; Suresh et al., 2021). However, IB relies on the class labels from the downstream task, making it unsuitable for self-supervised training where there are few or no labels. Instead of following previous works that adversarially train the encoder and augmentation networks, which may fail to preserve the semantics in time series data, we train the augmentation network based on the Principle of Relevant Information (PRI) (Principe, 2010). Unlike supervised IB which relies on another variable as well as their joint distributions, PRI only exploits the self-organization of a random variable, making it fully unsupervised. Specifically, with PRI training the augmentation network, we aim to achieve a reduced statistical representation v∗ by minimizing the following function. βH(v∗) + D(Px||Pv∗), (6) where β is the trade-off hyper-parameter, H(v∗) is the entropy of representation variable v∗, and D(Px||Pv∗) is the divergence between distributions of the original variablex and transformed variable v∗. The minimization of H(v∗) aims to reduce uncertainty and obtain statistical regularity in v∗ and the second term is for preserving the descriptive power of v∗ about x. Given an instancex, the transformed informative partv∗ is obtained by applying a binary factorization mask h ∈ {0, 1}T on x and then an invertible transformation function, thus, minimizing the first term in Eq. (6), H(v∗), can be achieved by minimizing the number of non-zero elements in the factorization mask, i.e. ||h||0. According to the calculation of h in Eq. (5), we have ||h||0 = TX t=1 \u0012 1 − σ(τ log −γ(1 − πt) ζπt ) \u0013 . (7) To preserve the descriptive power ofv∗ about x, we follow existing works and estimate the second term D(Px||Pv∗) with the maximum mean discrepancy, MMD(Px, Pv∗) (Gretton et al., 2012). In practice, given a mini-batch of samples, X, and its associated view set V∗, we first compute the embeddings by passing all instances from X through the function f. The same procedure applies to V∗. Then, the loss function to train the augmentation network is shown as follows, LPRI = 1 |X| X x∈X β||h(x)||0 + ∥ 1 |X| X x∈X f(x) − 1 |V∗| X v∗∈V∗ f(v∗)∥2. (8) 6Published as a conference paper at ICLR 2024 Regularization of temporal consistency.As shown in previous studies (Luo et al., 2023), informative signals tend to be continuous. Thus, we include regularization of temporal consistency when generating the factorization mask. Specifically, given a batch X, for each instance x ∈ X, we randomly select a time point a as the anchor. Then we randomly select a time point p from the left or right position of a to create a positive pair (a, p). Their mask values h(x) a and h(x) p should be similar, compared to another point n that is far away from a, whose mask value is denoted by h(x) n . Formally, we have the following triplet loss. Lt = 1 |X| X x∈X (|h(x) a − h(x) p | − |h(x) a − h(x) n |). (9) With a trade-off parameter λ, the final augmentation network loss could be formulated as: Laug = LPRI + λLt (10) Alternative Training. To train encoder and augmentation networks, we follow GAN (Goodfellow et al., 2020) to use an alternating training schedule that trains the encoder network M times and then trains the augmentation network one time. M is a hyper-parameter determined by grid search. 4 E XPERIMENTS We compare AutoTCLwith extensive baselines on both forecasting and classification tasks. We also conduct ablation studies to show insights into each component in AutoTCL. Detailed experimental setups, full experimental results, and extensive experiments are presented in the Appendix. 4.1 T IME SERIES FORECASTING Datasets and baselines. Six benchmark datasets, ETTh1, ETTh2, ETTm1, (Zhou et al., 2021), Electricity (Dua & Graff, 2017), Weather2, and Lora dataset are adopted for time series forecasting in both univariate and multivariate settings. Lora dataset is a new introduced real-world dataset that captures the wireless signal data using the LoRa devices 3. It contains 74 days of data with timestamps. The proposed AutoTCL model is compared to representative state-of-the-art methods such as TS2Vec (Yue et al., 2022), Informer (Zhou et al., 2021), StemGNN (Cao et al., 2020), TCN (Bai et al., 2018), LogTrans (Li et al., 2019), LSTnet (Lai et al., 2018), CoST (Woo et al., 2022), TNC(Tonekaboni et al., 2021), TS-TCC (Eldele et al., 2021), InfoTS (Luo et al., 2023) and N-BEATS (Oreshkin et al., 2020), with N-BEATS being exclusive to univariate forecasting and StemGNN to multivariate. Setup. We follow CoST (Woo et al., 2022) network architecture. A multi-layer dilated CNN module is used for the backbone and we remove the seasonal feature disentangler module. The Augmentation network has the same feature extract architecture and two projectors as shown in Fig. 1. In addition, the proposed AutoTCL is a general contrastive learning framework that can also be combined with more recent methods, such as BTSF (Yang & Hong, 2022), TF-C (Zhang et al., 2022), and LsST (Wang et al., 2022) to further improve accuracy performances. We leave this as our future work. Time series forecasting aims to predict future time stamps, using the last Lx observations. Following the method presented in (Yue et al., 2022), a linear model regularized with L2 norm penalty, is trained to make predictions. Specifically, After pretraining by contrastive learning, the encoder network will be frozen in the following fine-tuning. A linear model is used to map representations to results. The linear model is trained by Linear least squares with l2 regularization in the package sk-learn Pedregosa et al. (2011). We use the default setting during training. This part is kept the same for other competing methods for a fair comparison. In the univariate case, the model’s output has a dimension of Ly, while in the multivariate case, it has a dimension of Ly × F. The evaluation is based on standard regression metrics, Mean Squared Error (MSE), and Mean Absolute Error (MAE). To comprehensively evaluate the performances, we consider different prediction lengths,Ly. 2https://www.ncei.noaa.gov/data/local-climatological-data/ 3https://lora-alliance.org/ 7Published as a conference paper at ICLR 2024 Results. For each dataset, we calculate the average forecasting performances in both univariate and multivariate settings. The results are shown in Table 1 and Table 2, respectively. The detailed results of univariate and multivariate time series forecasting can be found in Table 10 and Table 11 in the Appendix. From these tables, we have several observations. First, in general, contrastive learning methods, including AutoTCL, TS2vec, CoST, and InfoTS, achieve better performances compared to traditional baselines, indicating the effectiveness of contrastive learning for learning time series representations. Second, the consistent improvement of our method over CoST indicates that universal data augmentations may not be the most informative for generating positive pairs in various datasets. Specifically, Compared to CoST, AutoTCL decreases both MAE and MSE in all datasets in the univariate setting. On average, AutoTCL decreases the average MSE by 6.5% and the average MAE by 4.8% in the univariate setting. This is because AutoTCL can adaptively learn the most suitable augmentations in a data-driven manner, preserving semantics and ensuring sufficient variance. Encoders trained with these informative augmentations lead to representations with higher quality. In the multivariate setting, AutoTCL outperforms CoST in 7 cases. On average, it decreases the average MSE by 2.9% and the average MAE by 1.2%. Table 1: Univariate time series forecasting results. AutoTCL TS2Vec Informer LogTrans N-BEATS TCN CoST TNC TS-TCC InfoTS Dataset MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1 0.076 0.2070.110 0.252 0.186 0.347 0.196 0.365 0.218 0.375 0.263 0.431 0.0910.228 0.150 0.303 0.168 0.316 0.0910.227ETTh2 0.1580.2990.170 0.321 0.204 0.358 0.217 0.391 0.326 0.442 0.219 0.362 0.161 0.3070.168 0.322 0.298 0.4280.149 0.299ETTm1 0.046 0.1540.069 0.186 0.241 0.382 0.270 0.416 0.162 0.326 0.200 0.349 0.054 0.164 0.069 0.191 0.158 0.299 0.0500.157Elec. 0.366 0.3450.393 0.370 0.464 0.388 0.744 0.528 0.727 0.482 0.525 0.423 0.375 0.353 0.378 0.359 0.511 0.603 0.3690.348WTH0.160 0.2870.181 0.308 0.243 0.370 0.280 0.411 0.256 0.374 0.1660.2910.183 0.307 0.175 0.303 0.302 0.442 0.176 0.304Lora 0.177 0.2730.356 0.385 1.574 0.999 0.656 0.550 0.311 0.349 1.160 0.927 0.1860.2820.620 0.565 0.490 0.591 0.333 0.325 Avg. 0.157 0.2580.207 0.301 0.486 0.477 0.382 0.441 0.320 0.388 0.419 0.465 0.1680.2710.256 0.340 0.315 0.441 0.188 0.274 Table 2: Multivariate time series forecasting results. AutoTCL TS2Vec Informer LogTrans StemGNN TCN CoST TNC TS-TCC InfoTS Dataset MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1 0.6560.5900.788 0.646 0.907 0.739 1.043 0.890 0.738 0.632 1.021 0.8160.650 0.5850.904 0.702 0.748 0.635 0.784 1.622ETTh2 1.191 0.8151.566 0.937 2.371 1.199 2.898 1.356 1.940 1.077 2.574 1.265 1.2830.8511.869 1.053 2.120 1.109 1.474 0.914ETTm1 0.4090.4410.628 0.553 0.749 0.640 0.965 0.914 0.729 0.626 0.818 0.8490.409 0.4390.740 0.599 0.612 0.564 0.568 0.521Elec. 0.1750.2720.319 0.397 0.495 0.488 0.351 0.412 0.501 0.489 0.332 0.4040.165 0.2680.387 0.446 0.511 0.602 0.289 0.376WTH 0.4230.4570.451 0.474 0.574 0.552 0.645 0.6170.3530.593 0.440 0.461 0.430 0.4640.441 0.466 0.483 0.535 0.455 0.472Lora 0.346 0.3720.356 0.384 0.743 0.586 0.766 0.5200.2580.492 1.013 0.814 0.350 0.378 0.590 0.518 0.490 0.591 0.3450.368 Avg. 0.545 0.4990.697 0.571 0.990 0.708 1.138 0.798 0.753 0.651 1.057 0.781 0.5610.5050.837 0.637 0.838 0.675 0.665 0.556 4.2 T IME SERIES CLASSIFICATION Datasets and baselines. For the classification task, we evaluate our method on the UEA dataset (Dau et al., 2019), which contains 30 multivariate time series datasets. We compare our method with 8 state-of-the-art baselines, including TS2Vec (Yue et al., 2022), T-Loss (Franceschi et al., 2019), TNC (Tonekaboni et al., 2021), TS-TCC (Eldele et al., 2021), TST (Zerveas et al., 2021), DTW (Chen et al., 2013), TF-C (Zhang et al., 2022) and InfoTS (Luo et al., 2023). Setup. We use TS2Vec (Yue et al., 2022) network architecture. In the training stage, we use the same strategy as the forecasting tasks which could be found in Appendix. We follow the previous setting (Yue et al., 2022) that the evaluation is conducted in a standard supervised manner. A radial basis function kernel SVM classifier is trained on the training set and then makes predictions on test data. We report two metrics in the results, accuracy(ACC) and rank(RANK). Results. The results on the 30 UEA datasets are summarized in Table 3. The detailed results can be found in Table 12 in the Appendix. Overall, AutoTCL substantially outperforms baselines with an average rank value 2.3. As shown in Table 12, our method achieves the best results in 16 out of 30 datasets. In addition, it improves the classification accuracy by1.6%, on average, over the second-best baseline, InfoTS. The comprehensive comparison indicates the effectiveness of the proposed method. 4.3 A BLATION STUDY AND MODEL ANALYSIS . In this set of experiments, we conduct ablation studies to investigate the effectiveness of each component in the proposed method. To present deep insights into automatic data augmentation and 8Published as a conference paper at ICLR 2024 Table 3: Classification result of the UEA dataset Dataset AutoTCL TS2Vec T-Loss TNC TS-TCC TST DTW TF-C InfoTS Avg. ACC 0.742 0.704 0.658 0.670 0.668 0.617 0.629 0.298 0.730 Avg. RANK 2.300 3.700 4.667 5.433 5.133 6.133 5.400 8.200 2.367 factorization, we compare AutoTCL with multiple groups of variants. (1)W/o h(x), W/o g(x), and W/o ∆v are ablation studies about the effectiveness of each part of AutoTCL. In our experiments, W/o h(x) means the whole input instance would be regarded as the informative part. W/o g(x) represents the transformation head g(x) would be replaced by all 1 vectors and no noise will be added in W/o ∆v setting. (2) Cutout and Jitter are two commonly adopted data augmentation techniques for time series contrastive learning. We replace the augmentation network in AutoTCL with these two static transformations as variants. (3) Adversarial training is routinely adopted in the literature to learn views for contrastive learning. For this variant, we adversarially train the augmentation network by minimizing the mutual information between views and original instances, approximated by the InfoNCE (Tian et al., 2020). (4), Random Aug. randomly select augmentation operations from Cutout and Jitter with different parameters. The parameter of cutout ranges from 0.3 to 0.8. The mean of Jitter is set to 0, and the standard deviation ranges from 0.3 to 1.0. We report the averaged performances in Table 4 and the full results are shown in Table 6 in Appendix. We have several observations in Table 4. First, by removing the factorization head, W/o h(x) increase the MSE by 10.19% and MAE by 4.65% respectively, verifying the effectiveness of the factorization. The comparison between AutoTCLand W/o g(x), indicates the importance of invertible view generation. Specifically, W/o g(x) increases the MSE by 37.6% and MAE by 9.3%; The difference between AutoTCLand W/o ∆v indicates the importance of diversity in data augmentation. Second, the comparison between W/o Aug and Cutout shows that universal and non-parametric augmentation techniques may harm the performances of time series contrastive learning. On average, Cutout performs even worse than W/o Aug. This observation is consistent with the conclusion drawn in TS2Vec (Yue et al., 2022). By adaptive learning suitable augmentations, our methods can consistently and significantly outperform these baselines. Third, with the augmentation network trained in an adversarial manner, the variant, Adversarial improves the performances, indicating the necessity of adaptive augmentation for time series data. However, overlooking the semantic preservation may generate trivial augmented views, hindering the performance of downstream contrastive learning. On the other hand, our method achieves the best performances in most cases, especially for forecasting long periods, which verifies the advantage of our training algorithm. The comparison between AutoTCL and Random Aug. further indicates the advantage of parametric augmentation. Table 4: Ablation studies and model analysis AutoTCL W/oh(x) W/og(x) W/o∆v W/o Aug Cutout Jitter Adversarial Random Aug. Dataset MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1 0.076 0.2070.0770.2080.078 0.209 0.086 0.219 0.095 0.231 0.088 0.221 0.086 0.219 0.089 0.224 0.112 0.254ETTh2 0.158 0.2990.168 0.3050.178 0.312 0.176 0.311 0.170 0.309 0.1600.306 0.173 0.317 0.187 0.319 0.168 0.321ETTm1 0.046 0.1540.052 0.161 0.0500.1590.051 0.163 0.053 0.162 0.053 0.164 0.056 0.170 0.052 0.163 0.065 0.187Elec. 0.3650.348 0.371 0.3490.3650.348 0.366 0.3470.368 0.354 0.3670.3450.3660.3440.365 0.3450.376 0.358WTH 0.160 0.2870.172 0.301 0.1660.295 0.164 0.2930.183 0.309 0.167 0.294 0.174 0.304 0.1660.294 0.184 0.310Lora 0.177 0.2730.237 0.309 0.489 0.385 0.304 0.361 0.711 0.412 0.783 0.442 0.2850.3460.445 0.373 0.191 0.299 Avg. 0.157 0.2580.1730.2700.216 0.282 0.185 0.280 0.260 0.294 0.266 0.394 0.184 0.281 0.212 0.284 0.176 0.286 5 C ONCLUSION AND FUTURE WORK We present a novel factorization-based augmentation framework for time series representation learning in the self-supervised setting. Theoretical analysis from the information theory perspective shows that the proposed framework is more flexible to persevere semantics and includes sufficient variances to augment views. On top of that, we provide a simple and effective instantiation and an efficient training algorithm. With time series forecasting as the downstream task, we compare the proposed method, AutoTCL, with representative methods and verify its effectiveness. In addition, AutoTCL exploits the informative part of time series data, which might help users better understand the time series data. In the future, we plan to investigate the usage of parametric augmentation in other contrastive learning frameworks and extend the technique to other domains. 9Published as a conference paper at ICLR 2024 ACKNOWLEDGMENTS This project was partially supported by NSF grants IIS-2331908 and CNS-2150010. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies. REFERENCES Amine Mohamed Aboussalah, Minjae Kwon, Raj G Patel, Cheng Chi, and Chi-Guhn Lee. Re- cursive time series data augmentation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=5lgD4vU-l24s. Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, et al. Spectral temporal graph neural network for multivariate time-series forecasting. Advances in neural information processing systems, 33:17766–17778, 2020. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, pp. 1597–1607, 2020. Yanping Chen, Bing Hu, Eamonn Keogh, and Gustavo EAPA Batista. Dtw-d: time series semi- supervised learning from a single example. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 383–391, 2013. Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In CVPR, pp. 113–123, 2019. Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn Keogh. The ucr time series archive. IEEE/CAA Journal of Automatica Sinica, 6(6):1293–1305, 2019. doi: 10.1109/JAS.2019.1911747. Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive. ics.uci.edu/ml. Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting. In Ambuj Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng Yan, Ravi Kumar, Fatma Ozcan, and Jieping Ye (eds.),Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023, pp. 459–469. ACM, 2023. doi: 10.1145/3580305.3599533. URL https://doi.org/10.1145/3580305.3599533. Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In Zhi-Hua Zhou (ed.), Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pp. 2352–2359. ijcai.org, 2021. doi: 10.24963/ijcai.2021/324. URL https://doi.org/10.24963/ijcai. 2021/324. Haoyi Fan, Fengbin Zhang, and Yue Gao. Self-supervised time series representation learning by inter-intra relational reasoning. arXiv preprint arXiv:2011.13548, 2020. Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation learning for multivariate time series. Advances in neural information processing systems, 32, 2019. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http: //www.deeplearningbook.org. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020. 10Published as a conference paper at ICLR 2024 Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723–773, 2012. Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Faster autoaugment: Learning augmentation strategies using backpropagation. In ECCV, pp. 1–16. Springer, 2020. Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation: Efficient learning of augmentation policy schedules. In International conference on machine learning, pp. 2731–2741. PMLR, 2019. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id=rkE3y85ee. Bulat Khaertdinov, Esam Ghaleb, and Stylianos Asteriadis. Contrastive self-supervised learning for sensor-based human activity recognition. In 2021 IEEE International Joint Conference on Biometrics (IJCB), pp. 1–8. IEEE, 2021. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Ivan Kobyzev, Simon JD Prince, and Marcus A Brubaker. Normalizing flows: An introduction and review of current methods. IEEE transactions on pattern analysis and machine intelligence, 43 (11):3964–3979, 2020. Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In SIGIR, pp. 95–104, 2018. Jiangmeng Li, Wenwen Qiang, Changwen Zheng, Bing Su, and Hui Xiong. Metaug: Contrastive learning via meta feature augmentation. In International Conference on Machine Learning, pp. 12964–12978. PMLR, 2022. Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In NeurIPS, pp. 5243–5253, 2019. Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M. Robertson, and Yongxin Yang. Differentiable automatic data augmentation. In Computer Vision – ECCV 2020: 16th Euro- pean Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXII, pp. 580–595, Berlin, Heidelberg, 2020. Springer-Verlag. ISBN 978-3-030-58541-9. doi: 10.1007/978-3-030-58542-6_ 35. URL https://doi.org/10.1007/978-3-030-58542-6_35 . Shengsheng Lin, Weiwei Lin, Wentai Wu, Songbo Wang, and Yongxiang Wang. Petformer: Long-term time series forecasting via placeholder-enhanced transformer. arXiv preprint arXiv:2308.04791, 2023. Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through l_0 regularization. In International Conference on Learning Representations, 2018. Dongsheng Luo, Wei Cheng, Yingheng Wang, Dongkuan Xu, Jingchao Ni, Wenchao Yu, Xuchao Zhang, Yanchi Liu, Yuncong Chen, Haifeng Chen, et al. Time series contrastive learning with information-aware augmentations. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 4534–4542, 2023. Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=S1jE5L5gl. Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Confer- ence on Learning Representations, 2023. URL https://openreview.net/forum?id= Jbdc0vTOcol. 11Published as a conference paper at ICLR 2024 Manuel T Nonnenmacher, Lukas Oldenburg, Ingo Steinwart, and David Reeb. Utilizing expert features for contrastive learning of time-series representations. In International Conference on Machine Learning, pp. 16969–16989. PMLR, 2022. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis ex- pansion analysis for interpretable time series forecasting. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=r1ecqn4YwB. Yilmazcan Ozyurt, Stefan Feuerriegel, and Ce Zhang. Contrastive learning for unsupervised domain adaptation of time series. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=xPkJYRsQGM. F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten- hofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. Jose C Principe. Information theoretic learning: Renyi’s entropy and kernel perspectives. Springer Science & Business Media, 2010. Cédric Rommel, Thomas Moreau, Joseph Paillard, and Alexandre Gramfort. Cadda: Class-wise automatic differentiable data augmentation for eeg signals. InICLR 2022-International Conference on Learning Representations, 2022. David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3): 1181–1191, 2020. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014. Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve graph contrastive learning. Advances in Neural Information Processing Systems, 34:15920–15933, 2021. Alex Tamkin, Mike Wu, and Noah Goodman. Viewmaker networks: Learning views for unsupervised representation learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=enoVQWLsfyL. Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? Advances in neural information processing systems, 33:6827–6839, 2020. Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000. Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. Unsupervised representation learning for time series with temporal neighborhood coding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=8qDwejCuCN. Vikas Verma, Thang Luong, Kenji Kawaguchi, Hieu Pham, and Quoc Le. Towards domain-agnostic contrastive learning. In International Conference on Machine Learning, pp. 10530–10541. PMLR, 2021. Tianchun Wang, Farzaneh Mirzazadeh, Xiang Zhang, and Jie Chen. Gc-flow: A graph-based flow network for effective clustering. arXiv preprint arXiv:2305.17284, 2023. Zhiyuan Wang, Xovee Xu, Goce Trajcevski, Weifeng Zhang, Ting Zhong, and Fan Zhou. Learning latent seasonal-trend representations for time series forecasting. In Advances in Neural Information Processing Systems, 2022. 12Published as a conference paper at ICLR 2024 Qingsong Wen, Liang Sun, Xiaomin Song, Jingkun Gao, Xue Wang, and Huan Xu. Time series data augmentation for deep learning: A survey. In AAAI, 2021. Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=PilZY3omXV2. Jing Wu, Jennifer Hobbs, and Naira Hovakimyan. Hallucination improves the performance of unsupervised visual representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16132–16143, 2023. Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. Advances in neural information processing systems, 33:6256–6268, 2020. Dongkuan Xu, Wei Cheng, Dongsheng Luo, Haifeng Chen, and Xiang Zhang. Infogcl: Information- aware graph contrastive learning. Advances in Neural Information Processing Systems , 34: 30414–30425, 2021. Ling Yang and Shenda Hong. Unsupervised time-series representation learning with iterative bilinear temporal-spectral fusion. In International Conference on Machine Learning, pp. 25038–25054. PMLR, 2022. Yihang Yin, Qingzhong Wang, Siyu Huang, Haoyi Xiong, and Xiang Zhang. Autogcl: Automated graph contrastive learning via learnable view generators. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 8892–8900, 2022. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. In NeurIPS, pp. 5812–5823, 2020. Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. Bringing your own view: Graph contrastive learning without prefabricated data augmentations. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, pp. 1300–1309, 2022. Chengqing Yu, Fei Wang, Zezhi Shao, Tao Sun, Lin Wu, and Yongjun Xu. Dsformer: A dou- ble sampling transformer for multivariate time series long-term prediction. arXiv preprint arXiv:2308.03274, 2023. Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. Ts2vec: Towards universal representation of time series. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 8980–8987, 2022. George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A transformer-based framework for multivariate time series representation learning. In SIGKDD, pp. 2114–2124, 2021. Wenrui Zhang, Ling Yang, Shijia Geng, and Shenda Hong. Self-supervised time series representation learning via cross reconstruction transformer.IEEE Transactions on Neural Networks and Learning Systems, 2023a. Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised contrastive pre-training for time series via time-frequency consistency. Advances in Neural Information Processing Systems, 35:3988–4003, 2022. Zhiqiang Zhang, Yuxuan Chen, Dandan Zhang, Yining Qian, and Hongbing Wang. Ctfnet: Long- sequence time-series forecasting based on convolution and time–frequency analysis. IEEE Trans- actions on Neural Networks and Learning Systems, 2023b. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In AAAI, 2021. 13Published as a conference paper at ICLR 2024 APPENDIX : PARAMETRIC AUGMENTATION FOR TIME SERIES CONTRASTIVE LEARNING A N OTATIONS Important notations are summarized in Table 5. Table 5: Notations and their meanings. Symbol Meaning x, x Time series x∗, x∗ Informative part of x ∆x, ∆x Task-irrelevant part ofx v, v Augmented view v∗, v∗ Informative part of v ∆v, ∆v Task-irrelevant noise y, y Downstream task label T Length of time series F Number of features of time series D Dimensions of hidden representations f Encoder function g Transformation function h Factorization function η Augmentation function H(·) Entropy MI (·; ·) Mutual information h Factorization mask g Transformation mask hi i-th element of h ˜hi Intermediate result from concrete distribution πi Parameter in Bern. distribution ϵ Random variable from uniform distribution τ Temperature in Concrete distribution ζ, γ Hyper-parameters in hard concrete distribution X A batch/set of time series instances a, p, n Anchor, positive and negative time stamp β, λ Trade-off hyper-parameters M Hyper-parameter for training B D ETAILED PROOFS Property 1. If view v is generated fromx with an invertible function v = g(x). Then H(v) = H(x) = MI (x; v), where H(x), H(v) are entropy of variables x and v, respectively; MI (v; x) is the mutual information between v and x. Proof. Since g is an invertible function and v = g(x), we have an one-to-one mapping between variables v and x. Thus, v = g(x) for each pair of x and v. We have P[x = x] = P[v = v]. From the definition of Shannon entropy, we have H(x) = − X x p(x) logp(x) = − X x P[x = x] logP[x = x] = − X v P[v = v] logP[v = v] = − X x p(v) logp(v) = H(v). 14Published as a conference paper at ICLR 2024 From the definition of conditional entropy, we have H(x|v) = X v,x p(v, x) log p(v, x) p(v) , H(x|v) = X v,x p(v) log p(v) p(v) = 0. The above results in the mutual information between v and x, given by MI(v; x) = H(x) − H(x|v) = H(v). Property 2 (Task agnostic label preserving). If a variable v is a good view of x, and the downstream task label y (although not visible to training) is independent to noise in x, the mutual information between v and y is equivalent to that between raw input x and y, i.e., MI(v; y) = MI(x; y). Proof. From the definition of the good view, we have x = x∗ + ∆x v = η(g(x∗), ∆v). We first analyze the relationship between MI(x, y) and MI(x∗, y). MI(x, y) = H(y) − H(y|x) = H(y) + X x,y p(x, y) log p(x, y) p(x) = H(y) + X x∗,∆x,y p(x∗, ∆x, y) log p(x∗, ∆x, y) p(x∗, ∆x) = H(y) + X x∗,∆x,y p(x∗, ∆x, y) log p(∆x, y|x∗) p(∆x|x∗) . With the safe independence assumption, we have p(∆x, y|x∗) = p(∆x|x∗)p(y|x∗). Thus, we show that MI(x, y) = H(y) + X x∗,∆x,y p(x∗, ∆x, y) log p(x∗, y) p(x∗) = H(y) + X x∗,y p(x∗, y) log p(x∗, y) p(x∗) = H(y) − H(y|x∗) = MI(x∗, y). Letting v∗ = g(x∗), from the Property 1, we have MI(x∗, y) = MI(v∗, y). Since ∆v is a random noise and is independent to the label y, similarly, we have MI(v, y) = MI(η(v∗, ∆v), y) = MI((v∗, ∆v), y) = MI(v∗, y) Combining them together results in MI(v, y) = MI(x, y). 15Published as a conference paper at ICLR 2024 Property 3. (Containing more information). A good view v contains more information comparing to the raw input x, i.e., H(v) ≥ H(x). Proof. Since ∆v denotes the included random noise, we assume that its generation is independent of the augmented view v∗. Thus we have MI(∆v, v∗) = 0. (11) Further, with our decomposing model, we can rewrite the entropy of x as the joint entropy of x∗ and ∆x. Formally, we have H(x) = H(x∗, ∆x) = H(x∗) + H(∆x) − MI(∆x, x∗). Then H(x∗) = H(v∗) holds (Property 1). From the definition of the good view, we have H(∆v) ≥ H(∆x). Thus, we have H(x) = H(x∗) + H(∆x) − MI(∆x, x∗) ≤ H(v∗) + H(∆v) = H(v∗) + H(∆v) − MI(∆v, v∗) = H(∆v, v∗) = H(η(v∗, ∆v)) = H(v). Derivation of Eq. (7) As described in Section 3.4, the factorization mask h is generated with a hard concrete distribution. Thus, the number of non-zero entries in h can be reformulated with ||h||0 = X t (1 − Pht(0)), where Pht(0) is the cumulative distribution function (CDF) of ht (before clipping). We let S(·) be an affine function of the stretch process in Eq. (5), such that ht = S(˜ht) = ˜ht(ζ − γ) + γ, where γ ∈ (−∞, 0) and ζ ∈ (1, ∞). As derived in (Maddison et al., 2017), the density of ht is pht(ht) = ταth−τ−1 t (1 − ht)−τ−1 (αth−τ t + (1 − ht)−τ )2 , where αt = log πt 1−πt . The CDF of ht reads Pht(ht) = σ((log ht − log(1 − ht))τ − αt). Thus, the probability density function of ht is pht(ht) = p˜ht (S−1(ht)) \f\f\f\f ∂ ∂ht S−1(ht) \f\f\f\f = (ζ − γ)ταt(ht − γ)−τ−1(ζ − ht)−τ−1 (αt(ht − γ)−τ + (ζ − ht)−τ )2 . The CDF of ht is given by Pht(ht) = P˜ht (S−1(ht)) = σ((log(ht − γ) − log(ζ − ht))τ − αt). When setting ht = 0, we have the Pht(0) = σ(τ log −γ ζ − αt). 16Published as a conference paper at ICLR 2024 C I MPLEMENTATION DETAILS C.1 T RAINING THE ENCODER WITH LOCAL AND GLOBAL CONTRASTS . Similar to the augmentation network, our method can work with different architectures. We formulate the feature extraction encoder as f : RT×F → RD, where D is the dimensionality of output embeddings. Following existing work (Luo et al., 2023), we use both global and local contrastive losses. Global contrast aims to improve the inter-instance robustness for representation learning. Given a batch of time series instances X, for each instance x ∈ X, we generate an augmented view v. Such a pair of the original instance x and the corresponding view v is then used as a positive pair. Other pairs of instances and views are treated as negative pairs. Formally,(x, v′) is a negative pair, where v′ is an augmented view of x′ and x′ ̸= x. Following (Chen et al., 2020), we use the InfoNCE as the global-wise contrastive loss to train the encoder network. Formally, we have Lg = − 1 |X| X x∈X log exp(sim(zx, zv))P x′∈XB exp(sim(zx, zv′)), (12) where zx = f(x), zv = f(v), zx′ = f(x′), zv′ = f(v′) are representations of x, v, x′ and v′, respectively. Local contrast is designed to enhance the encoder network to capture the intra-instance relationship. Given an augmented viewv, we first segment it into a set of subsequencesS, where each subsequence s ∈ S has length L. Following (Tonekaboni et al., 2021), two close subsequences(s, p) are considered as a positive pair, and the ones with a large distance lead to a negative pair. Formally, the loss of local contrast is: Llx = − 1 |S| X s∈S log exp(sim(zs, zp)) exp(sim(zs, zp)) + P j∈ ¯Ns exp(sim(zs, zj)), (13) where ¯Ns is the set of negative pairs for a subsequence s. zs = f(s), zp = f(p), zn = f(n) are representations of s, pand n generated by function f, respectively. Considering all instances in a batch, we have Ll = 1 |X| X x∈X Llx. (14) With both local and global contrasts, we have our contrastive loss as follows. Lcon = Lg + αLl, (15) where α is the hyper-parameter to achieve the trade-off between global and local losses. C.2 T RAINING ALGORITHM In the training stage, AutoTCL optimizes the augmentation network and encoder network simul- taneously. Similar to GAN (Goodfellow et al., 2016), these networks were randomly initialized. Different from GAN, AutoTCL is less affected by the problem of gradient explosion and mode collapse, because our encoder network aims to embed the information part from different views rather than distinguish them. Although our argumentation network tries to reduce the distribution between original instances and arguments, AutoTCL augmentations preserve the information part by using a reversible mapping function, which alleviates the mode collapse problem. Our training algorithm is shown in Alg. 1 . D E XPERIMENTAL SETTINGS All experiments are conducted on a Linux machine with 8 NVIDIA A100 GPUs, each with 40GB of memory. The software environment is CUDA 11.6 and Driver Version 520.61.05. We used Python 3.9.13 and Pytorch 1.12.1 to construct our project. 17Published as a conference paper at ICLR 2024 Algorithm 1 AutoTCL training algorithm Require: augmentation network faug, encoder network fenc, epochs E, a hyperparameter M, epoch ← 0 while epoch < Edo for x in dataset do xa ← faug(x) zx ← fenc(x) za ← fenc(xa) if epoch%M == 0 then Compute loss with using Eq. (10) Update parameters in faug with backpropagation end if Compute loss with using Eq. (15) Update parameters in fenc with backpropagation end for epoch ← epoch + 1 end while D.1 B ASELINE SETTINGS In forecasting tasks, we conducted baseline methods on six benchmark datasets by following the experiment setting of TS2Vec(Yue et al., 2022) for most baseline methods, such as Informer (Zhou et al., 2021), (Tonekaboni et al., 2021), StemGNN (Cao et al., 2020), TCN (Bai et al., 2018), N-BEATS (Oreshkin et al., 2020), etc. For TS2Vec(Yue et al., 2022), CoST (Woo et al., 2022), we followed its code default setting for Lora and Weather datasets. The representation dimension was 320 and the learning rate and batch size were 0.001 and 8. For InfoTS (Luo et al., 2023), We used the default setting to conduct experiments. As for TS-TCC (Eldele et al., 2021) in forecasting tasks, we used the Epilepsy config as the default config and modified the network model to make the input and output channels remain the same. Due to its pooling layers, the network would require 3 times the lengths of inputs of other baselines which is unfair for forecasting tasks. In the experiments, we used another interpolate layer to make the length of input data and prediction data the same. In classification tasks, similar to the forecasting task, we followed the experiment setting of TS2Vec(Yue et al., 2022). In TF-C (Zhang et al., 2022) classification experiments, we use its HAR config as the default setting. Similar to TS-TCC, we modifie the network so that the transformer encoder could fit the input length and the pre-train dataset is the same as the fine tune dataset. D.2 H YPERPARAMETERS In our experiments, we used grid search to obtain the best performance. We used the same strategy in forecasting and classification tasks that each dataset had its own group of hyperparameters. We provided all of the hyperparameters as well as their configurations in the following: • Optimizer: Two Adam optimizers (Kingma & Ba, 2014) were used for the augmentation network and feature extraction network with learning rate and other hyperparameters were setting with default decay rates setting to 0.001 and (0.9,0.999) respectively. • Encoder architecture: The depth of the multi-layer dilated CNN module and the hidden dimension were designed to be able to change, which were searched in {6, 7, 8, 9, 10} and {256, 128, 64, 32, 16, 8}. In training, we used a designed dropout rate to avoid overfitting, which was tuned in [0.01, 1]. • Augmentation architecture: Same as encoder, the depth of multi-layer dilated CNN module and hidden dimension are hyperparameters, searched in {1, 2, 3, 4, 5} and {256, 128, 64, 32, 16, 8} and as mention in equation Eq. 5, ζ is another hyperparameter, tuned in [0.0005, 0.5]. • Trade-off hyperparameters: β in Eq. (8), and λ in Eq. (10) are tuned in [0, 0.3]. • Alternating training hyperparameters: M in Sec. (3.4) is tuned in {1, 2, 4, 8, 16, 32}. 18Published as a conference paper at ICLR 2024 D.3 E XTRA EXPERIMENTS D.3.1 P ERFORMANCE WITH DIFFERENT BACKBONES As a general framework for time series contrastive learning, AutoTCL can be used as a plug-and-play component to boost performance. To further verify the generalization capacity of AutoTCL. Table 6 shows the full results of ablation studies and model analysis with CoST as the backbone. In addition, we adopt Ts2vec (Yue et al., 2022) as the backbone and show the results in Table 7. We can draw similar conclusions that by adaptively selecting the optimal augmentations with the principle of relevant information, AutoTCL can outperform the vanilla TS2vec and other baselines. Table 6: Ablation studies using CoST backbone AutoTCL W/oh(x) W/og(x) W/o∆V W/o Aug Cutout Jitter Adversarial Random Aug. DatasetLy MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1 24 0.0370.1480.0370.1480.0370.1480.0380.1490.0370.1480.037 0.1470.0380.1470.039 0.149 0.056 0.17848 0.0540.1760.0540.1760.0540.177 0.055 0.180 0.055 0.1780.053 0.1750.0540.1760.056 0.180 0.076 0.2081680.078 0.2100.0790.2100.080 0.211 0.083 0.217 0.100 0.2370.078 0.2100.081 0.212 0.090 0.227 0.132 0.278336 0.0930.2310.0930.2310.094 0.232 0.096 0.234 0.108 0.2510.092 0.2300.095 0.233 0.106 0.250 0.137 0.2897200.120 0.2720.1210.2740.124 0.277 0.157 0.317 0.175 0.340 0.179 0.345 0.163 0.325 0.152 0.313 0.157 0.318 ETTh2 24 0.079 0.206 0.0770.204 0.0760.2050.079 0.209 0.078 0.2080.076 0.2040.092 0.217 0.078 0.207 0.090 0.22948 0.117 0.2550.124 0.258 0.1130.256 0.118 0.259 0.127 0.2590.110 0.2530.135 0.272 0.124 0.265 0.125 0.2721680.176 0.3190.1910.3290.212 0.346 0.240 0.358 0.220 0.347 0.1910.340 0.207 0.356 0.227 0.361 0.175 0.3313360.193 0.3440.201 0.350 0.243 0.371 0.204 0.3490.2000.357 0.201 0.355 0.212 0.366 0.253 0.375 0.207 0.368720 0.2230.3730.246 0.384 0.246 0.380 0.238 0.379 0.227 0.3740.2200.3760.2170.3740.251 0.385 0.245 0.405 ETTm1 24 0.016 0.091 0.0140.0870.015 0.090 0.015 0.0890.013 0.0850.017 0.092 0.0150.091 0.0150.092 0.020 0.10748 0.026 0.120 0.0250.119 0.0250.1170.027 0.1220.024 0.1160.028 0.123 0.027 0.124 0.028 0.125 0.032 0.13896 0.0360.1450.0370.146 0.038 0.146 0.039 0.1500.036 0.1440.039 0.150 0.043 0.158 0.040 0.151 0.045 0.1652880.063 0.1910.074 0.205 0.0720.2040.0720.205 0.080 0.216 0.078 0.211 0.082 0.218 0.075 0.205 0.093 0.2386720.090 0.2250.108 0.250 0.0980.2390.104 0.248 0.114 0.248 0.104 0.246 0.112 0.260 0.100 0.240 0.134 0.285 Elec. 24 0.2400.266 0.244 0.266 0.2410.267 0.2420.2640.243 0.272 0.2410.264 0.240 0.2640.242 0.2650.251 0.27948 0.2850.294 0.291 0.295 0.2850.295 0.287 0.294 0.290 0.300 0.2860.291 0.2840.2920.287 0.294 0.298 0.3091680.3920.371 0.400 0.3710.3920.366 0.3940.367 0.398 0.372 0.3940.365 0.3950.3620.393 0.3640.413 0.383336 0.542 0.461 0.547 0.465 0.5410.464 0.542 0.461 0.5410.470 0.545 0.4600.5450.457 0.539 0.4570.541 0.460 WTH 24 0.093 0.211 0.098 0.220 0.0920.2090.091 0.2070.096 0.215 0.0920.210 0.093 0.212 0.0920.2090.100 0.22248 0.131 0.256 0.139 0.266 0.130 0.2550.127 0.2500.141 0.266 0.1290.2520.134 0.260 0.131 0.257 0.142 0.2661680.182 0.3110.194 0.324 0.185 0.3140.1840.316 0.208 0.336 0.186 0.315 0.195 0.327 0.185 0.315 0.207 0.3353360.195 0.3250.210 0.342 0.208 0.342 0.206 0.341 0.231 0.357 0.209 0.339 0.215 0.349 0.2030.3350.225 0.3557200.198 0.3300.218 0.353 0.217 0.353 0.2110.3490.240 0.369 0.220 0.352 0.231 0.370 0.218 0.352 0.244 0.371 Lora 24 0.052 0.1410.060 0.1520.138 0.219 0.128 0.213 0.078 0.171 0.067 0.170 0.158 0.223 0.0570.154 0.070 0.18548 0.080 0.1810.092 0.196 0.117 0.225 0.181 0.264 0.127 0.223 0.112 0.218 0.185 0.257 0.0840.1890.096 0.2181680.155 0.2630.246 0.317 0.1960.3080.232 0.334 0.481 0.393 0.676 0.433 0.311 0.359 0.235 0.323 0.169 0.2903360.229 0.3350.3020.3720.395 0.444 0.363 0.433 0.941 0.532 1.403 0.619 0.378 0.429 0.535 0.475 0.245 0.3537200.370 0.4450.483 0.509 1.60 0.729 0.617 0.561 1.926 0.739 1.655 0.771 0.3950.4611.315 0.722 0.375 0.450 Avg. 0.157 0.2580.1730.2700.216 0.282 0.185 0.280 0.260 0.294 0.266 0.394 0.184 0.281 0.212 0.284 0.176 0.286 D.3.2 V ISUALIZATION OF AUGMENTATION The intuitive understanding of Property 3 is that if the map between x and v is one-to-many, the generated view will contain more information compared to the raw input x. In other words, a good augmentation should preserve the underlying semantics that two differentx cannot map the samev. In order to further explore the effectiveness of AutoTCL in generating diverse and semantic-preserving views, we used T-SNE to visualize the embeddings of different augmented views in Figure 2. We chose an instance, denoted by x, from dataset ETTh1 and compare different augmentation methods, including Cutout, Jitter, and Adversarial. To avoid the special case, we reported 10 augmented views for AutoTCL. We also include another x′ instance as a reference. As shown in Figure 2, the instances augmented by AutoTCL include more diversity compared with Jitter and Cutout. Moreover, the augmentation generated by the Adversarial is closer to x′ or x, indicating that it fails to preserve the underlying semantics. D.3.3 V ISUALIZATION OF CONVERGENCE To show the convergence of our method, we plotted the curves of Eq.( 10) and Eq. (15) on different datasets. As shown in Figure 3, our method converged easily in both the argumentation network and the embedding network. In Figure 3(a) and 3(d), we observed that after the argumentation network converged to a certain level, the encoding network still benefited from that. In Figure 3(b), 3(c), and 3(e), they have the same patterns that the augmentation loss arrived the convergence level almost the 19Published as a conference paper at ICLR 2024 Table 7: Ablation studies using TS2Vec backbone AutoTCL W/oh(x) W/og(x) W/o∆V W/o Aug Cutout Jitter Adversarial Random Aug. DatasetLy MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1 24 0.039 0.146 0.0390.1480.047 0.165 0.047 0.1640.0390.149 0.043 0.155 0.0400.1500.041 0.151 0.047 0.16648 0.058 0.1800.0600.1850.075 0.212 0.073 0.205 0.063 0.190 0.063 0.187 0.063 0.190 0.062 0.186 0.069 0.2031680.106 0.2450.115 0.259 0.145 0.298 0.131 0.278 0.119 0.264 0.118 0.262 0.1110.2530.114 0.255 0.127 0.2803360.121 0.2660.139 0.289 0.159 0.317 0.148 0.303 0.141 0.291 0.133 0.285 0.1300.2770.132 0.280 0.140 0.3007200.154 0.3140.181 0.347 0.198 0.365 0.180 0.344 0.193 0.359 0.1670.3310.164 0.323 0.167 0.330 0.171 0.342 ETTh2 24 0.106 0.252 0.108 0.2500.095 0.2350.1040.2480.108 0.251 0.105 0.250 0.105 0.249 0.107 0.252 0.101 0.24648 0.1310.2840.134 0.2850.129 0.2790.136 0.289 0.140 0.290 0.133 0.287 0.135 0.287 0.137 0.288 0.129 0.2811680.182 0.3430.1850.3440.212 0.365 0.211 0.366 0.203 0.360 0.198 0.355 0.194 0.3530.195 0.353 0.185 0.3443360.190 0.3510.1910.3510.205 0.362 0.209 0.366 0.206 0.367 0.204 0.363 0.201 0.362 0.199 0.3600.201 0.362720 0.204 0.370 0.204 0.368 0.203 0.366 0.2000.3640.205 0.369 0.205 0.367 0.2000.364 0.1940.3590.234 0.329 ETTm1 24 0.014 0.0850.018 0.098 0.0150.0890.0140.0870.0140.0870.0150.0890.0140.0870.014 0.0850.021 0.10948 0.026 0.1170.0270.121 0.028 0.1230.0260.1200.0270.121 0.0270.121 0.0270.1210.026 0.1170.036 0.14496 0.038 0.1470.0390.1490.0390.1470.041 0.153 0.041 0.153 0.041 0.1530.038 0.147 0.038 0.1470.050 0.1712880.0810.2160.083 0.219 0.0820.2160.084 0.222 0.084 0.222 0.084 0.2220.081 0.215 0.0810.2160.106 0.2516720.119 0.2630.123 0.269 0.122 0.266 0.124 0.271 0.124 0.270 0.121 0.267 0.1200.2650.119 0.2630.176 0.329 Elec. 24 0.247 0.2690.249 0.271 0.2480.269 0.2470.2700.250 0.271 0.2480.2700.249 0.273 0.250 0.2700.251 0.27748 0.2970.3010.302 0.306 0.2970.3010.297 0.303 0.298 0.3020.2960.3020.2970.307 0.298 0.3020.393 0.3031680.4080.3800.413 0.3810.4080.3800.4100.3800.408 0.377 0.4080.383 0.4100.3840.408 0.3770.405 0.3723360.541 0.4680.553 0.4720.5410.4690.547 0.470 0.5420.471 0.545 0.470 0.470 0.547 0.5420.471 0.554 0.458 WTH 24 0.0930.212 0.096 0.214 0.0940.2100.096 0.214 0.0940.2110.099 0.215 0.096 0.213 0.096 0.213 0.104 0.22748 0.133 0.258 0.134 0.2590.130 0.2530.134 0.258 0.134 0.257 0.1320.256 0.135 0.259 0.133 0.2540.142 0.268168 0.1880.3160.192 0.3220.184 0.3130.192 0.322 0.197 0.324 0.189 0.317 0.192 0.321 0.193 0.322 0.195 0.3233360.201 0.3330.208 0.341 0.2020.3350.208 0.341 0.216 0.347 0.208 0.338 0.211 0.342 0.212 0.344 0.209 0.3407200.204 0.3390.210 0.3470.204 0.3390.210 0.347 0.225 0.357 0.2110.3430.220 0.353 0.217 0.352 0.211 0.344 Lora 24 0.053 0.1400.0610.1560.076 0.169 0.0610.1560.188 0.219 0.062 0.158 0.080 0.173 0.062 0.157 0.076 0.17748 0.082 0.1880.087 0.1930.136 0.229 0.0860.1930.296 0.285 0.086 0.192 0.155 0.238 0.088 0.195 0.115 0.2221680.161 0.2780.210 0.306 0.210 0.308 0.344 0.360 0.341 0.371 0.1880.2940.229 0.318 0.219 0.310 0.219 0.3213360.231 0.3470.454 0.431 0.2740.3690.299 0.391 0.400 0.420 0.369 0.407 0.281 0.373 0.488 0.438 0.323 0.401720 0.375 0.451 0.369 0.4470.400 0.473 0.370 0.428 0.484 0.482 0.3670.4460.443 0.4930.3640.454 0.465 0.512 Avg. 0.165 0.2710.179 0.280 0.178 0.284 0.180 0.283 0.199 0.291 0.1750.2790.176 0.284 0.179 0.279 0.185 0.292 (a) Samples a  (b) Samples b Figure 2: T-SNE visualization of different augmentation instances. In samples a and b, AutoTCL- generated samples are closer to the original instance x than other instances x′ with large variety same as the contrastive loss. While the situation was different in Figure 3(f), at the beginning the augmentation network benefited from encoding loss, then two losses converged gradually. D.3.4 P ARAMETER SENSITIVITY STUDIES In the proposed AutoTCL, we have three hyper-parameters, α in Eq. (15), β in Eq. (8), and γ in Eq. (10), to get the trade-off in training the augmentation network and the encoder network. In this part, we chose different values for these three variables in the range from 0.0001 to 0.3 and reported MSE and MAE scores in the ETTh1 dataset. The results of this part could be found in Figure 4. The sensitivity studies result of three hyper-parameters are shown in Figure 4. From this figure, some results could be observed that our method is able to achieve comparative performances with a wide range of choices for these three hyperparameters, indicating the robustness of our method. The β in 20Published as a conference paper at ICLR 2024 (a) ETTh1  (b) ETTh2 (c) ETTm1  (d) Elec. (e) WTH  (f) Lora Figure 3: The augmentation loss, Eq. (10) and contrastive loss, Eq. (15), in the training process 21Published as a conference paper at ICLR 2024 Eq. (8), and γ in Eq. (10) have the opposite effect as the weight goes up. Second, we observe that small values, such as 0.001, give good performances on ETTh1 datasets as well as others. 1E-4 3E-4 1E-3 3E-3 1E-2 3E-2 1E-1 3E-1 0.077 0.078 0.079 0.080 0.081MSE (a) MSE 1E-4 3E-4 1E-3 3E-3 1E-2 3E-2 1E-1 3E-1 0.208 0.209 0.210 0.211 0.212 0.213 0.214MAE (b) MAE Figure 4: Parameter sensitivity studies on ETTh1. D.3.5 C ASE STUDY To further explore the augmentation of AutoTCL, we have done the case study in this section. We selected four instances to show the effectiveness of our method in Table 9. As shown in Table 9, we used the CricketX dataset as input instances and got the informative part by using the augmentation network to get masks, the result of h(x). With the regularization loss help in Eq. (9), our method could have a continuous mask that makes the informative part more consistent. From the results, the informative parts detected by h() appear to retain the prominent peaks and troughs of the original sequence, which are typically considered significant by domain experts. In time series analysis, such prominent features often correspond to critical events or changes in the underlying system’s behavior. D.3.6 E XPERIMENTS ON OTHER DOMAIN DATASET In order to further verify the adaptability of our method, we conducted experiments on Traffic4 dataset. This dataset comprises hourly information sourced from the California Department of Transportation. 4https://pems.dot.ca.gov/ 22Published as a conference paper at ICLR 2024 This dataset delineates road occupancy rates as observed by various sensors deployed on the freeways in the San Francisco Bay area. Following the default setting, we adopt CoST as the backbone and conduct forecasting in both univariate and multivariate forecasting settings. We also include another SOTA method, TS2Vec as a comparison. The results are shown in Table 8. We observe that, in both univariate and multivariate forecasting settings, AutoTCL achieves the best results in all prediction lengths. On average, AutoTCL decreases MSE by 9.4%, MAE by 5.7% in the univariate forecasting setting and MSE by 5.0%, MAE by 9.3% in the multivariate forecasting setting comparing to the baseline. Table 8: Forecasting results on Traffic dataset. AutoTCL TS2Vec CoST Settings Ly MSE MAE MSE MAE MSE MAE Univariate 96 0.253 0.353 0.431 0.484 0.284 0.379 192 0.271 0.373 0.437 0.489 0.302 0.398 336 0.312 0.414 0.453 0.500 0.340 0.435 720 0.357 0.447 0.464 0.508 0.390 0.474 Avg. 0.298 0.397 0.446 0.495 0.329 0.421 Multivariate 96 0.715 0.396 1.038 0.574 0.759 0.442 192 0.722 0.396 1.042 0.588 0.757 0.434 336 0.730 0.396 1.064 0.594 0.765 0.435 720 0.746 0.403 1.085 0.604 0.784 0.444 Avg. 0.728 0.398 1.057 0.590 0.766 0.439 D.4 F ULL EXPERIMENTS Univariate forecasting. Full experiment results of univariate time series forecasting results can be found in Table 10. In these experiments, AutoTCL achieved minimum error in most cases. Compared to the state-of-the-art CoST method, AutoTCL reduced the average MSE error by 6.5% and the average MAE by 4.8%. Multivariate forecasting. We provided our full experiment results of multivariate time series forecasting results in Table 11. In multivariate forecasting tasks, our method achieved fewer best results than univariate forecasting. AutoTCL reduced the average MSE error by2.9% and the average MAE by 1.2% than CoST. In the column of stemGNN, because of the error out-of-memory, we can’t report part results. Classification. In Table 12, the full results of 30 class datasets are provided. AutoTCL is the most powerful method than other baselines with the highest average accuracy rate and ranks. Some results are not available due to out-of-memory errors. 23Published as a conference paper at ICLR 2024 Table 9: Case study of parametric augmentation. The inputs are from the CricketX dataset, which is a univariate time series dataset. We demonstrate the informative parts in two settings, w/ and w/o (γ = 0) regularization loss. Input γ h(x) x∗ 0 50 100 150 200 250 300 0 2 4 6 0.0 0 50 100 150 200 250 300 0.0 0.2 0.4 0.6 0.8 1.0 0 50 100 150 200 250 300 0 2 4 6 0.2 0 50 100 150 200 250 300 0.0 0.2 0.4 0.6 0.8 1.0 0 50 100 150 200 250 300 0 2 4 6 0 50 100 150 200 250 300 2 1 0 1 2 3 0.0 0 50 100 150 200 250 300 0.0 0.2 0.4 0.6 0.8 1.0 0 50 100 150 200 250 300 2 1 0 1 2 3 0.2 0 50 100 150 200 250 300 0.0 0.2 0.4 0.6 0.8 1.0 0 50 100 150 200 250 300 2 1 0 1 2 3 0 50 100 150 200 250 300 2 1 0 1 2 0.0 0 50 100 150 200 250 300 0.0 0.2 0.4 0.6 0.8 1.0 0 50 100 150 200 250 300 2 1 0 1 2 0.2 0 50 100 150 200 250 300 0.0 0.2 0.4 0.6 0.8 1.0 0 50 100 150 200 250 300 2 1 0 1 2 0 50 100 150 200 250 300 1 0 1 2 0.0 0 50 100 150 200 250 300 0.0 0.2 0.4 0.6 0.8 1.0 0 50 100 150 200 250 300 1 0 1 2 0.2 0 50 100 150 200 250 300 0.0 0.2 0.4 0.6 0.8 1.0 0 50 100 150 200 250 300 1 0 1 2 24Published as a conference paper at ICLR 2024 Table 10: Univariate time series forecasting results. AutoTCL TS2Vec Informer LogTrans N-BEATS TCN CoST TNC TS-TCC InfoTS DatasetLy MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1 24 0.037 0.1480.0390.152 0.098 0.247 0.103 0.259 0.094 0.238 0.075 0.210 0.040 0.1520.057 0.184 0.103 0.237 0.0390.14948 0.054 0.1760.062 0.191 0.158 0.319 0.167 0.328 0.210 0.367 0.227 0.402 0.060 0.186 0.094 0.239 0.139 0.279 0.0560.1791680.078 0.2100.134 0.282 0.183 0.346 0.207 0.375 0.232 0.391 0.316 0.493 0.0970.2360.171 0.329 0.253 0.408 0.100 0.2393360.093 0.2310.154 0.310 0.222 0.387 0.230 0.398 0.232 0.388 0.306 0.495 0.1120.2580.192 0.357 0.155 0.318 0.117 0.2647200.120 0.2720.163 0.327 0.269 0.435 0.273 0.463 0.322 0.490 0.390 0.557 0.148 0.306 0.235 0.408 0.190 0.337 0.1410.302 ETTh2 24 0.079 0.2060.090 0.229 0.093 0.240 0.102 0.255 0.198 0.345 0.103 0.2490.0790.2070.097 0.238 0.239 0.391 0.0810.21548 0.1170.2550.124 0.273 0.155 0.314 0.169 0.348 0.234 0.386 0.142 0.290 0.118 0.2590.131 0.281 0.260 0.4050.1150.261168 0.1760.3190.208 0.360 0.232 0.389 0.246 0.422 0.331 0.453 0.227 0.376 0.189 0.339 0.197 0.354 0.291 0.4200.1710.327336 0.1930.3440.213 0.369 0.263 0.417 0.267 0.437 0.431 0.508 0.296 0.430 0.206 0.360 0.207 0.366 0.336 0.4530.183 0.341720 0.223 0.373 0.214 0.374 0.277 0.431 0.303 0.493 0.437 0.517 0.325 0.463 0.214 0.371 0.2070.3700.362 0.4720.194 0.357 ETTm1 24 0.016 0.091 0.0150.092 0.030 0.137 0.065 0.202 0.054 0.184 0.041 0.157 0.0150.0880.019 0.103 0.089 0.2280.014 0.08748 0.0260.1200.027 0.126 0.069 0.203 0.078 0.220 0.190 0.361 0.101 0.2570.025 0.1170.036 0.142 0.134 0.2800.025 0.11796 0.0360.1450.044 0.161 0.194 0.372 0.199 0.386 0.183 0.353 0.142 0.311 0.0380.147 0.054 0.178 0.159 0.3050.036 0.1422880.063 0.1910.103 0.246 0.401 0.554 0.411 0.572 0.186 0.362 0.318 0.472 0.077 0.209 0.098 0.244 0.204 0.327 0.0710.2006720.090 0.2250.156 0.307 0.512 0.644 0.598 0.702 0.197 0.368 0.397 0.547 0.113 0.257 0.136 0.290 0.206 0.354 0.1020.240 Elec. 24 0.241 0.2620.260 0.288 0.251 0.275 0.528 0.447 0.427 0.330 0.263 0.279 0.2430.2640.252 0.278 0.379 0.561 0.245 0.26948 0.287 0.2920.319 0.324 0.346 0.339 0.409 0.414 0.551 0.392 0.373 0.344 0.2920.3000.300 0.308 0.453 0.600 0.294 0.3011680.394 0.3650.427 0.394 0.544 0.424 0.959 0.612 0.893 0.538 0.609 0.462 0.405 0.3750.412 0.384 0.575 0.616 0.4020.367336 0.5430.4600.565 0.474 0.713 0.512 1.079 0.639 1.035 0.669 0.855 0.606 0.560 0.473 0.548 0.466 0.637 0.6330.533 0.453 WTH 24 0.093 0.2110.096 0.215 0.117 0.251 0.136 0.279 0.136 0.264 0.109 0.217 0.0960.2130.102 0.221 0.221 0.386 0.101 0.22248 0.131 0.2560.140 0.264 0.178 0.318 0.206 0.356 0.198 0.319 0.143 0.269 0.1380.2620.139 0.264 0.255 0.406 0.141 0.2661680.182 0.3110.207 0.335 0.266 0.398 0.309 0.439 0.309 0.420 0.188 0.319 0.207 0.334 0.1980.3280.339 0.458 0.199 0.328336 0.1950.3250.231 0.360 0.297 0.416 0.359 0.484 0.369 0.4600.192 0.3200.230 0.356 0.215 0.347 0.372 0.491 0.220 0.3517200.1980.3300.233 0.365 0.359 0.466 0.388 0.499 0.270 0.4060.198 0.3290.242 0.370 0.219 0.353 0.322 0.467 0.218 0.353 Lora 24 0.052 0.1410.212 0.268 0.917 0.720 0.264 0.371 0.072 0.170 0.981 0.899 0.0530.1440.206 0.273 0.365 0.514 0.058 0.14948 0.080 0.1810.267 0.316 1.067 0.786 0.364 0.424 0.115 0.223 0.981 0.898 0.0820.1840.286 0.349 0.426 0.562 0.090 0.1921680.155 0.2630.355 0.389 1.745 1.067 0.452 0.465 0.286 0.350 1.276 0.946 0.166 0.2740.523 0.549 0.481 0.587 0.1560.2673360.229 0.3350.425 0.441 1.661 1.050 0.950 0.683 0.405 0.429 1.273 0.943 0.2520.3550.772 0.724 0.588 0.645 0.313 0.3867200.370 0.4450.523 0.509 2.482 1.370 1.248 0.807 0.679 0.573 1.290 0.950 0.3790.4511.313 0.929 0.592 0.649 1.047 0.635 Avg. 0.157 0.2580.207 0.301 0.486 0.477 0.382 0.441 0.320 0.388 0.419 0.465 0.1680.2710.256 0.340 0.315 0.441 0.188 0.274 Table 11: Multivariate time series forecasting results. AutoTCL TS2Vec Informer LogTrans StemGNN TCN CoST TNC TS-TCC InfoTS DatasetLy MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1 24 0.3890.4390.599 0.534 0.577 0.549 0.686 0.604 0.614 0.571 0.767 0.6120.386 0.4290.708 0.592 0.516 0.508 0.564 0.52048 0.4470.4770.629 0.555 0.685 0.625 0.766 0.757 0.748 0.618 0.713 0.6170.437 0.4640.749 0.619 0.644 0.579 0.607 0.5531680.615 0.5740.755 0.636 0.931 0.752 1.002 0.846 0.663 0.608 0.995 0.738 0.6430.5820.884 0.699 0.678 0.619 0.746 0.6383360.802 0.6710.907 0.717 1.128 0.873 1.362 0.952 0.927 0.730 1.175 0.800 0.8120.6791.020 0.768 0.967 0.754 0.904 0.722720 1.028 0.789 1.048 0.790 1.215 0.896 1.397 1.291 – – 1.453 1.311 0.9700.7711.157 0.8300.935 0.7151.098 0.811 ETTh2 24 0.337 0.4330.398 0.4610.720 0.665 0.828 0.750 1.292 0.883 1.365 0.888 0.447 0.502 0.612 0.595 0.782 0.666 0.3830.46248 0.5720.5760.578 0.5731.457 1.001 1.806 1.034 1.099 0.847 1.395 0.960 0.699 0.637 0.840 0.716 1.357 0.8810.5670.5821681.470 0.9471.901 1.065 3.489 1.515 4.070 1.681 2.282 1.228 3.166 1.407 1.5490.9822.359 1.213 4.318 1.728 1.789 1.0483361.685 1.0272.304 1.215 2.723 1.340 3.875 1.763 3.086 1.351 3.256 1.481 1.7491.0422.782 1.349 2.097 1.145 2.120 1.1617201.890 1.0922.650 1.373 3.467 1.473 3.913 1.552 – – 3.690 1.588 1.9711.0922.753 1.394 2.047 1.1272.511 1.316 ETTm1 24 0.2560.3390.443 0.436 0.323 0.369 0.419 0.412 0.620 0.570 0.324 0.3740.246 0.3290.522 0.472 0.403 0.455 0.391 0.40848 0.3390.3960.582 0.515 0.494 0.503 0.507 0.583 0.744 0.628 0.477 0.4500.331 0.3860.695 0.567 0.618 0.552 0.503 0.47596 0.3760.4220.622 0.549 0.678 0.614 0.768 0.792 0.709 0.624 0.636 0.602 0.3780.4190.731 0.595 0.607 0.572 0.537 0.5032880.464 0.4840.709 0.609 1.056 0.786 1.462 1.320 0.843 0.683 1.270 1.351 0.4720.4860.818 0.649 0.722 0.638 0.653 0.5796720.608 0.5660.786 0.655 1.192 0.926 1.669 1.461 – – 1.381 1.467 0.6200.5740.932 0.712 0.708 0.601 0.757 0.642 Elec. 24 0.1530.2500.287 0.374 0.312 0.387 0.297 0.374 0.439 0.388 0.305 0.3840.136 0.2420.354 0.423 0.379 0.561 0.255 0.35048 0.1670.2640.307 0.388 0.392 0.431 0.316 0.389 0.413 0.455 0.317 0.3920.153 0.2580.376 0.438 0.453 0.600 0.279 0.368168 0.1790.2750.332 0.4070.515 0.509 0.426 0.466 0.506 0.518 0.358 0.4230.175 0.2750.402 0.456 0.575 0.616 0.302 0.385336 0.1990.2970.349 0.420 0.759 0.625 0.365 0.417 0.647 0.596 0.349 0.4160.196 0.2960.417 0.466 0.637 0.633 0.320 0.399 WTH 24 0.302 0.364 0.307 0.3630.335 0.381 0.435 0.4770.2830.507 0.321 0.367 0.2980.3600.320 0.373 0.356 0.463 0.316 0.36948 0.361 0.4120.374 0.418 0.395 0.459 0.426 0.4950.3370.573 0.386 0.423 0.3590.4110.380 0.421 0.429 0.500 0.381 0.420168 0.4550.4840.491 0.506 0.608 0.567 0.727 0.6710.3970.652 0.491 0.501 0.464 0.4910.479 0.495 0.511 0.550 0.490 0.501336 0.4870.5050.525 0.530 0.702 0.620 0.754 0.6700.3940.639 0.502 0.507 0.497 0.517 0.505 0.514 0.575 0.584 0.532 0.527720 0.5080.5190.556 0.552 0.831 0.731 0.885 0.773 – –0.498 0.5080.533 0.542 0.519 0.525 0.545 0.577 0.554 0.543 Lora 24 0.1980.2520.212 0.267 0.376 0.345 0.456 0.3940.1610.373 0.854 0.775 0.202 0.259 0.264 0.302 0.365 0.514 0.1980.24348 0.2540.3010.266 0.316 0.428 0.420 0.663 0.4670.2040.439 0.851 0.774 0.258 0.307 0.319 0.345 0.426 0.562 0.2540.297168 0.346 0.3770.354 0.389 0.734 0.597 0.682 0.5100.2700.536 1.118 0.839 0.350 0.383 0.474 0.477 0.481 0.587 0.3450.374336 0.414 0.4280.425 0.441 0.995 0.738 1.068 0.6080.3950.618 1.111 0.836 0.417 0.432 0.625 0.588 0.588 0.645 0.4120.427720 0.5170.5020.522 0.509 1.181 0.831 0.959 0.622 – – 1.131 0.844 0.524 0.5071.266 0.876 0.592 0.6490.514 0.501 Avg. 0.545 0.4990.697 0.571 0.990 0.708 1.138 0.798 0.753 0.651 1.057 0.781 0.5610.5050.837 0.637 0.838 0.675 0.665 0.556 25Published as a conference paper at ICLR 2024 Table 12: Classification result of the UEA dataset Dataset AutoTCL TS2Vec T-Loss TNC TS-TCC TST DTW TF-C InfoTS ArticularyWordRecognition 0.983 0.987 0.943 0.973 0.953 0.977 0.987 0.467 0.993 AtrialFibrillation 0.467 0.200 0.133 0.133 0.267 0.067 0.200 0.040 0.267 BasicMotions 1.000 0.975 1.000 0.975 1.000 0.975 0.975 0.475 1.000 CharacterTrajectories 0.976 0.995 0.993 0.967 0.985 0.975 0.989 0.090 0.987 Cricket 1.000 0.972 0.972 0.958 0.917 1.000 1.000 0.125 1.000 DuckDuckGeese 0.700 0.680 0.650 0.460 0.380 0.620 0.600 0.340 0.600 EigenWorms 0.901 0.847 0.840 0.840 0.779 0.748 0.618 – 0.748 Epilepsy 0.978 0.964 0.971 0.957 0.957 0.949 0.964 0.217 0.993 ERing 0.944 0.874 0.133 0.852 0.904 0.874 0.133 0.167 0.953 EthanolConcentration 0.354 0.308 0.205 0.297 0.285 0.262 0.323 0.247 0.323 FaceDetection 0.581 0.501 0.513 0.536 0.544 0.534 0.529 0.502 0.525 FingerMovements 0.640 0.480 0.580 0.470 0.460 0.560 0.530 0.510 0.620 HandMovementDirection 0.432 0.338 0.351 0.324 0.243 0.243 0.231 0.405 0.514 Handwriting 0.384 0.515 0.451 0.249 0.498 0.225 0.286 0.051 0.554 Heartbeat 0.785 0.683 0.741 0.746 0.751 0.746 0.717 0.737 0.771 JapaneseV owels 0.984 0.984 0.989 0.978 0.930 0.978 0.949 0.135 0.986 Libras 0.833 0.867 0.883 0.817 0.822 0.656 0.870 0.067 0.889 LSST 0.554 0.537 0.509 0.595 0.474 0.408 0.551 0.314 0.593 MotorImagery 0.570 0.510 0.580 0.500 0.610 0.500 0.500 0.500 0.610 NATOPS 0.944 0.928 0.917 0.911 0.822 0.850 0.883 0.533 0.939 PEMS-SF 0.838 0.682 0.676 0.699 0.734 0.740 0.711 0.312 0.757 PenDigits 0.984 0.989 0.981 0.979 0.974 0.560 0.977 0.236 0.989 PhonemeSpectra 0.218 0.233 0.222 0.207 0.252 0.085 0.151 0.026 0.233 RacketSports 0.914 0.855 0.855 0.776 0.816 0.809 0.803 0.480 0.829 SelfRegulationSCP1 0.891 0.812 0.843 0.799 0.823 0.754 0.775 0.502 0.887 SelfRegulationSCP2 0.578 0.578 0.539 0.550 0.533 0.550 0.539 0.500 0.527 SpokenArabicDigits 0.925 0.932 0.905 0.934 0.970 0.923 0.963 0.100 0.988 StandWalkJump 0.533 0.467 0.333 0.400 0.333 0.267 0.200 0.333 0.467 UWaveGestureLibrary 0.893 0.884 0.875 0.759 0.753 0.575 0.903 0.125 0.906 InsectWingbeat 0.488 0.466 0.156 0.469 0.264 0.105 – 0.108 0.472 Avg. ACC 0.742 0.704 0.658 0.670 0.668 0.617 0.629 0.298 0.730 Avg. RANK 2.300 3.700 4.667 5.433 5.133 6.133 5.400 8.200 2.367 26",
      "references": [
        "Recursive time series data augmentation.",
        "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.",
        "Spectral temporal graph neural network for multivariate time-series forecasting.",
        "A simple framework for contrastive learning of visual representations.",
        "DTW-D: time series semi-supervised learning from a single example.",
        "Autoaugment: Learning augmentation strategies from data.",
        "The ucr time series archive.",
        "UCI machine learning repository.",
        "Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting.",
        "Time-series representation learning via temporal and contextual contrasting.",
        "Self-supervised time series representation learning by inter-intra relational reasoning.",
        "Unsupervised scalable representation learning for multivariate time series.",
        "Deep Learning.",
        "Generative adversarial networks.",
        "A kernel two-sample test.",
        "Faster autoaugment: Learning augmentation strategies using backpropagation.",
        "Population based augmentation: Efficient learning of augmentation policy schedules.",
        "Categorical reparameterization with gumbel-softmax.",
        "Contrastive self-supervised learning for sensor-based human activity recognition.",
        "Adam: A method for stochastic optimization.",
        "Normalizing flows: An introduction and review of current methods.",
        "Modeling long-and short-term temporal patterns with deep neural networks.",
        "Metaug: Contrastive learning via meta feature augmentation.",
        "Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting.",
        "Differentiable automatic data augmentation.",
        "Petformer: Long-term time series forecasting via placeholder-enhanced transformer.",
        "Learning sparse neural networks through l0 regularization.",
        "Time series contrastive learning with information-aware augmentations.",
        "The concrete distribution: A continuous relaxation of discrete random variables.",
        "A time series is worth 64 words: Long-term forecasting with transformers.",
        "Utilizing expert features for contrastive learning of time-series representations.",
        "Representation learning with contrastive predictive coding.",
        "N-beats: Neural basis expansion analysis for interpretable time series forecasting.",
        "Contrastive learning for unsupervised domain adaptation of time series.",
        "Scikit-learn: Machine learning in Python.",
        "Information theoretic learning: Renyi’s entropy and kernel perspectives.",
        "Cadda: Class-wise automatic differentiable data augmentation for eeg signals.",
        "Dsformer: A double sampling transformer for multivariate time series long-term prediction.",
        "Ts2vec: Towards universal representation of time series.",
        "A transformer-based framework for multivariate time series representation learning.",
        "CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting.",
        "Infogcl: Information-aware graph contrastive learning.",
        "Unsupervised time-series representation learning with iterative bilinear temporal-spectral fusion.",
        "Unsupervised data augmentation for consistency training.",
        "Hallucination improves the performance of unsupervised visual representation learning.",
        "Time series data augmentation for deep learning: A survey.",
        "Informer: Beyond efficient transformer for long sequence time-series forecasting."
      ],
      "meta_data": {
        "arxiv_id": "2402.10434v1",
        "authors": [
          "Xu Zheng",
          "Tianchun Wang",
          "Wei Cheng",
          "Aitian Ma",
          "Haifeng Chen",
          "Mo Sha",
          "Dongsheng Luo"
        ],
        "published_date": "2024-02-16T03:51:14Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This paper tackles the challenge of selecting effective data augmentations for time-series contrastive learning by introducing a parametric, factorization-based augmentation framework called AutoTCL. It defines Good Views as ones that decompose a time series x into an informative part x* and a task-irrelevant part Δx, and then uses a learnable mask h and an invertible transform g to produce lossless, more diverse views v from x*. By training the augmentation network with a Principle of Relevant Information (PRI) objective and a temporal-consistency regularizer, and training the encoder with contrastive losses, AutoTCL adapts augmentations to each instance. The approach is encoder-agnostic and yields competitive or state-of-the-art results on univariate and multivariate forecasting and on time-series classification, with average improvements over strong baselines (e.g., ~6.5% MSE and ~4.7% MAE in forecasting; ~1.2% accuracy in classification). It is validated through extensive ablations and backbone-agnostic experiments.",
        "methodology": "Core ideas and components: (1) factorization of each time-series x into informative x* and task-irrelevant Δx via a learnable mask h(x); (2) lossless transformation v* = g(x*) using an invertible function g, with a parametric mask g(x) to generate a view v = η(g(x*) ⊙ h(x), Δv) that increases diversity; (3) a practical instantiation where h and g are implemented by a shared dilated CNN backbone with two projection heads (factorization head for h and transformation head for g) and a stochastic binary concrete mechanism to sample h; (4) augmentation objective based on PRI: LPRI = β H(v*) + D(Px || Pv*), approximated by MMD to encourage informative yet compact representations; (5) sparsity of the mask ||h||0 and a regularization term to encourage temporal consistency via a triplet loss Lt; (6) an alternating training schedule: M encoder updates per augmentation update; (7) encoder training with global (InfoNCE) and local (subsequence-based) contrasts, forming Lcon = Lg + αLl; (8) plug-and-play with backbones such as TS2Vec and CoST, enabling broad applicability across encoder architectures.",
        "experimental_setup": "Datasets: forecasting on six datasets (ETTh1, ETTh2, ETTm1, Electricity, Weather, Lora) in univariate/multivariate settings; classification on the 30-dataset UEA collection. Baselines include TS2Vec, Informer, LogTrans, N-BEATS, TCN, CoST, TNC, TS-TCC, InfoTS, etc. Evaluation: forecasting uses a linear model on frozen pretrained embeddings to predict future values for varying horizons; metrics MSE and MAE; classification uses a linear SVM on embeddings with accuracy and rank as metrics. Implementation details: encoder backbone is a multi-layer dilated CNN; augmentation network shares architecture and has two projection heads; training uses Adam optimizers on both networks; runs on 8 NVIDIA A100 GPUs with PyTorch; hyperparameters tuned by grid search; appendix contains full results and ablations. Additional validations include traffic data (Traffic4) and a CricketX case study to illustrate learned informative parts; visualization via t-SNE and convergence plots.",
        "limitations": "Potential limitations include increased computational overhead due to joint training of augmentation and encoder networks and the need for careful hyperparameter tuning (β, γ, α, M). The method relies on learnable factorization masks and invertible transforms, whose success depends on the quality of the learned x* vs Δx decomposition and the expressiveness of g/h architectures. Results are demonstrated on time-series forecasting and classification; generalization to other domains or tasks remains to be validated. Dependency on backbone choice and data characteristics (e.g., noise, nonstationarity) may affect performance; scalability to very long sequences or very large datasets could pose practical challenges.",
        "future_research_directions": "Extend parametric augmentation to other self-supervised and supervised contrastive frameworks and to domains beyond time-series (e.g., audio, sensor, or graph data). Explore more expressive invertible transforms and masking strategies (e.g., flow-based or attention-based masks) and alternative PRI variants. Investigate robustness to distribution shifts, nonstationarity, and extremely long sequences; study scalability with larger encoders and datasets; apply AutoTCL to other tasks such as anomaly detection, domain adaptation, or time-series segmentation. Combine with more diverse backbones and consider unsupervised domain adaptation or transfer learning scenarios to further exploit learned representations.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Multifaceted Uncertainty Estimation for Label-Efficient Deep Learning",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Label-efficient Segmentation via Affinity Propagation",
      "full_text": "Label-efficient Segmentation via Affinity Propagation Wentong Li1∗, Yuqian Yuan1∗, Song Wang1, Wenyu Liu1, Dongqi Tang2, Jian Liu2, Jianke Zhu1†, Lei Zhang3 1Zhejiang University 2Ant Group 3The Hong Kong Polytechnical University https://LiWentomng.github.io/apro/ Abstract Weakly-supervised segmentation with label-efficient sparse annotations has at- tracted increasing research attention to reduce the cost of laborious pixel-wise labeling process, while the pairwise affinity modeling techniques play an essen- tial role in this task. Most of the existing approaches focus on using the local appearance kernel to model the neighboring pairwise potentials. However, such a local operation fails to capture the long-range dependencies and ignores the topology of objects. In this work, we formulate the affinity modeling as an affinity propagation process, and propose a local and a global pairwise affinity terms to generate accurate soft pseudo labels. An efficient algorithm is also developed to reduce significantly the computational cost. The proposed approach can be conveniently plugged into existing segmentation networks. Experiments on three typical label-efficient segmentation tasks, i.e. box-supervised instance segmenta- tion, point/scribble-supervised semantic segmentation and CLIP-guided semantic segmentation, demonstrate the superior performance of the proposed approach. 1 Introduction Segmentation is a widely studied problem in computer vision, aiming at generating a mask prediction for a given image, e.g., grouping each pixel to an object instance (instance segmentation) or assigning each pixel a category label (semantic segmentation). While having achieved promising performance, most of the existing approaches are trained in a fully supervised manner, which heavily depend on the pixel-wise mask annotations, incurring tedious labeling costs [1]. Weakly-supervised methods have been proposed to reduce the dependency on dense pixel-wise labels with label-efficient sparse annotations, such as points [2, 3, 4], scribbles [5, 6, 7], bounding boxes [8, 9, 10, 11] and image-level labels [12, 13, 14, 15]. Such methods make dense segmentation more accessible with less annotation costs for new categories or scene types. Most of the existing weakly-supervised segmentation methods [16, 13, 8, 17, 3, 10] adopt the local appearance kernel to model the neighboring pairwise affinities, where spatially nearby pixels with similar color ( i.g., LAB color space [ 8, 3] or RGB color space [ 13, 16, 17, 10]) are likely to be in the same class. Though having proved to be effective, these methods suffer from two main limitations. First, the local operation cannot capture global context cues and capture long-range affinity dependencies. Second, the appearance kernel fails to take the intrinsic topology of objects into account, and lacks capability of detail preservation. To address the first issue, one can directly enlarge the kernel size to obtain a large receptive filed. However, this will make the segmentation model insensitive to local details and increase the com- putational cost greatly. Some methods [14, 12] model the long-range affinity via random walk [18], but they cannot model the fine-grained semantic affinities. As for the second issue, the tree-based ∗Equal contribution †Correspondence author 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.10533v2  [cs.CV]  17 Oct 2023box-supervised instance segmentation point/scribble-supervised semantic segmentation annotation-free semantic segmentation input image supervise GT pseudo label  GT supervise (a) (b) or CLIP supervise (c) or input image input image pseudo label refinedpredictionprediction prediction pseudo label APro APro APro Figure 1: The proposed approach upon the typical weakly-supervised segmentation tasks with label-efficient annotations, including (a) box-supervised instance segmentation, (b) point/scribble- supervised semantic segmentation and (c) annotation-free semantic segmentation with CLIP pre- trained model. approaches [7, 19] are able to preserve the geometric structures of objects, and employ the minimum spanning tree [20] to capture the pairwise relationship. However, the affinity interactions with distant nodes will decay rapidly as the distance increases along the spanning tree, which still focuses on the local nearby regions. LTF-V2 [21] enables the long-range tree-based interactions but it fails to model the valid pairwise affinities for label-efficient segmentation task. With the above considerations, we propose a novel component, named Affinity Propagation (APro), which can be easily embedded in existing methods for label-efficient segmentation. Firstly, we define the weakly-supervised segmentation from a new perspective, and formulate it as a uniform affinity propagation process. The modelled pairwise term propagates the unary term to other nearby and distant pixels and updates the soft pseudo labels progressively. Then, we introduce the global affinity propagation, which leverages the topology-aware tree-based graph and relaxes the geometric constraints of spanning tree to capture the long-range pairwise affinity. With the efficient design, the O(N2) complexity of brute force implementation is reduced to O(N log N), and the global propagation approach can be performed with much less resource consumption for practical applica- tions. Although the long-range pairwise affinity is captured, it inevitably brings in noises based on numerous pixels in a global view. To this end, we introduce a local affinity propagation to encourage the piece-wise smoothness with spatial consistency. The formulated APro can be embedded into the existing segmentation networks to generate accurate soft pseudo labels online for unlabeled regions. As shown in Fig. 1, it can be seamlessly plugged into the existing segmentation networks for various tasks to achieve weakly-supervised segmentation with label-efficient sparse annotations. We perform experiments on three typical label-efficient segmentation tasks, i.e. box-supervised instance segmentation, point/scribble-supervised semantic segmentation and annotation-free semantic segmentation with pretrained CLIP model, and the results demonstrated the superior performance of our proposed universal label-efficient approach. 2 Related Work Label-efficient Segmentation. Label-efficient segmentation, which is based on the weak supervi- sion from partial or sparse labels, has been widely explored [ 1]. Different from semi-supervised settings [22, 23], this paper mainly focuses on the segmentation with sparse labels. In earlier lit- erature [24, 25, 26, 27, 28], it primarily pertained to image-level labels. Recently, diverse sparse annotations have been employed, including point, scribble, bounding box , image-level label and the combinations of them. We briefly review the weakly-supervised instance segmentation, semantic segmentation and panoptic segmentation tasks in the following. For weakly-supervised instance segmentation, box-supervised methods [ 16, 8, 17, 9, 29, 30, 11, 10, 31] are dominant and perform on par with fully-supervised segmentation approaches. Besides, the “points + bounding box” annotation can also achieve competitive performance [ 32, 33]. As for weakly-supervised semantic segmentation, previous works mainly focus on the point-level supervision [2, 34, 6] and scribble-level supervision [5, 35, 36], which utilize the spatial and color information of the input image and are trained with two stages. Liang et al. [7] introduced an effective tree energy loss based on the low-level and high-level features for point/scribble/block- supervised semantic segmentation. For semantic segmentation, the supervision of image-level labels has been well explored [12, 37, 15, 14]. Recently, some works have been proposed to make use of the large-scale pretrained CLIP model [38] to achieve weakly-supervised or annotation-free semantic 2Network Pairwise Affinity Propagation Global affinity propagation Local affinity propagation + (a) (b) ()f I Y Y I g u Figure 2: Overview of our APro approach. (a) The general weakly supervised segmentation frame- work with the proposed pairwise affinity propagation. (b) The proposed approach consists of global affinity propagation (GP) and local affinity propagation (LP) to generate accurate pseudo labels. segmentation [39, 40]. In addition, weakly-supervised panoptic segmentation methods [ 3, 41, 4] with a single and multiple points annotation have been proposed. In this paper, we aim to develop a universal component for various segmentation tasks, which can be easily plugged into the existing segmentation networks. Pairwise Affinity Modeling. Pairwise affinity modeling plays an important role in many computer vision tasks. Classical methods, like CRF [42], make full use of the color and spatial information to model the pairwise relations in the semantic labeling space. Some works use it as a post-processing module [43], while others integrate it as a jointly-trained part into the deep neural network [44, 45]. Inspired by CRF, some recent approaches explore the local appearance kernel to tap the neighboring pairwise affinities, where spatially nearby pixels with similar colors are more likely to fall into the same class. Tian et al. [8] proposed the local pairwise loss, which models the neighboring relationship in LAB color space. Some works [10, 16, 17] focus on the pixel relation in RGB color space directly, and achieve competitive performance. However, the local operation fails to capture global context cues and lacks the long-range affinity dependencies. Furthermore, the appearance kernel cannot reflect the topology of objects, missing the details of semantic objects. To model the structural pair-wise relationship, tree-based approaches [7, 19] leverage the minimum spanning tree [20] to capture the topology of objects in the image. However, the affinity interactions with distant nodes decay rapidly as the distance increases along the spanning tree. Besides, Zhang et al. [35] proposed an affinity network to convert an image to a weighted graph, and model the node affinities by the graph neural network (GNN). Different from these methods, in this work we model the pairwise affinity from a new affinity propagation perspective both globally and locally. 3 Methodology In this section, we introduce our proposed affinity propagation ( APro) approach to label-efficient segmentation. First, we define the problem and model the APro framework in Section 3.1. Then, we describe the detailed pairwise affinity propagation method in Section 3.2. Finally, we provide an efficient implementation of our method in Section 3.3. 3.1 Problem Definition and Modeling Given an input image I = {xi}N with N pixels and its available sparse ground truth labeling ¯Y (i.e. points, scribble, or bounding box), and let fθ(I) be the output of a segmentation network with the learnable parameters θ, the whole segmentation network can be regarded as a neural network optimization problem as follows: min θ \b Lg(fθ(I), ¯Y ) + Lu(fθ(I), Y) \t , (1) where Lg is a ground truth loss on the set of labeled pixels Ωg with ¯Y and Lu is a loss on the unlabeled regions Ωu with the pseudo label Y . As shown in Fig. 2-(a), our goal is to obtain the accurate pseudo label Y by leveraging the image pixel affinities for unlabeled regions. As in the classic MRF/CRF model [ 46, 42], the unary term reflects the per-pixel confidence of assigning labels, while pairwise term captures the inter-pixel constraints. We define the generation of 3pseudo label Y as an affinity propagation process, which can be formulated as follows: yi = 1 zi X j∈τ ϕ(xj)ψ(xi, xj), (2) where ϕ(xj) denotes the unary potential term, which is used to align yj and the corresponding network prediction based on the available sparse labels. ψ(xi, xj) indicates the pairwise potential, which models the inter-pixel relationships to constrain the predictions and produce accurate pseudo labels. τ is the region with different receptive fields.zi is the summation of pairwise affinityψ(xi, xj) along with j to normalize the response. Notably, we unify both global and local pairwise potentials in an affinity propagation process formulated in Eq. 2. As shown in Fig. 2-(b), the global affinity propagation (GP) can capture the pairwise affinities with topological consistency in a global view, while the local affinity propagation (LP) can obtain the pairwise affinities with local spatial consistency. Through the proposed component, the soft pseudo labels Y can be obtained. We assign each yi from GP and LP with the network prediction pi and directly employ the distance measurement function as the objective for unlabeled regions Ωu. Simple L1 distance is empirically adopted in our implementation. 3.2 Pairwise Affinity Propagation 3.2.1 Global Affinity Propagation We firstly provide a solution to model the global affinity efficiently based on the input image. Specifically, we represent an input image as a 4-connected planar graph G, where each node is adjacent to up to 4 neighbors. The weight of the edge measures the image pixel distance between adjacent nodes. Inspired by tree-based approaches [47, 19], we employ the minimum spanning tree (MST) algorithm [20] to remove the edge with a large distance to obtain the tree-based sparse graph GT , i.e., GT ← MST(G) and GT = {V, E}, where V = {Vi}N is the set of nodes and E = {Ei}N−1 denotes the set of edges. Then, we model the global pairwise potential by iterating over each node. To be specific, we take the current node as the root of the spanning tree GT and propagate the long-range affinities to other nodes. While the distant nodes along the spanning tree need to pass through nearby nodes along the path of spanning tree, the distance-insensitive max affinity function can alleviate this geometric constraint and relax the affinity decay for long-range nodes. Hence, we define the global pairwise potential ψg as follows: ψg(xi, xj) = T (Ii, Ij) ∀j∈V = exp(− max ∀(k,l)∈Ei,j wk,l ζg 2 ), (3) where T denotes the global tree. Ei,j is the set of edges in the path of T from node j to node i. wk,l indicates the edge weight between the adjacent nodes k and l, which is represented as the Euclidean distance between pixel values of two adjacent nodes, i.e., wk,l = |Ik − Il|2. ζg controls the degree of similarity with the long-range pixels. In this way, the global affinity propagation (GP) process to obtain the soft label yg can be formulated as follows: yg i = 1 zg i X j∈V ϕ(xj)ψg(xi, xj), zg i = X j∈V ψg(xi, xj), (4) where interactions with distant nodes are performed over tree-based topology. In addition to utilizing low-level image, we empirically employ high-level feature as input to propagate semantic affinity. 3.2.2 Local Affinity Propagation The long-range pairwise affinity is inevitably noisy since it is computed based on the susceptible image features in a global view. The spatially nearby pixels are more likely to have the same label, while they have certain difference in color and intensity,etc. Hence, we further introduce the local affinity propagation (LP) to promote the piece-wise smoothness. The Gaussian kernel is widely used to capture the local relationship among the neighbouring pixels in previous works [45, 8, 10]. Different from these works, we define the local pairwise affinity via the formulated affinity propagation process. The local pairwise term ψs is defined as: ψs(xi, xj) = K(Ii, Ij) j∈N(i) = exp   −|Ii − Ij|2 ζ2s ! , (5) 4where K denotes the Gaussian kernel, N(i) is the set containing all local neighbor pixels. The degree of similarity is controlled by parameterζs. Then the pseudo label ys can be obtained via the following affinity propagation: ys i = 1 zs i X j∈N(i) ϕ(xj)ψs(xi, xj), zs i = X j∈N(i) ψs(xi, xj), (6) where the local spatial consistency is maintained based on high-contrast neighbors. To obtain a robust segmentation performance, multiple iterations are required. Notably, our LP process ensures a fast convergence, which is 5× faster than MeanField-based method [10, 42]. The details can be found in the experimental section 4.4. 3.3 Efficient Implementation Given a tree-based graph GT = {V, E} in the GP process, we define the maximum w value of the path through any two vertices as the transmission cost C. One straightforward approach to get yg i of vertex i is to traverse each vertex j by Depth First Search or Breadth First Search to get the transmission cost Ci,j accordingly. Consequently, the computational complexity required to acquire the entire output yg is O(N2), making it prohibitive in real-world applications. Instead of calculating and updating the transmission cost of any two vertices, we design a lazy update algorithm to accelerate the GP process. Initially, each node is treated as a distinct union, represented by U. Unions are subsequently connected based on each edge wk,l in ascending order of w. We show that when connecting two unions Uk and Ul, wk,l is equivalent to the transmission cost for all nodes within Uk and Ul. This is proved in the supplementary material. To efficiently update values, we introduce a Lazy Propagationscheme. We only update the value of the root node and postpone the update of its descendants. The update information is retained in a lazy tag Z and is updated as follows: Z(δ)k∗ = Z(δ)k∗ + \u001aexp(−wk,l/ζg 2)S(δ)l Uk.rank > Ul.rank, exp(−wk,l/ζg 2)S(δ)l − Z(δ)l∗ otherwise, (7) where S(δ)i = P j∈Ui δj, δ means different inputs, including the dense prediction ϕ(x) and all-one matrix Λ. k∗/l∗ denotes the root node of node k/l. Once all unions are connected, the lazy tags can be propagated downward from the root node to its descendants. For the descendants, the global affinity propagation term is presented as follows: LP rop(δ)i = δi + X r∈AscGT (i)∪{i} Z(δ)r, (8) where AscGT (i) represents the ascendants of node i in the tree GT . As shown in Algorithm 1, the disjoint-set data structure is employed to implement the proposed algorithm. In our implementation, a Path Compression strategy is applied, connecting each node on the path directly to the root node. Consequently, it is sufficient to consider the node itself and its parent node to obtain LP rop. Time complexity. For each channel of the input, the average time complexity of sorting is O(N log N). In the merge step, we utilize the Path Compression and Union-by-Rank strategies, which have a complexity of O(α(N))[48]. After merging all the concatenated blocks, the lazy tags can be propagated in O(N) time. Hence, the overall complexity is O(N log N). Note that the batches and channels are independent of each other. Thus, the algorithm can be executed in parallel for both batches and channels for practical implementations. As a result, the proposed algorithm reduces the computational complexity dramatically. 4 Experiments 4.1 Weakly-supervised Instance Segmentation Datasets. As in prior arts [8, 9, 16, 17], we conduct experiments on two widely used datasets for the weakly box-supervised instance segmentation task: • COCO [49], which has 80 classes with 115K train2017 images and 5K val2017 images. 5Algorithm 1: Algorithm for GP process Input: Tree GT ∈ Ne×2; Pairwise distance w ∈ RN ; Dense predictions ϕ(x) ∈ RN ; Vertex numN; Edge num e = N − 1; Set of vertices V. Output: yg ∈ RN . Λ ← 1 ∈ RN F ← {0, 1, 2, ..., N− 1} ▷ Initialize each vertex as a connected block Sort {GT , w} in ascending order of w. ▷ Quick Sort for (k, l) ∈ GT , wi ∈ w do a ← find(k), b ← find(l) ▷ Find the root node with Path Compression Update{Z(ϕ)a, Z(Λ)a, Z(ϕ)b, Z(Λ)b} ▷ Add lazy tag if Sa < Sb then swap(a, b) ▷ Merge by Rank Fb ← a ▷ Merge two connected blocks for v ∈ Vdo p ← find(v) for δ ∈ {ϕ, Λ} do if p = v then LP rop(δ)v = Z(δ)v + δv else LP rop(δ)v = Z(δ)p + Z(δ)v + δv yg v = LProp (ϕ)v LProp (Λ)v ▷ Normalization return yg • Pascal VOC [43] augmented by SBD [50] based on the original Pascal VOC 2012 [51], which has 20 classes with 10,582 trainaug images and 1,449 val images. Base Architectures and Competing Methods. In the evaluation, we apply our proposed APro to two representative instance segmentation architectures, SOLOv2 [ 52] and Mask2Former [ 53], with different backbones (i.e., ResNet [54], Swin-Transformer [55]) following Box2Mask [ 11]. We compare our approach with its counterparts that model the pairwise affinity based on the image pixels without modifying the base segmentation network for box-supervised setting. Specifically, the compared methods include Pairwise Loss [8], TreeEnergy Loss [7] and CRF Loss [10]. For fairness, we re-implement these models using the default setting in MMDetection [56]. Implementation Details. We follow the commonly used training settings on each dataset as in MMDetection [56]. All models are initialized with ImageNet [57] pretrained backbone. For SOLOv2 framework [52], the scale jitter is used, where the shorter image side is randomly sampled from 640 to 800 pixels. For Mask2Former framework [53], the large-scale jittering augmentation scheme [58] is employed with a random scale sampled within range [0.1, 2.0], followed by a fixed size crop to 1024×1024. The initial learning rate is set to 10−4 and the weight decay is 0.05 with 16 images per mini-batch. The box projection loss [8, 9] is employed to constrain the network prediction within the bounding box label as the unary term ϕ. COCO-style mask AP (%) is adopted for evaluation. Quantitative Results. Table 1 shows the quantitative results. We compare the approaches with the same architecture for fair comparison. The state-of-the-art methods are listed for reference. One can see that our APro method outperforms its counterparts across Pascal VOC and COCO datasets. • Pascal VOC [43] val. Under the SOLOv2 framework, our approach achieves 37.1% AP and 38.4% AP with 12 epochs and 36 epochs, respectively, outperforming other methods by 1.4%-2.5% mask AP with ResNet-50. With the Mask2Former framework, our approach also outperforms its counterparts. Furthermore, with the Swin-L backbone [55], our proposed approach achieves very promising performance, 49.6% mask AP with 50 epochs. • COCO [49] val. Under the SOLOv2 framework, our approach achieves 32.0% AP and 32.9% AP with 12 epochs and 36 epochs, and surpasses its best counterpart by 1.0% AP and 0.4% AP using ResNet-50, respectively. Under the Mask2Former framework, our method still achieves the best performance with ResNet-50 backbone. Furthermore, equipped with stronger backbones, our approach obtains more robust performance, achieving 38.0% mask AP with ResNet-101, and 41.0% mask AP using Swin-L backbone. 6Table 1: Quantitative results (§4.1) on Pascal VOC [43] and COCO val [49] with mask AP(%). Pascal VOC COCOMethod Backbone #Epoch AP AP50 AP75 AP AP50 AP75 BBTP[NeurIPS19][16] ResNet-101 12 23.1 54.1 17.1 21.1 45.5 17.2 BoxInst[CVPR21][8] ResNet-50 36 34.3 58.6 34.6 31.8 54.4 32.5 DiscoBox[ICCV21][17] ResNet-50 36 - 59.8 35.5 31.4 52.6 32.2 BoxLevelset[ECCV22][9] ResNet-50 36 36.3 64.2 35.9 31.4 53.7 31.8 SOLOv2 Framework Pairwise Loss[CVPR21][8] ResNet-50 12 35.7 64.3 35.1 31.0 52.8 31.5 TreeEnergy Loss[CVPR22][7] ResNet-50 12 35.0 64.4 34.7 30.9 52.9 31.3 CRF Loss[CVPR23][10] ResNet-50 12 35.0 64.7 34.9 30.9 53.1 31.4 APro(Ours) ResNet-50 12 37.1 65.1 37.0 32.0 53.4 32.9 Pairwise Loss[CVPR21][8] ResNet-50 36 36.5 63.4 38.1 32.4 54.5 33.4 TreeEnergy Loss[CVPR22][7] ResNet-50 36 36.1 63.5 36.1 31.4 54.0 31.2 CRF Loss[CVPR23][10] ResNet-50 36 35.9 64.0 35.7 32.5 54.9 33.2 APro(Ours) ResNet-50 36 38.4 65.4 39.8 32.9 55.2 33.6 APro(Ours) ResNet-101 36 40.5 67.9 42.6 34.3 57.0 35.3 Mask2Former Framework Pairwise Loss[CVPR21][8] ResNet-50 12 35.2 62.9 33.9 33.8 57.1 34.0 TreeEnergy Loss[CVPR22][7] ResNet-50 12 36.0 65.0 34.3 33.5 56.7 33.7 CRF Loss[CVPR23][10] ResNet-50 12 35.7 64.3 35.2 33.5 57.5 33.8 APro(Ours) ResNet-50 12 37.0 65.1 37.0 34.4 57.7 35.3 APro(Ours) ResNet-50 50 42.3 70.6 44.5 36.1 62.0 36.7 APro(Ours) ResNet-101 50 43.6 72.0 45.7 38.0 63.6 38.7 APro(Ours) Swin-L 50 49.6 77.6 53.1 41.0 68.3 41.9 Qualitative Results. Fig. 3 illustrates the visual comparisons on affinity maps of our APro and other approaches, and Fig. 4 compares the segmentation results. One can clearly see that our method captures accurate pairwise affinity with object’s topology and yields more fine-grained predictions. 4.2 Weakly-supervised Semantic Segmentation Datasets. We conduct experiments on the widely-used Pascal VOC2012 dataset [51], which contains 20 object categories and a background class. As in [ 6, 7], the augmented Pascal VOC dataset is adopted here. The point [2] and scribble [5] annotations are employed forweakly point-supervised and scribble-supervised settings, respectively. Table 2: Quantitative results (§4.2) on Pascal VOC2012 [51] val with mean IoU(%). Method BackboneSupervisionCRF Post.mIoU †KernelCut Loss[ECCV18][6] DeepLabV2 ✓ 57.0 ∗TEL[CVPR22][7] LTF ✗ 66.8 APro(Ours) LTF Point ✗ 67.7 †NormCut Loss[CVPR18][59] DeepLabV2 ✓ 74.5 †DenseCRF Loss[ECCV18][6] DeepLabV2 ✓ 75.0 †KernelCut Loss[ECCV18][6] DeepLabV2 ✓ 75.0 †GridCRF Loss[ICCV19][36] DeepLabV2 ✗ 72.8 PSI[ICCV21][36] DeepLabV3 ✗ 74.9 ∗TEL[CVPR22][7] LTF ✗ 76.2 APro(Ours) LTF Scribble ✗ 76.6 †:adopting multi-stage training,∗:our re-implementation. Implementation Details. As in [7], we adopt LTF [19] as the base segmentation model. The input size is 512 × 512. The SGD optimizer with momentum of 0.9 and weight decay of 10−4 is used. The initial learning rate is 0.001, and there are 80k train- ing iterations. The same data augmentations as in [ 7] are uti- lized. We employ the partial cross-entropy loss to make full use of the available point/scrib- ble labels and constrain the unary term. ResNet-101 [54] pretrained on ImageNet [57] is adopted as backbone network for all methods. Quantitative Results. As shown in Table 2, we compare our APro approach with the state-of-the-art methods on point-supervised and scribble-supervised semantic segmentation, respectively. • Point-wise supervision. With DeepLabV2 [ 43], KernelCut Loss [ 6] achieves 57.0% mIoU. Equipped with LTF [19], TEL [7] achieves 66.8% mIoU. Our APro achieves 67.7% mIoU, outper- forming the previous best method TEL [7] by 0.9% mIoU. • Scribble-wise supervision. The scribble-supervised approaches are popular in weakly supervised semantic segmentation. We apply the proposed approach under the single-stage training framework without calling for CRF post-processing during testing. Compared with the state-of-the-art methods, our approach achieves better performance with 76.6% mIoU. 7Image GPTF LP Init. Soft pseudo L# Figure 3: Visual comparisons of pairwise affinity maps based on the RGB image for a specific position (green star), including TreeFilter (TF) [19], local kernels with full image size (L#), and our presented LP and GP processes. The GP process can capture the long-range pairwise affinity with object’s topology, while LP retrieves the local similarities. Our APro approach smooths the noisy initial network predictions (init.) to obtain cleaner soft pseudo labels. 4.3 CLIP-guided Semantic Segmentation Datasets. To more comprehensively evaluate our proposed approach, we conduct experiments on CLIP-guided annotation-free semantic segmentation with three widely used datasets: • Pascal VOC 2012 [51] introduced in Section 4.2. • Pascal Context [60], which contains 59 foreground classes and a background class with 4,996 train images and 5,104 val images. • COCO-Stuff [61], which has 171 common semantic object/stuff classes on 164K images, containing 118,287 train images and 5,000 val images. Base Architectures and Backbones. We employ MaskCLIP+ [40] as our base architecture, which leverages the semantic priors of pretrained CLIP [38] model to achieve the annotation-free dense semantic segmentation. In the experiments, we couple MaskCLIP+ with our APro approach under ResNet-50, ResNet-50×16 and ViT-B/16 [62]. The dense semantic predictions of MaskCLIP [40] are used as the unary term, and our proposed method can refine it and generate more accurate pseudo labels for training target networks. Implementation Details. For fair comparison, we keep the same settings as MaskCLIP+ [40]. We keep the text encoder of CLIP unchanged and take prompts with target classes as the input. For text embedding, we feed prompt engineered texts into the text encoder of CLIP with 85 prompt templates, and average the results with the same class. For ViT-B/16, the bicubic interpolation is adopted for the pretrained positional embeddings. The initial learning rate is set to 10−4. We train all models with batch size 32 and 2k/4k/8k iterations. DeepLabv2-ResNet101 is used as the backbone. Table 3: Quantitative results (§4.3) on Pascal VOC2012 [51] val, Pascal Context [60] val, and COCO-Stuff [61] val with mean IoU (%). Method CLIP Model VOC2012ContextCOCO. MaskCLIP+[ECCV22][40] 58.0 23.9 13.6 APro(Ours) ResNet-50 61.6↑3.6 25.4↑1.5 14.6↑1.0 MaskCLIP+[ECCV22][40] 67.5 25.2 17.3 APro(Ours) ResNet-50×16 70.4↑2.9 26.5↑1.3 18.2↑0.9 MaskCLIP+[ECCV22][40] 73.6 31.1 18.0 APro(Ours) ViT-B/16 75.1↑1.5 32.6↑1.5 19.5↑1.5 Quantitative Results. Ta- ble A1 compares our ap- proach with MaskCLIP+ [40] for annotaion-free semantic segmentation. We have the following observations. • Pascal VOC2012 [51] val. With ResNet-50 as the im- age encoder in pretrained CLIP model, our approach outperforms MaskCLIP+ by 3.6% mIoU. With ResNet-50×16 and ViT-B/16 as the image encoders, our method surpasses MaskCLIP+ by 2.9% and 1.5% mIoU, respectively. • Pascal Context [60] val. Our proposed method outperforms MaskCLIP+ consistently with different image encoders (about +1.5% mIoU). • COCO-Stuff [61] val. COCO-Stuff consists of hundreds of semantic categories. Our method still brings +1.0%, +0.9% and +1.5% performance gains over MaskCLIP+ with ResNet-50, ResNet- 50×16 and ViT-B/16 image encoders, respectively. 8only GP only LP APro Figure 4: Qualitative results with different pairwise affinity terms on weakly supervised instance segmentation. Our method only with GP process preserves details without local consistency, while the model with only LP process encourages the local smoothness without topology-wise details. Our APro approach yields high-quality predictions with fine-grained details. 4.4 Diagnostic Experiments For in-depth analysis, we conduct ablation studies on Pascal VOC [ 51] upon the weakly box- supervised instance segmentation task. Table 4: Effects of unary and pairwise terms. UnaryGlobal PairwiseLocal PairwiseAP AP50 AP75 ✓ 25.9 57.0 20.4 ✓ ✓ 36.3 63.9 37.0 ✓ ✓ 36.0 64.3 35.6 ✓ ✓ ✓ 38.4 65.4 39.8 Unary and Pairwise Terms. Table 4 shows the evaluation results with different unary and pairwise terms. When using the unary term only, our method achieves 25.9% AP. When the global pairwise term is employed, our method achieves a much better performance of 36.3% AP. Using the local pairwise term only, our method obtains 36.0% AP. When both the global and local pairwise terms are adopted, our method achieves the best performance of 38.4% AP. Table 5: Comparisons with tree-based methods. Method AP AP50 AP75 TreeFilter [19] 36.1 63.5 36.1 TreeFilter [19] + Local Pairwise 36.8 64.4 36.5 Global + Local Pairwise (Ours) 38.4 65.4 39.8 Tree-based Long-range Affinity Modeling. The previous works [ 19, 7] explore tree- based filters for pairwise relationship model- ing. Table 5 compares our method with them. TreeFilter can capture the relationship with dis- tant nodes to a certain extent (see Fig. 3). Directly using TreeFilter as the pairwise term leads to 36.1% AP. By combining TreeFilter with our local pairwise term, the model obtains 36.8%AP. In comparison, our proposed approach achieves 38.4% AP. Table 6: Comparisons on local pairwise affinity modeling. LP(Ours) MeanField[10] Iteration AP Iteration AP 10 35.8 20 35.2 20 36.0 30 35.5 30 35.7 50 35.5 50 35.6 100 35.9 Iterated Local Affinity Modeling. We evaluate our local affinity propagation (LP) with different iterations, and compare it with the classical MeanField method [10, 42]. Table 6 reports the compar- ison results. Our APro with the LP process achieves 36.0% AP after 20 iterations. However, replacing our local affinity propa- gation with MeanFiled-based method [10] costs 100 iterations to obtain 35.9% AP. This indicates that our LP method possesses the attribute of fast convergence. Table 7: Generation of soft pseudo labels. Method AP AP50 AP75 GP-LP-C 36.8 63.7 37.8 LP-GP-C 37.7 65.1 39.1 GP-LP-P 38.4 65.4 39.8 Soft Pseudo-label Generation. With the formulated GP and LP methods, we study how to integrate them to generate the soft pseudo- labels in Table 7. We can cascade GP and LP sequentially to refine the pseudo labels. Putting GP before LP (denoted as GP-LP-C) achieves 36.8% AP, and putting LP before GP (denoted as LP- GP-C) performs better with 37.7% AP. In addition, we can use GP and LP in parallel (denoted as GP-LP-P) to produce two pseudo labels, and employ both of them to optimize the segmentation network with L1 distance. Notably, GP-LP-P achieves the best performance with 38.4% mask AP. This indicates that our proposed affinity propagation in global and local views are complementary for optimizing the segmentation network. 9Table 8: Average runtime (ms) with and without the ef- ficient implementation. Effic. Imple.Ave. Runtime ✗ 4.3×103 ✓ 0.8 Runtime Analysis. We report the average runtime of our method in Table 8. The experiment is conducted on a single GeForce RTX 3090 with batch size 1. Here we report the average runtime for one GP process duration of an epoch on the Pascal VOC dataset. When directly using Breadth First Search for each node with N times, the runtime is 4.3×103 ms with O(N2) time complexity. While employing the proposed efficient implementation, the runtime is only 0.8 ms with O(N log N) time complexity. This demonstrates that the proposed efficient implementation reduces the computational complexity dramatically. 5 Conclusion In this work, we proposed a novel universal component for weakly-supervised segmentation by formulating it as an affinity propagation process. A global and a local pairwise affinity term were introduced to generate the accurate soft pseudo labels. An efficient implementation with the light computational overhead was developed. The proposed approach, termed as APro, can be embedded into the existing segmentation networks for label-efficient segmentation. Experiments on three typical label-efficient segmentation tasks, i.e., box-supervised instance segmentation, point/scribble- supervised semantic segmentation and CLIP-guided annotation-free semantic segmentation, proved the effectiveness of proposed method. Acknowledgments This work is supported by National Natural Science Foundation of China under Grants (61831015). It is also supported by the Information Technology Center and State Key Lab of CAD&CG, Zhejiang University. References [1] Wei Shen, Zelin Peng, Xuehui Wang, Huayu Wang, Jiazhong Cen, Dongsheng Jiang, Lingxi Xie, Xiaokang Yang, and Q Tian. A survey on label-efficient deep image segmentation: Bridging the gap between weak supervision and dense prediction. TPAMI, 2023. 1, 2 [2] Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li Fei-Fei. What’s the point: Semantic segmenta- tion with point supervision. In ECCV, pages 549–565, 2016. 1, 2, 7 [3] Junsong Fan, Zhaoxiang Zhang, and Tieniu Tan. Pointly-supervised panoptic segmentation. In ECCV, pages 319–336, 2022. 1, 3 [4] Wentong Li, Yuqian Yuan, Song Wang, Jianke Zhu, Jianshu Li, Jian Liu, and Lei Zhang. Point2mask: Point-supervised panoptic segmentation via optimal transport. In ICCV, pages 572–581, 2023. 1, 3 [5] Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised convolutional networks for semantic segmentation. In CVPR, pages 3159–3167, 2016. 1, 2, 7 [6] Meng Tang, Federico Perazzi, Abdelaziz Djelouah, Ismail Ben Ayed, Christopher Schroers, and Yuri Boykov. On regularized losses for weakly-supervised cnn segmentation. In ECCV, pages 507–522, 2018. 1, 2, 7 [7] Zhiyuan Liang, Tiancai Wang, Xiangyu Zhang, Jian Sun, and Jianbing Shen. Tree energy loss: Towards sparsely annotated semantic segmentation. In CVPR, pages 16907–16916, 2022. 1, 2, 3, 6, 7, 9, 16, 17, 18 [8] Zhi Tian, Chunhua Shen, Xinlong Wang, and Hao Chen. Boxinst: High-performance instance segmentation with box annotations. In CVPR, pages 5443–5452, 2021. 1, 2, 3, 4, 5, 6, 7, 16, 17 [9] Wentong Li, Wenyu Liu, Jianke Zhu, Miaomiao Cui, Xian-Sheng Hua, and Lei Zhang. Box-supervised instance segmentation with level set evolution. In ECCV, pages 1–18, 2022. 1, 2, 5, 6, 7, 16 [10] Shiyi Lan, Xitong Yang, Zhiding Yu, Zuxuan Wu, Jose M Alvarez, and Anima Anandkumar. Vision transformers are good mask auto-labelers. In CVPR, 2023. 1, 2, 3, 4, 5, 6, 7, 9, 17 [11] Wentong Li, Wenyu Liu, Jianke Zhu, Miaomiao Cui, Risheng Yu, Xiansheng Hua, and Lei Zhang. Box2mask: Box-supervised instance segmentation via level-set evolution.arXiv preprint arXiv:2212.01579, 2022. 1, 2, 6, 16 [12] Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation. In CVPR, pages 4981–4990, 2018. 1, 2 10[13] Jiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly supervised learning of instance segmentation with inter-pixel relations. In CVPR, pages 2209–2218, 2019. 1 [14] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learning affinity from attention: end-to-end weakly- supervised semantic segmentation with transformers. In CVPR, pages 16846–16855, 2022. 1, 2 [15] Jinlong Li, Zequn Jie, Xu Wang, Xiaolin Wei, and Lin Ma. Expansion and shrinkage of localization for weakly-supervised semantic segmentation. In NeurIPS, 2022. 1, 2 [16] Cheng-Chun Hsu, Kuang-Jui Hsu, Chung-Chi Tsai, Yen-Yu Lin, and Yung-Yu Chuang. Weakly supervised instance segmentation using the bounding box tightness prior. In NeurIPS, volume 32, 2019. 1, 2, 3, 5, 7 [17] Shiyi Lan, Zhiding Yu, Christopher Choy, Subhashree Radhakrishnan, Guilin Liu, Yuke Zhu, Larry S Davis, and Anima Anandkumar. Discobox: Weakly supervised instance segmentation and semantic correspondence from box supervision. In ICCV, pages 3406–3416, 2021. 1, 2, 3, 5, 7 [18] Paul Vernaza and Manmohan Chandraker. Learning random-walk label propagation for weakly-supervised semantic segmentation. In CVPR, pages 7158–7166, 2017. 1 [19] Lin Song, Yanwei Li, Zeming Li, Gang Yu, Hongbin Sun, Jian Sun, and Nanning Zheng. Learnable tree filter for structure-preserving feature transform. In NeurIPS, volume 32, 2019. 2, 3, 4, 7, 8, 9 [20] Joseph B Kruskal. On the shortest spanning subtree of a graph and the traveling salesman problem. Proceedings of the American Mathematical society, 7(1):48–50, 1956. 2, 3, 4 [21] Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and Nanning Zheng. Rethinking learnable tree filter for generic feature transform. In NeurIPS, volume 33, pages 3991–4002, 2020. 2 [22] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In ICML, pages 912–919, 2003. 2 [23] Xiaojin Zhu. Semi-supervised learning literature survey. 2005. 2 [24] Alexander Vezhnevets and Joachim M Buhmann. Towards weakly supervised semantic segmentation by means of multiple instance and multitask learning. In CVPR, pages 3249–3256, 2010. 2 [25] Deepak Pathak, Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully convolutional multi-class multiple instance learning. arXiv preprint arXiv:1412.7144, 2014. 2 [26] Deepak Pathak, Philipp Krahenbuhl, and Trevor Darrell. Constrained convolutional neural networks for weakly supervised segmentation. In ICCV, pages 1796–1804, 2015. 2 [27] Pedro O Pinheiro and Ronan Collobert. From image-level to pixel-level labeling with convolutional networks. In CVPR, pages 1713–1721, 2015. 2 [28] George Papandreou, Liang-Chieh Chen, Kevin P Murphy, and Alan L Yuille. Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation. In ICCV, pages 1742–1750, 2015. 2 [29] Ruihuang Li, Chenhang He, Yabin Zhang, Shuai Li, Liyi Chen, and Lei Zhang. Sim: Semantic-aware instance mask generation for box-supervised instance segmentation. In CVPR, 2023. 2 [30] Tianheng Cheng, Xinggang Wang, Shaoyu Chen, Qian Zhang, and Wenyu Liu. Boxteacher: Exploring high-quality pseudo labels for weakly supervised instance segmentation. In CVPR, 2023. 2 [31] Rui Yang, Lin Song, Yixiao Ge, and Xiu Li. Boxsnake: Polygonal instance segmentation with box supervision. In ICCV, 2023. 2 [32] Bowen Cheng, Omkar Parkhi, and Alexander Kirillov. Pointly-supervised instance segmentation. In CVPR, pages 2617–2626, 2022. 2 [33] Chufeng Tang, Lingxi Xie, Gang Zhang, Xiaopeng Zhang, Qi Tian, and Xiaolin Hu. Active pointly- supervised instance segmentation. In ECCV, pages 606–623, 2022. 2 [34] Hongjun Chen, Jinbao Wang, Hong Cai Chen, Xiantong Zhen, Feng Zheng, Rongrong Ji, and Ling Shao. Seminar learning for click-level weakly supervised semantic segmentation. In ICCV, pages 6920–6929, 2021. 2 [35] Bingfeng Zhang, Jimin Xiao, Jianbo Jiao, Yunchao Wei, and Yao Zhao. Affinity attention graph neural network for weakly supervised semantic segmentation. TPAMI, 44(11):8082–8096, 2021. 2, 3 [36] Dmitrii Marin, Meng Tang, Ismail Ben Ayed, and Yuri Boykov. Beyond gradient descent for regularized segmentation losses. In CVPR, pages 10187–10196, 2019. 2, 7 [37] Yun Liu, Yu-Huan Wu, Peisong Wen, Yujun Shi, Yu Qiu, and Ming-Ming Cheng. Leveraging instance-, image-and dataset-level information for weakly supervised instance segmentation. TPAMI, 44(3):1415– 1428, 2020. 2 11[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748–8763, 2021. 2, 8 [39] Yuqi Lin, Minghao Chen, Wenxiao Wang, Boxi Wu, Ke Li, Binbin Lin, Haifeng Liu, and Xiaofei He. Clip is also an efficient segmenter: A text-driven approach for weakly supervised semantic segmentation. In CVPR, 2023. 3 [40] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In ECCV, pages 696–712, 2022. 3, 8, 16, 18 [41] Yanwei Li, Hengshuang Zhao, Xiaojuan Qi, Yukang Chen, Lu Qi, Liwei Wang, Zeming Li, Jian Sun, and Jiaya Jia. Fully convolutional networks for panoptic segmentation with point-based supervision. TPAMI, 2022. 3 [42] Philipp Krähenbühl and Vladlen Koltun. Efficient inference in fully connected crfs with gaussian edge potentials. In NeurIPS, volume 24, 2011. 3, 5, 9 [43] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI, 40(4):834–848, 2017. 3, 6, 7 [44] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip HS Torr. Conditional random fields as recurrent neural networks. In ICCV, pages 1529–1537, 2015. 3 [45] Anton Obukhov, Stamatios Georgoulis, Dengxin Dai, and Luc Van Gool. Gated crf loss for weakly supervised semantic image segmentation. In NeurIPS, 2019. 3, 4 [46] William T Freeman, Egon C Pasztor, and Owen T Carmichael. Learning low-level vision. IJCV, 40:25–47, 2000. 3 [47] Qingxiong Yang. Stereo matching using tree filtering. IEEE TPAMI, 37(4):834–846, 2014. 4 [48] Robert Endre Tarjan. A class of algorithms which require nonlinear time to maintain disjoint sets. Journal of computer and system sciences, 18(2):110–127, 1979. 5 [49] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740–755, 2014. 5, 6, 7, 16 [50] Bharath Hariharan, Pablo Arbeláez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In ICCV, pages 991–998, 2011. 6 [51] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 88(2):303–338, 2010. 6, 7, 8, 9, 16, 17 [52] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast instance segmentation. In NeurIPS, volume 33, pages 17721–17732, 2020. 6, 17 [53] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked- attention mask transformer for universal image segmentation. In CVPR, pages 1290–1299, 2022. 6 [54] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 6, 7 [55] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pages 10012–10022, 2021. 6 [56] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019. 6, 16 [57] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet: large scale visual recognition challenge. IJCV, 115:211–252, 2015. 6, 7 [58] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In CVPR, pages 2918–2928, 2021. 6 [59] Meng Tang, Abdelaziz Djelouah, Federico Perazzi, Yuri Boykov, and Christopher Schroers. Normalized cut loss for weakly-supervised cnn segmentation. In CVPR, pages 1818–1827, 2018. 7 [60] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, pages 891–898, 2014. 8, 16 12[61] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In CVPR, 2018. 8, 16 [62] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2020. 8 [63] MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. https://github.com/open-mmlab/mmsegmentation, 2020. 16 [64] openseg.pytorch Contributors. openseg.pytorch. https://github.com/openseg-group/openseg. pytorch, 2020. 16 [65] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 16 13Supplementary Material In this document, we provide more details, additional experimental results and discussions on our approach. The supplementary material is organized as follows: • §A: more details on the efficient implementation; • §B: additional graphical illustration; • §C: more performance comparisons; • §D: additional visualization results; • §E: discussions. A More Details on the Efficient Implementation. In this section, we first present the proofs of our claims about transmission cost and lazy propagation in our proposed lazy update algorithm. Then, we provide the pseudo-code of the find function in Algorithm 1 of the main paper. The symbols in this document follow the same definitions as the main paper. A.1 Proofs on Transmission Cost and Lazy Propagation Lemma 1. Given edge E(k,l) in GT with edge weight wk,l, ∀a ∈ Uk, b ∈ Ul, the transmission cost between vertex a and b is wk,l. Proof. Since there are no loops in the tree, the shortest path between any two vertices is unique. Therefore, there exists a path a−k in Uk that connects vertices a and k, and a path b−l that connects b and l in Ul. When connecting unions Uk and Ul through edge E(k,l), there is exactly a single path connecting a and b, denoted as a−k−l−b. As the weight w is sorted in ascending order, for any edge Ei with wi between a−k in Uk, we have wi ≤ wk,l. The same conclusion applies to l−b. Hence, the maximum weight in path a−k−l−b is wk,l. Consequently, once k and l are connected, wk,l is equivalent to the transmission cost for all nodes within Uk and Ul. Lemma 2. When connecting vertices k and l, lazy tags Z(δ)k∗ and Z(δ)l∗ can be updated as follows: Z(δ)k∗ = Z(δ)k∗ + \u001aexp(−wk,l/ζg 2)S(δ)l Uk.rank > Ul.rank, exp(−wk,l/ζg 2)S(δ)l − Z(δ)l∗ otherwise. (9) Proof. Given a ∈ Uk, for ∀b ∈ Ul, the transmission cost between a and b is wk,l. We have: ∆LP rop(δ)a = X i∈Ul (exp(−wk,l/ζg 2)δi) = exp(−wk,l/ζg 2)S(δ)l. (10) First, let Uk.rank > Ul.rank. When merging unions Uk and Ul, we choose k∗ as the root node and let l∗ be its descendant. There is: ∆LP rop(δ)a = ∆Z(δ)k∗, (11) ∴ ∆Z(δ)k∗ = exp(−wk,l/ζg 2)S(δ)l. (12) Second, let Uk.rank ≤ Ul.rank. When merging unions Uk and Ul, we instead choose l∗ as the root node and let k∗ be its descendant. Then we have: ∆LP rop(δ)a = Z(δ)l∗ + ∆Z(δ)k∗ = exp(−wk,l/ζg 2)S(δ)l, (13) 14∴ ∆Z(δ)k∗ = exp(−wk,l/ζg 2)S(δ)l − Z(δ)l∗. (14) A.2 Pseudo Code The pseudo-code of the find function is shown in Algorithm A1, which finds the root rode with Path Compression. Algorithm A1: Pseudo-code of the find function with Path Compression /* fa: the parent of the i-th node, shape: (N) tag: the lazy tag of numbers ptag: the lazy tag of predictions */ int find(int x){ /* x: the node index to query return: the root node of x */ int fx = fa[x]; if(fx == x) return x; fa[x] = find(fx); // Path Compression if(fa[x] != fx){ tag[x] += tag[fx]; // Downlink lazy tag ptag[x] += ptag[fx]; } return fa[x]; } B Additional Graphical Illustration To facilitate a better comprehension, we provide a detailed graphical illustration in Fig. A1 to describe our global affinity propagation process. Initially, an input image is represented as a 4-connected planar graph. Subsequently, the Minimum Spanning Tree (MST) is constructed based on the edge weights to obtain the tree-based graph GT . ψg(xi, xj) is calculated as exp(−d), where d is the maximum value along the path Ei,j from node xi to node xj. This pairwise similarity ψg(xi, xj) is then multiplied by the unary term to obtain soft pseudo predictions. Note that Fig. A1 serves purely as a visual illustration of our method. In the implementation, it is unnecessary to compute as it explicitly. As detailed in Section 3.3 of main paper, we alternatively design a lazy propagation scheme to efficiently update these values. 4-connected planar graph edge(k, l) weight: Input image 1 2 5 4 1 4 2 3 2 3 4 7 6 3 2 8 9 8 75 5 63 4 MST 1 2 1 3 2 3 2 32 7 6 5 3 5 7 x0 1 e-1 e-2 e-3 e-3 e-1 e-2 e-3 e-3 e-3 e-3 e-5 e-7 e-6 e-5 e-7 1 1 2 3 2 x0 x3 Figure A1: The graphical illustration of the detailed process of global affinity propagation. In the green dashed box, we present the calculation of ψg(x0, x3) as a simple example. 15C More Performance Comparisons For annotation-free semantic segmentation with pretrained CLIP model, Key Smoothing (KS) pro- posed in MaskCLIP [40] also aims to realize the global affinity propagation. To better explore their efforts, we conduct detailed comparisons between KS and our APro method based on training-free MaskCLIP [40]. The experimental results are shown in Table A1. Table A1: Quantitative results on Pascal Context [60] val and COCO-Stuff [61] val with mean IoU (%). Method CLIP Model Context COCO. MaskCLIP [40] 18.46 10.17 +KS 21.0 12.42 +APro(Ours) ResNet-50 21.67 12.70 MaskCLIP [40] 21.57 13.55 +KS 22.65 15.50 +APro(Ours) ResNet-50×16 24.03 16.30 MaskCLIP [40] 21.68 12.51 +KS 23.87 13.79 +KS+PD 25.45 14.62 +APro(Ours) 28.91 16.69 +APro(Ours) +PD ViT-B/16 29.42 16.71 Both KS and our APro method bring per- formance gains. Compared with KS, APro achieves better performance with different CLIP-based models. Especially, for ViT- B/16 model, our approach outperforms KS by +5.04% mIoU on Pascal Context and +2.90% mIoU on COCO, repectively. Equipped with Prompt Denoising (PD), the models could achieve further improvements. We have the following further discussions: KS relies on the calculation of key feature similar- ities, which predominantly stems from high- level features of CLIP and computes pairwise terms within each pair of patches. Compared with KS of MaskCLIP, our method is built on a tree-based graph derived from low-level images, which is capable of capturing finer topological details. D Additional Visualization Results To further show the performance of our proposed APro approach, we provide more visualization results. Fig. A2 shows the qualitative comparisons with the state-of-the-art methods upon box- supervised instance segmentation task [8, 9, 11]. It can be seen that our proposed APro approach is able to generate more accurate boundaries. For weakly-supervised semantic segmentation, we compare our method with the prior art TEL [7] upon point-wise supervision in Fig. A3. APro captures the fine-grained details of objects with the fitting boundaries. As for CLIP-guided annotation-free semantic segmentation, Fig. A4 provides the comparison results with MaskCLIP+ [ 40]. It can be observed that our approach eliminates the noisy predictions from the pretrained CLIP model effectively, achieving high-quality mask predictions. In addition, Fig. A5 provides the qualitative results of our method on general COCO dataset. E Discussions Asset License and Consent. We use four image segmentation datasets, i.e., COCO [ 49], Pascal VOC 2012 [ 51], COCO-Stuff [ 61] and Pascal Context [ 60], which are all pub- licly and freely available for academic research. We implement all models with MMDetection [ 56], MMSegmentation [ 63] and openseg.pytorch [ 64] codebases. COCO (https://cocodataset.org/) is released under the CC BY 4.0. Pascal VOC 2012 (http://host.robots.ox.ac.uk/pascal/VOC/voc2012/) is released under the Flickr Terms of use for images. COCO-Stuff v1.1 ( https://github.com/nightrome/cocostuff) is released under the Flickr Terms of use for images and the CC BY 4.0 for annota- tions. MMDetection ( https://github.com/open-mmlab/mmdetection) and MMSegmen- tation ( https://github.com/open-mmlab/mmsegmentation) codebases are released under the Apache-2.0 license. Openseg.pytorch ( https://github.com/openseg-group/openseg. pytorch) codebase is released under the MIT license. Limitations. The presented affinity propagation method is performed under the guidance of the similarities of image intensity and color. Our proposed method may have difficulties in accurately capturing the pairwise affinities under the challenging scenarios like motion blur, occlusions, and cluttered scenes, etc. Actually, this is a common problem for many segmentation methods. In the future work, we will explore how to integrate our method into the large-scale foundation models, such as SAM [65], to take advantage of their strong features for more promising segmentation results. 16Broader Impact. This work presents an effective component for weakly-supervised segmentation with label-efficient annotations. We have demonstrated its effectiveness over three typical label- efficient segmentation tasks. On the positive side, our approach has the potential to benefit a wide variety of real-world applications, such as autonomous vehicles, medical imaging, remote sensing and image editing, which can significantly reduce the labeling costs. On the other side, erroneous predictions in real-world applications (i.e., medical imaging analysis and tasks involving autonomous vehicles) raise the safety issues of human beings. In order to avoid the potentially negative effects, we suggest to adopt a highly stringent security protocol in case that our approach fails to function properly in real-world applications. APro(Ours)CRF LossTreeEnergyLossPairwise Loss Figure A2: Qualitative comparisons on Pascal VOC [51]. We compare our APro approach with CRF loss [10], TreeEnergy loss [7] and Pairwise loss [8] under the SOLOv2 [52] framework. Our method obtains more fine-grained predictions with detail preserved. 17Image Ground Truth APro(Ours) TEL Figure A3: Qualitative comparisons on point-supervised semantic segmentation. Compared with the state-of-the-art TEL [7], our method segments objects with more accurate boundaries. MaskCLIP+ APro(Ours) MaskCLIP+ APro(Ours) Figure A4: Visual comparison results on Pascal Context with ViT-B/16 image encoder. Compared with the prior art MaskCLIP+ [ 40], our method obtains more accurate predictions with fitting boundaries. 18Figure A5: Qualitative results of our APro on COCO with ResNet-101 under the SOLOv2 framework upon box-supervised instance segmentation. 19",
      "references": [
        "A survey on label-efficient deep image segmentation: Bridging the gap between weak supervision and dense prediction.",
        "What’s the point: Semantic segmentation with point supervision.",
        "Pointly-supervised panoptic segmentation.",
        "Point2mask: Point-supervised panoptic segmentation via optimal transport.",
        "Scribblesup: Scribble-supervised convolutional networks for semantic segmentation.",
        "On regularized losses for weakly-supervised cnn segmentation.",
        "Tree energy loss: Towards sparsely annotated semantic segmentation.",
        "Boxinst: High-performance instance segmentation with box annotations.",
        "Box-supervised instance segmentation with level set evolution.",
        "Vision transformers are good mask auto-labelers.",
        "Box2mask: Box-supervised instance segmentation via level-set evolution.",
        "Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation.",
        "Weakly supervised learning of instance segmentation with inter-pixel relations.",
        "Learning affinity from attention: end-to-end weakly-supervised semantic segmentation with transformers.",
        "Expansion and shrinkage of localization for weakly-supervised semantic segmentation.",
        "Weakly supervised instance segmentation using the bounding box tightness prior.",
        "Discobox: Weakly supervised instance segmentation and semantic correspondence from box supervision.",
        "Learning random-walk label propagation for weakly-supervised semantic segmentation.",
        "Learnable tree filter for structure-preserving feature transform.",
        "On the shortest spanning subtree of a graph and the traveling salesman problem.",
        "Rethinking learnable tree filter for generic feature transform.",
        "Semi-supervised learning using gaussian fields and harmonic functions.",
        "Semi-supervised learning literature survey.",
        "Towards weakly supervised semantic segmentation by means of multiple instance and multitask learning.",
        "Fully convolutional multi-class multiple instance learning.",
        "Constrained convolutional neural networks for weakly supervised segmentation.",
        "From image-level to pixel-level labeling with convolutional networks.",
        "Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation.",
        "Sim: Semantic-aware instance mask generation for box-supervised instance segmentation.",
        "Boxteacher: Exploring high-quality pseudo labels for weakly supervised instance segmentation.",
        "Boxsnake: Polygonal instance segmentation with box supervision.",
        "Pointly-supervised instance segmentation.",
        "Active pointly- supervised instance segmentation.",
        "Seminar learning for click-level weakly supervised semantic segmentation.",
        "Affinity attention graph neural network for weakly supervised semantic segmentation.",
        "Beyond gradient descent for regularized segmentation losses.",
        "Leveraging instance-, image-and dataset-level information for weakly supervised instance segmentation.",
        "Learning transferable visual models from natural language supervision.",
        "Clip is also an efficient segmenter: A text-driven approach for weakly supervised semantic segmentation.",
        "Extract free dense labels from clip.",
        "Fully convolutional networks for panoptic segmentation with point-based supervision.",
        "Efficient inference in fully connected crfs with gaussian edge potentials.",
        "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.",
        "Conditional random fields as recurrent neural networks.",
        "Gated crf loss for weakly supervised semantic image segmentation.",
        "Learning low-level vision.",
        "Stereo matching using tree filtering.",
        "A class of algorithms which require nonlinear time to maintain disjoint sets.",
        "Microsoft coco: Common objects in context.",
        "Semantic contours from inverse detectors.",
        "The pascal visual object classes (voc) challenge.",
        "Solov2: Dynamic and fast instance segmentation.",
        "Masked-attention mask transformer for universal image segmentation.",
        "Deep residual learning for image recognition.",
        "Swin transformer: Hierarchical vision transformer using shifted windows.",
        "Mmdetection: Open mmlab detection toolbox and benchmark.",
        "Imagenet: large scale visual recognition challenge.",
        "Simple copy-paste is a strong data augmentation method for instance segmentation.",
        "Normalized cut loss for weakly-supervised cnn segmentation.",
        "The role of context for object detection and semantic segmentation in the wild.",
        "Coco-stuff: Thing and stuff classes in context.",
        "An image is worth 16x16 words: Transformers for image recognition at scale.",
        "MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark.",
        "openseg.pytorch.",
        "Segment anything."
      ],
      "meta_data": {
        "arxiv_id": "2310.10533v2",
        "authors": [
          "Wentong Li",
          "Yuqian Yuan",
          "Song Wang",
          "Wenyu Liu",
          "Dongqi Tang",
          "Jian Liu",
          "Jianke Zhu",
          "Lei Zhang"
        ],
        "published_date": "2023-10-16T15:54:09Z",
        "github_url": "https://github.com/open-mmlab/mmsegmentation"
      },
      "llm_extracted_info": {
        "main_contributions": "We propose Affinity Propagation (APro), a universal, plug‑in component for label-efficient segmentation that jointly models global and local pairwise affinities to generate accurate soft pseudo labels for unlabeled pixels. The method combines a global affinity propagation (GP) on a topology-aware tree (minimum spanning tree) to capture long-range, topology-consistent relationships with a local affinity propagation (LP) based on local color/spatial similarity to enforce piecewise smoothness. An efficient lazy-update algorithm reduces the naive O(N^2) computation to O(N log N). APro can be embedded into existing segmentation networks and is validated across three tasks—box-supervised instance segmentation, point/scribble-supervised semantic segmentation, and CLIP-guided annotation-free semantic segmentation—achieving state‑of‑the‑art or competitive results on COCO, Pascal VOC, Pascal Context, and COCO-Stuff datasets.",
        "methodology": "Formulation: generate soft pseudo labels Y via affinity propagation yi = (1/zi) sum_j ϕ(xj) ψ(xi, xj) over regions τ, unifying global and local pairwise terms. Global affinity propagation uses a tree GT obtained by MST on a 4-connected image graph; ψg(xi,xj) = exp(- max_{(k,l) in path(i,j)} w_{k,l} / ζ_g^2), where w_{k,l} = ||Ik - Il||^2. Local affinity propagation uses a Gaussian kernel over local neighbors N(i): ψs(xi,xj) = exp(-||Ii - Ij||^2 / ζ_s^2). Unaries ϕ(xj) align predictions with sparse labels. The GP and LP are combined (GP-LP) to refine pseudo labels; two terms can be cascaded or computed in parallel (GP-LP-P) to maximize accuracy. Efficient implementation uses a lazy propagation with disjoint-set union to merge graph blocks in ascending edge weight, updating root-level lazy tags Z(δ) and propagating down, achieving O(N log N) per channel. The model is trained with a standard framework: Lg on labeled pixels and Lu on unlabeled pixels using L1 distance between pseudo labels and network predictions.",
        "experimental_setup": "Datasets and tasks: (1) Weakly supervised box‑based instance segmentation on COCO and Pascal VOC with SOLOv2 and Mask2Former backbones; baselines include Pairwise Loss, TreeEnergy Loss and CRF Loss. (2) Weakly supervised semantic segmentation on Pascal VOC2012 with point and scribble supervision using LTF as base model. (3) CLIP‑guided annotation-free semantic segmentation using MaskCLIP+ as base and evaluating on Pascal VOC2012, Pascal Context, and COCO-Stuff with ResNet-50, ResNet-50×16 and ViT‑B/16 image encoders. Evaluation metrics include mask AP for instance segmentation and mIoU for semantic segmentation. Ablation studies analyze unary vs global/local terms, compare with tree filters, assess iterations against MF, and explore GP/LP cascade and parallel variants. Runtime analysis shows substantial speedups (4.3e3 ms naive vs 0.8 ms with lazy propagation).",
        "limitations": "Relies on pixel intensity/color similarities and local textural cues; may struggle under challenging conditions such as motion blur, heavy occlusion, cluttered scenes, or poor initial unary cues; effectiveness may degrade if global topology is not well captured by MST; additional overhead for integrating the GP/LP machinery into some architectures, though the paper shows minimal impact on training time; current formulation focuses on 2D RGB/feature spaces and may need adaptation for other modalities.",
        "future_research_directions": "Explore integration with large-scale foundation models (e.g., SAM) to leverage richer features and robustness; extend APro to other dense prediction tasks (panoptic segmentation, depth estimation); investigate more robust affinity measures beyond color/intensity (e.g., learned or multimodal affinities); study dynamic graphs or end-to-end trainable MST construction; apply APro in video segmentation to model temporal affinities; combine with stronger CLIP or text-driven priors for annotation-free segmentation.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Efficient Testable Learning of Halfspaces with Adversarial Label Noise",
      "full_text": "arXiv:2303.05485v1  [cs.LG]  9 Mar 2023 Eﬃcient T estable Learning of Halfspaces with Adversarial Label Noise Ilias Diakonikolas ∗ UW Madison ilias@cs.wisc.edu Daniel M. Kane † UC San Diego dakane@ucsd.edu V asilis Kontonis‡ UW Madison kontonis@wisc.edu Sihan Liu UC San Diego sil046@ucsd.edu Nikos Zariﬁs § UW Madison zarifis@wisc.edu March 10, 2023 Abstract W e give the ﬁrst polynomial-time algorithm for the testable learning of halfspaces in the presence of adversarial label noise under the Gaussian dist ribution. In the recently introduced testable learning model, one is required to produce a tester -learner such that if the data passes the tester, then one can trust the output of the robust learne r on the data. Our tester-learner runs in time poly(d/ǫ) and outputs a halfspace with misclassiﬁcation error O(opt) + ǫ, where opt is the 0-1 error of the best ﬁtting halfspace. At a technical l evel, our algorithm employs an iterative soft localization technique enhanced with appro priate testers to ensure that the data distribution is suﬃciently similar to a Gaussian. ∗ Supported by NSF Medium Award CCF-2107079, NSF Award CCF-16 52862 (CAREER), a Sloan Research F ellowship, and a DARP A Learning with Less Labels (LwLL) grant. † Supported by NSF Award CCF-1553288 (CAREER) and a Sloan Research F ellowship. ‡ Supported in part by NSF Award CCF-2144298 (CAREER). § Supported in part by NSF Award CCF-1652862 (CAREER) and a DAR P A Learning with Less Labels (LwLL) grant.1 Introduction A (homogeneous) halfspace is a Boolean function h : Rd → {± 1} of the form hw(x) = sign ( w · x), where w ∈ Rd is the corresponding weight vector and the function sign : R → {± 1} is deﬁned as sign(t) = 1 if t ≥ 0 and sign(t) = −1 otherwise. Learning halfspaces from random labeled exampl es is a classical task in machine learning, with history going b ack to the Perceptron algorithm [ Ros58]. In the realizable P AC model [ V al84] (i.e., with consistent labels), the class of halfspaces is known to be eﬃciently learnable without distributional assumption s. On the other hand, in the agnostic (or adversarial label noise) model [ Hau92, KSS94] even weak learning is computationally intractable in the distribution-free setting [ Dan16, DKMR22, Tie22]. These intractability results have served as a motivation fo r the study of agnostic learning in the distribution-speciﬁc setting, i.e., when the marginal dis tribution on examples is assumed to be well- behaved. In this context, a number of algorithmic results ar e known. The L1-regression algorithm of [ KKMS08] agnostically learns halfspaces within near-optimal 0-1 e rror of opt+ ǫ, where opt is the 0-1 error of the best-ﬁtting halfspace. The running time of t he L1-regression algorithm is d ˜O(1/ǫ2) under the assumption that the marginal distribution on exam ples is the standard Gaussian (and for a few other structured distributions) [ DGJ+10, DKN10]. While the L1 regression method leads to improper learners, a proper agnostic learner with qualit atively similar complexity was recently given in [ DKK+21]. The exponential dependence on 1/ǫ in the running time of these algorithms is known to be inherent, in both the Statistical Query model [ DKZ20, GGK20, DKPZ21] and under standard cryptographic assumptions [ DKR23]. Interestingly , it is possible to circumvent the super-poly nomial dependence on 1/ǫ by relaxing the ﬁnal error guarantee — namely , by obtaining a hypothesis wit h 0-1 error f(opt)+ǫ, for some function f(t) that goes to 0 when t → 0. (V anilla agnostic learning corresponds to the case that f(t) = t.) A number of algorithmic works, starting with [ KLS09], developed eﬃcient algorithms with relaxed error guarantees; see, e.g., [ ABL17, Dan15, DKS18, DKTZ20b]. The most relevant results in the context of the current paper are the works [ ABL17, DKS18] which gave poly(d/ǫ) time algorithms with error Copt + ǫ, for some universal constant C > 1, for learning halfspaces with adversarial label noise under the Gaussian distribution. Given the afor ementioned computational hardness results, these constant-factor approximations are best po ssible within the class of polynomial-time algorithms. A drawback of distribution-speciﬁc agnostic learning is th at it provides no guarantees if the assumption on the marginal distribution on examples is not s atisﬁed. Ideally , one would additionally like an eﬃcient method to test these distributional assumptions, so that: (1) if our teste r accepts, then we can trust the output of the learner, and (2) it is unlik ely that the tester rejects if the data satisﬁes the distributional assumptions. This state- of-aﬀairs motivated the deﬁnition of a new model — introduced in [ R V22] and termed testable learning — formally deﬁned below: Deﬁnition 1.1 (T estable Learning with Adversarial Label Noise [ R V22]). Fix ǫ, τ ∈ (0, 1] and let f : [0 , 1] ↦→R+. A tester-learner A (approximately) testably learns a concept class C with respect to the distribution Dx on Rd with N samples and failure probability τ if for any distribution D on Rd × {±1}, the tester-learner A draws a set S of N i.i.d. samples from D and either rejects S or accepts S and produces a hypothesis h : Rd ↦→ {±1}. Moreover, the following conditions must be met: • (Completeness) If D truly has marginal Dx, A accepts with probability at least 1 − τ. • (Soundness) The probability that A accepts and outputs a hypothesis h for which Pr(x,y)∼D[h(x) ̸= y] > f (opt) + ǫ , where opt := min g∈C Pr(x,y)∼D[g(x) ̸= y] is at most τ. 1The probability in the above statements is over the randomne ss of the sample S and the internal randomness of the tester-learner A. The initial work [ R V22] and the followup paper [ GKK22] focused on the setting where f(t) = t (i.e., achieving optimal error of opt + ǫ). These works developed general moment-matching based algorithms that yield testable learners for a range of conce pt classes, including halfspaces. F or the class of halfspaces in particular, they gave a testable agno stic learner under the Gaussian distribution with sample complexity and runtime d ˜O(1/ǫ2) — essentially matching the complexity of the problem in the standard agnostic P AC setting (without the testable r equirement). Since the testable learning setting is at least as hard as the standard P AC setting, the af orementioned hardness results imply that the exponential complexity dependence in 1/ǫ cannot be improved. In this work, we continue this line of investigation. W e ask w hether we can obtain fully poly- nomial time testable learning algorithms with relaxed error guarantee s — ideally matching the standard (non-testable) learning setting. Concretely , we study the following question: Is there a poly(d/ǫ) time tester-learner for halfspaces with error f(opt) + ǫ? Speciﬁcally, is there a constant-factor approximation? As our main result, we provide an aﬃrmative answer to this que stion in the strongest possible sense — by providing an eﬃcient constant-factor approximate test er-learner. Main Result Our main result is the ﬁrst polynomial-time tester-learner for homogeneous halfs- paces with respect to the Gaussian distribution in the prese nce of adversarial label noise. F ormally , we establish the following theorem: Theorem 1.2 (T estable Learning Halfspaces under Gaussian Marginals) . Let ǫ, τ ∈ (0, 1) and C be the class of homogeneous halfspaces on Rd. There exists a poly(d, 1/ǫ) log(1/τ)-time tester-learner for C with respect to N (0, I) up to 0-1 error O(opt) + ǫ, where opt is the 0-1 error of the best ﬁtting function in C and τ is the failure probability. Before we provide an overview of our technical approach, some remarks are in order. Theorem 1.2 gives the ﬁrst algorithm for testable learning of halfspace s that runs in poly(d/ǫ) time and achieves dimension-independent error (i.e., error of the form f(opt) + ǫ, where f satisﬁes limt→0 f(t) = 0.) Moreover, the constant-factor approximation achieved i s best possible, matching the known guarantees without the testable requirement and complexit y lower bounds. Prior to our work, the only known result in the testable setting, due to [ R V22, GKK22], achieves error opt + ǫ with complexity dpoly(1/ǫ). A novel (and seemingly necessary) feature of our approach i s that the testing components of our algorithm depend on the labels (as opposed to the label-oblivious testers of [R V22, GKK22]). As will be explained in the proceeding discussion, to pro ve Theorem 1.2 we develop a testable version of the well-known localization t echnique that may be of broader interest. Independent W ork In concurrent and independent work, [ GKSV23] gave an eﬃcient tester- learner for homogeneous halfspaces under the Gaussian dist ribution (and strongly log-concave dis- tributions) achieving dimension-independent error guara ntees. Speciﬁcally , their algorithm achieves 0-1 error O(k1/2opt1−1/k) with sample complexity and running time of poly(d ˜O(k), (1/ǫ) ˜O(k)). That is, they obtain error O(optc), where c < 1 is a universal constant, in polyc(d/ǫ) time; and error ˜O(opt) in quasi-polynomial (d/ǫ)polylog(d) time. 21.1 Overview of T echniques Our tester-learner is based on the well-known localization technique that has been used in the context of learning halfspaces with noise; see, e.g., [ ABL17, DKS18]. At a high-level, the idea of localization hinges on updating a given hypothesis by usi ng “the most informative” examples, speciﬁcally examples that have very small margin with respe ct to the current hypothesis. Natu- rally , the correctness of this geometric technique leverag es structural properties of the underlying distribution over examples, namely concentration, anti-c oncentration, and anti-anti-concentration properties (see, e.g., [ DKTZ20a]). While the Gaussian distribution satisﬁes these propert ies, they are unfortunately hard to test. In this work, we show that loc alization can be eﬀectively combined with appropriate eﬃcient testing routines to provide an eﬃc ient tester-learner. Localization and a (W eak) Proper T estable Learner Assume that we are given a halfspace deﬁned by the unit vector w with small 0-1 error, namely Prx∼Dx [sign(v∗ · x) ̸= sign( w · x)] ≤ δ, for some small δ > 0, where v∗ is the unit vector deﬁning an optimal halfspace. The localiz ation approach improves the current hypothesis, deﬁned by w, by considering the conditional distribution D′ on the points that fall in a thin slice around w, i.e., the set of points x satisfying |w · x| ≤ O(δ). The goal is to compute a new (unit) weight vector w′ that is close to an optimal halfspace, deﬁned by v∗, with respect to D′, i.e., Prx′∼D′x [sign(v∗ · x′) ̸= sign( w′ · x′)] ≤ α, for an appropriate α > 0. W e can then show that the halfspace deﬁned by w′ will be closer to the target halfspace (deﬁned by v∗) with respect to the original distribution, i.e., we have that Prx∼Dx [sign(v∗ · x) ̸= sign( w′ · x)] ≤ O(δα). By repeating the above step, we iteratively reduce the disag reement with v∗ until we reach our target error of O(opt). Similarly to [ DKS18], instead of “hard” conditioning on a thin slice, we perform a “soft” localization step where (by rejection samp ling) we transform the x-marginal to a Gaussian whose covariance is O(δ2) in the direction of w and identity in the orthogonal directions, i.e., Σ = I − (1 − δ2)ww⊤; see Fact 3.3 . A crucial ingredient of our approach is a proper testable, weak agnostic learner with respect to the Gaussian distribution. More precisely , our tester-l earner runs in polynomial time and either reports that the x-marginal is not N (0, I) or outputs a unit vector w with small constant distance to the target v∗, i.e., ∥w − v∗∥2 ≤ 1/100; see Proposition 2.1 . Our weak proper tester-learner ﬁrst veriﬁes that the given x-marginal approximately matches constantly many low-degr ee moments with the standard Gaussian; and if it does, it returns the vector d eﬁned by the degree- 1 Chow parameters, i.e., c = E(x,y)∼D[yx]. Our main structural result in this context shows that if Dx approximately matches its low-degree moments with the standard Gaussian, then the Chow parameters of any homogeneous L TF with respect to Dx are close to its Chow parameters with respect to N (0, I), i.e., for any homogeneous L TF f(x), we have that Ex∼Dx [f(x)x] ≈ Ex∗∼N (0,I)[f(x∗)x∗]; see Lemma 2.3 . Since the Chow vector of a homogeneous L TF with respect to the Gaussian distribution is parallel to its normal vector v∗ (see Fact 2.2 ), it is not hard to show that the Chow vector of the L TF with respect to Dx will not be very far from v∗ and will satisfy the (weak) learning guarantee of ∥c−v∗∥2 ≤ 1/100. Finally , to deal with label noise, we show that if x′ has bounded second moments (a condition that we can eﬃciently test), we can robustly est imate Ex∼Dx [f(x)x] with samples from D up to error O(√opt) (see Lemma 2.7 ), which suﬃces for our purpose of weak learning. The detailed description of our weak, proper tester-learner ca n be found in Section 2 . F rom Parameter Distance to Zero-One Error Having a (weak) testable proper learner, we can now use it on the localized (conditional) distribution D′ and obtain a vector w′ that is closer to v∗ in ℓ2 distance; see Lemma 3.4 . However, our goal is to obtain a vector that has small zero-o ne disagreement with the target halfspace v∗. Assuming that the underlying x-marginal is a standard 3Θ( i) v∗ uδ δ δ w θ = Θ( δ) Figure 1: The disagreement region between a halfspace with n ormal vector w and the target v∗ is shown in green. The unit direction u corresponds to the projection of v∗ on the orthogonal complement of w. W e assume that the ℓ2 distance of the two halfspaces is δ (and thus their angle is Θ( δ)). Since the slabs Si = {iδ ≤ | x · w| ≤ (i + 1)δ} have width δ, the x-coordinate of the start of the i-th box is Θ( i). normal distribution, and that ∥w−v∗∥2 = δ, it holds that Prx∼Dx [sign(w·x) ̸= sign( v∗ ·x)] = O(δ), which implies that achieving ℓ2-distance O(opt)+ǫ suﬃces. W e give an algorithm that can eﬃciently either certify that small ℓ2-distance implies small zero-one disagreement with respec t to the given marginal Dx or declare that Dx is not the standard normal. The disagreement region of v∗ and w is a union of two “wedges” (intersection of two halfspaces); see Figure 1 . In order for our algorithm to work, we need to verify that the se wedges do not contain too much probability mass. Similarly to our approach for the weak tester-leaner, one could try a moment-matching approach and argue that if Dx matches its “low”-degree moments with N (0, I), then small ℓ2-distance translates to small zero-one disagreement. Howe ver, we will need to use this result for vectors that are very close to the target (but stil l not close enough), namely ∥w−v∗∥2 = δ, where δ = Θ( ǫ); this would require matching poly(1/δ) many moments (as we essentially need to approximate the wedge of Figure 1 with a polynomial) and would thus lead to an exponential runtime of dpoly(1/δ). Instead of trying to approximate the disagreement region wi th a polynomial, we will make use of the fact that our algorithm knows w (but not v∗) and approximate the disagreement region by a union of cylindrical slabs. W e consider slabs of the form Si = {x : iδ ≤ | w · x| ≤ (i + 1) δ}. If the target distribution is Gaussian, we know that the set |w · x| ≫ √ log(1/ǫ) has mass O(δ) and we can essentially ignore it. Therefore, we can cover the whole space by considering roughly M = O( √ log(1/δ)/δ) slabs of width δ and split the disagreement region into the disagreement region inside each slab Si. W e have that Pr x∼Dx [sign(w · x) ̸= sign( v∗ · x)] ≤ M∑ i=1 Pr[|u · x| ≥ i | x ∈ Si] Pr[Si] , where u is the unit direction parallel to the projection of the targe t v∗ onto the orthogonal com- plement of w, see Figure 1 . By the anti-concentration of the Gaussian distribution we k now that each slab should have mass at most O(δ). Note that this is easy to test by sampling and computing empirical estimates of the mass of each slab. Moreover, assu ming that underlying distribution is N (0, I), we have that, conditional on Si the orthogonal direction u · x ∼ N (0, 1) (see Figure 1 ) and in particular u · x has bounded second moment. W e do not know the orthogonal dire ction u as it depends on the unknown v∗ but we can check that, conditional on the slab Si, the projection of Dx onto the orthogonal complement of w is (approximately) mean-zero and has bounded covari- ance (i.e., bounded above by 2I). Note that both these conditions hold when x ∼ N (0, I) and can 4be eﬃciently tested with samples in time poly(d, 1/δ). Under those conditions we have that that Pr[Si] = O(δ) for all i. Moreover, when the conditional distribution on Si (projected on the orthogo- nal complement of w) has bounded second moment, we have that Pr[|u·x| ≥ i | x ∈ Si] ≤ O(1/i2) . Combining the above, we obtain that under those assumptions the total probability of disagreement is at most O(δ). The detailed analysis is given in Section 3.1 . 1.2 Preliminaries W e use small boldface characters for vectors and capital bol d characters for matrices. W e use [d] to denote the set {1, 2, . . . , d }. F or a vector x ∈ Rd and i ∈ [d], xi denotes the i-th coordinate of x, and ∥x∥2 := √ ∑ d i=1 x2 i the ℓ2 norm of x. W e use x · y := ∑ n i=1 xiyi as the inner product between them. W e use /BD {E} to denote the indicator function of some event E. W e use Ex∼D[x] for the expectation of the random variable x according to the distribution D and Pr[E] for the probability of event E. F or simplicity of notation, we may omit the distribution when it is clear from the context. F or µ ∈ Rd, Σ ∈ Rd×d, we denote by N (µ, Σ ) the d-dimensional Gaussian distribution with mean µ and covariance Σ . F or (x, y) ∈ X distributed according to D, we denote Dx to be the marginal distribution of x. Let f : Rd ↦→ {±1} be a boolean function and D a distribution over Rd. The degree- 1 Chow parameter vector of f with respect to D is deﬁned as Ex∼D [f(x)x]. F or a halfspace h(x) = sign( v · x), we say that v is the deﬁning vector of h. Moment-Matching In what follows, we use the phrase “A distribution D on Rd matches k moments with a distribution Q up to error ∆ ”. Similarly to [ GKK22], we formally deﬁne approximate moment-matching as follows. Deﬁnition 1.3 (Approximate Moment-Matching). Let k ∈ N be a degree parameter and let M(k, d) be the set of d-variate monomials of degree up to k. Moreover, let ∆ ∈ R|M(k,d)| + be a slack parameter (indexed by the monomials of M(k, d)), satisfying ∆ 0 = 0 . We say that two distributions D, Q match k moments up to error ∆ if | Ex∼D[m(x)] − Ex∼Q[m(x)]| ≤ ∆ m for every monomial m(x) ∈ M(k, d). When the error bound ∆ is the same for all monomials we overload notation and simply use ∆ instead of the parameter ∆ . 2 W eak T estable Proper Agnostic Learning As our starting point, we give an algorithm that performs tes table proper learning of homogeneous halfspaces in the presence of adversarial label noise with r espect to the Gaussian distribution. The main result of this section is the following: Proposition 2.1 (Proper T estable Learner with Adversarial Label Noise) . Let D be a distribution on labeled examples (x, y) ∈ Rd × {±1}. Suppose that there exists a unit vector v∗ ∈ Rd such that Pr(x,y)∼D [sign(v∗ · x) ̸= y] ≤ opt. There exists an algorithm ( Algorithm 1 ) that given τ, η ∈ (0, 1), and N = d ˜O(1/η2) log(1/τ) i.i.d. samples from D, runs in time poly(d, N ) and does one of the following: • The algorithm reports that the x-marginal of D is not N (0, I). • The algorithm outputs a unit vector w ∈ Rd. With probability at least 1 − τ the following holds: (1) if the algorithm reports anything, the report is correct, and (2) if the algorithm returns a vector w, it holds ∥v∗ − w∥2 ≤ CA √ opt + η, where CA > 0 is an absolute constant. 5A couple of remarks are in order. First, notice that if the alg orithm outputs a vector w, we only have the guarantee that ∥v∗ − w∥2 is small — instead of that the hypothesis halfspace hw(x) = sign(w · x) achieves small 0-1 error. Nonetheless, as we will show in the next section, conditioned on D passing some test, the error of the halfspace hw will be at most opt plus a constant multiple of ∥v∗ − w∥2 (see Lemma 3.1 ). Second, unlike the testable improper learners in [ R V22, GKK22] — which achieve error of opt + η with similar running time and sample complexity — our testab le proper learner achieves the weaker error guarantee of O(√opt + η). This suﬃces for our purposes for the following reason: in the context of our localization -based approach, we only need an eﬃcient proper weak learner that achieves suﬃciently small constant error. This holds for our proper testable learner, as long as both opt and η are bounded above by some other suﬃciently small constant. T o obtain a proper learner, we proceed to directly estimate t he deﬁning vector v∗ of the target halfspace h∗(x) = sign( v∗ · x), where we assume without loss of generality that v∗ is a unit vector. The following simple fact relating the degree- 1 Chow-parameters of a homogeneous halfspace and its deﬁning vector will be useful for us. F act 2.2 (see, e.g., Lemma 4.3 of [ DKS18]). Let v be a unit vector and h(x) = sign( v · x) be the corresponding halfspace. If x is drawn from N (0, I), then we have that Ex∼N (0,I) [h(x)x] = √ 2/π v. T o apply Fact 2.2 in our context, we need to overcome two hurdles: (i) the x marginal of D is not necessarily the standard Gaussian, and (ii) the label s are not always consistent with h∗(x). The second issue can be circumvented by following the approa ch of [ DKS18]. In particular, if the x marginal of D is indeed Gaussian, we can just treat D as a corrupted version of (x, h∗(x)), where x ∼ N (0, I) and estimate the Chow parameters robustly . T o deal with the ﬁrst issue, we borrow tools from [ GKK22]. At a high level, we certify that the low-degree moments of Dx — the x marginal of D — approximately match the corresponding moments of N (0, I) before estimating the Chow parameters. T o establish the cor rectness of our algorithm, we show that, for any distribution B that passes the moment test, the Chow parameters of a halfspace under B will still be close to its deﬁning vector. F ormally , we prove the following lemma: Lemma 2.3 (F rom Moment-Matching to Chow Distance) . Fix η > 0. Let k = C log(1/η)/η2 and ∆ = 1 kdk ( 1 C √ k ) k+1 , where C > 0 is a suﬃciently large absolute constant. Let B be a distribution whose moments up to degree k match with those of N (0, I) up to additive error ∆ . Let h(x) = sign(v · x) be a halfspace. Then we have that      E x∼B [h(x)x] − √ 2 π v      2 ≤ O(√ η) . Proof. It suﬃces to show that for any unit vector u ∈ Rd, the following holds: ⏐ ⏐ ⏐ ⏐ E x∼B [h(x)x · u] − E x∼N (0,I) [h(x)x · u] ⏐ ⏐ ⏐ ⏐≤ O(√ η) . The following fact expresses a real number a as an integral of the sign function. F act 2.4. For any a ∈ R, it holds a = 1 2 ∫ ∞ 0 (sign(a − t) + sign( a + t))dt. 6W e apply Fact 2.4 to the term u · x, which gives ⏐ ⏐ ⏐ ⏐ E x∼B [h(x)x · u] − E x∼N (0,I) [h(x)x · u] ⏐ ⏐ ⏐ ⏐= 1 2 ⏐ ⏐ ⏐ ⏐ E x∼B [ h(x) ∫ t≥0 (sign(u · x − t) + sign( u · x + t)) dt ] − E x∼N (0,I) [ h(x) ∫ t≥0 (sign(u · x − t) + sign( u · x + t)) dt ] ⏐ ⏐ ⏐ ⏐ = 1 2 ⏐ ⏐ ⏐ ⏐ ∫ t≥0 ( E x∼B [h(x) (sign(u · x − t) + sign( u · x + t))] − E x∼N (0,I) [h(x) (sign(u · x − t) + sign( u · x + t))] ) dt ⏐ ⏐ ⏐ ⏐, where in the last line we switch the order of the integral of t and x by F ubini’s theorem. W e then split the above integral over t into two parts based on the magnitude of t (t > 1/√ η versus 0 ≤ t ≤ 1/√η) and apply the triangle inequality: ⏐ ⏐ ⏐ ⏐ E x∼B [h(x)x · u] − E x∼N (0,I) [h(x)x · u] ⏐ ⏐ ⏐ ⏐ ≤ 1 2 ⏐ ⏐ ⏐ ⏐ ⏐ ∫ 0≤t≤1/√ η ( E x∼B [h(x)sign(u · x − t)] − E x∼N (0,I) [h(x)sign(u · x − t)] ) dt ⏐ ⏐ ⏐ ⏐ ⏐ + 1 2 ⏐ ⏐ ⏐ ⏐ ⏐ ∫ 0≤t≤1/√ η ( E x∼B [h(x)sign(u · x + t)] − E x∼N (0,I) [h(x)sign(u · x + t)] ) dt ⏐ ⏐ ⏐ ⏐ ⏐ + 1 2 ⏐ ⏐ ⏐ ⏐ ∫ t≥1/√ η ( E x∼B [h(x) (sign(u · x − t) + sign( u · x + t))] − E x∼N (0,I) [h(x) (sign(u · x − t) + sign( u · x + t))] ) dt ⏐ ⏐ ⏐ ⏐. (1) W e start by bounding the integral for t ≥ 1/√ η. Lemma 2.5 (Chow-Distance T ail). Let Q be distribution over Rd with Ex∼Q[xx⊤] ≼ 2I. Moreover, let g(x) : Rd ↦→R be a bounded function, i.e., |g(x)| ≤ 1 for all x ∈ Rd. It holds ⏐ ⏐ ⏐ ⏐ ⏐ ∫ t≥1/√ η E x∼Q [g(x) (sign(u · x − t) + sign( u · x + t))] dt ⏐ ⏐ ⏐ ⏐ ⏐≤ O(√ η) . Proof. W e split the expectation into two parts based on the relative sizes of |u · x| and t. Speciﬁcally , we can write: ⏐ ⏐ ⏐ ⏐ ⏐ ∫ t≥1/√ η E x∼Q [g(x) (sign(u · x − t) + sign( u · x + t))] dt ⏐ ⏐ ⏐ ⏐ ⏐ ≤ ⏐ ⏐ ⏐ ⏐ ⏐ ∫ t≥1/√ η E x∼Q [g(x) (sign(u · x − t) + sign( u · x + t)) /BD {|u · x| ≥ t}] dt ⏐ ⏐ ⏐ ⏐ ⏐ + ⏐ ⏐ ⏐ ⏐ ⏐ ∫ t≥1/√ η E x∼Q [g(x) (sign(u · x − t) + sign( u · x + t)) /BD {|u · x| ≤ t}] dt ⏐ ⏐ ⏐ ⏐ ⏐. (2) F or the second term in Equation (2) , we rely on the following observation: when |u · x| ≤ t, the quantities u·x−t and u·x+t have opposite signs. Hence, we conclude the integrand is 0 everywhere 7and therefore the second term is also 0. F or the ﬁrst term, we have ⏐ ⏐ ⏐ ⏐ ⏐ ∫ t≥1/√ η E x∼Q [g(x) (sign(u · x − t) + sign( u · x + t)) /BD {|u · x| ≥ t}] ⏐ ⏐ ⏐ ⏐ ⏐ ≤ ∫ t≥1/√ η E x∼Q [ ⏐ ⏐ ⏐ ⏐g(x) (sign(u · x − t) + sign( u · x + t)) /BD {|u · x| ≥ t} ⏐ ⏐ ⏐ ⏐ ] ≤ ∫ t≥1/√ η E x∼Q [2/BD {|u · x| ≥ t}] ≤ 4 ∫ t≥1/√η 1 t2 ≤ O(√η) , where the ﬁrst inequality follows from the triangle inequal ity , the second inequality uses the fact that the sign(·) function is at most 1 and the third inequality follows from Chebyshev’s inequali ty using the fact that the E[xx⊤] ≼ 2I. Combining our analysis for the two terms in Equation (2) , we can then conclude the proof of Lemma 2.5 . Using the triangle inequality and applying Lemma 2.5 on the distributions B and N (0, I), we have that 1 2 ⏐ ⏐ ⏐ ⏐ ∫ t≥1/√ η ( E x∼B [h(x) (sign(u · x − t) + sign( u · x + t))] − E x∼N (0,I) [h(x) (sign(u · x − t) + sign( u · x + t))] ) dt ⏐ ⏐ ⏐ ⏐≤ O(√ η) . (3) W e then turn our attention to the terms⏐ ⏐ ⏐ ⏐ ⏐ ∫ 0≤t≤1/√ η ( E x∼B [h(x)sign(u · x − t)] − E x∼N (0,I) [h(x)sign(u · x − t)] ) dt ⏐ ⏐ ⏐ ⏐ ⏐ . (4) ⏐ ⏐ ⏐ ⏐ ⏐ ∫ 0≤t≤1/√ η ( E x∼B [h(x)sign(u · x + t)] − E x∼N (0,I) [h(x)sign(u · x + t)] ) dt ⏐ ⏐ ⏐ ⏐ ⏐ . (5) T o bound Equations (4) and (5), we need the following fact from [ GKK22]. F act 2.6 (Theorem 5.6 of [ GKK22]). Let h : Rd ↦→ {±1} be a function of p halfspaces, i.e., h(x) = g (h1(x), · · · , hp(x)) where hi are halfspaces and g : {±1}p ↦→ {±1}. For any k ∈ N, let ∆ = √p 2k 1 dk ( 1 C′√ k ) k+1 for some suﬃciently large absolute constant C′ > 0. Then, for any distribution B whose moments up to order k match those of N (0, I) up to ∆ , we have ⏐ ⏐ ⏐ ⏐ E x∼N (0,I) [h(x)] − E x∼B [h(x)] ⏐ ⏐ ⏐ ⏐≤ 1 √ k √p ( C log ( √ pk )) 2p . for some constant C > 0. F or a ﬁxed t, note that h(x)sign(u · x − t) is a function of two halfspaces. Moreover, from the assumptions of Lemma 2.3 , the distributions B and N (0, I) match k = C log(1/η)/η2 moments up to error ∆ = 1 kdk ( 1 C √ k ) k+1 , where C > 0 is a suﬃciently large absolute constant. Therefore, applying Fact 2.6 and the triangle inequality gives 1 2 ⏐ ⏐ ⏐ ⏐ ⏐ ∫ 0≤t≤1/√ η ( E x∼B [h(x)sign(u · x − t)] − E x∼N (0,I) [h(x)sign(u · x − t)] ) dt ⏐ ⏐ ⏐ ⏐ ⏐ ≤ O(1) ∫ 0≤t≤1/√ η ηdt = O(√η) . (6) 8Similarly , we can show that Equation (5) is bounded by O(√η). Substituting the bounds from Equations (3) and (6) into Equation (1) then gives ⏐ ⏐ ⏐ ⏐ E x∼B [h(x)x · u] − E x∼N (0,I) [h(x)x · u] ⏐ ⏐ ⏐ ⏐≤ O(√ η). Since u is chosen as an arbitrary unit vector, this implies that     E x∼B [h(x)x] − E x∼N (0,I) [h(x)x]     2 ≤ O(√ η). Combining this with Fact 2.2 concludes the proof of Lemma 2.3 . With Lemma 2.3 in hand, we know it suﬃces to estimate the Chow parameters of h∗ with respect to Dx. This would then give us a good approximation to v∗ conditioned on Dx indeed having its low-degree moments approximately match those of N (0, I). W e use the following algorithm, which estimates the Chow parameters robustly under adversarial label noise. Lemma 2.7. Let G be a distribution over Rd × {±1} such that Ex∼Gx [xx⊤] ≼ 2I. Let v ∈ Rd be a unit vector such that v = argmin w∈Rd Pr(x,y)∼G[sign(w · x) ̸= y] and assume that Pr(x,y)∼G[sign(v · x) ̸= y] ≤ ǫ. Then there exists an algorithm that takes N = poly( d, 1/ǫ) samples, runs in time poly(N), and outputs a vector w such that     E x∼Gx [sign(v · x)x] − w     2 ≤ O(√ ǫ) . Proof. W e ﬁrst show that  Ex∼Gx [sign(v · x)x] − E(x,y)∼G [yx]   2 ≤ O(√ ǫ). F or any unit vector u, we have that E x∼Gx [sign(v · x)u · x] − E (x,y)∼G [yu · x] = E (x,y)∼G [(sign(v · x) − y)u · x] ≤ √ E (x,y)∼G [(sign(v · x) − y)2] E x∼Gx [(u · x)2] ≤ 4√ǫ , where we used the Cauchy-Schwarz inequality and the fact tha t Pr(x,y)∼G[sign(v·x) ̸= y] ≤ ǫ. There- fore, we have that  Ex∼Gx [sign(v · x)x] − E(x,y)∼G [yx]   2 ≤ 4√ ǫ. Let (x(1), y(1)), . . . , (x(N1), y(N1)) be samples drawn from D, where N1 = O(d/ǫ2). Then, let ˜wi = (1 /N1) ∑ N1 i=1 y(i)x(i) · ei. F rom, Markov’s inequality , we have that Pr[|˜wi − E(x,y)∼D[yx]| ≥ ǫ/ √ d] ≤ 4d/(N1ǫ2) ≤ 1/4. Therefore, using the standard median technique, we can ﬁnd a wmedian i , so that Pr[|wmedian i − E(x,y)∼D[yx]| ≥ ǫ/ √ d] ≤ τ/d, using N2 = O(N1 log(d/τ)) samples. Let w = ( wmedian 1 , . . . , wmedian d ), then we have that  E(x,y)∼G [yx] − w   2 ≤ O(ǫ) with probability at least 1−τ. Then, using the triangle inequality , we have that ∥Ex∼Gx [sign(v · x)x] − w∥2 ≤ O(√ ǫ), which concludes the proof of Lemma 2.7 . W e are ready to present the algorithm and conclude the proof o f Proposition 2.1 . Proof of Proposition 2.1 . Let k, ∆ , N be deﬁned as in Algorithm 1 . If Dx is N (0, I), the moments up to degree k of the x-marginal of the empirical distribution ˆDN (obtained after drawing N i.i.d. samples from D) are close to those of N (0, I) up to additive error ∆ with probability at least 1 − τ/10. 9If Algorithm 1 did not terminate on Line 3, we then have that the moments up to degree k of the x-marginal of ˆDN are close to those of N (0, I) up to additive error ∆ with probability at least 1 − τ/10. Let ( ˆDN )x be the x-marginal of ˆDN . Then, applying Lemma 2.3 with B = ( ˆDN )x and h(x) = sign( v∗ · x), we get that      E x∼( ˆDN )x [sign(v∗ · x)x] − √ 2/πv∗      2 ≤ O(√ η). (7) By our assumption, the error of sign(v∗ · x) under D is at most opt. Hence, the error of sign(v∗ · x) under ˆDN is at most opt + η with probability at least 1 − τ/10. Assuming that this holds, by Lemma 2.7 , with probability at least 1 − τ/10, the vector w computed on Line 5 of Algorithm 1 satisﬁes      E x∼( ˆDN )x [sign(v∗ · x)x] − w      2 ≤ O ( √ opt + η ) . (8) Combining Equations (7) and (8), we get that   w − √ 2/πv∗    2 ≤ O ( √ opt + η ) , as desired. Input: Sample access to a distribution D over labeled examples; certiﬁcation range η; failure probability τ. Output: Either reports that the Dx is not N (0, I); or returns a unit vector w ∈ Rd such that ∥v∗ − w∥2 ≤ CA √opt + η. 1. Set k = C log(1/η)/η2 and ∆ = 1 kdk ( 1 C √ k ) k+1 , where C > 0 is a suﬃciently large absolute constant. 2. Draw N = dCk log k log(1/τ) samples from D and construct the empirical distribution ˆDN . 3. Certify that the moments of ˆDN up to degree k match with those of N (0, I) up to error ∆ . 4. If the above does not hold; report that Dx is not N (0, I) and terminate. 5. Use algorithm from Lemma 2.7 on ˆDN and obtain w. Return w/∥w∥2. Algorithm 1: Proper T estable Learner 3 Eﬃcient T estable Learning of Halfspaces In this section, we give our tester-learner for homogeneous halfspaces under the Gaussian distri- bution, thereby proving Theorem 1.2 . Throughout this section, we will ﬁx an optimal halfspace h∗(x) = sign( v∗ · x), i.e., a halfspace with optimal 0-1 error. The structure of this section is as follows: In Section 3.1 , we present a tester which certiﬁes that the probability of the disagreement region between two half spaces whose deﬁning vectors are close to each other is small under Dx. In Section 3.2 , we present and analyze our localization step and combine it with the tester from Section 3.1 to obtain our ﬁnal algorithm. 103.1 F rom Parameter Distance to 0-1 Error F or two homogeneous halfspaces hu(x) = sign( u · x) and hv(x) = sign( v · x), where u, v are unit vectors, if Dx is the standard Gaussian, N (0, I), we can express the probability mass of their disagreement region as follows (see, e.g., Lemma 4.2 of [ DKS18]): Pr x∼Dx [hu(x) ̸= hv(x)] ≤ O (∥u − v∥2) . (9) Hence, learning homogeneous halfspaces under Gaussian mar ginals can often be reduced to approx- imately learning the deﬁning vector of some optimal halfspa ce h∗. This is no longer the case if Dx is an arbitrary distribution, which may well happen in our re gime. W e show in this section that it is still possible to “certify” whether some relationship si milar to the one in Equation (9) holds. Input: Sample access to a distribution Dx over Rd; tolerance parameter η > 0; unit vector v ∈ Rd; failure probability τ ∈ (0, 1). Output: Certiﬁes that for all unit vectors w such that ∥w − v∥2 ≤ η it holds that Prx∼Dx [sign(v · x) ̸= sign( w · x)] ≤ Cη, for some absolute constant C > 1, or reports that Dx is not N (0, I). 1. Set B = ⌈ √ log(1/η)/η⌉. 2. Let ˜D be the empirical distribution obtained by drawing poly(d, 1/η) log(1/τ) many samples from Dx. 3. F or integers −B − 1 ≤ i ≤ B, deﬁne Ei to be the event that {v · x ∈ [iη, (i + 1)η]} and EB+1 to be the event that {|v · x| ≥ √ log(1/η)}. 4. V erify that B+1∑ i=−B−1 ⏐ ⏐ ⏐ ⏐ Pr N (0,I) [Ei] − Pr ˜D [Ei] ⏐ ⏐ ⏐ ⏐ ≤ η. 5. Let Si be the distribution of ˜D conditioned on Ei and S⊥ i be Si projected into the subspace orthogonal to v. 6. F or each i, verify that S⊥ i has bounded covariance, i.e., check that Ex∼S⊥ i [xx⊤] ≼ 2I . Algorithm 2: W edge-Bound In particular, given a known vector v, we want to make sure that for any other vector w that is close to v, the mass of the disagreement region between the halfspaces deﬁned by by v, w respectively is small. T o do so, we will decompose the space into many thin “ slabs” that are stacked on top of each other in the direction of v. Then, we will certify the mass of disagreement restricted t o each of the slab is not too large. F or slabs that are close to the hal fspace sign(v · x), we can check these slabs must not themselves be too heavy . F or slabs that are far away from the halfspace, we use the observation that the points in the disagreement region m ust then have large components in the subspace perpendicular to v. Hence, as long as D has its second moment bounded, we can bound the mass of the disagreement region in these far-away slabs u sing standard concentration inequality . Lemma 3.1 (W edge Bound). Let Dx be a distribution over Rd. Given a unit vector v and parame- ters η, τ ∈ (0, 1/2), there exists an algorithm ( Algorithm 2 ) that draws i.i.d. samples from Dx, runs in time poly(d, 1/η) log(1/τ), and reports either one of the following: 11(i) For all unit vectors w such that ∥w − v∥2 ≤ η it holds Prx∼Dx [sign(v ·x) ̸= sign( w ·x)] ≤ Cη, for some absolute constant C > 1. (ii) Dx is not the standard Gaussian N (0, I). Moreover, with probability at least 1 − τ, the report is accurate. Proof. Recall that ˜D is the empirical distribution made up of N i.i.d. samples from Dx where N = poly( d, 1/η) log(1/τ). W e consider ˜D restricted to a set of thin “slabs” stacked on each other in the direction of v. More formally , we deﬁne Si to be the distribution of ˜D conditioned on v · x ∈ [(i − 1)η, iη], for i ∈ [− √ log(1/η)/η, √ log(1/η)/η], and S⊥ i to be the distribution Si projected into the subspace orthogonal to v. Suppose that Dx is indeed N (0, I). Then the distribution ˜D is the empirical distribution formed by samples taken from N (0, I). In this case, it is easy to see that both Line 4 and 6 of Algorithm 2 pass with high probability . Claim 3.2. Assume that Dx = N (0, I). Then the tests at Line 4 and 6 of Algorithm 2 pass with probability at least 1 − τ/10. Proof. If x ∼ N (0, I), then v · x ∼ N (0, 1). If we concatenate the values of PrN (0,I)[Ei] into a vector, it can be viewed as the discretization of N (0, 1) into 2B + 3 many buckets. On the other hand, Pr ˜D[Ei] is an empirical version of this discrete distribution compo sed of N i.i.d. samples where N = poly( d, 1/η) log(1/τ). Since we can learn any discrete distribution with support n up to error η in total variation distance with Θ( n/η2) log(1/τ) samples with probability at least 1 − τ, it follows that Line 4 will pass with high probability as long as we take more than Θ( B/η2) log(1/τ) ≤ poly(d, 1/η) log(1/τ) many samples. F or Line 6, we remark that S⊥ i is the empirical version of a (d − 1)-dimensional standard Gaussian. Since the empirical mean and the empirical covari ance concentrates around the true mean and covariance with probability at least 1 − τ if one takes more than Θ( d2/η2) log(1/τ) many samples, it follows that Line 4 will pass with high probability as long as we take more than Θ( d2/η2) log(1/τ) ≤ poly(d, 1/η) log(1/τ) many samples. Suppose that both lines pass. W e claim that this implies the f ollowing: for all unit vectors w such that ∥w − v∥2 ≤ η it holds Pr x∼ ˜D [sign(v · x) ̸= sign( w · x)] ≤ Cη . (10) for some absolute constant C > 1. Given this, we can deduce that the same equation must also hold for Dx with high probability — albeit with a larger constant C′. T o see this, we remark that the left hand side of the equation can be treated as the error o f the halfspace sign(w · x) if the true labels are generated by sign(v · x). Since the VC-dimension of the class of homogeneous halfspa ces is d, we have that the error for all w under Dx is well-approximated by that under ˜D up to an additive η with probability at least 1 − τ given N = poly( d, 1/η) log(1/τ) many samples. Hence, conditioned on Equation ( 10), it holds with probability at least 1 − τ that Pr x∼Dx [sign(v · x) ̸= sign( w · x)] ≤ (C + 1)η , for all w. 12W e now proceed to show Equation ( 10) holds if the Algorithm 2 did not terminate on Lines 4 and 6. Conditioned on Line 6, for any unit vector u ∈ Rd that is orthogonal to v, we have |E [u · x]| ≤ O(1) and Var [u · x] ≤ 2. Using Chebyshev’s inequality , for any α > 0, it holds Pr x∼S⊥ i [|u · x| ≥ α] ≤ O ( 1 + η2 α2 ) . (11) W e can now bound Prx∼ ˜D [sign(v · x) ̸= sign( w · x)] for an arbitrary unit vector w satisfying ∥w − v∥2 ≤ η. W e proceed to rewrite w as (1− γ2)1/2v + γu for some unit vector u ∈ Rd that is or- thogonal to v and γ ∈ (0, η). Denote γ′ = γ/(1−γ2)1/2, then the event that sign(v ·x) ̸= sign( w ·x) implies that γ′ |u · x| ≥ | v · x|. Therefore, we have that Pr x∼Si [sign(v · x) ̸= sign( w · x)] ≤ Pr x∼Si [ γ′ |u · x| ≥ | v · x| ] ≤ Pr x∼S⊥ i [ γ′ |u · x| ≥ iη ] ≤ O ( 1 + η2 i2 ) , (12) where in the second inequality we use the deﬁnition of Si, and in the third inequality we use that γ ≤ η and Equation (11) . W e now bound from above the total disagreement probability between w and v under ˜D. W e have that Pr x∼ ˜D [sign(v · x) ̸= sign( w · x)] ≤ Pr x∼ ˜D [ |v · x| ≥ √ log(1/η) ] + Pr x∼ ˜D [|v · x| ≤ η] + √ log(1/η)/η∑ |i|>1 Pr x∼Si [sign(v · x) ̸= sign( w · x)] Pr x∼ ˜D [(i − 1)η ≤ x · v ≤ iη] ≤ 6η + O (η) √ log(1/η)/η∑ |i|>1 1 + η2 i2 ≤ O(η) , where we used that Prx∼ ˜D[|x · v| > √ log(1/η)] ≤ 3η and Prx∼ ˜D[|x · v| < η ] ≤ 3η, since in Line 4 we veriﬁed that the probabilities Prx∼ ˜D[v · x ∈ [(i − 1)η, iη]] are close to the probabilities under N (0, I) and hence bounded by O(η), Equation (12) , and the fact that the series ∑ i 1 i2 is convergent and less than π2/6. 3.2 Algorithm and Analysis: Proof of Theorem 1.2 W e employ the idea of “soft” localization used in [ DKS18]. In particular, given a vector v and a parameter σ, we use rejection sampling to deﬁne a new distribution Dv,σ that “focuses” on the region near the halfspace sign(v · x). F act 3.3 (Rejection Sampling, Lemma 4.7 of [ DKS18]). Let D be a distribution on labeled examples (x, y) ∈ Rd × {±1}. Let v ∈ Rd be a unit vector and σ ∈ (0, 1). We deﬁne the distribution Dv,σ as follows: draw a sample (x, y) from D and accept it with probability e−(v·x)2·(σ−2−1)/2. Then, Dv,σ is the distribution of (x, y) conditional on acceptance. If the x-marginal of D is N (0, I), then the x-marginals of Dv,σ is N (0, Σ ), where Σ = I − (1 − σ2)vvT . Moreover, the acceptance probability of a point is σ. The main idea of localization is the following. Let v be a vector such that ∥v − v∗∥2 ≤ δ. Suppose that we use localization to the distribution Dv,δ. If we can learn a halfspace with deﬁning 13vector w that achieves suﬃciently small constant error with respect to the new distribution Dv,δ, we can then combine our knowledge of w and v to produce a new halfspace with signiﬁcantly improved error guarantees under the original distribution D. The following lemma formalizes this geometric intuition. Lemma 3.4. Let v∗, v be two unit vectors in Rd such that ∥v − v∗∥2 ≤ δ ≤ 1/100. Let Σ = I − (1 − δ2)vvT and w be a unit vector such that    w − Σ 1/ 2v∗ ∥Σ 1/ 2v∗∥2     2 ≤ ζ ≤ 1/100. Then it holds     Σ −1/ 2w ∥Σ −1/ 2w∥2 − v∗     2 ≤ 5(δ2 + δζ). Before we give the proof of Lemma 3.4 , we provide a few useful remarks regarding the relevant parameters. Remark 3.5. Observe that in Lemma 3.4 we require that the distance of v and v∗ is smaller than 1/100. While this constant is not the best possible, we remark that some non-trivial error is indeed necessary so that the localization step works. F or example, assume that v and v∗ are orthogonal, i.e., v · v∗ = 0 , and that Σ = I − (1 − ξ2)vvT for some ξ ∈ [0, 1]. Observe that Σ 1/2 scales vectors by a factor of ξ in the direction of v and leaves orthogonal directions unchanged. Similarly , it s inverse Σ −1/2 scales vectors by a factor of 1/ξ in the direction of v and leaves orthogonal directions unchanged. Without loss of generality , assume that v = e1, v∗ = e2. Then Σ 1/2v∗ = v∗. Moreover, assume that w = ae1 + be2 (with a2 + b2 = 1 ). W e observe that ∥w − Σ 1/2v∗/∥Σ 1/2v∗∥2∥2 2 = ∥w−v∗∥2 2 = 2 −2b. However, observe that s = Σ −1/2w = ( a/ξ)e1 +be2. Therefore, ∥s/∥s∥2 −v∗∥2 2 = 2 − 2b/ √ (a/ξ)2 + b2. W e observe that for all ξ ∈ [0, 1] it holds that s/∥s∥2 is further away from v∗ than w, i.e., rescaling by Σ −1/2 worsens the error. Proof of Lemma 3.4 . Since ∥v − v∗∥2 ≤ δ, we can write v∗ = 1√ 1 + κ2 (v + κu) (13) for some κ ∈ [0, δ] and some unit vector u perpendicular to v. By deﬁnition, Σ 1/2 shrinks in the direction of v by a factor of δ and leaves other orthogonal directions unchanged. Hence, i t holds Σ 1/2v∗ = 1√ 1 + κ2 (δv + κu) . Then, using the triangle inequality , we obtain γ :=   Σ 1/2v∗    2 ≤ 1 √ 1 + κ2 (δ ∥v∥2 + κ ∥u∥2) ≤ 2δ√ 1 + κ2 ≤ 2δ , since κ is upper bounded by δ. Since    w − Σ 1/ 2v∗ ∥Σ 1/ 2v∗∥2     2 ≤ ζ, we can write w = Σ 1/2v∗  Σ 1/2v∗  2 + av + bu′ , for some |a| , b ∈ [0, ζ] and u′ perpendicular to v. W e can multiply both sides by γ and get γw = Σ 1/2v∗ + aγv + bγu′ , 14which implies that γΣ −1/2w = v∗ + aγ/δv + bγu′ . Using Equation (13) , we then have γΣ −1/2w = ( 1√ 1 + κ2 + aγ δ ) v + κ√ 1 + κ2 u + bγu′. (14) Let λ := 1√ 1+κ2 + aγ/δ be the coeﬃcient before v. W e next identify the range of λ. Claim 3.6. It holds that λ ∈ [1 − δ − 2ζ, 1 + 2 ζ]. Proof. Recall that κ ∈ [0, δ], |a| ∈ [0, ζ] and γ ∈ [0, 2δ]. If we view λ as a function of κ, a, γ , it is minimized when κ = δ, a = −ζ, γ = 2 δ, which then gives λ ≥ 1√ 1 + δ2 − 2ζ ≥ 1 − δ − 2ζ , where in the second inequality we use the fact that 1√ 1+x2 ≥ 1 − x for x ≥ 0. On the other hand, λ is maximized when κ = 0 , a = ζ, γ = 2 δ, which gives λ ≤ 1 + 2 ζ. Hence, we can conclude that λ ∈ [1 − δ − 2ζ, 1 + 2 ζ]. W e multiply both sides of Equation (14) by 1 λ √ 1+κ2 , which gives γ λ √ 1 + κ2 Σ −1/2w = 1√ 1 + κ2 v + κ λ (1 + κ2)u + b κ λ √ 1 + κ2 u′ = v∗ + ( κ λ (1 + κ2) − κ√ 1 + κ2 ) u + b γ λ √ 1 + κ2 u′ , where in the second equality we use Equation (13) . W e then bound from above and below the norm, and we get that ⏐ ⏐ ⏐ ⏐     γ λ √ 1 + κ2 Σ −1/2w     2 − 1 ⏐ ⏐ ⏐ ⏐≤ κ 1 + κ2 ⏐ ⏐ ⏐ ⏐ 1 λ − √ 1 + κ2 ⏐ ⏐ ⏐ ⏐+ bγ λ √ 1 + κ2 , where we used triangle inequality . Note that |(1/λ) − √ 1 + κ2| ≤ | 1/λ − 1| + |1 − √ 1 + κ2| ≤ (δ + 2ζ)/(1 − δ − 2ζ) + κ and that κ ≤ δ. Therefore, we obtain that ⏐ ⏐ ⏐ ⏐     γ λ √ 1 + κ2 Σ −1/2w     2 − 1 ⏐ ⏐ ⏐ ⏐≤ 4(δ2 + δζ) . Let A =    γ λ √ 1+κ2 Σ −1/2w    2 . W e have that      Σ −1/2w  Σ −1/2w   2 − v∗      2 ≤ ∥ v∗∥2|1 − 1/A| + |1/A − 1| ≤ 5(δ2 + δζ) . This concludes the proof. 15W e are ready to present our main algorithm and its analysis. A t a high level, we ﬁrst use Algorithm 1 from Proposition 2.1 to learn a vector v that is close to v∗ in ℓ2-distance up to some suﬃciently small constant. Then we localize to the learned h alfspace and re-apply Algorithm 1 to iteratively improve v. In particular, we will argue that, whenever the learned hal fspace is still signiﬁcantly suboptimal, the algorithm either detec ts that the underlying distribution is not Gaussian or keeps making improvements such that v gets closer to v∗ conditioned on v still being sub-optimal. After at most a logarithmic number of iteratio ns, we know that v must be close to v∗, and we can then use Algorithm 2 from Lemma 3.1 to certify that the disagreement between the learned halfspace and h∗(x) is small. Input: Sample access to a distribution D over labeled examples; ǫ > 0; unit vector v ∈ Rd. Output: Either reports that Dx is not N (0, I) or computes a hypothesis h such that Pr(x,y)∼D[h(x) ̸= y] = O(opt). 1. Set τ = ( ǫ/(C log(1/ǫ))) for a suﬃciently large constant C > 0. 2. Set η = 1 /(20000CA) where CA is the constant from Proposition 2.1 . 3. Run Algorithm 1 on D with accuracy η to obtain unit vector v(0). 4. If Algorithm 1 reports that Dx is not N (0, I), then report it and terminate. 5. F or t = 0 . . . log(1/ǫ) (a) Set δ = (1 /100)2−t. (b) Run Algorithm 4 with parameters v(t), δ, η and obtain v(t+1). 6. F or i = 0 . . . log(1/ǫ) (a) Run Algorithm 2 with v(i) and η = jǫ for j ∈ [1/ǫ] on the x-marginals of D. (b) Set h(i)(x) = sign( v(i) · x). 7. Return the halfspace h(i) for all i ≤ log(1/ǫ) with the smallest empirical error using O(log(1/ǫ)/ǫ2) samples from D. Algorithm 3: T estable Localization Lemma 3.7. Suppose  v∗ − v(t)  2 ≤ δ ≤ 1/100. There is an algorithm ( Algorithm 4 ) that with probability at least 1 − τ, either (i) correctly reports that the x-marginal of D is not N (0, I) or (ii) computes a unit vector v(t+1) so that either  v∗ − v(t+1)  2 ≤ δ/2 or  v∗ − v(t)  2 ≤ Copt, where C > 0 is an absolute constant. Proof. F or simplicity , we denote v(t) as v. W e apply the Rejection Sampling procedure from Fact 3.3 in the direction of v with σ = δ. If the x-marginal of D is N (0, I), the acceptance probability of Dv,δ is exactly δ. W e then estimate the acceptance probability with accuracy ǫ; if it is not lying inside the interval [δ/2, 3δ/2] (see Line 2 of Algorithm 4 ), we report that the x-marginal of D is not standard normal and terminate. Conditioned on the event that the algorithm did not terminat e, we can sample from Dv,δ, using 16Input: Sample access to a distribution D over labeled examples; unit vector v ∈ Rd; parameters τ, η, δ > 0. Output: Either reports that Dx is not N (0, I) or computes a unit vector v′ such that: either ∥v∗ − v′∥2 ≤ δ/2 or ∥v∗ − v′∥2 ≤ O(opt) 1. Let Dv,δ be the distribution obtained by running Rejection Sampling with parameters v and δ. 2. Check the acceptance probability is within [δ/2, 3δ/2]. Otherwise, report x-marginals of D is not N (0, I) . 3. Let G be the distribution obtained by applying the transformatio n Σ −1/2 on the x marginals of Dv,δ where Σ = I − (1 − δ2)vv⊤. 4. Run Algorithm 1 on G with accuracy η = 1 /(20000C2 A) to obtain a unit vector w. 5. If the algorithm reports the marginal of G is not N (0, I), terminate and report it. 6. Set v′ = Σ 1/2w/  Σ 1/2w   2 and return v′. Algorithm 4: T estable Localized-Update O(1/δ) samples from D. Note that, under the distribution Dv,δ, the error of the v∗ is Pr (x,y)∼Dv,δ [sign(v∗ · x) ̸= y] = Pr (x,y)∼D [sign(v∗ · x) ̸= y | (x, y) is accepted ] ≤ Pr (x,y)∼D [sign(v∗ · x) ̸= y]/ Pr (x,y)∼D [(x, y) is accepted ] ≤ 2opt/δ , (15) where we used that the probability of the acceptance is at lea st δ/2. Denote by G the distribution of (Σ −1/2x, y), where (x, y) ∼ Dv,δ and Σ = I − (1 − δ2)vv⊤. W e note that if the x-marginal of D were the standard normal, then x-marginal of G is the standard normal. Hence, we can apply the algorithm Algorithm 1 from Proposition 2.1 . Under the transformed distribution, we have that the new optimal vector (v∗)′ := Σ 1/2v∗/∥Σ 1/2v∗∥2. F rom Proposition 2.1 , after running Algorithm 1 on the normalized distribution G with error parameter η ≤ 1/(2000C2 A), conditioned on the event that it succeeds (which happens wi th prob- ability at least 1 − τ), it either (i) reports that the x-marginal of G is not standard Gaussian (ii) returns a unit vector w such that  w − (v∗)′  2 ≤ CA √ Pr (x,y)∼G [sign((v∗)′ · x) ̸= y] + η. (16) In case (i), we can directly report that x-marginal of D is not N (0, I) since the x-marginal of G ought to be N (0, I). In case (ii), we claim that at least one of the following hold ( a) v before the localized update is already good enough, i.e. ∥v∗ − v∥2 ≤ 40000CA2opt; (b) it holds ∥(v∗)′ − w∥2 ≤ 1/100. Sup- pose that (a) does not hold; we will show that it then must hold ∥(v∗)′ − w∥2 ≤ 1/100. Since ∥v∗ − v∥2 ≤ δ and ∥v∗ − v∥2 > 40000CA2opt, we have that opt ≤ δ/(40000C2 A). F urthermore, from Equation (15) we have that Pr(x,y)∼Dv,δ [sign(v∗ · x) ̸= y] ≤ 2opt/δ, it then follows that 17Pr(x,y)∼G [sign((v∗)′ · x) ̸= y] ≤ 2opt/δ ≤ 1/(20000C2 A). Substituting this into Equation (16) then gives  w − (v∗)′  2 ≤ 1/100 . Using our assumption that ∥v − v∗∥2 ≤ δ < 1/100, we can apply Lemma 3.4 , which gives that      Σ −1/2w  Σ −1/2w   2 − v∗      2 ≤ δ/2 . Hence, we set v(t+1) =     Σ −1/ 2w ∥Σ −1/ 2w∥2     2 and this completes the proof. Proof of Theorem 1.2 . Denote by v∗, a unit vector with error at most opt, i.e., Pr(x,y)∼D[sign(v∗ · x) ̸= y] ≤ opt. W e start by analyzing Algorithm 3 . In Line 3, Algorithm 3 uses Algorithm 1 with parameter η = 1 /(20000C2 A) to get a hypothesis v(0) with small distance with v∗. F rom Proposition 2.1 , Algorithm 1 either reports that x-marginal of D is not N (0, I) or outputs a vector v(0) small distance with v∗. If Algorithm 1 reports that the x-marginal of D is not N (0, I), we can terminate the algorithm. Conditioned on the event that the a lgorithm did not terminate, then we have that   v(0) − v∗    2 ≤ CA √ opt + η. (17) W e consider two cases depending on how large the value of opt is. If opt > 1/(20000C2 A), then any unit vector achieves constant error; therefore, condition ed that the algorithm did not terminate on any proceeding test, any vector we output will satisfy the gu arantees of Theorem 1.2 . F or the rest of the proof, we consider the case where opt ≤ 1/(20000C2 A). In this case,  v(0) − v∗  2 ≤ 1/100, this means that Algorithm 3 on Lines 5-5b will decrease the distance between the current hypothesis and v∗. Conditioned on the event that the algorithm did not terminat e at Lines 5-5b of Algorithm 3 , we claim that there must have some 0 ≤ t∗ ≤ log(1/ǫ) such that  v(t∗) − v∗  2 ≤ O (opt + ǫ). Let t′ ∈ N be the maximum value so that 2−t′ /100 ≥ 40000opt, then, for all t ≤ min(t′, log(1/ǫ)) it holds that 2−t/100 ≥ 40000opt. F rom Lemma 3.7 , we have that for all t ≤ min(t′, log(1/ǫ)) it holds that   v∗ − v(t)    2 ≤ 2−t−1/100 . F rom the above, note that if t′ > log(1/ǫ) then  v(log(1/ǫ)) − v∗  2 ≤ ǫ/100. If t′ ≤ log(1/ǫ), we have that   v(t′) − v∗    2 ≤ O(opt), which proves our claim. It remains to show that Algorithm 3 will return a vector v′ so that Pr(x,y)∼D[sign(v′ · x) ̸= y] = O(opt + ǫ). F rom Lemma 3.1 , conditioned that Algorithm 3 did not terminate on Lines 6-6b, we have that for all vectors v(0), . . . , v(log(1/ǫ)) generated on Lines 5-5b of Algorithm 3 , we have that Pr (x,y)∼D [sign(v(t) · x) ̸= y] ≤ O (   v(t) − v∗    2 ) . Hence, we can conclude that Pr(x,y)∼D[sign(v(t′) · x) ̸= y] ≤ O (opt + ǫ). F rom VC inequality , we have that with at most ˜O(log(1/τ)/ǫ2) samples, we can estimate the empirical probabilities of Pr(x,y)∼D[sign(v·x) ̸= y] for all the vectors generated by our algorithm up to error ǫ. Since we return the one with the smallest empirical error, the returned hypo thesis has error less than O (opt + ǫ). T o conclude the proof, note that Algorithms 1 , 2 and 4 use N = poly( d, 1/ǫ) samples and poly(d, N ) runtime and Algorithm 3 use each algorithm at most O(log(1/ǫ)) many iterations, therefore the total sample complexity of Algorithm 3 is N = poly( d, 1/ǫ) with poly(d, N ) runtime. 18References [ABL17] P . A wasthi, M. F. Balcan, and P . M. Long. The power of loc alization for eﬃciently learning linear separators with noise. J. ACM , 63(6):50:1–50:27, 2017. [Dan15] A. Daniely . A PT AS for agnostically learning halfsp aces. In Proceedings of The 28 th Conference on Learning Theory, COLT 2015 , pages 484–502, 2015. [Dan16] A. Daniely . Complexity theoretic limitations on le arning halfspaces. In Proceedings of the 48 th Annual Symposium on Theory of Computing, STOC 2016 , pages 105–117, 2016. [DGJ+10] I. Diakonikolas, P . Gopalan, R. Jaiswal, R. Servedio, an d E. Viola. Bounded indepen- dence fools halfspaces. SIAM Journal on Computing , 39(8):3441–3462, 2010. [DKK+21] I. Diakonikolas, D. M. Kane, V. Kontonis, C. T zamos, and N . Zariﬁs. Agnostic proper learning of halfspaces under gaussian marginals. In Proceedings of The 34 th Conference on Learning Theory, COLT , 2021. [DKMR22] I. Diakonikolas, D. M. Kane, P . Manurangsi, and L. R en. Cryptographic hardness of learning halfspaces with massart noise. CoRR, abs/2207.14266, 2022. [DKN10] I. Diakonikolas, D. M. Kane, and J. Nelson. Bounded in dependence fools degree- 2 threshold functions. In FOCS, pages 11–20, 2010. [DKPZ21] I. Diakonikolas, D. M. Kane, T. Pittas, and N. Zariﬁ s. The optimality of polynomial re- gression for agnostic learning under gaussian marginals in the SQ model. In Proceedings of The 34 th Conference on Learning Theory, COLT , 2021. [DKR23] I. Diakonikolas, D. M. Kane, and L. Ren. Near-optima l cryptographic hardness of agnostically learning halfspaces and relu regression unde r gaussian marginals. CoRR, abs/2302.06512, 2023. [DKS18] I. Diakonikolas, D. M. Kane, and A. Stewart. Learnin g geometric concepts with nasty noise. In Proceedings of the 50 th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018 , pages 1061–1073, 2018. [DKTZ20a] I. Diakonikolas, V. Kontonis, C. T zamos, and N. Za riﬁs. Learning halfspaces with massart noise under structured distributions. In Conference on Learning Theory, COLT , 2020. [DKTZ20b] I. Diakonikolas, V. Kontonis, C. T zamos, and N. Za riﬁs. Non-convex SGD learns halfspaces with adversarial label noise. In Advances in Neural Information Processing Systems, NeurIPS , 2020. [DKZ20] I. Diakonikolas, D. M. Kane, and N. Zariﬁs. Near-opt imal SQ lower bounds for agnosti- cally learning halfspaces and ReLUs under Gaussian margina ls. In Advances in Neural Information Processing Systems, NeurIPS , 2020. [GGK20] S. Goel, A. Gollakota, and A. R. Klivans. Statistica l-query lower bounds via functional gradients. In Advances in Neural Information Processing Systems, NeurIP S, 2020. 19[GKK22] A. Gollakota, A. Klivans, and P . Kothari. A moment-m atching approach to testable learning and a new characterization of rademacher complexi ty . arXiv preprint arXiv:2211.13312, 2022. [GKSV23] A. Gollakota, A. R. Klivans, K. Stavropoulos, and A . V asilyan. An eﬃcient tester- learner for halfspaces. CoRR, abs/2302.14853, 2023. [Hau92] D. Haussler. Decision theoretic generalizations o f the P AC model for neural net and other learning applications. Information and Computation , 100:78–150, 1992. [KKMS08] A. Kalai, A. Klivans, Y. Mansour, and R. Servedio. A gnostically learning halfspaces. SIAM Journal on Computing , 37(6):1777–1805, 2008. Special issue for FOCS 2005. [KLS09] A. Klivans, P . Long, and R. Servedio. Learning Halfs paces with Malicious Noise. Jour- nal of Machine Learning Research , 10:2715–2740, 2009. [KSS94] M. Kearns, R. Schapire, and L. Sellie. T oward Eﬃcien t Agnostic Learning. Machine Learning, 17(2/3):115–141, 1994. [Ros58] F. Rosenblatt. The Perceptron: a probabilistic mod el for information storage and organization in the brain. Psychological Review , 65:386–407, 1958. [R V22] R. Rubinfeld and A. V asilyan. T esting distributiona l assumptions of learning algorithms. (arXiv:2204.07196), 2022. [Tie22] S. Tiegel. Hardness of agnostically learning halfs paces from worst-case lattice problems. CoRR, abs/2207.14030, 2022. [V al84] L. G. V aliant. A theory of the learnable. In Proc. 16th Annual ACM Symposium on Theory of Computing (STOC) , pages 436–445. ACM Press, 1984. 20",
      "references": [
        "The power of localization for efficiently learning linear separators with noise.",
        "A PTAS for agnostically learning halfspaces.",
        "Complexity theoretic limitations on learning halfspaces.",
        "Bounded independence fools halfspaces.",
        "Agnostic proper learning of halfspaces under gaussian marginals.",
        "Cryptographic hardness of learning halfspaces with massart noise.",
        "Bounded independence fools degree-2 threshold functions.",
        "The optimality of polynomial regression for agnostic learning under gaussian marginals in the SQ model.",
        "Near-optimal cryptographic hardness of agnostically learning halfspaces and ReLU regression under gaussian marginals.",
        "Learning geometric concepts with nasty noise.",
        "Learning halfspaces with massart noise under structured distributions.",
        "Non-convex SGD learns halfspaces with adversarial label noise.",
        "Near-optimal SQ lower bounds for agnostically learning halfspaces and ReLUs under Gaussian marginals.",
        "Statistical-query lower bounds via functional gradients.",
        "A moment-matching approach to testable learning and a new characterization of rademacher complexity.",
        "An efficient tester-learner for halfspaces.",
        "Decision theoretic generalizations of the PAC model for neural net and other learning applications.",
        "Agnostically learning halfspaces.",
        "Learning Halfspaces with Malicious Noise.",
        "Toward Efficient Agnostic Learning.",
        "The Perceptron: a probabilistic model for information storage and organization in the brain.",
        "Testing distributional assumptions of learning algorithms.",
        "Hardness of agnostically learning halfspaces from worst-case lattice problems.",
        "A theory of the learnable."
      ],
      "meta_data": {
        "arxiv_id": "2303.05485v1",
        "authors": [
          "Ilias Diakonikolas",
          "Daniel M. Kane",
          "Vasilis Kontonis",
          "Sihan Liu",
          "Nikos Zarifis"
        ],
        "published_date": "2023-03-09T18:38:46Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "We provide the first polynomial-time tester-learner for homogeneous halfspaces with Gaussian marginals under adversarial label noise in the testable learning model. The algorithm runs in time poly(d,1/ε) and outputs a halfspace with misclassification error O(opt) + ε, where opt is the 0-1 error of the best-fitting halfspace. The work introduces a testable weak learner combined with a localization procedure that uses appropriate testers to certify Gaussian-like behavior of the data. It establishes a robust, moment-based (low-degree) analysis linking moment matching to the Chow parameters of homogeneous halfspaces, yielding a constant-factor approximation that matches non-testable guarantees up to constants. The approach is complemented by an independent concurrent result by GKSV23 achieving related tester-learner guarantees under Gaussian and strongly log-concave settings.",
        "methodology": "The method builds on soft localization: iteratively refine a current halfspace by focusing on a narrow slab around the current decision boundary and updating the weight vector to better align with the target halfspace. Key components include (i) a testable, weak proper agnostic learner for Gaussian marginals that either certifies Gaussianity of the x-marginal or outputs a direction close to the target v*, (ii) a moment-to-Chow-distance connection establishing that if Dx closely matches low-degree Gaussian moments, then the Chow parameters of any homogeneous LT F with respect to Dx are close to those under N(0,I) (Lemma 2.3), (iii) robust estimation of the degree-1 Chow vector under adversarial label noise (Algorithm 1 and Lemma 2.7), (iv) a wedge/ slab decomposition to bound the zero-one error from parameter distance (Section 3.1, Lemma 3.1), (v) soft localization via rejection sampling to produce a Gaussian with covariance Σ = I − (1 − δ^2)vv^T (Fact 3.3), and (vi) a localization refinement step (Algorithm 4, Lemma 3.4) that translates localized improvements into actual agreement gains with the target under the original distribution. The main theoretical result is Theorem 1.2, establishing a poly(d,1/ε) time tester-learner achieving 0-1 error O(opt) + ε for Gaussian marginals.",
        "experimental_setup": "The work is theoretical and does not perform experiments. The setting considers i.i.d. samples from an unknown distribution D on Rd × {±1}, with completeness assumed under the Gaussian marginal N(0,I). The algorithm uses polynomially many samples N = poly(d,1/η) in the weak learner and poly(d,1/ε) overall, with total runtime poly(d,1/ε) × poly(log 1/τ). The data are tested for Gaussianity via low-degree moment matching up to a chosen k, and localization uses rejection sampling to create near-Gaussian marginals with controlled covariance. The analysis yields a tester-learner that either certifies non-Gaussianity or outputs a unit vector w with ∥w − v*∥2 ≤ O(sqrt(opt)) plus small additive terms, leading to final 0-1 error guarantees. No empirical datasets or benchmarks are used; the results are validated through rigorous probabilistic and geometric arguments (moments, Chow parameters, slabs, and anti-concentration properties of Gaussian).",
        "limitations": "Assumes Gaussian marginals (Homogeneous halfspaces) and relies on moment-matching tests for Gaussianity. The error guarantee is a constant-factor approximation of opt (i.e., O(opt) + ε) rather than opt + ε, and the constants are hidden in asymptotic bounds. The method uses rejection sampling and requires opt to be reasonably small to translate parameter distance into small 0-1 error. It handles adversarial label noise but not general distribution families beyond Gaussian or strongly log-concave extensions without further work. The approach is primarily theoretical, and practical performance or extension to non-homogeneous halfspaces remains unaddressed in this work.",
        "future_research_directions": "Directions include extending the tester-learner to non-Gaussian marginals (e.g., strongly log-concave or general distributions), improving constants and runtime, handling non-homogeneous halfspaces and ReLU-like models, reducing reliance on moment-matching (e.g., via distributional testing with fewer assumptions), and merging with or contrasting against concurrent results such as GKSV23. Additional avenues include empirical validation, exploring broader distribution families, and tightening the gap between the achieved O(opt) + ε guarantees and opt + ε in the testable learning setting.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ]
}